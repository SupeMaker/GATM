2024-03-23 20:58:15,467 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\roberta-base
2024-03-23 20:58:15,467 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\roberta-base. Creating a new one with MEAN pooling.
2024-03-23 20:58:17,373 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-23 20:58:17,954 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\roberta-base
2024-03-23 20:58:17,954 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\roberta-base. Creating a new one with MEAN pooling.
2024-03-23 20:58:19,439 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-23 20:58:19,696 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\roberta-base
2024-03-23 20:58:19,697 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\roberta-base. Creating a new one with MEAN pooling.
2024-03-23 20:58:21,069 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-23 20:58:22,475 - train - INFO - PretrainedBaseline(
  (model): RobertaForSequenceClassification(
    (roberta): RobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(50265, 768, padding_idx=0)
        (position_embeddings): Embedding(514, 768, padding_idx=0)
        (token_type_embeddings): Embedding(1, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (classifier): RobertaClassificationHead(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (out_proj): Linear(in_features=768, out_features=15, bias=True)
    )
  )
)
Trainable params: 46,690,575
Freeze params: 0
2024-03-23 21:00:11,929 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\roberta-base
2024-03-23 21:00:11,929 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\roberta-base. Creating a new one with MEAN pooling.
2024-03-23 21:00:13,588 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-23 21:00:14,073 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\roberta-base
2024-03-23 21:00:14,073 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\roberta-base. Creating a new one with MEAN pooling.
2024-03-23 21:00:14,540 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-23 21:00:14,800 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\roberta-base
2024-03-23 21:00:14,801 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\roberta-base. Creating a new one with MEAN pooling.
2024-03-23 21:00:15,201 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-23 21:00:15,631 - train - INFO - PretrainedBaseline(
  (model): RobertaForSequenceClassification(
    (roberta): RobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(50265, 768, padding_idx=0)
        (position_embeddings): Embedding(514, 768, padding_idx=0)
        (token_type_embeddings): Embedding(1, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (classifier): RobertaClassificationHead(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (out_proj): Linear(in_features=768, out_features=15, bias=True)
    )
  )
)
Trainable params: 46,690,575
Freeze params: 0
2024-03-23 21:07:23,772 - trainer - INFO -     epoch          : 1
2024-03-23 21:07:23,772 - trainer - INFO -     loss           : 1.002336
2024-03-23 21:07:23,772 - trainer - INFO -     accuracy       : 0.684024
2024-03-23 21:07:23,772 - trainer - INFO -     macro_f        : 0.661118
2024-03-23 21:07:23,772 - trainer - INFO -     precision      : 0.678469
2024-03-23 21:07:23,772 - trainer - INFO -     recall         : 0.684024
2024-03-23 21:07:23,773 - trainer - INFO -     val_loss       : 0.890411
2024-03-23 21:07:23,773 - trainer - INFO -     val_accuracy   : 0.710615
2024-03-23 21:07:23,773 - trainer - INFO -     val_macro_f    : 0.700324
2024-03-23 21:07:23,773 - trainer - INFO -     val_precision  : 0.725649
2024-03-23 21:07:23,773 - trainer - INFO -     val_recall     : 0.710615
2024-03-23 21:07:23,773 - trainer - INFO -     test_loss      : 0.886463
2024-03-23 21:07:23,774 - trainer - INFO -     test_accuracy  : 0.713836
2024-03-23 21:07:23,774 - trainer - INFO -     test_macro_f   : 0.704704
2024-03-23 21:07:23,774 - trainer - INFO -     test_precision : 0.730177
2024-03-23 21:07:23,774 - trainer - INFO -     test_recall    : 0.713836
2024-03-23 21:14:36,874 - trainer - INFO -     epoch          : 2
2024-03-23 21:14:36,874 - trainer - INFO -     loss           : 0.802813
2024-03-23 21:14:36,874 - trainer - INFO -     accuracy       : 0.738531
2024-03-23 21:14:36,875 - trainer - INFO -     macro_f        : 0.726686
2024-03-23 21:14:36,875 - trainer - INFO -     precision      : 0.751214
2024-03-23 21:14:36,875 - trainer - INFO -     recall         : 0.738531
2024-03-23 21:14:36,875 - trainer - INFO -     val_loss       : 0.912271
2024-03-23 21:14:36,876 - trainer - INFO -     val_accuracy   : 0.70655
2024-03-23 21:14:36,876 - trainer - INFO -     val_macro_f    : 0.699657
2024-03-23 21:14:36,876 - trainer - INFO -     val_precision  : 0.72796
2024-03-23 21:14:36,876 - trainer - INFO -     val_recall     : 0.70655
2024-03-23 21:14:36,877 - trainer - INFO -     test_loss      : 0.919429
2024-03-23 21:14:36,877 - trainer - INFO -     test_accuracy  : 0.705093
2024-03-23 21:14:36,877 - trainer - INFO -     test_macro_f   : 0.699327
2024-03-23 21:14:36,877 - trainer - INFO -     test_precision : 0.728432
2024-03-23 21:14:36,877 - trainer - INFO -     test_recall    : 0.705093
2024-03-23 21:21:47,670 - trainer - INFO -     epoch          : 3
2024-03-23 21:21:47,670 - trainer - INFO -     loss           : 0.700656
2024-03-23 21:21:47,670 - trainer - INFO -     accuracy       : 0.766182
2024-03-23 21:21:47,670 - trainer - INFO -     macro_f        : 0.75793
2024-03-23 21:21:47,671 - trainer - INFO -     precision      : 0.782787
2024-03-23 21:21:47,671 - trainer - INFO -     recall         : 0.766182
2024-03-23 21:21:47,671 - trainer - INFO -     val_loss       : 0.954349
2024-03-23 21:21:47,671 - trainer - INFO -     val_accuracy   : 0.701948
2024-03-23 21:21:47,671 - trainer - INFO -     val_macro_f    : 0.700847
2024-03-23 21:21:47,672 - trainer - INFO -     val_precision  : 0.735766
2024-03-23 21:21:47,672 - trainer - INFO -     val_recall     : 0.701948
2024-03-23 21:21:47,672 - trainer - INFO -     test_loss      : 0.938405
2024-03-23 21:21:47,672 - trainer - INFO -     test_accuracy  : 0.709388
2024-03-23 21:21:47,672 - trainer - INFO -     test_macro_f   : 0.708198
2024-03-23 21:21:47,672 - trainer - INFO -     test_precision : 0.741687
2024-03-23 21:21:47,673 - trainer - INFO -     test_recall    : 0.709388
2024-03-23 21:22:28,245 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\roberta-base
2024-03-23 21:22:28,246 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\roberta-base. Creating a new one with MEAN pooling.
2024-03-23 21:22:29,388 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-23 21:22:29,669 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\roberta-base
2024-03-23 21:22:29,670 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\roberta-base. Creating a new one with MEAN pooling.
2024-03-23 21:22:30,423 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-23 21:22:30,741 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\roberta-base
2024-03-23 21:22:30,742 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\roberta-base. Creating a new one with MEAN pooling.
2024-03-23 21:22:31,203 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-23 21:22:31,866 - train - INFO - PretrainedBaseline(
  (model): RobertaForSequenceClassification(
    (roberta): RobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(50265, 768, padding_idx=0)
        (position_embeddings): Embedding(514, 768, padding_idx=0)
        (token_type_embeddings): Embedding(1, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (classifier): RobertaClassificationHead(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (out_proj): Linear(in_features=768, out_features=15, bias=True)
    )
  )
)
Trainable params: 46,690,575
Freeze params: 0
2024-03-23 21:34:11,836 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\roberta-base
2024-03-23 21:34:11,838 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\roberta-base. Creating a new one with MEAN pooling.
2024-03-23 21:34:12,875 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-23 21:34:13,312 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\roberta-base
2024-03-23 21:34:13,312 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\roberta-base. Creating a new one with MEAN pooling.
2024-03-23 21:34:13,829 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-23 21:34:14,013 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\roberta-base
2024-03-23 21:34:14,028 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\roberta-base. Creating a new one with MEAN pooling.
2024-03-23 21:34:14,660 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-23 21:34:15,959 - train - INFO - PretrainedBaseline(
  (model): BertForSequenceClassification(
    (bert): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (classifier): Linear(in_features=768, out_features=15, bias=True)
  )
)
Trainable params: 31,527,183
Freeze params: 0
2024-03-23 21:40:52,995 - trainer - INFO -     epoch          : 1
2024-03-23 21:40:52,996 - trainer - INFO -     loss           : 1.045879
2024-03-23 21:40:52,996 - trainer - INFO -     accuracy       : 0.674848
2024-03-23 21:40:52,996 - trainer - INFO -     macro_f        : 0.64902
2024-03-23 21:40:52,997 - trainer - INFO -     precision      : 0.66411
2024-03-23 21:40:52,997 - trainer - INFO -     recall         : 0.674848
2024-03-23 21:40:52,997 - trainer - INFO -     val_loss       : 0.917026
2024-03-23 21:40:52,997 - trainer - INFO -     val_accuracy   : 0.709848
2024-03-23 21:40:52,997 - trainer - INFO -     val_macro_f    : 0.694697
2024-03-23 21:40:52,997 - trainer - INFO -     val_precision  : 0.71316
2024-03-23 21:40:52,997 - trainer - INFO -     val_recall     : 0.709848
2024-03-23 21:40:52,998 - trainer - INFO -     test_loss      : 0.909149
2024-03-23 21:40:52,998 - trainer - INFO -     test_accuracy  : 0.711305
2024-03-23 21:40:52,998 - trainer - INFO -     test_macro_f   : 0.697556
2024-03-23 21:40:52,998 - trainer - INFO -     test_precision : 0.718847
2024-03-23 21:40:52,998 - trainer - INFO -     test_recall    : 0.711305
2024-03-23 21:47:35,182 - trainer - INFO -     epoch          : 2
2024-03-23 21:47:35,182 - trainer - INFO -     loss           : 0.828027
2024-03-23 21:47:35,182 - trainer - INFO -     accuracy       : 0.73181
2024-03-23 21:47:35,182 - trainer - INFO -     macro_f        : 0.718226
2024-03-23 21:47:35,182 - trainer - INFO -     precision      : 0.741677
2024-03-23 21:47:35,182 - trainer - INFO -     recall         : 0.73181
2024-03-23 21:47:35,182 - trainer - INFO -     val_loss       : 0.917874
2024-03-23 21:47:35,182 - trainer - INFO -     val_accuracy   : 0.710538
2024-03-23 21:47:35,182 - trainer - INFO -     val_macro_f    : 0.685271
2024-03-23 21:47:35,182 - trainer - INFO -     val_precision  : 0.699128
2024-03-23 21:47:35,182 - trainer - INFO -     val_recall     : 0.710538
2024-03-23 21:47:35,182 - trainer - INFO -     test_loss      : 0.91513
2024-03-23 21:47:35,182 - trainer - INFO -     test_accuracy  : 0.709925
2024-03-23 21:47:35,182 - trainer - INFO -     test_macro_f   : 0.687053
2024-03-23 21:47:35,182 - trainer - INFO -     test_precision : 0.702697
2024-03-23 21:47:35,182 - trainer - INFO -     test_recall    : 0.709925
2024-03-23 21:54:15,565 - trainer - INFO -     epoch          : 3
2024-03-23 21:54:15,565 - trainer - INFO -     loss           : 0.734874
2024-03-23 21:54:15,565 - trainer - INFO -     accuracy       : 0.756527
2024-03-23 21:54:15,565 - trainer - INFO -     macro_f        : 0.746379
2024-03-23 21:54:15,565 - trainer - INFO -     precision      : 0.771286
2024-03-23 21:54:15,565 - trainer - INFO -     recall         : 0.756527
2024-03-23 21:54:15,565 - trainer - INFO -     val_loss       : 0.928501
2024-03-23 21:54:15,565 - trainer - INFO -     val_accuracy   : 0.706703
2024-03-23 21:54:15,565 - trainer - INFO -     val_macro_f    : 0.692796
2024-03-23 21:54:15,565 - trainer - INFO -     val_precision  : 0.717654
2024-03-23 21:54:15,565 - trainer - INFO -     val_recall     : 0.706703
2024-03-23 21:54:15,565 - trainer - INFO -     test_loss      : 0.926675
2024-03-23 21:54:15,565 - trainer - INFO -     test_accuracy  : 0.711766
2024-03-23 21:54:15,565 - trainer - INFO -     test_macro_f   : 0.69826
2024-03-23 21:54:15,565 - trainer - INFO -     test_precision : 0.722575
2024-03-23 21:54:15,565 - trainer - INFO -     test_recall    : 0.711766
2024-03-23 21:54:55,689 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\roberta-base
2024-03-23 21:54:55,691 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\roberta-base. Creating a new one with MEAN pooling.
2024-03-23 21:54:56,337 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-23 21:54:56,524 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\roberta-base
2024-03-23 21:54:56,524 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\roberta-base. Creating a new one with MEAN pooling.
2024-03-23 21:54:57,008 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-23 21:54:57,198 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\roberta-base
2024-03-23 21:54:57,198 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\roberta-base. Creating a new one with MEAN pooling.
2024-03-23 21:54:57,687 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-23 21:54:58,983 - train - INFO - PretrainedBaseline(
  (model): BertForSequenceClassification(
    (bert): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (classifier): Linear(in_features=768, out_features=15, bias=True)
  )
)
Trainable params: 31,527,183
Freeze params: 0
