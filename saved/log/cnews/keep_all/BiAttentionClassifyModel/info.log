2024-03-25 14:47:16,905 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=10, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=1200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1200, out_features=60, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,992,759
Freeze params: 0
2024-03-25 14:52:44,425 - trainer - INFO -     epoch          : 1
2024-03-25 14:52:44,425 - trainer - INFO -     loss           : 0.283425
2024-03-25 14:52:44,425 - trainer - INFO -     accuracy       : 0.91374
2024-03-25 14:52:44,425 - trainer - INFO -     macro_f        : 0.912696
2024-03-25 14:52:44,425 - trainer - INFO -     precision      : 0.935028
2024-03-25 14:52:44,425 - trainer - INFO -     recall         : 0.91374
2024-03-25 14:52:44,425 - trainer - INFO -     doc_entropy    : 3.909798
2024-03-25 14:52:44,425 - trainer - INFO -     val_loss       : 0.382544
2024-03-25 14:52:44,425 - trainer - INFO -     val_accuracy   : 0.8824
2024-03-25 14:52:44,425 - trainer - INFO -     val_macro_f    : 0.881139
2024-03-25 14:52:44,425 - trainer - INFO -     val_precision  : 0.91541
2024-03-25 14:52:44,425 - trainer - INFO -     val_recall     : 0.8824
2024-03-25 14:52:44,425 - trainer - INFO -     val_doc_entropy: 3.762221
2024-03-25 14:52:44,425 - trainer - INFO -     test_loss      : 0.295398
2024-03-25 14:52:44,425 - trainer - INFO -     test_accuracy  : 0.9065
2024-03-25 14:52:44,425 - trainer - INFO -     test_macro_f   : 0.904959
2024-03-25 14:52:44,425 - trainer - INFO -     test_precision : 0.930056
2024-03-25 14:52:44,425 - trainer - INFO -     test_recall    : 0.9065
2024-03-25 14:52:44,425 - trainer - INFO -     test_doc_entropy: 3.767061
2024-03-25 14:58:27,000 - trainer - INFO -     epoch          : 2
2024-03-25 14:58:27,000 - trainer - INFO -     loss           : 0.219759
2024-03-25 14:58:27,000 - trainer - INFO -     accuracy       : 0.93146
2024-03-25 14:58:27,000 - trainer - INFO -     macro_f        : 0.931456
2024-03-25 14:58:27,000 - trainer - INFO -     precision      : 0.950062
2024-03-25 14:58:27,000 - trainer - INFO -     recall         : 0.93146
2024-03-25 14:58:27,000 - trainer - INFO -     doc_entropy    : 3.605229
2024-03-25 14:58:27,000 - trainer - INFO -     val_loss       : 0.332218
2024-03-25 14:58:27,000 - trainer - INFO -     val_accuracy   : 0.9006
2024-03-25 14:58:27,000 - trainer - INFO -     val_macro_f    : 0.900051
2024-03-25 14:58:27,000 - trainer - INFO -     val_precision  : 0.924705
2024-03-25 14:58:27,000 - trainer - INFO -     val_recall     : 0.9006
2024-03-25 14:58:27,000 - trainer - INFO -     val_doc_entropy: 3.727381
2024-03-25 14:58:27,000 - trainer - INFO -     test_loss      : 0.294818
2024-03-25 14:58:27,016 - trainer - INFO -     test_accuracy  : 0.91
2024-03-25 14:58:27,016 - trainer - INFO -     test_macro_f   : 0.91069
2024-03-25 14:58:27,016 - trainer - INFO -     test_precision : 0.932471
2024-03-25 14:58:27,016 - trainer - INFO -     test_recall    : 0.91
2024-03-25 14:58:27,016 - trainer - INFO -     test_doc_entropy: 3.720291
2024-03-25 15:04:07,864 - trainer - INFO -     epoch          : 3
2024-03-25 15:04:07,864 - trainer - INFO -     loss           : 0.182702
2024-03-25 15:04:07,879 - trainer - INFO -     accuracy       : 0.94152
2024-03-25 15:04:07,879 - trainer - INFO -     macro_f        : 0.941053
2024-03-25 15:04:07,879 - trainer - INFO -     precision      : 0.956629
2024-03-25 15:04:07,879 - trainer - INFO -     recall         : 0.94152
2024-03-25 15:04:07,879 - trainer - INFO -     doc_entropy    : 3.554615
2024-03-25 15:04:07,879 - trainer - INFO -     val_loss       : 0.472829
2024-03-25 15:04:07,879 - trainer - INFO -     val_accuracy   : 0.8618
2024-03-25 15:04:07,879 - trainer - INFO -     val_macro_f    : 0.850839
2024-03-25 15:04:07,879 - trainer - INFO -     val_precision  : 0.889375
2024-03-25 15:04:07,879 - trainer - INFO -     val_recall     : 0.8618
2024-03-25 15:04:07,879 - trainer - INFO -     val_doc_entropy: 3.218522
2024-03-25 15:04:07,879 - trainer - INFO -     test_loss      : 0.390322
2024-03-25 15:04:07,879 - trainer - INFO -     test_accuracy  : 0.8916
2024-03-25 15:04:07,879 - trainer - INFO -     test_macro_f   : 0.886114
2024-03-25 15:04:07,879 - trainer - INFO -     test_precision : 0.916367
2024-03-25 15:04:07,879 - trainer - INFO -     test_recall    : 0.8916
2024-03-25 15:04:07,879 - trainer - INFO -     test_doc_entropy: 3.19393
2024-03-25 15:05:03,173 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=10, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=1200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1200, out_features=60, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,992,759
Freeze params: 0
2024-03-26 14:57:02,807 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=10, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=3600, bias=True)
    (1): Tanh()
    (2): Linear(in_features=3600, out_features=180, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 28,414,479
Freeze params: 0
2024-03-26 14:57:21,975 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=10, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=3600, bias=True)
    (1): Tanh()
    (2): Linear(in_features=3600, out_features=180, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 28,414,479
Freeze params: 0
2024-03-26 15:03:48,157 - trainer - INFO -     epoch          : 1
2024-03-26 15:03:49,017 - trainer - INFO -     loss           : 0.304617
2024-03-26 15:03:49,017 - trainer - INFO -     accuracy       : 0.90744
2024-03-26 15:03:49,017 - trainer - INFO -     macro_f        : 0.906648
2024-03-26 15:03:49,017 - trainer - INFO -     precision      : 0.931268
2024-03-26 15:03:49,017 - trainer - INFO -     recall         : 0.90744
2024-03-26 15:03:49,017 - trainer - INFO -     doc_entropy    : 3.615125
2024-03-26 15:03:49,017 - trainer - INFO -     val_loss       : 0.371487
2024-03-26 15:03:49,017 - trainer - INFO -     val_accuracy   : 0.8862
2024-03-26 15:03:49,017 - trainer - INFO -     val_macro_f    : 0.880788
2024-03-26 15:03:49,017 - trainer - INFO -     val_precision  : 0.906868
2024-03-26 15:03:49,017 - trainer - INFO -     val_recall     : 0.8862
2024-03-26 15:03:49,017 - trainer - INFO -     val_doc_entropy: 3.736018
2024-03-26 15:03:49,017 - trainer - INFO -     test_loss      : 0.346858
2024-03-26 15:03:49,017 - trainer - INFO -     test_accuracy  : 0.8984
2024-03-26 15:03:49,017 - trainer - INFO -     test_macro_f   : 0.894995
2024-03-26 15:03:49,017 - trainer - INFO -     test_precision : 0.921228
2024-03-26 15:03:49,017 - trainer - INFO -     test_recall    : 0.8984
2024-03-26 15:03:49,017 - trainer - INFO -     test_doc_entropy: 3.730008
2024-03-26 15:10:24,653 - trainer - INFO -     epoch          : 2
2024-03-26 15:10:24,653 - trainer - INFO -     loss           : 0.250763
2024-03-26 15:10:24,653 - trainer - INFO -     accuracy       : 0.92238
2024-03-26 15:10:24,653 - trainer - INFO -     macro_f        : 0.922077
2024-03-26 15:10:24,653 - trainer - INFO -     precision      : 0.942807
2024-03-26 15:10:24,653 - trainer - INFO -     recall         : 0.92238
2024-03-26 15:10:24,653 - trainer - INFO -     doc_entropy    : 3.625876
2024-03-26 15:10:24,653 - trainer - INFO -     val_loss       : 0.424292
2024-03-26 15:10:24,653 - trainer - INFO -     val_accuracy   : 0.8704
2024-03-26 15:10:24,653 - trainer - INFO -     val_macro_f    : 0.864871
2024-03-26 15:10:24,653 - trainer - INFO -     val_precision  : 0.89889
2024-03-26 15:10:24,653 - trainer - INFO -     val_recall     : 0.8704
2024-03-26 15:10:24,653 - trainer - INFO -     val_doc_entropy: 3.633125
2024-03-26 15:10:24,653 - trainer - INFO -     test_loss      : 0.381958
2024-03-26 15:10:24,653 - trainer - INFO -     test_accuracy  : 0.8869
2024-03-26 15:10:24,653 - trainer - INFO -     test_macro_f   : 0.882068
2024-03-26 15:10:24,653 - trainer - INFO -     test_precision : 0.912502
2024-03-26 15:10:24,653 - trainer - INFO -     test_recall    : 0.8869
2024-03-26 15:10:24,653 - trainer - INFO -     test_doc_entropy: 3.618705
2024-03-26 15:17:03,164 - trainer - INFO -     epoch          : 3
2024-03-26 15:17:03,164 - trainer - INFO -     loss           : 0.227985
2024-03-26 15:17:03,164 - trainer - INFO -     accuracy       : 0.9277
2024-03-26 15:17:03,164 - trainer - INFO -     macro_f        : 0.927238
2024-03-26 15:17:03,164 - trainer - INFO -     precision      : 0.94629
2024-03-26 15:17:03,164 - trainer - INFO -     recall         : 0.9277
2024-03-26 15:17:03,164 - trainer - INFO -     doc_entropy    : 3.462171
2024-03-26 15:17:03,164 - trainer - INFO -     val_loss       : 0.400701
2024-03-26 15:17:03,164 - trainer - INFO -     val_accuracy   : 0.8824
2024-03-26 15:17:03,164 - trainer - INFO -     val_macro_f    : 0.878022
2024-03-26 15:17:03,164 - trainer - INFO -     val_precision  : 0.907032
2024-03-26 15:17:03,164 - trainer - INFO -     val_recall     : 0.8824
2024-03-26 15:17:03,164 - trainer - INFO -     val_doc_entropy: 3.417889
2024-03-26 15:17:03,164 - trainer - INFO -     test_loss      : 0.342472
2024-03-26 15:17:03,164 - trainer - INFO -     test_accuracy  : 0.8973
2024-03-26 15:17:03,164 - trainer - INFO -     test_macro_f   : 0.895419
2024-03-26 15:17:03,164 - trainer - INFO -     test_precision : 0.922799
2024-03-26 15:17:03,164 - trainer - INFO -     test_recall    : 0.8973
2024-03-26 15:17:03,164 - trainer - INFO -     test_doc_entropy: 3.39509
2024-03-26 15:23:41,517 - trainer - INFO -     epoch          : 4
2024-03-26 15:23:41,517 - trainer - INFO -     loss           : 0.231119
2024-03-26 15:23:41,517 - trainer - INFO -     accuracy       : 0.92632
2024-03-26 15:23:41,517 - trainer - INFO -     macro_f        : 0.926042
2024-03-26 15:23:41,517 - trainer - INFO -     precision      : 0.94577
2024-03-26 15:23:41,517 - trainer - INFO -     recall         : 0.92632
2024-03-26 15:23:41,517 - trainer - INFO -     doc_entropy    : 3.193148
2024-03-26 15:23:41,517 - trainer - INFO -     val_loss       : 0.460132
2024-03-26 15:23:41,517 - trainer - INFO -     val_accuracy   : 0.86
2024-03-26 15:23:41,517 - trainer - INFO -     val_macro_f    : 0.853651
2024-03-26 15:23:41,517 - trainer - INFO -     val_precision  : 0.893209
2024-03-26 15:23:41,517 - trainer - INFO -     val_recall     : 0.86
2024-03-26 15:23:41,517 - trainer - INFO -     val_doc_entropy: 3.174521
2024-03-26 15:23:41,517 - trainer - INFO -     test_loss      : 0.398912
2024-03-26 15:23:41,517 - trainer - INFO -     test_accuracy  : 0.8829
2024-03-26 15:23:41,517 - trainer - INFO -     test_macro_f   : 0.878965
2024-03-26 15:23:41,517 - trainer - INFO -     test_precision : 0.910914
2024-03-26 15:23:41,517 - trainer - INFO -     test_recall    : 0.8829
2024-03-26 15:23:41,517 - trainer - INFO -     test_doc_entropy: 3.168467
2024-03-26 15:30:20,335 - trainer - INFO -     epoch          : 5
2024-03-26 15:30:20,335 - trainer - INFO -     loss           : 0.216333
2024-03-26 15:30:20,335 - trainer - INFO -     accuracy       : 0.931
2024-03-26 15:30:20,335 - trainer - INFO -     macro_f        : 0.930847
2024-03-26 15:30:20,335 - trainer - INFO -     precision      : 0.94886
2024-03-26 15:30:20,335 - trainer - INFO -     recall         : 0.931
2024-03-26 15:30:20,335 - trainer - INFO -     doc_entropy    : 3.064166
2024-03-26 15:30:20,335 - trainer - INFO -     val_loss       : 0.389077
2024-03-26 15:30:20,335 - trainer - INFO -     val_accuracy   : 0.8884
2024-03-26 15:30:20,335 - trainer - INFO -     val_macro_f    : 0.884591
2024-03-26 15:30:20,335 - trainer - INFO -     val_precision  : 0.915171
2024-03-26 15:30:20,335 - trainer - INFO -     val_recall     : 0.8884
2024-03-26 15:30:20,335 - trainer - INFO -     val_doc_entropy: 3.079742
2024-03-26 15:30:20,335 - trainer - INFO -     test_loss      : 0.353917
2024-03-26 15:30:20,335 - trainer - INFO -     test_accuracy  : 0.9008
2024-03-26 15:30:20,335 - trainer - INFO -     test_macro_f   : 0.896878
2024-03-26 15:30:20,335 - trainer - INFO -     test_precision : 0.921828
2024-03-26 15:30:20,335 - trainer - INFO -     test_recall    : 0.9008
2024-03-26 15:30:20,335 - trainer - INFO -     test_doc_entropy: 3.067108
2024-03-26 15:31:20,182 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=10, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=3600, bias=True)
    (1): Tanh()
    (2): Linear(in_features=3600, out_features=180, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 28,414,479
Freeze params: 0
2024-03-26 15:37:58,938 - trainer - INFO -     epoch          : 1
2024-03-26 15:37:58,938 - trainer - INFO -     loss           : 0.30538
2024-03-26 15:37:58,938 - trainer - INFO -     accuracy       : 0.90666
2024-03-26 15:37:58,938 - trainer - INFO -     macro_f        : 0.905957
2024-03-26 15:37:58,938 - trainer - INFO -     precision      : 0.931048
2024-03-26 15:37:58,938 - trainer - INFO -     recall         : 0.90666
2024-03-26 15:37:58,938 - trainer - INFO -     doc_entropy    : 2.961535
2024-03-26 15:37:58,938 - trainer - INFO -     val_loss       : 0.377151
2024-03-26 15:37:58,938 - trainer - INFO -     val_accuracy   : 0.8834
2024-03-26 15:37:58,938 - trainer - INFO -     val_macro_f    : 0.88043
2024-03-26 15:37:58,938 - trainer - INFO -     val_precision  : 0.91311
2024-03-26 15:37:58,938 - trainer - INFO -     val_recall     : 0.8834
2024-03-26 15:37:58,938 - trainer - INFO -     val_doc_entropy: 3.079382
2024-03-26 15:37:58,938 - trainer - INFO -     test_loss      : 0.329959
2024-03-26 15:37:58,938 - trainer - INFO -     test_accuracy  : 0.9021
2024-03-26 15:37:58,938 - trainer - INFO -     test_macro_f   : 0.898965
2024-03-26 15:37:58,938 - trainer - INFO -     test_precision : 0.92762
2024-03-26 15:37:58,938 - trainer - INFO -     test_recall    : 0.9021
2024-03-26 15:37:58,938 - trainer - INFO -     test_doc_entropy: 3.074173
2024-03-26 15:44:39,493 - trainer - INFO -     epoch          : 2
2024-03-26 15:44:39,493 - trainer - INFO -     loss           : 0.213513
2024-03-26 15:44:39,493 - trainer - INFO -     accuracy       : 0.93322
2024-03-26 15:44:39,493 - trainer - INFO -     macro_f        : 0.932935
2024-03-26 15:44:39,493 - trainer - INFO -     precision      : 0.950364
2024-03-26 15:44:39,493 - trainer - INFO -     recall         : 0.93322
2024-03-26 15:44:39,493 - trainer - INFO -     doc_entropy    : 3.262429
2024-03-26 15:44:39,493 - trainer - INFO -     val_loss       : 0.384687
2024-03-26 15:44:39,493 - trainer - INFO -     val_accuracy   : 0.8864
2024-03-26 15:44:39,493 - trainer - INFO -     val_macro_f    : 0.879136
2024-03-26 15:44:39,493 - trainer - INFO -     val_precision  : 0.913966
2024-03-26 15:44:39,493 - trainer - INFO -     val_recall     : 0.8864
2024-03-26 15:44:39,493 - trainer - INFO -     val_doc_entropy: 3.237411
2024-03-26 15:44:39,493 - trainer - INFO -     test_loss      : 0.330345
2024-03-26 15:44:39,493 - trainer - INFO -     test_accuracy  : 0.9012
2024-03-26 15:44:39,493 - trainer - INFO -     test_macro_f   : 0.895072
2024-03-26 15:44:39,493 - trainer - INFO -     test_precision : 0.922797
2024-03-26 15:44:39,493 - trainer - INFO -     test_recall    : 0.9012
2024-03-26 15:44:39,493 - trainer - INFO -     test_doc_entropy: 3.252147
2024-03-26 15:51:19,420 - trainer - INFO -     epoch          : 3
2024-03-26 15:51:19,420 - trainer - INFO -     loss           : 0.194365
2024-03-26 15:51:19,420 - trainer - INFO -     accuracy       : 0.93964
2024-03-26 15:51:19,420 - trainer - INFO -     macro_f        : 0.939173
2024-03-26 15:51:19,420 - trainer - INFO -     precision      : 0.954752
2024-03-26 15:51:19,420 - trainer - INFO -     recall         : 0.93964
2024-03-26 15:51:19,420 - trainer - INFO -     doc_entropy    : 3.332423
2024-03-26 15:51:19,420 - trainer - INFO -     val_loss       : 0.340717
2024-03-26 15:51:19,420 - trainer - INFO -     val_accuracy   : 0.9008
2024-03-26 15:51:19,420 - trainer - INFO -     val_macro_f    : 0.895811
2024-03-26 15:51:19,420 - trainer - INFO -     val_precision  : 0.920116
2024-03-26 15:51:19,436 - trainer - INFO -     val_recall     : 0.9008
2024-03-26 15:51:19,436 - trainer - INFO -     val_doc_entropy: 3.223814
2024-03-26 15:51:19,436 - trainer - INFO -     test_loss      : 0.334287
2024-03-26 15:51:19,436 - trainer - INFO -     test_accuracy  : 0.9035
2024-03-26 15:51:19,436 - trainer - INFO -     test_macro_f   : 0.899684
2024-03-26 15:51:19,436 - trainer - INFO -     test_precision : 0.924797
2024-03-26 15:51:19,436 - trainer - INFO -     test_recall    : 0.9035
2024-03-26 15:51:19,436 - trainer - INFO -     test_doc_entropy: 3.219225
2024-03-26 15:58:00,678 - trainer - INFO -     epoch          : 4
2024-03-26 15:58:00,678 - trainer - INFO -     loss           : 0.238781
2024-03-26 15:58:00,678 - trainer - INFO -     accuracy       : 0.92412
2024-03-26 15:58:00,678 - trainer - INFO -     macro_f        : 0.923591
2024-03-26 15:58:00,678 - trainer - INFO -     precision      : 0.944046
2024-03-26 15:58:00,678 - trainer - INFO -     recall         : 0.92412
2024-03-26 15:58:00,678 - trainer - INFO -     doc_entropy    : 3.02508
2024-03-26 15:58:00,678 - trainer - INFO -     val_loss       : 0.461935
2024-03-26 15:58:00,678 - trainer - INFO -     val_accuracy   : 0.8656
2024-03-26 15:58:00,678 - trainer - INFO -     val_macro_f    : 0.861449
2024-03-26 15:58:00,678 - trainer - INFO -     val_precision  : 0.901512
2024-03-26 15:58:00,678 - trainer - INFO -     val_recall     : 0.8656
2024-03-26 15:58:00,678 - trainer - INFO -     val_doc_entropy: 2.70227
2024-03-26 15:58:00,678 - trainer - INFO -     test_loss      : 0.43661
2024-03-26 15:58:00,678 - trainer - INFO -     test_accuracy  : 0.8723
2024-03-26 15:58:00,678 - trainer - INFO -     test_macro_f   : 0.868657
2024-03-26 15:58:00,678 - trainer - INFO -     test_precision : 0.901715
2024-03-26 15:58:00,678 - trainer - INFO -     test_recall    : 0.8723
2024-03-26 15:58:00,678 - trainer - INFO -     test_doc_entropy: 2.715741
2024-03-26 16:04:42,094 - trainer - INFO -     epoch          : 5
2024-03-26 16:04:42,094 - trainer - INFO -     loss           : 0.199826
2024-03-26 16:04:42,094 - trainer - INFO -     accuracy       : 0.93608
2024-03-26 16:04:42,094 - trainer - INFO -     macro_f        : 0.936077
2024-03-26 16:04:42,094 - trainer - INFO -     precision      : 0.95315
2024-03-26 16:04:42,094 - trainer - INFO -     recall         : 0.93608
2024-03-26 16:04:42,109 - trainer - INFO -     doc_entropy    : 2.836269
2024-03-26 16:04:42,109 - trainer - INFO -     val_loss       : 0.524143
2024-03-26 16:04:42,109 - trainer - INFO -     val_accuracy   : 0.8586
2024-03-26 16:04:42,109 - trainer - INFO -     val_macro_f    : 0.850457
2024-03-26 16:04:42,109 - trainer - INFO -     val_precision  : 0.892068
2024-03-26 16:04:42,109 - trainer - INFO -     val_recall     : 0.8586
2024-03-26 16:04:42,109 - trainer - INFO -     val_doc_entropy: 2.94756
2024-03-26 16:04:42,109 - trainer - INFO -     test_loss      : 0.422867
2024-03-26 16:04:42,109 - trainer - INFO -     test_accuracy  : 0.8847
2024-03-26 16:04:42,109 - trainer - INFO -     test_macro_f   : 0.879719
2024-03-26 16:04:42,109 - trainer - INFO -     test_precision : 0.912865
2024-03-26 16:04:42,109 - trainer - INFO -     test_recall    : 0.8847
2024-03-26 16:04:42,109 - trainer - INFO -     test_doc_entropy: 2.941069
2024-03-26 16:05:43,456 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=10, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=3600, bias=True)
    (1): Tanh()
    (2): Linear(in_features=3600, out_features=180, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 28,414,479
Freeze params: 0
2024-03-26 16:12:24,422 - trainer - INFO -     epoch          : 1
2024-03-26 16:12:24,422 - trainer - INFO -     loss           : 0.347115
2024-03-26 16:12:24,422 - trainer - INFO -     accuracy       : 0.89468
2024-03-26 16:12:24,438 - trainer - INFO -     macro_f        : 0.894051
2024-03-26 16:12:24,438 - trainer - INFO -     precision      : 0.922278
2024-03-26 16:12:24,438 - trainer - INFO -     recall         : 0.89468
2024-03-26 16:12:24,438 - trainer - INFO -     doc_entropy    : 3.365782
2024-03-26 16:12:24,438 - trainer - INFO -     val_loss       : 0.465679
2024-03-26 16:12:24,438 - trainer - INFO -     val_accuracy   : 0.855
2024-03-26 16:12:24,438 - trainer - INFO -     val_macro_f    : 0.849086
2024-03-26 16:12:24,438 - trainer - INFO -     val_precision  : 0.885755
2024-03-26 16:12:24,438 - trainer - INFO -     val_recall     : 0.855
2024-03-26 16:12:24,438 - trainer - INFO -     val_doc_entropy: 3.123657
2024-03-26 16:12:24,438 - trainer - INFO -     test_loss      : 0.395243
2024-03-26 16:12:24,438 - trainer - INFO -     test_accuracy  : 0.8803
2024-03-26 16:12:24,438 - trainer - INFO -     test_macro_f   : 0.878271
2024-03-26 16:12:24,438 - trainer - INFO -     test_precision : 0.905161
2024-03-26 16:12:24,438 - trainer - INFO -     test_recall    : 0.8803
2024-03-26 16:12:24,438 - trainer - INFO -     test_doc_entropy: 3.105278
2024-03-26 16:19:07,932 - trainer - INFO -     epoch          : 2
2024-03-26 16:19:07,932 - trainer - INFO -     loss           : 0.252677
2024-03-26 16:19:07,932 - trainer - INFO -     accuracy       : 0.92232
2024-03-26 16:19:07,932 - trainer - INFO -     macro_f        : 0.921994
2024-03-26 16:19:07,932 - trainer - INFO -     precision      : 0.942241
2024-03-26 16:19:07,932 - trainer - INFO -     recall         : 0.92232
2024-03-26 16:19:07,932 - trainer - INFO -     doc_entropy    : 3.355206
2024-03-26 16:19:07,932 - trainer - INFO -     val_loss       : 0.487464
2024-03-26 16:19:07,932 - trainer - INFO -     val_accuracy   : 0.8604
2024-03-26 16:19:07,932 - trainer - INFO -     val_macro_f    : 0.851544
2024-03-26 16:19:07,932 - trainer - INFO -     val_precision  : 0.896885
2024-03-26 16:19:07,932 - trainer - INFO -     val_recall     : 0.8604
2024-03-26 16:19:07,932 - trainer - INFO -     val_doc_entropy: 3.360725
2024-03-26 16:19:07,932 - trainer - INFO -     test_loss      : 0.422977
2024-03-26 16:19:07,932 - trainer - INFO -     test_accuracy  : 0.8825
2024-03-26 16:19:07,932 - trainer - INFO -     test_macro_f   : 0.877346
2024-03-26 16:19:07,932 - trainer - INFO -     test_precision : 0.910668
2024-03-26 16:19:07,932 - trainer - INFO -     test_recall    : 0.8825
2024-03-26 16:19:07,932 - trainer - INFO -     test_doc_entropy: 3.338961
2024-03-26 16:25:53,534 - trainer - INFO -     epoch          : 3
2024-03-26 16:25:53,534 - trainer - INFO -     loss           : 0.2103
2024-03-26 16:25:53,534 - trainer - INFO -     accuracy       : 0.9329
2024-03-26 16:25:53,534 - trainer - INFO -     macro_f        : 0.932835
2024-03-26 16:25:53,534 - trainer - INFO -     precision      : 0.950763
2024-03-26 16:25:53,534 - trainer - INFO -     recall         : 0.9329
2024-03-26 16:25:53,534 - trainer - INFO -     doc_entropy    : 3.379903
2024-03-26 16:25:53,534 - trainer - INFO -     val_loss       : 0.393065
2024-03-26 16:25:53,534 - trainer - INFO -     val_accuracy   : 0.8956
2024-03-26 16:25:53,534 - trainer - INFO -     val_macro_f    : 0.893296
2024-03-26 16:25:53,534 - trainer - INFO -     val_precision  : 0.918548
2024-03-26 16:25:53,534 - trainer - INFO -     val_recall     : 0.8956
2024-03-26 16:25:53,534 - trainer - INFO -     val_doc_entropy: 3.395082
2024-03-26 16:25:53,534 - trainer - INFO -     test_loss      : 0.360943
2024-03-26 16:25:53,534 - trainer - INFO -     test_accuracy  : 0.8993
2024-03-26 16:25:53,534 - trainer - INFO -     test_macro_f   : 0.89636
2024-03-26 16:25:53,534 - trainer - INFO -     test_precision : 0.921944
2024-03-26 16:25:53,534 - trainer - INFO -     test_recall    : 0.8993
2024-03-26 16:25:53,534 - trainer - INFO -     test_doc_entropy: 3.388734
2024-03-26 16:32:38,763 - trainer - INFO -     epoch          : 4
2024-03-26 16:32:38,763 - trainer - INFO -     loss           : 0.229339
2024-03-26 16:32:38,763 - trainer - INFO -     accuracy       : 0.92712
2024-03-26 16:32:38,763 - trainer - INFO -     macro_f        : 0.926705
2024-03-26 16:32:38,763 - trainer - INFO -     precision      : 0.946255
2024-03-26 16:32:38,763 - trainer - INFO -     recall         : 0.92712
2024-03-26 16:32:38,763 - trainer - INFO -     doc_entropy    : 3.346081
2024-03-26 16:32:38,763 - trainer - INFO -     val_loss       : 0.547395
2024-03-26 16:32:38,763 - trainer - INFO -     val_accuracy   : 0.8428
2024-03-26 16:32:38,763 - trainer - INFO -     val_macro_f    : 0.831143
2024-03-26 16:32:38,763 - trainer - INFO -     val_precision  : 0.873683
2024-03-26 16:32:38,763 - trainer - INFO -     val_recall     : 0.8428
2024-03-26 16:32:38,763 - trainer - INFO -     val_doc_entropy: 3.413887
2024-03-26 16:32:38,763 - trainer - INFO -     test_loss      : 0.45841
2024-03-26 16:32:38,763 - trainer - INFO -     test_accuracy  : 0.8703
2024-03-26 16:32:38,763 - trainer - INFO -     test_macro_f   : 0.862142
2024-03-26 16:32:38,763 - trainer - INFO -     test_precision : 0.897865
2024-03-26 16:32:38,763 - trainer - INFO -     test_recall    : 0.8703
2024-03-26 16:32:38,763 - trainer - INFO -     test_doc_entropy: 3.414093
2024-03-26 16:39:23,586 - trainer - INFO -     epoch          : 5
2024-03-26 16:39:23,586 - trainer - INFO -     loss           : 0.213269
2024-03-26 16:39:23,586 - trainer - INFO -     accuracy       : 0.9313
2024-03-26 16:39:23,586 - trainer - INFO -     macro_f        : 0.930921
2024-03-26 16:39:23,586 - trainer - INFO -     precision      : 0.948867
2024-03-26 16:39:23,586 - trainer - INFO -     recall         : 0.9313
2024-03-26 16:39:23,586 - trainer - INFO -     doc_entropy    : 3.294405
2024-03-26 16:39:23,586 - trainer - INFO -     val_loss       : 0.521263
2024-03-26 16:39:23,586 - trainer - INFO -     val_accuracy   : 0.8588
2024-03-26 16:39:23,586 - trainer - INFO -     val_macro_f    : 0.852921
2024-03-26 16:39:23,586 - trainer - INFO -     val_precision  : 0.890344
2024-03-26 16:39:23,586 - trainer - INFO -     val_recall     : 0.8588
2024-03-26 16:39:23,586 - trainer - INFO -     val_doc_entropy: 3.30312
2024-03-26 16:39:23,586 - trainer - INFO -     test_loss      : 0.465627
2024-03-26 16:39:23,586 - trainer - INFO -     test_accuracy  : 0.8781
2024-03-26 16:39:23,586 - trainer - INFO -     test_macro_f   : 0.871377
2024-03-26 16:39:23,586 - trainer - INFO -     test_precision : 0.901051
2024-03-26 16:39:23,586 - trainer - INFO -     test_recall    : 0.8781
2024-03-26 16:39:23,586 - trainer - INFO -     test_doc_entropy: 3.306303
2024-03-26 16:40:24,685 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=10, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=3600, bias=True)
    (1): Tanh()
    (2): Linear(in_features=3600, out_features=180, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 28,414,479
Freeze params: 0
2024-03-26 16:47:06,811 - trainer - INFO -     epoch          : 1
2024-03-26 16:47:06,811 - trainer - INFO -     loss           : 0.293362
2024-03-26 16:47:06,811 - trainer - INFO -     accuracy       : 0.9129
2024-03-26 16:47:06,811 - trainer - INFO -     macro_f        : 0.912057
2024-03-26 16:47:06,811 - trainer - INFO -     precision      : 0.934838
2024-03-26 16:47:06,811 - trainer - INFO -     recall         : 0.9129
2024-03-26 16:47:06,811 - trainer - INFO -     doc_entropy    : 3.613875
2024-03-26 16:47:06,811 - trainer - INFO -     val_loss       : 0.52172
2024-03-26 16:47:06,811 - trainer - INFO -     val_accuracy   : 0.8518
2024-03-26 16:47:06,811 - trainer - INFO -     val_macro_f    : 0.844303
2024-03-26 16:47:06,811 - trainer - INFO -     val_precision  : 0.892214
2024-03-26 16:47:06,811 - trainer - INFO -     val_recall     : 0.8518
2024-03-26 16:47:06,811 - trainer - INFO -     val_doc_entropy: 3.74194
2024-03-26 16:47:06,811 - trainer - INFO -     test_loss      : 0.45224
2024-03-26 16:47:06,811 - trainer - INFO -     test_accuracy  : 0.874
2024-03-26 16:47:06,811 - trainer - INFO -     test_macro_f   : 0.865764
2024-03-26 16:47:06,811 - trainer - INFO -     test_precision : 0.900521
2024-03-26 16:47:06,811 - trainer - INFO -     test_recall    : 0.874
2024-03-26 16:47:06,811 - trainer - INFO -     test_doc_entropy: 3.738863
2024-03-26 16:53:49,360 - trainer - INFO -     epoch          : 2
2024-03-26 16:53:49,360 - trainer - INFO -     loss           : 0.252538
2024-03-26 16:53:49,360 - trainer - INFO -     accuracy       : 0.92082
2024-03-26 16:53:49,360 - trainer - INFO -     macro_f        : 0.920555
2024-03-26 16:53:49,360 - trainer - INFO -     precision      : 0.941768
2024-03-26 16:53:49,360 - trainer - INFO -     recall         : 0.92082
2024-03-26 16:53:49,360 - trainer - INFO -     doc_entropy    : 3.571356
2024-03-26 16:53:49,360 - trainer - INFO -     val_loss       : 0.561306
2024-03-26 16:53:49,360 - trainer - INFO -     val_accuracy   : 0.8458
2024-03-26 16:53:49,360 - trainer - INFO -     val_macro_f    : 0.836808
2024-03-26 16:53:49,360 - trainer - INFO -     val_precision  : 0.876538
2024-03-26 16:53:49,360 - trainer - INFO -     val_recall     : 0.8458
2024-03-26 16:53:49,360 - trainer - INFO -     val_doc_entropy: 2.954165
2024-03-26 16:53:49,360 - trainer - INFO -     test_loss      : 0.538442
2024-03-26 16:53:49,360 - trainer - INFO -     test_accuracy  : 0.8501
2024-03-26 16:53:49,360 - trainer - INFO -     test_macro_f   : 0.842686
2024-03-26 16:53:49,360 - trainer - INFO -     test_precision : 0.881289
2024-03-26 16:53:49,360 - trainer - INFO -     test_recall    : 0.8501
2024-03-26 16:53:49,360 - trainer - INFO -     test_doc_entropy: 2.948158
2024-03-26 17:00:32,829 - trainer - INFO -     epoch          : 3
2024-03-26 17:00:32,829 - trainer - INFO -     loss           : 0.254183
2024-03-26 17:00:32,829 - trainer - INFO -     accuracy       : 0.9198
2024-03-26 17:00:32,829 - trainer - INFO -     macro_f        : 0.919031
2024-03-26 17:00:32,829 - trainer - INFO -     precision      : 0.940019
2024-03-26 17:00:32,829 - trainer - INFO -     recall         : 0.9198
2024-03-26 17:00:32,829 - trainer - INFO -     doc_entropy    : 3.367591
2024-03-26 17:00:32,829 - trainer - INFO -     val_loss       : 0.348968
2024-03-26 17:00:32,829 - trainer - INFO -     val_accuracy   : 0.8878
2024-03-26 17:00:32,829 - trainer - INFO -     val_macro_f    : 0.886191
2024-03-26 17:00:32,829 - trainer - INFO -     val_precision  : 0.917868
2024-03-26 17:00:32,829 - trainer - INFO -     val_recall     : 0.8878
2024-03-26 17:00:32,829 - trainer - INFO -     val_doc_entropy: 3.019524
2024-03-26 17:00:32,829 - trainer - INFO -     test_loss      : 0.308637
2024-03-26 17:00:32,829 - trainer - INFO -     test_accuracy  : 0.9035
2024-03-26 17:00:32,829 - trainer - INFO -     test_macro_f   : 0.901638
2024-03-26 17:00:32,829 - trainer - INFO -     test_precision : 0.925692
2024-03-26 17:00:32,829 - trainer - INFO -     test_recall    : 0.9035
2024-03-26 17:00:32,829 - trainer - INFO -     test_doc_entropy: 3.00062
2024-03-26 17:07:15,145 - trainer - INFO -     epoch          : 4
2024-03-26 17:07:15,145 - trainer - INFO -     loss           : 0.206981
2024-03-26 17:07:15,145 - trainer - INFO -     accuracy       : 0.93498
2024-03-26 17:07:15,145 - trainer - INFO -     macro_f        : 0.935021
2024-03-26 17:07:15,145 - trainer - INFO -     precision      : 0.952787
2024-03-26 17:07:15,145 - trainer - INFO -     recall         : 0.93498
2024-03-26 17:07:15,145 - trainer - INFO -     doc_entropy    : 3.142691
2024-03-26 17:07:15,145 - trainer - INFO -     val_loss       : 0.539401
2024-03-26 17:07:15,145 - trainer - INFO -     val_accuracy   : 0.8414
2024-03-26 17:07:15,145 - trainer - INFO -     val_macro_f    : 0.832783
2024-03-26 17:07:15,145 - trainer - INFO -     val_precision  : 0.879488
2024-03-26 17:07:15,145 - trainer - INFO -     val_recall     : 0.8414
2024-03-26 17:07:15,145 - trainer - INFO -     val_doc_entropy: 2.614058
2024-03-26 17:07:15,145 - trainer - INFO -     test_loss      : 0.444125
2024-03-26 17:07:15,145 - trainer - INFO -     test_accuracy  : 0.8665
2024-03-26 17:07:15,145 - trainer - INFO -     test_macro_f   : 0.859863
2024-03-26 17:07:15,145 - trainer - INFO -     test_precision : 0.895209
2024-03-26 17:07:15,145 - trainer - INFO -     test_recall    : 0.8665
2024-03-26 17:07:15,145 - trainer - INFO -     test_doc_entropy: 2.625292
2024-03-26 17:13:57,278 - trainer - INFO -     epoch          : 5
2024-03-26 17:13:57,278 - trainer - INFO -     loss           : 0.178098
2024-03-26 17:13:57,278 - trainer - INFO -     accuracy       : 0.94268
2024-03-26 17:13:57,278 - trainer - INFO -     macro_f        : 0.942542
2024-03-26 17:13:57,278 - trainer - INFO -     precision      : 0.957812
2024-03-26 17:13:57,278 - trainer - INFO -     recall         : 0.94268
2024-03-26 17:13:57,278 - trainer - INFO -     doc_entropy    : 3.20908
2024-03-26 17:13:57,278 - trainer - INFO -     val_loss       : 0.445159
2024-03-26 17:13:57,278 - trainer - INFO -     val_accuracy   : 0.865
2024-03-26 17:13:57,278 - trainer - INFO -     val_macro_f    : 0.859311
2024-03-26 17:13:57,278 - trainer - INFO -     val_precision  : 0.897951
2024-03-26 17:13:57,278 - trainer - INFO -     val_recall     : 0.865
2024-03-26 17:13:57,278 - trainer - INFO -     val_doc_entropy: 3.320227
2024-03-26 17:13:57,278 - trainer - INFO -     test_loss      : 0.366995
2024-03-26 17:13:57,278 - trainer - INFO -     test_accuracy  : 0.8883
2024-03-26 17:13:57,278 - trainer - INFO -     test_macro_f   : 0.883458
2024-03-26 17:13:57,278 - trainer - INFO -     test_precision : 0.913796
2024-03-26 17:13:57,278 - trainer - INFO -     test_recall    : 0.8883
2024-03-26 17:13:57,278 - trainer - INFO -     test_doc_entropy: 3.317941
2024-03-26 17:14:59,049 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=10, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=3600, bias=True)
    (1): Tanh()
    (2): Linear(in_features=3600, out_features=180, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 28,414,479
Freeze params: 0
2024-03-26 17:21:40,945 - trainer - INFO -     epoch          : 1
2024-03-26 17:21:40,945 - trainer - INFO -     loss           : 0.314864
2024-03-26 17:21:40,945 - trainer - INFO -     accuracy       : 0.90496
2024-03-26 17:21:40,945 - trainer - INFO -     macro_f        : 0.904235
2024-03-26 17:21:40,945 - trainer - INFO -     precision      : 0.929085
2024-03-26 17:21:40,945 - trainer - INFO -     recall         : 0.90496
2024-03-26 17:21:40,945 - trainer - INFO -     doc_entropy    : 3.580388
2024-03-26 17:21:40,945 - trainer - INFO -     val_loss       : 0.475561
2024-03-26 17:21:40,945 - trainer - INFO -     val_accuracy   : 0.8626
2024-03-26 17:21:40,945 - trainer - INFO -     val_macro_f    : 0.851696
2024-03-26 17:21:40,945 - trainer - INFO -     val_precision  : 0.887719
2024-03-26 17:21:40,945 - trainer - INFO -     val_recall     : 0.8626
2024-03-26 17:21:40,945 - trainer - INFO -     val_doc_entropy: 3.626201
2024-03-26 17:21:40,945 - trainer - INFO -     test_loss      : 0.471176
2024-03-26 17:21:40,945 - trainer - INFO -     test_accuracy  : 0.8707
2024-03-26 17:21:40,945 - trainer - INFO -     test_macro_f   : 0.860944
2024-03-26 17:21:40,945 - trainer - INFO -     test_precision : 0.893463
2024-03-26 17:21:40,945 - trainer - INFO -     test_recall    : 0.8707
2024-03-26 17:21:40,945 - trainer - INFO -     test_doc_entropy: 3.619032
2024-03-26 17:28:24,934 - trainer - INFO -     epoch          : 2
2024-03-26 17:28:24,934 - trainer - INFO -     loss           : 0.231826
2024-03-26 17:28:24,934 - trainer - INFO -     accuracy       : 0.9281
2024-03-26 17:28:24,934 - trainer - INFO -     macro_f        : 0.927779
2024-03-26 17:28:24,934 - trainer - INFO -     precision      : 0.947056
2024-03-26 17:28:24,934 - trainer - INFO -     recall         : 0.9281
2024-03-26 17:28:24,934 - trainer - INFO -     doc_entropy    : 3.606833
2024-03-26 17:28:24,934 - trainer - INFO -     val_loss       : 0.454488
2024-03-26 17:28:24,934 - trainer - INFO -     val_accuracy   : 0.877
2024-03-26 17:28:24,934 - trainer - INFO -     val_macro_f    : 0.870942
2024-03-26 17:28:24,934 - trainer - INFO -     val_precision  : 0.9068
2024-03-26 17:28:24,934 - trainer - INFO -     val_recall     : 0.877
2024-03-26 17:28:24,934 - trainer - INFO -     val_doc_entropy: 3.689876
2024-03-26 17:28:24,934 - trainer - INFO -     test_loss      : 0.402943
2024-03-26 17:28:24,934 - trainer - INFO -     test_accuracy  : 0.8899
2024-03-26 17:28:24,934 - trainer - INFO -     test_macro_f   : 0.885702
2024-03-26 17:28:24,934 - trainer - INFO -     test_precision : 0.913191
2024-03-26 17:28:24,934 - trainer - INFO -     test_recall    : 0.8899
2024-03-26 17:28:24,934 - trainer - INFO -     test_doc_entropy: 3.683
2024-03-26 17:35:08,123 - trainer - INFO -     epoch          : 3
2024-03-26 17:35:08,123 - trainer - INFO -     loss           : 0.212671
2024-03-26 17:35:08,123 - trainer - INFO -     accuracy       : 0.93392
2024-03-26 17:35:08,123 - trainer - INFO -     macro_f        : 0.933733
2024-03-26 17:35:08,123 - trainer - INFO -     precision      : 0.951799
2024-03-26 17:35:08,123 - trainer - INFO -     recall         : 0.93392
2024-03-26 17:35:08,123 - trainer - INFO -     doc_entropy    : 3.558966
2024-03-26 17:35:08,123 - trainer - INFO -     val_loss       : 0.412707
2024-03-26 17:35:08,123 - trainer - INFO -     val_accuracy   : 0.8842
2024-03-26 17:35:08,138 - trainer - INFO -     val_macro_f    : 0.878042
2024-03-26 17:35:08,138 - trainer - INFO -     val_precision  : 0.906014
2024-03-26 17:35:08,138 - trainer - INFO -     val_recall     : 0.8842
2024-03-26 17:35:08,138 - trainer - INFO -     val_doc_entropy: 3.112359
2024-03-26 17:35:08,138 - trainer - INFO -     test_loss      : 0.407175
2024-03-26 17:35:08,138 - trainer - INFO -     test_accuracy  : 0.8895
2024-03-26 17:35:08,138 - trainer - INFO -     test_macro_f   : 0.884147
2024-03-26 17:35:08,138 - trainer - INFO -     test_precision : 0.912662
2024-03-26 17:35:08,138 - trainer - INFO -     test_recall    : 0.8895
2024-03-26 17:35:08,138 - trainer - INFO -     test_doc_entropy: 3.10648
2024-03-26 17:41:50,886 - trainer - INFO -     epoch          : 4
2024-03-26 17:41:50,886 - trainer - INFO -     loss           : 0.199227
2024-03-26 17:41:50,886 - trainer - INFO -     accuracy       : 0.93618
2024-03-26 17:41:50,886 - trainer - INFO -     macro_f        : 0.936074
2024-03-26 17:41:50,886 - trainer - INFO -     precision      : 0.953061
2024-03-26 17:41:50,886 - trainer - INFO -     recall         : 0.93618
2024-03-26 17:41:50,886 - trainer - INFO -     doc_entropy    : 3.332808
2024-03-26 17:41:50,886 - trainer - INFO -     val_loss       : 0.401377
2024-03-26 17:41:50,886 - trainer - INFO -     val_accuracy   : 0.882
2024-03-26 17:41:50,886 - trainer - INFO -     val_macro_f    : 0.874556
2024-03-26 17:41:50,886 - trainer - INFO -     val_precision  : 0.905123
2024-03-26 17:41:50,886 - trainer - INFO -     val_recall     : 0.882
2024-03-26 17:41:50,886 - trainer - INFO -     val_doc_entropy: 3.240243
2024-03-26 17:41:50,886 - trainer - INFO -     test_loss      : 0.340501
2024-03-26 17:41:50,886 - trainer - INFO -     test_accuracy  : 0.9054
2024-03-26 17:41:50,886 - trainer - INFO -     test_macro_f   : 0.901575
2024-03-26 17:41:50,886 - trainer - INFO -     test_precision : 0.926979
2024-03-26 17:41:50,886 - trainer - INFO -     test_recall    : 0.9054
2024-03-26 17:41:50,886 - trainer - INFO -     test_doc_entropy: 3.254008
2024-03-26 17:48:33,973 - trainer - INFO -     epoch          : 5
2024-03-26 17:48:33,973 - trainer - INFO -     loss           : 0.182059
2024-03-26 17:48:33,973 - trainer - INFO -     accuracy       : 0.94204
2024-03-26 17:48:33,973 - trainer - INFO -     macro_f        : 0.9416
2024-03-26 17:48:33,973 - trainer - INFO -     precision      : 0.956314
2024-03-26 17:48:33,973 - trainer - INFO -     recall         : 0.94204
2024-03-26 17:48:33,973 - trainer - INFO -     doc_entropy    : 3.444199
2024-03-26 17:48:33,973 - trainer - INFO -     val_loss       : 0.366794
2024-03-26 17:48:33,973 - trainer - INFO -     val_accuracy   : 0.896
2024-03-26 17:48:33,973 - trainer - INFO -     val_macro_f    : 0.891811
2024-03-26 17:48:33,973 - trainer - INFO -     val_precision  : 0.919955
2024-03-26 17:48:33,973 - trainer - INFO -     val_recall     : 0.896
2024-03-26 17:48:33,973 - trainer - INFO -     val_doc_entropy: 3.513128
2024-03-26 17:48:33,973 - trainer - INFO -     test_loss      : 0.355237
2024-03-26 17:48:33,973 - trainer - INFO -     test_accuracy  : 0.8987
2024-03-26 17:48:33,973 - trainer - INFO -     test_macro_f   : 0.893708
2024-03-26 17:48:33,973 - trainer - INFO -     test_precision : 0.919413
2024-03-26 17:48:33,973 - trainer - INFO -     test_recall    : 0.8987
2024-03-26 17:48:33,973 - trainer - INFO -     test_doc_entropy: 3.511172
2024-04-01 12:14:03,854 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=10, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,645,139
Freeze params: 0
2024-04-01 12:22:58,423 - trainer - INFO -     epoch          : 1
2024-04-01 12:22:58,423 - trainer - INFO -     loss           : 0.260537
2024-04-01 12:22:58,423 - trainer - INFO -     accuracy       : 0.92156
2024-04-01 12:22:58,423 - trainer - INFO -     macro_f        : 0.9208
2024-04-01 12:22:58,423 - trainer - INFO -     precision      : 0.941312
2024-04-01 12:22:58,423 - trainer - INFO -     recall         : 0.92156
2024-04-01 12:22:58,423 - trainer - INFO -     doc_entropy    : 4.802288
2024-04-01 12:22:58,423 - trainer - INFO -     val_loss       : 0.335678
2024-04-01 12:22:58,423 - trainer - INFO -     val_accuracy   : 0.8978
2024-04-01 12:22:58,423 - trainer - INFO -     val_macro_f    : 0.897076
2024-04-01 12:22:58,423 - trainer - INFO -     val_precision  : 0.928984
2024-04-01 12:22:58,423 - trainer - INFO -     val_recall     : 0.8978
2024-04-01 12:22:58,423 - trainer - INFO -     val_doc_entropy: 3.866385
2024-04-01 12:22:58,423 - trainer - INFO -     test_loss      : 0.285956
2024-04-01 12:22:58,423 - trainer - INFO -     test_accuracy  : 0.9149
2024-04-01 12:22:58,423 - trainer - INFO -     test_macro_f   : 0.913131
2024-04-01 12:22:58,423 - trainer - INFO -     test_precision : 0.935507
2024-04-01 12:22:58,423 - trainer - INFO -     test_recall    : 0.9149
2024-04-01 12:22:58,423 - trainer - INFO -     test_doc_entropy: 3.881611
2024-04-01 12:31:56,999 - trainer - INFO -     epoch          : 2
2024-04-01 12:31:56,999 - trainer - INFO -     loss           : 0.172882
2024-04-01 12:31:56,999 - trainer - INFO -     accuracy       : 0.9466
2024-04-01 12:31:56,999 - trainer - INFO -     macro_f        : 0.946387
2024-04-01 12:31:57,000 - trainer - INFO -     precision      : 0.960782
2024-04-01 12:31:57,000 - trainer - INFO -     recall         : 0.9466
2024-04-01 12:31:57,000 - trainer - INFO -     doc_entropy    : 4.008621
2024-04-01 12:31:57,000 - trainer - INFO -     val_loss       : 0.292276
2024-04-01 12:31:57,000 - trainer - INFO -     val_accuracy   : 0.9158
2024-04-01 12:31:57,000 - trainer - INFO -     val_macro_f    : 0.913802
2024-04-01 12:31:57,000 - trainer - INFO -     val_precision  : 0.935836
2024-04-01 12:31:57,001 - trainer - INFO -     val_recall     : 0.9158
2024-04-01 12:31:57,001 - trainer - INFO -     val_doc_entropy: 3.278012
2024-04-01 12:31:57,001 - trainer - INFO -     test_loss      : 0.290219
2024-04-01 12:31:57,001 - trainer - INFO -     test_accuracy  : 0.9137
2024-04-01 12:31:57,001 - trainer - INFO -     test_macro_f   : 0.911554
2024-04-01 12:31:57,001 - trainer - INFO -     test_precision : 0.934391
2024-04-01 12:31:57,002 - trainer - INFO -     test_recall    : 0.9137
2024-04-01 12:31:57,002 - trainer - INFO -     test_doc_entropy: 3.308131
2024-04-01 12:40:54,941 - trainer - INFO -     epoch          : 3
2024-04-01 12:40:54,941 - trainer - INFO -     loss           : 0.12324
2024-04-01 12:40:54,941 - trainer - INFO -     accuracy       : 0.9605
2024-04-01 12:40:54,942 - trainer - INFO -     macro_f        : 0.960426
2024-04-01 12:40:54,942 - trainer - INFO -     precision      : 0.971181
2024-04-01 12:40:54,942 - trainer - INFO -     recall         : 0.9605
2024-04-01 12:40:54,942 - trainer - INFO -     doc_entropy    : 4.020708
2024-04-01 12:40:54,943 - trainer - INFO -     val_loss       : 0.379276
2024-04-01 12:40:54,943 - trainer - INFO -     val_accuracy   : 0.8868
2024-04-01 12:40:54,943 - trainer - INFO -     val_macro_f    : 0.883642
2024-04-01 12:40:54,943 - trainer - INFO -     val_precision  : 0.914748
2024-04-01 12:40:54,944 - trainer - INFO -     val_recall     : 0.8868
2024-04-01 12:40:54,944 - trainer - INFO -     val_doc_entropy: 4.141434
2024-04-01 12:40:54,944 - trainer - INFO -     test_loss      : 0.319863
2024-04-01 12:40:54,944 - trainer - INFO -     test_accuracy  : 0.9078
2024-04-01 12:40:54,944 - trainer - INFO -     test_macro_f   : 0.90418
2024-04-01 12:40:54,944 - trainer - INFO -     test_precision : 0.928377
2024-04-01 12:40:54,945 - trainer - INFO -     test_recall    : 0.9078
2024-04-01 12:40:54,945 - trainer - INFO -     test_doc_entropy: 4.133809
2024-04-01 12:42:14,859 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=10, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,645,139
Freeze params: 0
2024-04-01 12:51:13,881 - trainer - INFO -     epoch          : 1
2024-04-01 12:51:13,881 - trainer - INFO -     loss           : 0.252909
2024-04-01 12:51:13,881 - trainer - INFO -     accuracy       : 0.92422
2024-04-01 12:51:13,881 - trainer - INFO -     macro_f        : 0.923752
2024-04-01 12:51:13,882 - trainer - INFO -     precision      : 0.944121
2024-04-01 12:51:13,882 - trainer - INFO -     recall         : 0.92422
2024-04-01 12:51:13,882 - trainer - INFO -     doc_entropy    : 5.203938
2024-04-01 12:51:13,882 - trainer - INFO -     val_loss       : 0.295611
2024-04-01 12:51:13,883 - trainer - INFO -     val_accuracy   : 0.9074
2024-04-01 12:51:13,883 - trainer - INFO -     val_macro_f    : 0.903059
2024-04-01 12:51:13,883 - trainer - INFO -     val_precision  : 0.930935
2024-04-01 12:51:13,883 - trainer - INFO -     val_recall     : 0.9074
2024-04-01 12:51:13,884 - trainer - INFO -     val_doc_entropy: 5.230664
2024-04-01 12:51:13,884 - trainer - INFO -     test_loss      : 0.251544
2024-04-01 12:51:13,884 - trainer - INFO -     test_accuracy  : 0.9216
2024-04-01 12:51:13,884 - trainer - INFO -     test_macro_f   : 0.918217
2024-04-01 12:51:13,884 - trainer - INFO -     test_precision : 0.939669
2024-04-01 12:51:13,885 - trainer - INFO -     test_recall    : 0.9216
2024-04-01 12:51:13,885 - trainer - INFO -     test_doc_entropy: 5.230272
2024-04-01 13:00:15,823 - trainer - INFO -     epoch          : 2
2024-04-01 13:00:15,823 - trainer - INFO -     loss           : 0.148851
2024-04-01 13:00:15,839 - trainer - INFO -     accuracy       : 0.9541
2024-04-01 13:00:15,839 - trainer - INFO -     macro_f        : 0.953846
2024-04-01 13:00:15,839 - trainer - INFO -     precision      : 0.965923
2024-04-01 13:00:15,839 - trainer - INFO -     recall         : 0.9541
2024-04-01 13:00:15,839 - trainer - INFO -     doc_entropy    : 5.046727
2024-04-01 13:00:15,839 - trainer - INFO -     val_loss       : 0.614185
2024-04-01 13:00:15,839 - trainer - INFO -     val_accuracy   : 0.8308
2024-04-01 13:00:15,839 - trainer - INFO -     val_macro_f    : 0.80781
2024-04-01 13:00:15,839 - trainer - INFO -     val_precision  : 0.850872
2024-04-01 13:00:15,839 - trainer - INFO -     val_recall     : 0.8308
2024-04-01 13:00:15,840 - trainer - INFO -     val_doc_entropy: 4.789621
2024-04-01 13:00:15,840 - trainer - INFO -     test_loss      : 0.40222
2024-04-01 13:00:15,840 - trainer - INFO -     test_accuracy  : 0.8823
2024-04-01 13:00:15,840 - trainer - INFO -     test_macro_f   : 0.874598
2024-04-01 13:00:15,840 - trainer - INFO -     test_precision : 0.906692
2024-04-01 13:00:15,840 - trainer - INFO -     test_recall    : 0.8823
2024-04-01 13:00:15,841 - trainer - INFO -     test_doc_entropy: 4.785333
2024-04-01 13:09:15,088 - trainer - INFO -     epoch          : 3
2024-04-01 13:09:15,089 - trainer - INFO -     loss           : 0.148486
2024-04-01 13:09:15,089 - trainer - INFO -     accuracy       : 0.953
2024-04-01 13:09:15,089 - trainer - INFO -     macro_f        : 0.953128
2024-04-01 13:09:15,089 - trainer - INFO -     precision      : 0.966413
2024-04-01 13:09:15,090 - trainer - INFO -     recall         : 0.953
2024-04-01 13:09:15,090 - trainer - INFO -     doc_entropy    : 4.629353
2024-04-01 13:09:15,090 - trainer - INFO -     val_loss       : 0.293488
2024-04-01 13:09:15,090 - trainer - INFO -     val_accuracy   : 0.9132
2024-04-01 13:09:15,091 - trainer - INFO -     val_macro_f    : 0.91093
2024-04-01 13:09:15,091 - trainer - INFO -     val_precision  : 0.933676
2024-04-01 13:09:15,091 - trainer - INFO -     val_recall     : 0.9132
2024-04-01 13:09:15,091 - trainer - INFO -     val_doc_entropy: 4.658108
2024-04-01 13:09:15,092 - trainer - INFO -     test_loss      : 0.275091
2024-04-01 13:09:15,092 - trainer - INFO -     test_accuracy  : 0.9217
2024-04-01 13:09:15,092 - trainer - INFO -     test_macro_f   : 0.919056
2024-04-01 13:09:15,092 - trainer - INFO -     test_precision : 0.939162
2024-04-01 13:09:15,092 - trainer - INFO -     test_recall    : 0.9217
2024-04-01 13:09:15,092 - trainer - INFO -     test_doc_entropy: 4.654811
2024-04-01 13:10:34,864 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=10, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,645,139
Freeze params: 0
2024-04-01 13:19:33,471 - trainer - INFO -     epoch          : 1
2024-04-01 13:19:33,471 - trainer - INFO -     loss           : 0.246222
2024-04-01 13:19:33,471 - trainer - INFO -     accuracy       : 0.92514
2024-04-01 13:19:33,471 - trainer - INFO -     macro_f        : 0.924123
2024-04-01 13:19:33,471 - trainer - INFO -     precision      : 0.94297
2024-04-01 13:19:33,471 - trainer - INFO -     recall         : 0.92514
2024-04-01 13:19:33,471 - trainer - INFO -     doc_entropy    : 5.2918
2024-04-01 13:19:33,471 - trainer - INFO -     val_loss       : 0.386945
2024-04-01 13:19:33,471 - trainer - INFO -     val_accuracy   : 0.8824
2024-04-01 13:19:33,471 - trainer - INFO -     val_macro_f    : 0.877375
2024-04-01 13:19:33,471 - trainer - INFO -     val_precision  : 0.917497
2024-04-01 13:19:33,471 - trainer - INFO -     val_recall     : 0.8824
2024-04-01 13:19:33,471 - trainer - INFO -     val_doc_entropy: 5.350491
2024-04-01 13:19:33,471 - trainer - INFO -     test_loss      : 0.300696
2024-04-01 13:19:33,471 - trainer - INFO -     test_accuracy  : 0.9068
2024-04-01 13:19:33,471 - trainer - INFO -     test_macro_f   : 0.903789
2024-04-01 13:19:33,471 - trainer - INFO -     test_precision : 0.933765
2024-04-01 13:19:33,471 - trainer - INFO -     test_recall    : 0.9068
2024-04-01 13:19:33,471 - trainer - INFO -     test_doc_entropy: 5.351726
2024-04-01 13:28:32,488 - trainer - INFO -     epoch          : 2
2024-04-01 13:28:32,488 - trainer - INFO -     loss           : 0.14101
2024-04-01 13:28:32,488 - trainer - INFO -     accuracy       : 0.95632
2024-04-01 13:28:32,488 - trainer - INFO -     macro_f        : 0.95614
2024-04-01 13:28:32,488 - trainer - INFO -     precision      : 0.967901
2024-04-01 13:28:32,488 - trainer - INFO -     recall         : 0.95632
2024-04-01 13:28:32,488 - trainer - INFO -     doc_entropy    : 5.288177
2024-04-01 13:28:32,488 - trainer - INFO -     val_loss       : 0.243251
2024-04-01 13:28:32,488 - trainer - INFO -     val_accuracy   : 0.9274
2024-04-01 13:28:32,488 - trainer - INFO -     val_macro_f    : 0.925356
2024-04-01 13:28:32,488 - trainer - INFO -     val_precision  : 0.945467
2024-04-01 13:28:32,488 - trainer - INFO -     val_recall     : 0.9274
2024-04-01 13:28:32,488 - trainer - INFO -     val_doc_entropy: 5.171254
2024-04-01 13:28:32,488 - trainer - INFO -     test_loss      : 0.224995
2024-04-01 13:28:32,488 - trainer - INFO -     test_accuracy  : 0.9323
2024-04-01 13:28:32,488 - trainer - INFO -     test_macro_f   : 0.931563
2024-04-01 13:28:32,488 - trainer - INFO -     test_precision : 0.948594
2024-04-01 13:28:32,488 - trainer - INFO -     test_recall    : 0.9323
2024-04-01 13:28:32,488 - trainer - INFO -     test_doc_entropy: 5.178954
2024-04-01 13:37:31,415 - trainer - INFO -     epoch          : 3
2024-04-01 13:37:31,415 - trainer - INFO -     loss           : 0.116137
2024-04-01 13:37:31,415 - trainer - INFO -     accuracy       : 0.96288
2024-04-01 13:37:31,415 - trainer - INFO -     macro_f        : 0.962835
2024-04-01 13:37:31,416 - trainer - INFO -     precision      : 0.972928
2024-04-01 13:37:31,416 - trainer - INFO -     recall         : 0.96288
2024-04-01 13:37:31,416 - trainer - INFO -     doc_entropy    : 4.868155
2024-04-01 13:37:31,416 - trainer - INFO -     val_loss       : 0.349431
2024-04-01 13:37:31,416 - trainer - INFO -     val_accuracy   : 0.9018
2024-04-01 13:37:31,417 - trainer - INFO -     val_macro_f    : 0.895695
2024-04-01 13:37:31,417 - trainer - INFO -     val_precision  : 0.92435
2024-04-01 13:37:31,417 - trainer - INFO -     val_recall     : 0.9018
2024-04-01 13:37:31,417 - trainer - INFO -     val_doc_entropy: 4.793773
2024-04-01 13:37:31,417 - trainer - INFO -     test_loss      : 0.33442
2024-04-01 13:37:31,417 - trainer - INFO -     test_accuracy  : 0.9082
2024-04-01 13:37:31,417 - trainer - INFO -     test_macro_f   : 0.904255
2024-04-01 13:37:31,418 - trainer - INFO -     test_precision : 0.930803
2024-04-01 13:37:31,418 - trainer - INFO -     test_recall    : 0.9082
2024-04-01 13:37:31,418 - trainer - INFO -     test_doc_entropy: 4.793095
2024-04-01 13:38:51,144 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=10, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,645,139
Freeze params: 0
2024-04-01 13:47:49,426 - trainer - INFO -     epoch          : 1
2024-04-01 13:47:49,426 - trainer - INFO -     loss           : 0.265027
2024-04-01 13:47:49,426 - trainer - INFO -     accuracy       : 0.92034
2024-04-01 13:47:49,426 - trainer - INFO -     macro_f        : 0.919245
2024-04-01 13:47:49,426 - trainer - INFO -     precision      : 0.939951
2024-04-01 13:47:49,426 - trainer - INFO -     recall         : 0.92034
2024-04-01 13:47:49,426 - trainer - INFO -     doc_entropy    : 4.937583
2024-04-01 13:47:49,426 - trainer - INFO -     val_loss       : 0.343971
2024-04-01 13:47:49,426 - trainer - INFO -     val_accuracy   : 0.898
2024-04-01 13:47:49,426 - trainer - INFO -     val_macro_f    : 0.894386
2024-04-01 13:47:49,426 - trainer - INFO -     val_precision  : 0.92202
2024-04-01 13:47:49,426 - trainer - INFO -     val_recall     : 0.898
2024-04-01 13:47:49,426 - trainer - INFO -     val_doc_entropy: 4.679664
2024-04-01 13:47:49,426 - trainer - INFO -     test_loss      : 0.343772
2024-04-01 13:47:49,426 - trainer - INFO -     test_accuracy  : 0.904
2024-04-01 13:47:49,426 - trainer - INFO -     test_macro_f   : 0.898488
2024-04-01 13:47:49,426 - trainer - INFO -     test_precision : 0.922131
2024-04-01 13:47:49,426 - trainer - INFO -     test_recall    : 0.904
2024-04-01 13:47:49,426 - trainer - INFO -     test_doc_entropy: 4.674278
2024-04-01 13:56:47,237 - trainer - INFO -     epoch          : 2
2024-04-01 13:56:47,237 - trainer - INFO -     loss           : 0.186141
2024-04-01 13:56:47,237 - trainer - INFO -     accuracy       : 0.94262
2024-04-01 13:56:47,237 - trainer - INFO -     macro_f        : 0.942191
2024-04-01 13:56:47,237 - trainer - INFO -     precision      : 0.957369
2024-04-01 13:56:47,237 - trainer - INFO -     recall         : 0.94262
2024-04-01 13:56:47,237 - trainer - INFO -     doc_entropy    : 4.296387
2024-04-01 13:56:47,237 - trainer - INFO -     val_loss       : 0.361002
2024-04-01 13:56:47,237 - trainer - INFO -     val_accuracy   : 0.8894
2024-04-01 13:56:47,237 - trainer - INFO -     val_macro_f    : 0.881968
2024-04-01 13:56:47,237 - trainer - INFO -     val_precision  : 0.912899
2024-04-01 13:56:47,237 - trainer - INFO -     val_recall     : 0.8894
2024-04-01 13:56:47,237 - trainer - INFO -     val_doc_entropy: 4.504998
2024-04-01 13:56:47,237 - trainer - INFO -     test_loss      : 0.346003
2024-04-01 13:56:47,237 - trainer - INFO -     test_accuracy  : 0.8965
2024-04-01 13:56:47,237 - trainer - INFO -     test_macro_f   : 0.890365
2024-04-01 13:56:47,237 - trainer - INFO -     test_precision : 0.91702
2024-04-01 13:56:47,237 - trainer - INFO -     test_recall    : 0.8965
2024-04-01 13:56:47,237 - trainer - INFO -     test_doc_entropy: 4.511491
2024-04-01 14:05:44,110 - trainer - INFO -     epoch          : 3
2024-04-01 14:05:44,110 - trainer - INFO -     loss           : 0.152205
2024-04-01 14:05:44,111 - trainer - INFO -     accuracy       : 0.9518
2024-04-01 14:05:44,111 - trainer - INFO -     macro_f        : 0.951364
2024-04-01 14:05:44,111 - trainer - INFO -     precision      : 0.964251
2024-04-01 14:05:44,111 - trainer - INFO -     recall         : 0.9518
2024-04-01 14:05:44,112 - trainer - INFO -     doc_entropy    : 4.358429
2024-04-01 14:05:44,112 - trainer - INFO -     val_loss       : 0.269253
2024-04-01 14:05:44,112 - trainer - INFO -     val_accuracy   : 0.9204
2024-04-01 14:05:44,112 - trainer - INFO -     val_macro_f    : 0.916764
2024-04-01 14:05:44,112 - trainer - INFO -     val_precision  : 0.936623
2024-04-01 14:05:44,113 - trainer - INFO -     val_recall     : 0.9204
2024-04-01 14:05:44,113 - trainer - INFO -     val_doc_entropy: 4.367783
2024-04-01 14:05:44,113 - trainer - INFO -     test_loss      : 0.255594
2024-04-01 14:05:44,113 - trainer - INFO -     test_accuracy  : 0.923
2024-04-01 14:05:44,114 - trainer - INFO -     test_macro_f   : 0.920022
2024-04-01 14:05:44,114 - trainer - INFO -     test_precision : 0.939128
2024-04-01 14:05:44,114 - trainer - INFO -     test_recall    : 0.923
2024-04-01 14:05:44,114 - trainer - INFO -     test_doc_entropy: 4.365698
2024-04-01 14:07:04,320 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=10, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,645,139
Freeze params: 0
2024-04-01 14:16:05,809 - trainer - INFO -     epoch          : 1
2024-04-01 14:16:05,809 - trainer - INFO -     loss           : 0.250054
2024-04-01 14:16:05,809 - trainer - INFO -     accuracy       : 0.9246
2024-04-01 14:16:05,809 - trainer - INFO -     macro_f        : 0.923837
2024-04-01 14:16:05,809 - trainer - INFO -     precision      : 0.943798
2024-04-01 14:16:05,809 - trainer - INFO -     recall         : 0.9246
2024-04-01 14:16:05,809 - trainer - INFO -     doc_entropy    : 4.975009
2024-04-01 14:16:05,809 - trainer - INFO -     val_loss       : 0.527788
2024-04-01 14:16:05,809 - trainer - INFO -     val_accuracy   : 0.8342
2024-04-01 14:16:05,809 - trainer - INFO -     val_macro_f    : 0.828443
2024-04-01 14:16:05,825 - trainer - INFO -     val_precision  : 0.875126
2024-04-01 14:16:05,825 - trainer - INFO -     val_recall     : 0.8342
2024-04-01 14:16:05,825 - trainer - INFO -     val_doc_entropy: 4.346584
2024-04-01 14:16:05,825 - trainer - INFO -     test_loss      : 0.47837
2024-04-01 14:16:05,825 - trainer - INFO -     test_accuracy  : 0.8598
2024-04-01 14:16:05,825 - trainer - INFO -     test_macro_f   : 0.853627
2024-04-01 14:16:05,825 - trainer - INFO -     test_precision : 0.892874
2024-04-01 14:16:05,825 - trainer - INFO -     test_recall    : 0.8598
2024-04-01 14:16:05,825 - trainer - INFO -     test_doc_entropy: 4.357462
2024-04-01 14:25:06,403 - trainer - INFO -     epoch          : 2
2024-04-01 14:25:06,403 - trainer - INFO -     loss           : 0.223083
2024-04-01 14:25:06,403 - trainer - INFO -     accuracy       : 0.93192
2024-04-01 14:25:06,403 - trainer - INFO -     macro_f        : 0.931723
2024-04-01 14:25:06,403 - trainer - INFO -     precision      : 0.950244
2024-04-01 14:25:06,403 - trainer - INFO -     recall         : 0.93192
2024-04-01 14:25:06,403 - trainer - INFO -     doc_entropy    : 4.411812
2024-04-01 14:25:06,403 - trainer - INFO -     val_loss       : 0.301686
2024-04-01 14:25:06,403 - trainer - INFO -     val_accuracy   : 0.9062
2024-04-01 14:25:06,403 - trainer - INFO -     val_macro_f    : 0.900048
2024-04-01 14:25:06,403 - trainer - INFO -     val_precision  : 0.927059
2024-04-01 14:25:06,403 - trainer - INFO -     val_recall     : 0.9062
2024-04-01 14:25:06,403 - trainer - INFO -     val_doc_entropy: 4.692256
2024-04-01 14:25:06,403 - trainer - INFO -     test_loss      : 0.293849
2024-04-01 14:25:06,403 - trainer - INFO -     test_accuracy  : 0.9134
2024-04-01 14:25:06,403 - trainer - INFO -     test_macro_f   : 0.908262
2024-04-01 14:25:06,403 - trainer - INFO -     test_precision : 0.930808
2024-04-01 14:25:06,403 - trainer - INFO -     test_recall    : 0.9134
2024-04-01 14:25:06,403 - trainer - INFO -     test_doc_entropy: 4.691576
2024-04-01 14:34:07,928 - trainer - INFO -     epoch          : 3
2024-04-01 14:34:07,929 - trainer - INFO -     loss           : 0.160538
2024-04-01 14:34:07,929 - trainer - INFO -     accuracy       : 0.95
2024-04-01 14:34:07,929 - trainer - INFO -     macro_f        : 0.950093
2024-04-01 14:34:07,929 - trainer - INFO -     precision      : 0.963769
2024-04-01 14:34:07,930 - trainer - INFO -     recall         : 0.95
2024-04-01 14:34:07,930 - trainer - INFO -     doc_entropy    : 4.398058
2024-04-01 14:34:07,930 - trainer - INFO -     val_loss       : 0.395423
2024-04-01 14:34:07,930 - trainer - INFO -     val_accuracy   : 0.893
2024-04-01 14:34:07,931 - trainer - INFO -     val_macro_f    : 0.88748
2024-04-01 14:34:07,931 - trainer - INFO -     val_precision  : 0.9188
2024-04-01 14:34:07,931 - trainer - INFO -     val_recall     : 0.893
2024-04-01 14:34:07,931 - trainer - INFO -     val_doc_entropy: 4.058845
2024-04-01 14:34:07,932 - trainer - INFO -     test_loss      : 0.379657
2024-04-01 14:34:07,932 - trainer - INFO -     test_accuracy  : 0.9012
2024-04-01 14:34:07,932 - trainer - INFO -     test_macro_f   : 0.895366
2024-04-01 14:34:07,932 - trainer - INFO -     test_precision : 0.921664
2024-04-01 14:34:07,932 - trainer - INFO -     test_recall    : 0.9012
2024-04-01 14:34:07,932 - trainer - INFO -     test_doc_entropy: 4.053665
2024-04-01 14:41:06,345 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=10, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=400, bias=True)
    (1): Tanh()
    (2): Linear(in_features=400, out_features=20, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,313,519
Freeze params: 0
2024-04-01 14:49:47,539 - trainer - INFO -     epoch          : 1
2024-04-01 14:49:47,540 - trainer - INFO -     loss           : 0.251254
2024-04-01 14:49:47,540 - trainer - INFO -     accuracy       : 0.92522
2024-04-01 14:49:47,540 - trainer - INFO -     macro_f        : 0.924835
2024-04-01 14:49:47,540 - trainer - INFO -     precision      : 0.944875
2024-04-01 14:49:47,541 - trainer - INFO -     recall         : 0.92522
2024-04-01 14:49:47,541 - trainer - INFO -     doc_entropy    : 5.276646
2024-04-01 14:49:47,541 - trainer - INFO -     val_loss       : 0.294241
2024-04-01 14:49:47,541 - trainer - INFO -     val_accuracy   : 0.9154
2024-04-01 14:49:47,542 - trainer - INFO -     val_macro_f    : 0.911931
2024-04-01 14:49:47,542 - trainer - INFO -     val_precision  : 0.935341
2024-04-01 14:49:47,542 - trainer - INFO -     val_recall     : 0.9154
2024-04-01 14:49:47,542 - trainer - INFO -     val_doc_entropy: 5.279145
2024-04-01 14:49:47,542 - trainer - INFO -     test_loss      : 0.228637
2024-04-01 14:49:47,542 - trainer - INFO -     test_accuracy  : 0.9367
2024-04-01 14:49:47,543 - trainer - INFO -     test_macro_f   : 0.936271
2024-04-01 14:49:47,543 - trainer - INFO -     test_precision : 0.952714
2024-04-01 14:49:47,543 - trainer - INFO -     test_recall    : 0.9367
2024-04-01 14:49:47,543 - trainer - INFO -     test_doc_entropy: 5.280003
2024-04-01 14:58:30,903 - trainer - INFO -     epoch          : 2
2024-04-01 14:58:30,904 - trainer - INFO -     loss           : 0.149967
2024-04-01 14:58:30,904 - trainer - INFO -     accuracy       : 0.95348
2024-04-01 14:58:30,904 - trainer - INFO -     macro_f        : 0.953364
2024-04-01 14:58:30,904 - trainer - INFO -     precision      : 0.96583
2024-04-01 14:58:30,905 - trainer - INFO -     recall         : 0.95348
2024-04-01 14:58:30,905 - trainer - INFO -     doc_entropy    : 5.127848
2024-04-01 14:58:30,905 - trainer - INFO -     val_loss       : 0.330351
2024-04-01 14:58:30,905 - trainer - INFO -     val_accuracy   : 0.9018
2024-04-01 14:58:30,906 - trainer - INFO -     val_macro_f    : 0.89886
2024-04-01 14:58:30,906 - trainer - INFO -     val_precision  : 0.923524
2024-04-01 14:58:30,906 - trainer - INFO -     val_recall     : 0.9018
2024-04-01 14:58:30,906 - trainer - INFO -     val_doc_entropy: 5.102307
2024-04-01 14:58:30,906 - trainer - INFO -     test_loss      : 0.293958
2024-04-01 14:58:30,907 - trainer - INFO -     test_accuracy  : 0.9151
2024-04-01 14:58:30,907 - trainer - INFO -     test_macro_f   : 0.913707
2024-04-01 14:58:30,907 - trainer - INFO -     test_precision : 0.936868
2024-04-01 14:58:30,907 - trainer - INFO -     test_recall    : 0.9151
2024-04-01 14:58:30,907 - trainer - INFO -     test_doc_entropy: 5.102003
2024-04-01 15:07:15,709 - trainer - INFO -     epoch          : 3
2024-04-01 15:07:15,709 - trainer - INFO -     loss           : 0.118309
2024-04-01 15:07:15,709 - trainer - INFO -     accuracy       : 0.96206
2024-04-01 15:07:15,709 - trainer - INFO -     macro_f        : 0.962071
2024-04-01 15:07:15,710 - trainer - INFO -     precision      : 0.972353
2024-04-01 15:07:15,710 - trainer - INFO -     recall         : 0.96206
2024-04-01 15:07:15,710 - trainer - INFO -     doc_entropy    : 4.901272
2024-04-01 15:07:15,711 - trainer - INFO -     val_loss       : 0.244969
2024-04-01 15:07:15,711 - trainer - INFO -     val_accuracy   : 0.9272
2024-04-01 15:07:15,711 - trainer - INFO -     val_macro_f    : 0.926244
2024-04-01 15:07:15,711 - trainer - INFO -     val_precision  : 0.946356
2024-04-01 15:07:15,712 - trainer - INFO -     val_recall     : 0.9272
2024-04-01 15:07:15,712 - trainer - INFO -     val_doc_entropy: 5.006584
2024-04-01 15:07:15,712 - trainer - INFO -     test_loss      : 0.235428
2024-04-01 15:07:15,712 - trainer - INFO -     test_accuracy  : 0.9316
2024-04-01 15:07:15,712 - trainer - INFO -     test_macro_f   : 0.93103
2024-04-01 15:07:15,713 - trainer - INFO -     test_precision : 0.948739
2024-04-01 15:07:15,713 - trainer - INFO -     test_recall    : 0.9316
2024-04-01 15:07:15,713 - trainer - INFO -     test_doc_entropy: 5.005123
2024-04-01 15:08:34,286 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=10, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=400, bias=True)
    (1): Tanh()
    (2): Linear(in_features=400, out_features=20, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,313,519
Freeze params: 0
2024-04-01 15:17:19,957 - trainer - INFO -     epoch          : 1
2024-04-01 15:17:20,820 - trainer - INFO -     loss           : 0.252117
2024-04-01 15:17:20,821 - trainer - INFO -     accuracy       : 0.92508
2024-04-01 15:17:20,821 - trainer - INFO -     macro_f        : 0.924617
2024-04-01 15:17:20,821 - trainer - INFO -     precision      : 0.944541
2024-04-01 15:17:20,822 - trainer - INFO -     recall         : 0.92508
2024-04-01 15:17:20,822 - trainer - INFO -     doc_entropy    : 5.129752
2024-04-01 15:17:20,822 - trainer - INFO -     val_loss       : 0.425848
2024-04-01 15:17:20,822 - trainer - INFO -     val_accuracy   : 0.8538
2024-04-01 15:17:20,822 - trainer - INFO -     val_macro_f    : 0.844392
2024-04-01 15:17:20,822 - trainer - INFO -     val_precision  : 0.892604
2024-04-01 15:17:20,822 - trainer - INFO -     val_recall     : 0.8538
2024-04-01 15:17:20,823 - trainer - INFO -     val_doc_entropy: 4.941202
2024-04-01 15:17:20,823 - trainer - INFO -     test_loss      : 0.361683
2024-04-01 15:17:20,823 - trainer - INFO -     test_accuracy  : 0.8904
2024-04-01 15:17:20,823 - trainer - INFO -     test_macro_f   : 0.887834
2024-04-01 15:17:20,824 - trainer - INFO -     test_precision : 0.924538
2024-04-01 15:17:20,824 - trainer - INFO -     test_recall    : 0.8904
2024-04-01 15:17:20,824 - trainer - INFO -     test_doc_entropy: 4.945236
2024-04-01 15:26:09,080 - trainer - INFO -     epoch          : 2
2024-04-01 15:26:09,082 - trainer - INFO -     loss           : 0.164642
2024-04-01 15:26:09,082 - trainer - INFO -     accuracy       : 0.9492
2024-04-01 15:26:09,082 - trainer - INFO -     macro_f        : 0.949079
2024-04-01 15:26:09,082 - trainer - INFO -     precision      : 0.962667
2024-04-01 15:26:09,082 - trainer - INFO -     recall         : 0.9492
2024-04-01 15:26:09,083 - trainer - INFO -     doc_entropy    : 4.620153
2024-04-01 15:26:09,083 - trainer - INFO -     val_loss       : 0.306142
2024-04-01 15:26:09,083 - trainer - INFO -     val_accuracy   : 0.9108
2024-04-01 15:26:09,083 - trainer - INFO -     val_macro_f    : 0.906775
2024-04-01 15:26:09,084 - trainer - INFO -     val_precision  : 0.930104
2024-04-01 15:26:09,084 - trainer - INFO -     val_recall     : 0.9108
2024-04-01 15:26:09,084 - trainer - INFO -     val_doc_entropy: 4.732524
2024-04-01 15:26:09,084 - trainer - INFO -     test_loss      : 0.273155
2024-04-01 15:26:09,085 - trainer - INFO -     test_accuracy  : 0.9212
2024-04-01 15:26:09,085 - trainer - INFO -     test_macro_f   : 0.91786
2024-04-01 15:26:09,085 - trainer - INFO -     test_precision : 0.938099
2024-04-01 15:26:09,086 - trainer - INFO -     test_recall    : 0.9212
2024-04-01 15:26:09,086 - trainer - INFO -     test_doc_entropy: 4.730353
2024-04-01 15:34:58,448 - trainer - INFO -     epoch          : 3
2024-04-01 15:34:58,449 - trainer - INFO -     loss           : 0.132268
2024-04-01 15:34:58,449 - trainer - INFO -     accuracy       : 0.9578
2024-04-01 15:34:58,450 - trainer - INFO -     macro_f        : 0.957662
2024-04-01 15:34:58,450 - trainer - INFO -     precision      : 0.969074
2024-04-01 15:34:58,450 - trainer - INFO -     recall         : 0.9578
2024-04-01 15:34:58,450 - trainer - INFO -     doc_entropy    : 4.681765
2024-04-01 15:34:58,451 - trainer - INFO -     val_loss       : 0.298134
2024-04-01 15:34:58,451 - trainer - INFO -     val_accuracy   : 0.9106
2024-04-01 15:34:58,451 - trainer - INFO -     val_macro_f    : 0.906941
2024-04-01 15:34:58,451 - trainer - INFO -     val_precision  : 0.93281
2024-04-01 15:34:58,452 - trainer - INFO -     val_recall     : 0.9106
2024-04-01 15:34:58,452 - trainer - INFO -     val_doc_entropy: 4.643809
2024-04-01 15:34:58,452 - trainer - INFO -     test_loss      : 0.258411
2024-04-01 15:34:58,452 - trainer - INFO -     test_accuracy  : 0.9241
2024-04-01 15:34:58,453 - trainer - INFO -     test_macro_f   : 0.923402
2024-04-01 15:34:58,453 - trainer - INFO -     test_precision : 0.944932
2024-04-01 15:34:58,453 - trainer - INFO -     test_recall    : 0.9241
2024-04-01 15:34:58,453 - trainer - INFO -     test_doc_entropy: 4.648555
2024-04-01 15:36:18,835 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=10, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=400, bias=True)
    (1): Tanh()
    (2): Linear(in_features=400, out_features=20, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,313,519
Freeze params: 0
2024-04-01 15:45:09,689 - trainer - INFO -     epoch          : 1
2024-04-01 15:45:09,690 - trainer - INFO -     loss           : 0.24771
2024-04-01 15:45:09,691 - trainer - INFO -     accuracy       : 0.92434
2024-04-01 15:45:09,691 - trainer - INFO -     macro_f        : 0.923485
2024-04-01 15:45:09,691 - trainer - INFO -     precision      : 0.943374
2024-04-01 15:45:09,691 - trainer - INFO -     recall         : 0.92434
2024-04-01 15:45:09,692 - trainer - INFO -     doc_entropy    : 5.065331
2024-04-01 15:45:09,692 - trainer - INFO -     val_loss       : 0.317443
2024-04-01 15:45:09,692 - trainer - INFO -     val_accuracy   : 0.9084
2024-04-01 15:45:09,693 - trainer - INFO -     val_macro_f    : 0.901963
2024-04-01 15:45:09,693 - trainer - INFO -     val_precision  : 0.927896
2024-04-01 15:45:09,693 - trainer - INFO -     val_recall     : 0.9084
2024-04-01 15:45:09,693 - trainer - INFO -     val_doc_entropy: 4.89354
2024-04-01 15:45:09,694 - trainer - INFO -     test_loss      : 0.302345
2024-04-01 15:45:09,694 - trainer - INFO -     test_accuracy  : 0.9063
2024-04-01 15:45:09,694 - trainer - INFO -     test_macro_f   : 0.901909
2024-04-01 15:45:09,694 - trainer - INFO -     test_precision : 0.929666
2024-04-01 15:45:09,695 - trainer - INFO -     test_recall    : 0.9063
2024-04-01 15:45:09,695 - trainer - INFO -     test_doc_entropy: 4.892521
2024-04-01 15:53:59,123 - trainer - INFO -     epoch          : 2
2024-04-01 15:53:59,124 - trainer - INFO -     loss           : 0.154469
2024-04-01 15:53:59,124 - trainer - INFO -     accuracy       : 0.95206
2024-04-01 15:53:59,124 - trainer - INFO -     macro_f        : 0.951896
2024-04-01 15:53:59,125 - trainer - INFO -     precision      : 0.964888
2024-04-01 15:53:59,125 - trainer - INFO -     recall         : 0.95206
2024-04-01 15:53:59,125 - trainer - INFO -     doc_entropy    : 4.773398
2024-04-01 15:53:59,125 - trainer - INFO -     val_loss       : 0.266703
2024-04-01 15:53:59,125 - trainer - INFO -     val_accuracy   : 0.9244
2024-04-01 15:53:59,126 - trainer - INFO -     val_macro_f    : 0.921692
2024-04-01 15:53:59,126 - trainer - INFO -     val_precision  : 0.940728
2024-04-01 15:53:59,126 - trainer - INFO -     val_recall     : 0.9244
2024-04-01 15:53:59,126 - trainer - INFO -     val_doc_entropy: 4.805392
2024-04-01 15:53:59,126 - trainer - INFO -     test_loss      : 0.253258
2024-04-01 15:53:59,126 - trainer - INFO -     test_accuracy  : 0.9291
2024-04-01 15:53:59,127 - trainer - INFO -     test_macro_f   : 0.927301
2024-04-01 15:53:59,127 - trainer - INFO -     test_precision : 0.94541
2024-04-01 15:53:59,127 - trainer - INFO -     test_recall    : 0.9291
2024-04-01 15:53:59,127 - trainer - INFO -     test_doc_entropy: 4.807655
2024-04-01 16:02:49,993 - trainer - INFO -     epoch          : 3
2024-04-01 16:02:49,994 - trainer - INFO -     loss           : 0.129332
2024-04-01 16:02:49,994 - trainer - INFO -     accuracy       : 0.95898
2024-04-01 16:02:49,995 - trainer - INFO -     macro_f        : 0.958864
2024-04-01 16:02:49,995 - trainer - INFO -     precision      : 0.969737
2024-04-01 16:02:49,995 - trainer - INFO -     recall         : 0.95898
2024-04-01 16:02:49,996 - trainer - INFO -     doc_entropy    : 4.755331
2024-04-01 16:02:49,996 - trainer - INFO -     val_loss       : 0.365299
2024-04-01 16:02:49,996 - trainer - INFO -     val_accuracy   : 0.9038
2024-04-01 16:02:49,996 - trainer - INFO -     val_macro_f    : 0.894104
2024-04-01 16:02:49,996 - trainer - INFO -     val_precision  : 0.917165
2024-04-01 16:02:49,997 - trainer - INFO -     val_recall     : 0.9038
2024-04-01 16:02:49,997 - trainer - INFO -     val_doc_entropy: 4.947907
2024-04-01 16:02:49,997 - trainer - INFO -     test_loss      : 0.374189
2024-04-01 16:02:49,997 - trainer - INFO -     test_accuracy  : 0.8955
2024-04-01 16:02:49,998 - trainer - INFO -     test_macro_f   : 0.887264
2024-04-01 16:02:49,998 - trainer - INFO -     test_precision : 0.912583
2024-04-01 16:02:49,998 - trainer - INFO -     test_recall    : 0.8955
2024-04-01 16:02:49,998 - trainer - INFO -     test_doc_entropy: 4.944808
2024-04-01 16:04:12,407 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=10, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=400, bias=True)
    (1): Tanh()
    (2): Linear(in_features=400, out_features=20, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,313,519
Freeze params: 0
2024-04-01 16:13:01,463 - trainer - INFO -     epoch          : 1
2024-04-01 16:13:01,464 - trainer - INFO -     loss           : 0.25298
2024-04-01 16:13:01,465 - trainer - INFO -     accuracy       : 0.92284
2024-04-01 16:13:01,465 - trainer - INFO -     macro_f        : 0.922093
2024-04-01 16:13:01,465 - trainer - INFO -     precision      : 0.942058
2024-04-01 16:13:01,465 - trainer - INFO -     recall         : 0.92284
2024-04-01 16:13:01,466 - trainer - INFO -     doc_entropy    : 5.135784
2024-04-01 16:13:01,466 - trainer - INFO -     val_loss       : 0.287174
2024-04-01 16:13:01,466 - trainer - INFO -     val_accuracy   : 0.909
2024-04-01 16:13:01,466 - trainer - INFO -     val_macro_f    : 0.906856
2024-04-01 16:13:01,466 - trainer - INFO -     val_precision  : 0.934305
2024-04-01 16:13:01,467 - trainer - INFO -     val_recall     : 0.909
2024-04-01 16:13:01,467 - trainer - INFO -     val_doc_entropy: 5.102761
2024-04-01 16:13:01,467 - trainer - INFO -     test_loss      : 0.314011
2024-04-01 16:13:01,467 - trainer - INFO -     test_accuracy  : 0.9042
2024-04-01 16:13:01,467 - trainer - INFO -     test_macro_f   : 0.902764
2024-04-01 16:13:01,468 - trainer - INFO -     test_precision : 0.930422
2024-04-01 16:13:01,468 - trainer - INFO -     test_recall    : 0.9042
2024-04-01 16:13:01,468 - trainer - INFO -     test_doc_entropy: 5.10114
2024-04-01 16:21:51,573 - trainer - INFO -     epoch          : 2
2024-04-01 16:21:51,574 - trainer - INFO -     loss           : 0.16953
2024-04-01 16:21:51,575 - trainer - INFO -     accuracy       : 0.94582
2024-04-01 16:21:51,575 - trainer - INFO -     macro_f        : 0.94572
2024-04-01 16:21:51,575 - trainer - INFO -     precision      : 0.960633
2024-04-01 16:21:51,576 - trainer - INFO -     recall         : 0.94582
2024-04-01 16:21:51,576 - trainer - INFO -     doc_entropy    : 4.81673
2024-04-01 16:21:51,576 - trainer - INFO -     val_loss       : 0.291831
2024-04-01 16:21:51,577 - trainer - INFO -     val_accuracy   : 0.9032
2024-04-01 16:21:51,577 - trainer - INFO -     val_macro_f    : 0.897954
2024-04-01 16:21:51,577 - trainer - INFO -     val_precision  : 0.926238
2024-04-01 16:21:51,577 - trainer - INFO -     val_recall     : 0.9032
2024-04-01 16:21:51,578 - trainer - INFO -     val_doc_entropy: 4.518295
2024-04-01 16:21:51,578 - trainer - INFO -     test_loss      : 0.241513
2024-04-01 16:21:51,578 - trainer - INFO -     test_accuracy  : 0.9255
2024-04-01 16:21:51,578 - trainer - INFO -     test_macro_f   : 0.924734
2024-04-01 16:21:51,579 - trainer - INFO -     test_precision : 0.94539
2024-04-01 16:21:51,579 - trainer - INFO -     test_recall    : 0.9255
2024-04-01 16:21:51,579 - trainer - INFO -     test_doc_entropy: 4.518564
2024-04-01 16:30:40,823 - trainer - INFO -     epoch          : 3
2024-04-01 16:30:40,824 - trainer - INFO -     loss           : 0.116107
2024-04-01 16:30:40,824 - trainer - INFO -     accuracy       : 0.96324
2024-04-01 16:30:40,825 - trainer - INFO -     macro_f        : 0.963042
2024-04-01 16:30:40,825 - trainer - INFO -     precision      : 0.972729
2024-04-01 16:30:40,825 - trainer - INFO -     recall         : 0.96324
2024-04-01 16:30:40,825 - trainer - INFO -     doc_entropy    : 4.641704
2024-04-01 16:30:40,825 - trainer - INFO -     val_loss       : 0.278332
2024-04-01 16:30:40,826 - trainer - INFO -     val_accuracy   : 0.9164
2024-04-01 16:30:40,826 - trainer - INFO -     val_macro_f    : 0.915341
2024-04-01 16:30:40,826 - trainer - INFO -     val_precision  : 0.938793
2024-04-01 16:30:40,826 - trainer - INFO -     val_recall     : 0.9164
2024-04-01 16:30:40,826 - trainer - INFO -     val_doc_entropy: 4.927971
2024-04-01 16:30:40,827 - trainer - INFO -     test_loss      : 0.265181
2024-04-01 16:30:40,827 - trainer - INFO -     test_accuracy  : 0.9257
2024-04-01 16:30:40,827 - trainer - INFO -     test_macro_f   : 0.924723
2024-04-01 16:30:40,827 - trainer - INFO -     test_precision : 0.944119
2024-04-01 16:30:40,827 - trainer - INFO -     test_recall    : 0.9257
2024-04-01 16:30:40,828 - trainer - INFO -     test_doc_entropy: 4.932035
2024-04-01 16:32:02,230 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=10, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=400, bias=True)
    (1): Tanh()
    (2): Linear(in_features=400, out_features=20, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,313,519
Freeze params: 0
2024-04-01 16:40:48,308 - trainer - INFO -     epoch          : 1
2024-04-01 16:40:48,309 - trainer - INFO -     loss           : 0.247941
2024-04-01 16:40:48,309 - trainer - INFO -     accuracy       : 0.92632
2024-04-01 16:40:48,310 - trainer - INFO -     macro_f        : 0.925556
2024-04-01 16:40:48,310 - trainer - INFO -     precision      : 0.944924
2024-04-01 16:40:48,310 - trainer - INFO -     recall         : 0.92632
2024-04-01 16:40:48,310 - trainer - INFO -     doc_entropy    : 5.083278
2024-04-01 16:40:48,310 - trainer - INFO -     val_loss       : 0.397327
2024-04-01 16:40:48,310 - trainer - INFO -     val_accuracy   : 0.8826
2024-04-01 16:40:48,311 - trainer - INFO -     val_macro_f    : 0.874208
2024-04-01 16:40:48,311 - trainer - INFO -     val_precision  : 0.907878
2024-04-01 16:40:48,311 - trainer - INFO -     val_recall     : 0.8826
2024-04-01 16:40:48,311 - trainer - INFO -     val_doc_entropy: 4.958416
2024-04-01 16:40:48,311 - trainer - INFO -     test_loss      : 0.345967
2024-04-01 16:40:48,311 - trainer - INFO -     test_accuracy  : 0.8989
2024-04-01 16:40:48,312 - trainer - INFO -     test_macro_f   : 0.89316
2024-04-01 16:40:48,312 - trainer - INFO -     test_precision : 0.92085
2024-04-01 16:40:48,312 - trainer - INFO -     test_recall    : 0.8989
2024-04-01 16:40:48,312 - trainer - INFO -     test_doc_entropy: 4.959077
2024-04-01 16:49:35,293 - trainer - INFO -     epoch          : 2
2024-04-01 16:49:35,294 - trainer - INFO -     loss           : 0.157457
2024-04-01 16:49:35,294 - trainer - INFO -     accuracy       : 0.95116
2024-04-01 16:49:35,295 - trainer - INFO -     macro_f        : 0.951055
2024-04-01 16:49:35,295 - trainer - INFO -     precision      : 0.964019
2024-04-01 16:49:35,295 - trainer - INFO -     recall         : 0.95116
2024-04-01 16:49:35,295 - trainer - INFO -     doc_entropy    : 4.824051
2024-04-01 16:49:35,296 - trainer - INFO -     val_loss       : 0.438688
2024-04-01 16:49:35,296 - trainer - INFO -     val_accuracy   : 0.875
2024-04-01 16:49:35,296 - trainer - INFO -     val_macro_f    : 0.866096
2024-04-01 16:49:35,296 - trainer - INFO -     val_precision  : 0.900229
2024-04-01 16:49:35,297 - trainer - INFO -     val_recall     : 0.875
2024-04-01 16:49:35,297 - trainer - INFO -     val_doc_entropy: 4.839222
2024-04-01 16:49:35,297 - trainer - INFO -     test_loss      : 0.381533
2024-04-01 16:49:35,297 - trainer - INFO -     test_accuracy  : 0.896
2024-04-01 16:49:35,297 - trainer - INFO -     test_macro_f   : 0.893574
2024-04-01 16:49:35,298 - trainer - INFO -     test_precision : 0.925624
2024-04-01 16:49:35,298 - trainer - INFO -     test_recall    : 0.896
2024-04-01 16:49:35,298 - trainer - INFO -     test_doc_entropy: 4.840751
2024-04-01 16:58:22,687 - trainer - INFO -     epoch          : 3
2024-04-01 16:58:22,688 - trainer - INFO -     loss           : 0.121574
2024-04-01 16:58:23,550 - trainer - INFO -     accuracy       : 0.96108
2024-04-01 16:58:23,551 - trainer - INFO -     macro_f        : 0.960764
2024-04-01 16:58:23,552 - trainer - INFO -     precision      : 0.970904
2024-04-01 16:58:23,553 - trainer - INFO -     recall         : 0.96108
2024-04-01 16:58:23,553 - trainer - INFO -     doc_entropy    : 4.800997
2024-04-01 16:58:23,554 - trainer - INFO -     val_loss       : 0.294356
2024-04-01 16:58:23,555 - trainer - INFO -     val_accuracy   : 0.9166
2024-04-01 16:58:23,555 - trainer - INFO -     val_macro_f    : 0.914508
2024-04-01 16:58:23,556 - trainer - INFO -     val_precision  : 0.938155
2024-04-01 16:58:23,557 - trainer - INFO -     val_recall     : 0.9166
2024-04-01 16:58:23,557 - trainer - INFO -     val_doc_entropy: 4.961473
2024-04-01 16:58:23,557 - trainer - INFO -     test_loss      : 0.246252
2024-04-01 16:58:23,557 - trainer - INFO -     test_accuracy  : 0.9333
2024-04-01 16:58:23,558 - trainer - INFO -     test_macro_f   : 0.932166
2024-04-01 16:58:23,558 - trainer - INFO -     test_precision : 0.949314
2024-04-01 16:58:23,558 - trainer - INFO -     test_recall    : 0.9333
2024-04-01 16:58:23,558 - trainer - INFO -     test_doc_entropy: 4.956453
2024-04-08 12:10:09,757 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=10, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,645,139
Freeze params: 0
2024-04-08 12:19:09,100 - trainer - INFO -     epoch          : 1
2024-04-08 12:19:09,100 - trainer - INFO -     loss           : 0.2245
2024-04-08 12:19:09,100 - trainer - INFO -     accuracy       : 0.93492
2024-04-08 12:19:09,100 - trainer - INFO -     macro_f        : 0.933914
2024-04-08 12:19:09,100 - trainer - INFO -     precision      : 0.94949
2024-04-08 12:19:09,100 - trainer - INFO -     recall         : 0.93492
2024-04-08 12:19:09,100 - trainer - INFO -     doc_entropy    : 4.009716
2024-04-08 12:19:09,100 - trainer - INFO -     val_loss       : 0.179551
2024-04-08 12:19:09,100 - trainer - INFO -     val_accuracy   : 0.9434
2024-04-08 12:19:09,100 - trainer - INFO -     val_macro_f    : 0.942669
2024-04-08 12:19:09,100 - trainer - INFO -     val_precision  : 0.959335
2024-04-08 12:19:09,100 - trainer - INFO -     val_recall     : 0.9434
2024-04-08 12:19:09,100 - trainer - INFO -     val_doc_entropy: 3.803256
2024-04-08 12:19:09,100 - trainer - INFO -     test_loss      : 0.176427
2024-04-08 12:19:09,100 - trainer - INFO -     test_accuracy  : 0.9419
2024-04-08 12:19:09,100 - trainer - INFO -     test_macro_f   : 0.941168
2024-04-08 12:19:09,100 - trainer - INFO -     test_precision : 0.95848
2024-04-08 12:19:09,100 - trainer - INFO -     test_recall    : 0.9419
2024-04-08 12:19:09,100 - trainer - INFO -     test_doc_entropy: 3.801397
2024-04-08 12:28:13,342 - trainer - INFO -     epoch          : 2
2024-04-08 12:28:13,342 - trainer - INFO -     loss           : 0.084278
2024-04-08 12:28:13,342 - trainer - INFO -     accuracy       : 0.97292
2024-04-08 12:28:13,342 - trainer - INFO -     macro_f        : 0.972863
2024-04-08 12:28:13,342 - trainer - INFO -     precision      : 0.979863
2024-04-08 12:28:13,342 - trainer - INFO -     recall         : 0.97292
2024-04-08 12:28:13,342 - trainer - INFO -     doc_entropy    : 3.60622
2024-04-08 12:28:13,342 - trainer - INFO -     val_loss       : 0.187189
2024-04-08 12:28:13,342 - trainer - INFO -     val_accuracy   : 0.9462
2024-04-08 12:28:13,342 - trainer - INFO -     val_macro_f    : 0.945811
2024-04-08 12:28:13,342 - trainer - INFO -     val_precision  : 0.96236
2024-04-08 12:28:13,342 - trainer - INFO -     val_recall     : 0.9462
2024-04-08 12:28:13,342 - trainer - INFO -     val_doc_entropy: 3.485615
2024-04-08 12:28:13,342 - trainer - INFO -     test_loss      : 0.17796
2024-04-08 12:28:13,342 - trainer - INFO -     test_accuracy  : 0.9464
2024-04-08 12:28:13,342 - trainer - INFO -     test_macro_f   : 0.945482
2024-04-08 12:28:13,342 - trainer - INFO -     test_precision : 0.960322
2024-04-08 12:28:13,342 - trainer - INFO -     test_recall    : 0.9464
2024-04-08 12:28:13,342 - trainer - INFO -     test_doc_entropy: 3.4782
2024-04-08 12:37:18,186 - trainer - INFO -     epoch          : 3
2024-04-08 12:37:18,186 - trainer - INFO -     loss           : 0.049042
2024-04-08 12:37:18,186 - trainer - INFO -     accuracy       : 0.98444
2024-04-08 12:37:18,186 - trainer - INFO -     macro_f        : 0.984432
2024-04-08 12:37:18,186 - trainer - INFO -     precision      : 0.988525
2024-04-08 12:37:18,186 - trainer - INFO -     recall         : 0.98444
2024-04-08 12:37:18,186 - trainer - INFO -     doc_entropy    : 3.262647
2024-04-08 12:37:18,186 - trainer - INFO -     val_loss       : 0.148467
2024-04-08 12:37:18,186 - trainer - INFO -     val_accuracy   : 0.9592
2024-04-08 12:37:18,186 - trainer - INFO -     val_macro_f    : 0.958596
2024-04-08 12:37:18,186 - trainer - INFO -     val_precision  : 0.969757
2024-04-08 12:37:18,186 - trainer - INFO -     val_recall     : 0.9592
2024-04-08 12:37:18,186 - trainer - INFO -     val_doc_entropy: 3.370935
2024-04-08 12:37:18,186 - trainer - INFO -     test_loss      : 0.148563
2024-04-08 12:37:18,186 - trainer - INFO -     test_accuracy  : 0.9598
2024-04-08 12:37:18,186 - trainer - INFO -     test_macro_f   : 0.959601
2024-04-08 12:37:18,186 - trainer - INFO -     test_precision : 0.969585
2024-04-08 12:37:18,186 - trainer - INFO -     test_recall    : 0.9598
2024-04-08 12:37:18,186 - trainer - INFO -     test_doc_entropy: 3.359139
2024-04-08 12:38:41,889 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=10, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,645,139
Freeze params: 0
2024-04-08 12:47:47,509 - trainer - INFO -     epoch          : 1
2024-04-08 12:47:47,509 - trainer - INFO -     loss           : 0.225795
2024-04-08 12:47:47,509 - trainer - INFO -     accuracy       : 0.9343
2024-04-08 12:47:47,509 - trainer - INFO -     macro_f        : 0.933192
2024-04-08 12:47:47,509 - trainer - INFO -     precision      : 0.948664
2024-04-08 12:47:47,509 - trainer - INFO -     recall         : 0.9343
2024-04-08 12:47:47,509 - trainer - INFO -     doc_entropy    : 4.085825
2024-04-08 12:47:47,509 - trainer - INFO -     val_loss       : 0.180706
2024-04-08 12:47:47,509 - trainer - INFO -     val_accuracy   : 0.948
2024-04-08 12:47:47,509 - trainer - INFO -     val_macro_f    : 0.946813
2024-04-08 12:47:47,509 - trainer - INFO -     val_precision  : 0.961146
2024-04-08 12:47:47,509 - trainer - INFO -     val_recall     : 0.948
2024-04-08 12:47:47,509 - trainer - INFO -     val_doc_entropy: 3.848904
2024-04-08 12:47:47,509 - trainer - INFO -     test_loss      : 0.171639
2024-04-08 12:47:47,509 - trainer - INFO -     test_accuracy  : 0.9451
2024-04-08 12:47:47,509 - trainer - INFO -     test_macro_f   : 0.943674
2024-04-08 12:47:47,509 - trainer - INFO -     test_precision : 0.957437
2024-04-08 12:47:47,509 - trainer - INFO -     test_recall    : 0.9451
2024-04-08 12:47:47,509 - trainer - INFO -     test_doc_entropy: 3.842523
2024-04-08 12:56:53,319 - trainer - INFO -     epoch          : 2
2024-04-08 12:56:53,319 - trainer - INFO -     loss           : 0.082743
2024-04-08 12:56:53,319 - trainer - INFO -     accuracy       : 0.97374
2024-04-08 12:56:53,319 - trainer - INFO -     macro_f        : 0.973668
2024-04-08 12:56:53,319 - trainer - INFO -     precision      : 0.98039
2024-04-08 12:56:53,319 - trainer - INFO -     recall         : 0.97374
2024-04-08 12:56:53,319 - trainer - INFO -     doc_entropy    : 3.617146
2024-04-08 12:56:53,319 - trainer - INFO -     val_loss       : 0.168433
2024-04-08 12:56:53,319 - trainer - INFO -     val_accuracy   : 0.951
2024-04-08 12:56:53,319 - trainer - INFO -     val_macro_f    : 0.950622
2024-04-08 12:56:53,319 - trainer - INFO -     val_precision  : 0.963663
2024-04-08 12:56:53,319 - trainer - INFO -     val_recall     : 0.951
2024-04-08 12:56:53,319 - trainer - INFO -     val_doc_entropy: 3.58848
2024-04-08 12:56:53,319 - trainer - INFO -     test_loss      : 0.135087
2024-04-08 12:56:53,319 - trainer - INFO -     test_accuracy  : 0.9568
2024-04-08 12:56:53,319 - trainer - INFO -     test_macro_f   : 0.9566
2024-04-08 12:56:53,319 - trainer - INFO -     test_precision : 0.967008
2024-04-08 12:56:53,319 - trainer - INFO -     test_recall    : 0.9568
2024-04-08 12:56:53,319 - trainer - INFO -     test_doc_entropy: 3.581709
2024-04-08 13:05:58,532 - trainer - INFO -     epoch          : 3
2024-04-08 13:05:58,532 - trainer - INFO -     loss           : 0.04906
2024-04-08 13:05:58,532 - trainer - INFO -     accuracy       : 0.98472
2024-04-08 13:05:58,532 - trainer - INFO -     macro_f        : 0.984744
2024-04-08 13:05:58,532 - trainer - INFO -     precision      : 0.988945
2024-04-08 13:05:58,532 - trainer - INFO -     recall         : 0.98472
2024-04-08 13:05:58,532 - trainer - INFO -     doc_entropy    : 3.297702
2024-04-08 13:05:58,532 - trainer - INFO -     val_loss       : 0.199347
2024-04-08 13:05:58,532 - trainer - INFO -     val_accuracy   : 0.9446
2024-04-08 13:05:58,532 - trainer - INFO -     val_macro_f    : 0.943296
2024-04-08 13:05:58,532 - trainer - INFO -     val_precision  : 0.957884
2024-04-08 13:05:58,532 - trainer - INFO -     val_recall     : 0.9446
2024-04-08 13:05:58,532 - trainer - INFO -     val_doc_entropy: 3.370378
2024-04-08 13:05:58,532 - trainer - INFO -     test_loss      : 0.181225
2024-04-08 13:05:58,532 - trainer - INFO -     test_accuracy  : 0.9486
2024-04-08 13:05:58,532 - trainer - INFO -     test_macro_f   : 0.947976
2024-04-08 13:05:58,532 - trainer - INFO -     test_precision : 0.961559
2024-04-08 13:05:58,532 - trainer - INFO -     test_recall    : 0.9486
2024-04-08 13:05:58,532 - trainer - INFO -     test_doc_entropy: 3.371788
2024-04-08 13:07:22,526 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=10, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,645,139
Freeze params: 0
2024-04-08 13:16:31,139 - trainer - INFO -     epoch          : 1
2024-04-08 13:16:31,139 - trainer - INFO -     loss           : 0.229772
2024-04-08 13:16:31,139 - trainer - INFO -     accuracy       : 0.932
2024-04-08 13:16:31,139 - trainer - INFO -     macro_f        : 0.930371
2024-04-08 13:16:31,139 - trainer - INFO -     precision      : 0.945377
2024-04-08 13:16:31,139 - trainer - INFO -     recall         : 0.932
2024-04-08 13:16:31,139 - trainer - INFO -     doc_entropy    : 4.001159
2024-04-08 13:16:31,139 - trainer - INFO -     val_loss       : 0.192209
2024-04-08 13:16:31,139 - trainer - INFO -     val_accuracy   : 0.9404
2024-04-08 13:16:31,139 - trainer - INFO -     val_macro_f    : 0.939037
2024-04-08 13:16:31,139 - trainer - INFO -     val_precision  : 0.955769
2024-04-08 13:16:31,139 - trainer - INFO -     val_recall     : 0.9404
2024-04-08 13:16:31,139 - trainer - INFO -     val_doc_entropy: 3.66278
2024-04-08 13:16:31,139 - trainer - INFO -     test_loss      : 0.173384
2024-04-08 13:16:31,139 - trainer - INFO -     test_accuracy  : 0.946
2024-04-08 13:16:31,139 - trainer - INFO -     test_macro_f   : 0.94564
2024-04-08 13:16:31,139 - trainer - INFO -     test_precision : 0.96048
2024-04-08 13:16:31,139 - trainer - INFO -     test_recall    : 0.946
2024-04-08 13:16:31,139 - trainer - INFO -     test_doc_entropy: 3.671866
2024-04-08 13:25:35,852 - trainer - INFO -     epoch          : 2
2024-04-08 13:25:35,852 - trainer - INFO -     loss           : 0.085581
2024-04-08 13:25:35,852 - trainer - INFO -     accuracy       : 0.97318
2024-04-08 13:25:35,852 - trainer - INFO -     macro_f        : 0.973025
2024-04-08 13:25:35,852 - trainer - INFO -     precision      : 0.979804
2024-04-08 13:25:35,852 - trainer - INFO -     recall         : 0.97318
2024-04-08 13:25:35,852 - trainer - INFO -     doc_entropy    : 3.509135
2024-04-08 13:25:35,852 - trainer - INFO -     val_loss       : 0.165047
2024-04-08 13:25:35,852 - trainer - INFO -     val_accuracy   : 0.9516
2024-04-08 13:25:35,852 - trainer - INFO -     val_macro_f    : 0.950302
2024-04-08 13:25:35,852 - trainer - INFO -     val_precision  : 0.962054
2024-04-08 13:25:35,852 - trainer - INFO -     val_recall     : 0.9516
2024-04-08 13:25:35,852 - trainer - INFO -     val_doc_entropy: 3.428774
2024-04-08 13:25:35,852 - trainer - INFO -     test_loss      : 0.14934
2024-04-08 13:25:35,852 - trainer - INFO -     test_accuracy  : 0.9535
2024-04-08 13:25:35,852 - trainer - INFO -     test_macro_f   : 0.952694
2024-04-08 13:25:35,852 - trainer - INFO -     test_precision : 0.963989
2024-04-08 13:25:35,852 - trainer - INFO -     test_recall    : 0.9535
2024-04-08 13:25:35,852 - trainer - INFO -     test_doc_entropy: 3.436277
2024-04-08 13:34:40,171 - trainer - INFO -     epoch          : 3
2024-04-08 13:34:40,171 - trainer - INFO -     loss           : 0.051629
2024-04-08 13:34:40,171 - trainer - INFO -     accuracy       : 0.98336
2024-04-08 13:34:40,171 - trainer - INFO -     macro_f        : 0.98351
2024-04-08 13:34:40,171 - trainer - INFO -     precision      : 0.988054
2024-04-08 13:34:40,171 - trainer - INFO -     recall         : 0.98336
2024-04-08 13:34:40,171 - trainer - INFO -     doc_entropy    : 3.309989
2024-04-08 13:34:40,171 - trainer - INFO -     val_loss       : 0.204696
2024-04-08 13:34:40,171 - trainer - INFO -     val_accuracy   : 0.9404
2024-04-08 13:34:40,171 - trainer - INFO -     val_macro_f    : 0.939451
2024-04-08 13:34:40,171 - trainer - INFO -     val_precision  : 0.956711
2024-04-08 13:34:40,171 - trainer - INFO -     val_recall     : 0.9404
2024-04-08 13:34:40,171 - trainer - INFO -     val_doc_entropy: 3.477961
2024-04-08 13:34:40,171 - trainer - INFO -     test_loss      : 0.172319
2024-04-08 13:34:40,171 - trainer - INFO -     test_accuracy  : 0.9506
2024-04-08 13:34:40,171 - trainer - INFO -     test_macro_f   : 0.950233
2024-04-08 13:34:40,171 - trainer - INFO -     test_precision : 0.963862
2024-04-08 13:34:40,171 - trainer - INFO -     test_recall    : 0.9506
2024-04-08 13:34:40,171 - trainer - INFO -     test_doc_entropy: 3.478735
2024-04-08 13:36:04,156 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=10, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,645,139
Freeze params: 0
2024-04-08 13:45:07,445 - trainer - INFO -     epoch          : 1
2024-04-08 13:45:07,445 - trainer - INFO -     loss           : 0.228122
2024-04-08 13:45:07,445 - trainer - INFO -     accuracy       : 0.93326
2024-04-08 13:45:07,445 - trainer - INFO -     macro_f        : 0.931461
2024-04-08 13:45:07,445 - trainer - INFO -     precision      : 0.945952
2024-04-08 13:45:07,445 - trainer - INFO -     recall         : 0.93326
2024-04-08 13:45:07,445 - trainer - INFO -     doc_entropy    : 3.92328
2024-04-08 13:45:07,445 - trainer - INFO -     val_loss       : 0.171242
2024-04-08 13:45:07,445 - trainer - INFO -     val_accuracy   : 0.9482
2024-04-08 13:45:07,445 - trainer - INFO -     val_macro_f    : 0.947431
2024-04-08 13:45:07,445 - trainer - INFO -     val_precision  : 0.962105
2024-04-08 13:45:07,445 - trainer - INFO -     val_recall     : 0.9482
2024-04-08 13:45:07,445 - trainer - INFO -     val_doc_entropy: 3.795654
2024-04-08 13:45:07,445 - trainer - INFO -     test_loss      : 0.174197
2024-04-08 13:45:07,445 - trainer - INFO -     test_accuracy  : 0.944
2024-04-08 13:45:07,445 - trainer - INFO -     test_macro_f   : 0.943098
2024-04-08 13:45:07,445 - trainer - INFO -     test_precision : 0.958138
2024-04-08 13:45:07,445 - trainer - INFO -     test_recall    : 0.944
2024-04-08 13:45:07,445 - trainer - INFO -     test_doc_entropy: 3.805476
2024-04-08 13:54:11,655 - trainer - INFO -     epoch          : 2
2024-04-08 13:54:11,655 - trainer - INFO -     loss           : 0.082865
2024-04-08 13:54:11,655 - trainer - INFO -     accuracy       : 0.9742
2024-04-08 13:54:11,655 - trainer - INFO -     macro_f        : 0.974101
2024-04-08 13:54:11,655 - trainer - INFO -     precision      : 0.980477
2024-04-08 13:54:11,655 - trainer - INFO -     recall         : 0.9742
2024-04-08 13:54:11,655 - trainer - INFO -     doc_entropy    : 3.526627
2024-04-08 13:54:11,655 - trainer - INFO -     val_loss       : 0.172063
2024-04-08 13:54:11,655 - trainer - INFO -     val_accuracy   : 0.9454
2024-04-08 13:54:11,655 - trainer - INFO -     val_macro_f    : 0.943051
2024-04-08 13:54:11,655 - trainer - INFO -     val_precision  : 0.95855
2024-04-08 13:54:11,655 - trainer - INFO -     val_recall     : 0.9454
2024-04-08 13:54:11,671 - trainer - INFO -     val_doc_entropy: 3.574132
2024-04-08 13:54:11,671 - trainer - INFO -     test_loss      : 0.174548
2024-04-08 13:54:11,671 - trainer - INFO -     test_accuracy  : 0.9469
2024-04-08 13:54:11,671 - trainer - INFO -     test_macro_f   : 0.94606
2024-04-08 13:54:11,671 - trainer - INFO -     test_precision : 0.961069
2024-04-08 13:54:11,671 - trainer - INFO -     test_recall    : 0.9469
2024-04-08 13:54:11,671 - trainer - INFO -     test_doc_entropy: 3.569234
2024-04-08 14:03:16,046 - trainer - INFO -     epoch          : 3
2024-04-08 14:03:16,046 - trainer - INFO -     loss           : 0.049847
2024-04-08 14:03:16,046 - trainer - INFO -     accuracy       : 0.98396
2024-04-08 14:03:16,046 - trainer - INFO -     macro_f        : 0.983881
2024-04-08 14:03:16,046 - trainer - INFO -     precision      : 0.98792
2024-04-08 14:03:16,046 - trainer - INFO -     recall         : 0.98396
2024-04-08 14:03:16,046 - trainer - INFO -     doc_entropy    : 3.238556
2024-04-08 14:03:16,046 - trainer - INFO -     val_loss       : 0.171323
2024-04-08 14:03:16,046 - trainer - INFO -     val_accuracy   : 0.9528
2024-04-08 14:03:16,046 - trainer - INFO -     val_macro_f    : 0.952421
2024-04-08 14:03:16,046 - trainer - INFO -     val_precision  : 0.966485
2024-04-08 14:03:16,046 - trainer - INFO -     val_recall     : 0.9528
2024-04-08 14:03:16,046 - trainer - INFO -     val_doc_entropy: 3.44313
2024-04-08 14:03:16,046 - trainer - INFO -     test_loss      : 0.150749
2024-04-08 14:03:16,046 - trainer - INFO -     test_accuracy  : 0.956
2024-04-08 14:03:16,046 - trainer - INFO -     test_macro_f   : 0.955923
2024-04-08 14:03:16,046 - trainer - INFO -     test_precision : 0.967524
2024-04-08 14:03:16,046 - trainer - INFO -     test_recall    : 0.956
2024-04-08 14:03:16,046 - trainer - INFO -     test_doc_entropy: 3.441553
2024-04-08 14:04:38,934 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=10, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,645,139
Freeze params: 0
2024-04-08 14:13:45,478 - trainer - INFO -     epoch          : 1
2024-04-08 14:13:46,354 - trainer - INFO -     loss           : 0.225421
2024-04-08 14:13:46,354 - trainer - INFO -     accuracy       : 0.93462
2024-04-08 14:13:46,354 - trainer - INFO -     macro_f        : 0.933346
2024-04-08 14:13:46,354 - trainer - INFO -     precision      : 0.94868
2024-04-08 14:13:46,354 - trainer - INFO -     recall         : 0.93462
2024-04-08 14:13:46,354 - trainer - INFO -     doc_entropy    : 3.907186
2024-04-08 14:13:46,354 - trainer - INFO -     val_loss       : 0.17429
2024-04-08 14:13:46,354 - trainer - INFO -     val_accuracy   : 0.9502
2024-04-08 14:13:46,354 - trainer - INFO -     val_macro_f    : 0.949384
2024-04-08 14:13:46,354 - trainer - INFO -     val_precision  : 0.963022
2024-04-08 14:13:46,354 - trainer - INFO -     val_recall     : 0.9502
2024-04-08 14:13:46,354 - trainer - INFO -     val_doc_entropy: 3.465349
2024-04-08 14:13:46,354 - trainer - INFO -     test_loss      : 0.165208
2024-04-08 14:13:46,354 - trainer - INFO -     test_accuracy  : 0.9466
2024-04-08 14:13:46,354 - trainer - INFO -     test_macro_f   : 0.94564
2024-04-08 14:13:46,354 - trainer - INFO -     test_precision : 0.958253
2024-04-08 14:13:46,354 - trainer - INFO -     test_recall    : 0.9466
2024-04-08 14:13:46,354 - trainer - INFO -     test_doc_entropy: 3.447761
2024-04-08 14:22:59,997 - trainer - INFO -     epoch          : 2
2024-04-08 14:22:59,997 - trainer - INFO -     loss           : 0.084169
2024-04-08 14:22:59,997 - trainer - INFO -     accuracy       : 0.97278
2024-04-08 14:22:59,997 - trainer - INFO -     macro_f        : 0.972804
2024-04-08 14:22:59,997 - trainer - INFO -     precision      : 0.979847
2024-04-08 14:22:59,997 - trainer - INFO -     recall         : 0.97278
2024-04-08 14:22:59,997 - trainer - INFO -     doc_entropy    : 3.434442
2024-04-08 14:22:59,997 - trainer - INFO -     val_loss       : 0.149194
2024-04-08 14:22:59,997 - trainer - INFO -     val_accuracy   : 0.9586
2024-04-08 14:22:59,997 - trainer - INFO -     val_macro_f    : 0.957535
2024-04-08 14:22:59,997 - trainer - INFO -     val_precision  : 0.967857
2024-04-08 14:22:59,997 - trainer - INFO -     val_recall     : 0.9586
2024-04-08 14:22:59,997 - trainer - INFO -     val_doc_entropy: 3.171147
2024-04-08 14:22:59,997 - trainer - INFO -     test_loss      : 0.148362
2024-04-08 14:22:59,997 - trainer - INFO -     test_accuracy  : 0.9545
2024-04-08 14:22:59,997 - trainer - INFO -     test_macro_f   : 0.953967
2024-04-08 14:22:59,997 - trainer - INFO -     test_precision : 0.965327
2024-04-08 14:22:59,997 - trainer - INFO -     test_recall    : 0.9545
2024-04-08 14:22:59,997 - trainer - INFO -     test_doc_entropy: 3.183516
2024-04-08 14:32:08,919 - trainer - INFO -     epoch          : 3
2024-04-08 14:32:08,919 - trainer - INFO -     loss           : 0.048863
2024-04-08 14:32:08,919 - trainer - INFO -     accuracy       : 0.98466
2024-04-08 14:32:08,919 - trainer - INFO -     macro_f        : 0.984674
2024-04-08 14:32:08,919 - trainer - INFO -     precision      : 0.988544
2024-04-08 14:32:08,919 - trainer - INFO -     recall         : 0.98466
2024-04-08 14:32:08,919 - trainer - INFO -     doc_entropy    : 3.133979
2024-04-08 14:32:08,919 - trainer - INFO -     val_loss       : 0.172476
2024-04-08 14:32:08,935 - trainer - INFO -     val_accuracy   : 0.9528
2024-04-08 14:32:08,935 - trainer - INFO -     val_macro_f    : 0.951771
2024-04-08 14:32:08,935 - trainer - INFO -     val_precision  : 0.964142
2024-04-08 14:32:08,935 - trainer - INFO -     val_recall     : 0.9528
2024-04-08 14:32:08,935 - trainer - INFO -     val_doc_entropy: 3.085356
2024-04-08 14:32:08,935 - trainer - INFO -     test_loss      : 0.168997
2024-04-08 14:32:08,935 - trainer - INFO -     test_accuracy  : 0.9558
2024-04-08 14:32:08,935 - trainer - INFO -     test_macro_f   : 0.955204
2024-04-08 14:32:08,935 - trainer - INFO -     test_precision : 0.967065
2024-04-08 14:32:08,935 - trainer - INFO -     test_recall    : 0.9558
2024-04-08 14:32:08,935 - trainer - INFO -     test_doc_entropy: 3.088918
2024-04-09 22:50:19,413 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=10, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,645,139
Freeze params: 0
2024-04-09 22:59:11,111 - trainer - INFO -     epoch          : 1
2024-04-09 22:59:11,111 - trainer - INFO -     loss           : 0.2245
2024-04-09 22:59:11,111 - trainer - INFO -     accuracy       : 0.93492
2024-04-09 22:59:11,111 - trainer - INFO -     macro_f        : 0.933914
2024-04-09 22:59:11,111 - trainer - INFO -     precision      : 0.94949
2024-04-09 22:59:11,111 - trainer - INFO -     recall         : 0.93492
2024-04-09 22:59:11,111 - trainer - INFO -     doc_entropy    : 4.009716
2024-04-09 22:59:11,111 - trainer - INFO -     val_loss       : 0.179551
2024-04-09 22:59:11,111 - trainer - INFO -     val_accuracy   : 0.9434
2024-04-09 22:59:11,111 - trainer - INFO -     val_macro_f    : 0.942669
2024-04-09 22:59:11,111 - trainer - INFO -     val_precision  : 0.959335
2024-04-09 22:59:11,111 - trainer - INFO -     val_recall     : 0.9434
2024-04-09 22:59:11,111 - trainer - INFO -     val_doc_entropy: 3.803256
2024-04-09 22:59:11,111 - trainer - INFO -     test_loss      : 0.176427
2024-04-09 22:59:11,111 - trainer - INFO -     test_accuracy  : 0.9419
2024-04-09 22:59:11,111 - trainer - INFO -     test_macro_f   : 0.941168
2024-04-09 22:59:11,111 - trainer - INFO -     test_precision : 0.95848
2024-04-09 22:59:11,111 - trainer - INFO -     test_recall    : 0.9419
2024-04-09 22:59:11,111 - trainer - INFO -     test_doc_entropy: 3.801397
2024-04-09 23:08:06,797 - trainer - INFO -     epoch          : 2
2024-04-09 23:08:06,797 - trainer - INFO -     loss           : 0.084278
2024-04-09 23:08:06,797 - trainer - INFO -     accuracy       : 0.97292
2024-04-09 23:08:06,797 - trainer - INFO -     macro_f        : 0.972863
2024-04-09 23:08:06,797 - trainer - INFO -     precision      : 0.979863
2024-04-09 23:08:06,797 - trainer - INFO -     recall         : 0.97292
2024-04-09 23:08:06,797 - trainer - INFO -     doc_entropy    : 3.60622
2024-04-09 23:08:06,797 - trainer - INFO -     val_loss       : 0.187189
2024-04-09 23:08:06,797 - trainer - INFO -     val_accuracy   : 0.9462
2024-04-09 23:08:06,797 - trainer - INFO -     val_macro_f    : 0.945811
2024-04-09 23:08:06,797 - trainer - INFO -     val_precision  : 0.96236
2024-04-09 23:08:06,797 - trainer - INFO -     val_recall     : 0.9462
2024-04-09 23:08:06,797 - trainer - INFO -     val_doc_entropy: 3.485615
2024-04-09 23:08:06,797 - trainer - INFO -     test_loss      : 0.17796
2024-04-09 23:08:06,797 - trainer - INFO -     test_accuracy  : 0.9464
2024-04-09 23:08:06,797 - trainer - INFO -     test_macro_f   : 0.945482
2024-04-09 23:08:06,813 - trainer - INFO -     test_precision : 0.960322
2024-04-09 23:08:06,813 - trainer - INFO -     test_recall    : 0.9464
2024-04-09 23:08:06,813 - trainer - INFO -     test_doc_entropy: 3.4782
2024-04-09 23:17:01,745 - trainer - INFO -     epoch          : 3
2024-04-09 23:17:01,745 - trainer - INFO -     loss           : 0.049042
2024-04-09 23:17:01,745 - trainer - INFO -     accuracy       : 0.98444
2024-04-09 23:17:01,745 - trainer - INFO -     macro_f        : 0.984432
2024-04-09 23:17:01,745 - trainer - INFO -     precision      : 0.988525
2024-04-09 23:17:01,745 - trainer - INFO -     recall         : 0.98444
2024-04-09 23:17:01,745 - trainer - INFO -     doc_entropy    : 3.262647
2024-04-09 23:17:01,745 - trainer - INFO -     val_loss       : 0.148467
2024-04-09 23:17:01,745 - trainer - INFO -     val_accuracy   : 0.9592
2024-04-09 23:17:01,745 - trainer - INFO -     val_macro_f    : 0.958596
2024-04-09 23:17:01,745 - trainer - INFO -     val_precision  : 0.969757
2024-04-09 23:17:01,745 - trainer - INFO -     val_recall     : 0.9592
2024-04-09 23:17:01,745 - trainer - INFO -     val_doc_entropy: 3.370935
2024-04-09 23:17:01,745 - trainer - INFO -     test_loss      : 0.148563
2024-04-09 23:17:01,745 - trainer - INFO -     test_accuracy  : 0.9598
2024-04-09 23:17:01,745 - trainer - INFO -     test_macro_f   : 0.959601
2024-04-09 23:17:01,745 - trainer - INFO -     test_precision : 0.969585
2024-04-09 23:17:01,745 - trainer - INFO -     test_recall    : 0.9598
2024-04-09 23:17:01,745 - trainer - INFO -     test_doc_entropy: 3.359139
2024-04-09 23:18:21,175 - gensim.corpora.dictionary - INFO - adding document #0 to Dictionary<0 unique tokens: []>
2024-04-09 23:18:21,393 - gensim.corpora.dictionary - INFO - built Dictionary<8243 unique tokens: ['%', '(', ')', '+', '-']...> from 10000 documents (total 560212 corpus positions)
2024-04-09 23:18:21,409 - gensim.utils - INFO - Dictionary lifecycle event {'msg': "built Dictionary<8243 unique tokens: ['%', '(', ')', '+', '-']...> from 10000 documents (total 560212 corpus positions)", 'datetime': '2024-04-09T23:18:21.393946', 'gensim': '4.3.2', 'python': '3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}
2024-04-09 23:18:21,425 - gensim.topic_coherence.probability_estimation - INFO - using ParallelWordOccurrenceAccumulator<processes=11, batch_size=64> to estimate probabilities from sliding windows
2024-04-09 23:19:19,692 - gensim.topic_coherence.text_analysis - INFO - 1 batches submitted to accumulate stats from 64 documents (2693 virtual)
2024-04-09 23:19:19,692 - gensim.topic_coherence.text_analysis - INFO - 2 batches submitted to accumulate stats from 128 documents (5470 virtual)
2024-04-09 23:19:19,692 - gensim.topic_coherence.text_analysis - INFO - 3 batches submitted to accumulate stats from 192 documents (8874 virtual)
2024-04-09 23:19:19,692 - gensim.topic_coherence.text_analysis - INFO - 4 batches submitted to accumulate stats from 256 documents (11989 virtual)
2024-04-09 23:19:19,692 - gensim.topic_coherence.text_analysis - INFO - 5 batches submitted to accumulate stats from 320 documents (14374 virtual)
2024-04-09 23:19:19,692 - gensim.topic_coherence.text_analysis - INFO - 6 batches submitted to accumulate stats from 384 documents (17182 virtual)
2024-04-09 23:19:19,692 - gensim.topic_coherence.text_analysis - INFO - 7 batches submitted to accumulate stats from 448 documents (21527 virtual)
2024-04-09 23:19:19,708 - gensim.topic_coherence.text_analysis - INFO - 8 batches submitted to accumulate stats from 512 documents (24168 virtual)
2024-04-09 23:19:19,708 - gensim.topic_coherence.text_analysis - INFO - 9 batches submitted to accumulate stats from 576 documents (27902 virtual)
2024-04-09 23:19:19,708 - gensim.topic_coherence.text_analysis - INFO - 10 batches submitted to accumulate stats from 640 documents (31784 virtual)
2024-04-09 23:19:19,708 - gensim.topic_coherence.text_analysis - INFO - 11 batches submitted to accumulate stats from 704 documents (34718 virtual)
2024-04-09 23:19:19,708 - gensim.topic_coherence.text_analysis - INFO - 12 batches submitted to accumulate stats from 768 documents (37424 virtual)
2024-04-09 23:19:19,708 - gensim.topic_coherence.text_analysis - INFO - 13 batches submitted to accumulate stats from 832 documents (40286 virtual)
2024-04-09 23:19:19,708 - gensim.topic_coherence.text_analysis - INFO - 14 batches submitted to accumulate stats from 896 documents (43243 virtual)
2024-04-09 23:19:19,708 - gensim.topic_coherence.text_analysis - INFO - 15 batches submitted to accumulate stats from 960 documents (46126 virtual)
2024-04-09 23:19:19,723 - gensim.topic_coherence.text_analysis - INFO - 16 batches submitted to accumulate stats from 1024 documents (48682 virtual)
2024-04-09 23:19:19,723 - gensim.topic_coherence.text_analysis - INFO - 17 batches submitted to accumulate stats from 1088 documents (50683 virtual)
2024-04-09 23:19:19,723 - gensim.topic_coherence.text_analysis - INFO - 18 batches submitted to accumulate stats from 1152 documents (52033 virtual)
2024-04-09 23:19:19,723 - gensim.topic_coherence.text_analysis - INFO - 19 batches submitted to accumulate stats from 1216 documents (54074 virtual)
2024-04-09 23:19:19,723 - gensim.topic_coherence.text_analysis - INFO - 20 batches submitted to accumulate stats from 1280 documents (55852 virtual)
2024-04-09 23:19:19,723 - gensim.topic_coherence.text_analysis - INFO - 21 batches submitted to accumulate stats from 1344 documents (57199 virtual)
2024-04-09 23:19:19,739 - gensim.topic_coherence.text_analysis - INFO - 22 batches submitted to accumulate stats from 1408 documents (58228 virtual)
2024-04-09 23:19:19,739 - gensim.topic_coherence.text_analysis - INFO - 23 batches submitted to accumulate stats from 1472 documents (59571 virtual)
2024-04-09 23:19:19,739 - gensim.topic_coherence.text_analysis - INFO - 24 batches submitted to accumulate stats from 1536 documents (60967 virtual)
2024-04-09 23:19:19,739 - gensim.topic_coherence.text_analysis - INFO - 25 batches submitted to accumulate stats from 1600 documents (62124 virtual)
2024-04-09 23:19:19,739 - gensim.topic_coherence.text_analysis - INFO - 26 batches submitted to accumulate stats from 1664 documents (63042 virtual)
2024-04-09 23:19:19,739 - gensim.topic_coherence.text_analysis - INFO - 27 batches submitted to accumulate stats from 1728 documents (63717 virtual)
2024-04-09 23:19:19,755 - gensim.topic_coherence.text_analysis - INFO - 28 batches submitted to accumulate stats from 1792 documents (64218 virtual)
2024-04-09 23:19:19,755 - gensim.topic_coherence.text_analysis - INFO - 29 batches submitted to accumulate stats from 1856 documents (65578 virtual)
2024-04-09 23:19:19,755 - gensim.topic_coherence.text_analysis - INFO - 30 batches submitted to accumulate stats from 1920 documents (67634 virtual)
2024-04-09 23:19:19,770 - gensim.topic_coherence.text_analysis - INFO - 31 batches submitted to accumulate stats from 1984 documents (73849 virtual)
2024-04-09 23:19:19,770 - gensim.topic_coherence.text_analysis - INFO - 32 batches submitted to accumulate stats from 2048 documents (75524 virtual)
2024-04-09 23:19:19,770 - gensim.topic_coherence.text_analysis - INFO - 33 batches submitted to accumulate stats from 2112 documents (77130 virtual)
2024-04-09 23:19:19,786 - gensim.topic_coherence.text_analysis - INFO - 34 batches submitted to accumulate stats from 2176 documents (78474 virtual)
2024-04-09 23:19:19,786 - gensim.topic_coherence.text_analysis - INFO - 35 batches submitted to accumulate stats from 2240 documents (79468 virtual)
2024-04-09 23:19:19,786 - gensim.topic_coherence.text_analysis - INFO - 36 batches submitted to accumulate stats from 2304 documents (81417 virtual)
2024-04-09 23:19:19,802 - gensim.topic_coherence.text_analysis - INFO - 37 batches submitted to accumulate stats from 2368 documents (82927 virtual)
2024-04-09 23:19:19,802 - gensim.topic_coherence.text_analysis - INFO - 38 batches submitted to accumulate stats from 2432 documents (84179 virtual)
2024-04-09 23:19:19,802 - gensim.topic_coherence.text_analysis - INFO - 39 batches submitted to accumulate stats from 2496 documents (85539 virtual)
2024-04-09 23:19:19,802 - gensim.topic_coherence.text_analysis - INFO - 40 batches submitted to accumulate stats from 2560 documents (86803 virtual)
2024-04-09 23:19:19,802 - gensim.topic_coherence.text_analysis - INFO - 41 batches submitted to accumulate stats from 2624 documents (88906 virtual)
2024-04-09 23:19:19,802 - gensim.topic_coherence.text_analysis - INFO - 42 batches submitted to accumulate stats from 2688 documents (90045 virtual)
2024-04-09 23:19:19,802 - gensim.topic_coherence.text_analysis - INFO - 43 batches submitted to accumulate stats from 2752 documents (91459 virtual)
2024-04-09 23:19:19,802 - gensim.topic_coherence.text_analysis - INFO - 44 batches submitted to accumulate stats from 2816 documents (93273 virtual)
2024-04-09 23:19:19,817 - gensim.topic_coherence.text_analysis - INFO - 45 batches submitted to accumulate stats from 2880 documents (94745 virtual)
2024-04-09 23:19:19,817 - gensim.topic_coherence.text_analysis - INFO - 46 batches submitted to accumulate stats from 2944 documents (95959 virtual)
2024-04-09 23:19:19,817 - gensim.topic_coherence.text_analysis - INFO - 47 batches submitted to accumulate stats from 3008 documents (97731 virtual)
2024-04-09 23:19:19,817 - gensim.topic_coherence.text_analysis - INFO - 48 batches submitted to accumulate stats from 3072 documents (101658 virtual)
2024-04-09 23:19:19,833 - gensim.topic_coherence.text_analysis - INFO - 49 batches submitted to accumulate stats from 3136 documents (108115 virtual)
2024-04-09 23:19:19,833 - gensim.topic_coherence.text_analysis - INFO - 50 batches submitted to accumulate stats from 3200 documents (112196 virtual)
2024-04-09 23:19:19,833 - gensim.topic_coherence.text_analysis - INFO - 51 batches submitted to accumulate stats from 3264 documents (116184 virtual)
2024-04-09 23:19:19,833 - gensim.topic_coherence.text_analysis - INFO - 52 batches submitted to accumulate stats from 3328 documents (121825 virtual)
2024-04-09 23:19:19,848 - gensim.topic_coherence.text_analysis - INFO - 53 batches submitted to accumulate stats from 3392 documents (126803 virtual)
2024-04-09 23:19:19,848 - gensim.topic_coherence.text_analysis - INFO - 54 batches submitted to accumulate stats from 3456 documents (130920 virtual)
2024-04-09 23:19:19,848 - gensim.topic_coherence.text_analysis - INFO - 55 batches submitted to accumulate stats from 3520 documents (135856 virtual)
2024-04-09 23:19:19,848 - gensim.topic_coherence.text_analysis - INFO - 56 batches submitted to accumulate stats from 3584 documents (141505 virtual)
2024-04-09 23:19:19,864 - gensim.topic_coherence.text_analysis - INFO - 57 batches submitted to accumulate stats from 3648 documents (146310 virtual)
2024-04-09 23:19:19,880 - gensim.topic_coherence.text_analysis - INFO - 58 batches submitted to accumulate stats from 3712 documents (151981 virtual)
2024-04-09 23:19:19,880 - gensim.topic_coherence.text_analysis - INFO - 59 batches submitted to accumulate stats from 3776 documents (156731 virtual)
2024-04-09 23:19:19,895 - gensim.topic_coherence.text_analysis - INFO - 60 batches submitted to accumulate stats from 3840 documents (160267 virtual)
2024-04-09 23:19:19,895 - gensim.topic_coherence.text_analysis - INFO - 61 batches submitted to accumulate stats from 3904 documents (164699 virtual)
2024-04-09 23:19:19,895 - gensim.topic_coherence.text_analysis - INFO - 62 batches submitted to accumulate stats from 3968 documents (167130 virtual)
2024-04-09 23:19:19,911 - gensim.topic_coherence.text_analysis - INFO - 63 batches submitted to accumulate stats from 4032 documents (169846 virtual)
2024-04-09 23:19:19,911 - gensim.topic_coherence.text_analysis - INFO - 64 batches submitted to accumulate stats from 4096 documents (173362 virtual)
2024-04-09 23:19:19,911 - gensim.topic_coherence.text_analysis - INFO - 65 batches submitted to accumulate stats from 4160 documents (176494 virtual)
2024-04-09 23:19:19,911 - gensim.topic_coherence.text_analysis - INFO - 66 batches submitted to accumulate stats from 4224 documents (179288 virtual)
2024-04-09 23:19:19,911 - gensim.topic_coherence.text_analysis - INFO - 67 batches submitted to accumulate stats from 4288 documents (182080 virtual)
2024-04-09 23:19:19,927 - gensim.topic_coherence.text_analysis - INFO - 68 batches submitted to accumulate stats from 4352 documents (184198 virtual)
2024-04-09 23:19:19,927 - gensim.topic_coherence.text_analysis - INFO - 69 batches submitted to accumulate stats from 4416 documents (187386 virtual)
2024-04-09 23:19:19,927 - gensim.topic_coherence.text_analysis - INFO - 70 batches submitted to accumulate stats from 4480 documents (189912 virtual)
2024-04-09 23:19:19,927 - gensim.topic_coherence.text_analysis - INFO - 71 batches submitted to accumulate stats from 4544 documents (192554 virtual)
2024-04-09 23:19:19,927 - gensim.topic_coherence.text_analysis - INFO - 72 batches submitted to accumulate stats from 4608 documents (195960 virtual)
2024-04-09 23:19:19,927 - gensim.topic_coherence.text_analysis - INFO - 73 batches submitted to accumulate stats from 4672 documents (200012 virtual)
2024-04-09 23:19:19,942 - gensim.topic_coherence.text_analysis - INFO - 74 batches submitted to accumulate stats from 4736 documents (203330 virtual)
2024-04-09 23:19:20,052 - gensim.topic_coherence.text_analysis - INFO - 75 batches submitted to accumulate stats from 4800 documents (206888 virtual)
2024-04-09 23:19:20,052 - gensim.topic_coherence.text_analysis - INFO - 76 batches submitted to accumulate stats from 4864 documents (211715 virtual)
2024-04-09 23:19:20,052 - gensim.topic_coherence.text_analysis - INFO - 77 batches submitted to accumulate stats from 4928 documents (218875 virtual)
2024-04-09 23:19:20,083 - gensim.topic_coherence.text_analysis - INFO - 78 batches submitted to accumulate stats from 4992 documents (231392 virtual)
2024-04-09 23:19:20,098 - gensim.topic_coherence.text_analysis - INFO - 79 batches submitted to accumulate stats from 5056 documents (232287 virtual)
2024-04-09 23:19:20,098 - gensim.topic_coherence.text_analysis - INFO - 80 batches submitted to accumulate stats from 5120 documents (232833 virtual)
2024-04-09 23:19:20,114 - gensim.topic_coherence.text_analysis - INFO - 81 batches submitted to accumulate stats from 5184 documents (233462 virtual)
2024-04-09 23:19:20,114 - gensim.topic_coherence.text_analysis - INFO - 83 batches submitted to accumulate stats from 5312 documents (234218 virtual)
2024-04-09 23:19:20,114 - gensim.topic_coherence.text_analysis - INFO - 84 batches submitted to accumulate stats from 5376 documents (235214 virtual)
2024-04-09 23:19:20,302 - gensim.topic_coherence.text_analysis - INFO - 85 batches submitted to accumulate stats from 5440 documents (235765 virtual)
2024-04-09 23:19:20,302 - gensim.topic_coherence.text_analysis - INFO - 86 batches submitted to accumulate stats from 5504 documents (236082 virtual)
2024-04-09 23:19:20,302 - gensim.topic_coherence.text_analysis - INFO - 87 batches submitted to accumulate stats from 5568 documents (236431 virtual)
2024-04-09 23:19:20,302 - gensim.topic_coherence.text_analysis - INFO - 88 batches submitted to accumulate stats from 5632 documents (236761 virtual)
2024-04-09 23:19:20,302 - gensim.topic_coherence.text_analysis - INFO - 89 batches submitted to accumulate stats from 5696 documents (237120 virtual)
2024-04-09 23:19:20,302 - gensim.topic_coherence.text_analysis - INFO - 90 batches submitted to accumulate stats from 5760 documents (237187 virtual)
2024-04-09 23:19:20,302 - gensim.topic_coherence.text_analysis - INFO - 91 batches submitted to accumulate stats from 5824 documents (237524 virtual)
2024-04-09 23:19:20,302 - gensim.topic_coherence.text_analysis - INFO - 92 batches submitted to accumulate stats from 5888 documents (237645 virtual)
2024-04-09 23:19:20,302 - gensim.topic_coherence.text_analysis - INFO - 93 batches submitted to accumulate stats from 5952 documents (238006 virtual)
2024-04-09 23:19:20,302 - gensim.topic_coherence.text_analysis - INFO - 94 batches submitted to accumulate stats from 6016 documents (238721 virtual)
2024-04-09 23:19:20,302 - gensim.topic_coherence.text_analysis - INFO - 95 batches submitted to accumulate stats from 6080 documents (240305 virtual)
2024-04-09 23:19:20,302 - gensim.topic_coherence.text_analysis - INFO - 96 batches submitted to accumulate stats from 6144 documents (241115 virtual)
2024-04-09 23:19:20,317 - gensim.topic_coherence.text_analysis - INFO - 97 batches submitted to accumulate stats from 6208 documents (241946 virtual)
2024-04-09 23:19:20,317 - gensim.topic_coherence.text_analysis - INFO - 98 batches submitted to accumulate stats from 6272 documents (243834 virtual)
2024-04-09 23:19:20,317 - gensim.topic_coherence.text_analysis - INFO - 99 batches submitted to accumulate stats from 6336 documents (244474 virtual)
2024-04-09 23:19:20,317 - gensim.topic_coherence.text_analysis - INFO - 100 batches submitted to accumulate stats from 6400 documents (245373 virtual)
2024-04-09 23:19:20,317 - gensim.topic_coherence.text_analysis - INFO - 101 batches submitted to accumulate stats from 6464 documents (246734 virtual)
2024-04-09 23:19:20,317 - gensim.topic_coherence.text_analysis - INFO - 102 batches submitted to accumulate stats from 6528 documents (248196 virtual)
2024-04-09 23:19:20,317 - gensim.topic_coherence.text_analysis - INFO - 103 batches submitted to accumulate stats from 6592 documents (249199 virtual)
2024-04-09 23:19:20,317 - gensim.topic_coherence.text_analysis - INFO - 104 batches submitted to accumulate stats from 6656 documents (250226 virtual)
2024-04-09 23:19:20,333 - gensim.topic_coherence.text_analysis - INFO - 105 batches submitted to accumulate stats from 6720 documents (250964 virtual)
2024-04-09 23:19:20,333 - gensim.topic_coherence.text_analysis - INFO - 106 batches submitted to accumulate stats from 6784 documents (251892 virtual)
2024-04-09 23:19:20,333 - gensim.topic_coherence.text_analysis - INFO - 107 batches submitted to accumulate stats from 6848 documents (253322 virtual)
2024-04-09 23:19:20,333 - gensim.topic_coherence.text_analysis - INFO - 108 batches submitted to accumulate stats from 6912 documents (253966 virtual)
2024-04-09 23:19:20,333 - gensim.topic_coherence.text_analysis - INFO - 109 batches submitted to accumulate stats from 6976 documents (255153 virtual)
2024-04-09 23:19:20,348 - gensim.topic_coherence.text_analysis - INFO - 110 batches submitted to accumulate stats from 7040 documents (258116 virtual)
2024-04-09 23:19:20,348 - gensim.topic_coherence.text_analysis - INFO - 111 batches submitted to accumulate stats from 7104 documents (262863 virtual)
2024-04-09 23:19:20,348 - gensim.topic_coherence.text_analysis - INFO - 112 batches submitted to accumulate stats from 7168 documents (267402 virtual)
2024-04-09 23:19:20,348 - gensim.topic_coherence.text_analysis - INFO - 113 batches submitted to accumulate stats from 7232 documents (271466 virtual)
2024-04-09 23:19:20,348 - gensim.topic_coherence.text_analysis - INFO - 114 batches submitted to accumulate stats from 7296 documents (274734 virtual)
2024-04-09 23:19:20,364 - gensim.topic_coherence.text_analysis - INFO - 115 batches submitted to accumulate stats from 7360 documents (278585 virtual)
2024-04-09 23:19:20,364 - gensim.topic_coherence.text_analysis - INFO - 116 batches submitted to accumulate stats from 7424 documents (281720 virtual)
2024-04-09 23:19:20,364 - gensim.topic_coherence.text_analysis - INFO - 117 batches submitted to accumulate stats from 7488 documents (286124 virtual)
2024-04-09 23:19:20,380 - gensim.topic_coherence.text_analysis - INFO - 118 batches submitted to accumulate stats from 7552 documents (289525 virtual)
2024-04-09 23:19:20,380 - gensim.topic_coherence.text_analysis - INFO - 119 batches submitted to accumulate stats from 7616 documents (293514 virtual)
2024-04-09 23:19:20,395 - gensim.topic_coherence.text_analysis - INFO - 120 batches submitted to accumulate stats from 7680 documents (297582 virtual)
2024-04-09 23:19:20,395 - gensim.topic_coherence.text_analysis - INFO - 121 batches submitted to accumulate stats from 7744 documents (300713 virtual)
2024-04-09 23:19:20,411 - gensim.topic_coherence.text_analysis - INFO - 122 batches submitted to accumulate stats from 7808 documents (305851 virtual)
2024-04-09 23:19:20,411 - gensim.topic_coherence.text_analysis - INFO - 123 batches submitted to accumulate stats from 7872 documents (309307 virtual)
2024-04-09 23:19:20,411 - gensim.topic_coherence.text_analysis - INFO - 124 batches submitted to accumulate stats from 7936 documents (313187 virtual)
2024-04-09 23:19:20,427 - gensim.topic_coherence.text_analysis - INFO - 125 batches submitted to accumulate stats from 8000 documents (316848 virtual)
2024-04-09 23:19:20,427 - gensim.topic_coherence.text_analysis - INFO - 126 batches submitted to accumulate stats from 8064 documents (322161 virtual)
2024-04-09 23:19:20,427 - gensim.topic_coherence.text_analysis - INFO - 127 batches submitted to accumulate stats from 8128 documents (327330 virtual)
2024-04-09 23:19:20,442 - gensim.topic_coherence.text_analysis - INFO - 128 batches submitted to accumulate stats from 8192 documents (332307 virtual)
2024-04-09 23:19:20,442 - gensim.topic_coherence.text_analysis - INFO - 129 batches submitted to accumulate stats from 8256 documents (338388 virtual)
2024-04-09 23:19:20,442 - gensim.topic_coherence.text_analysis - INFO - 130 batches submitted to accumulate stats from 8320 documents (343860 virtual)
2024-04-09 23:19:20,458 - gensim.topic_coherence.text_analysis - INFO - 131 batches submitted to accumulate stats from 8384 documents (348751 virtual)
2024-04-09 23:19:20,473 - gensim.topic_coherence.text_analysis - INFO - 132 batches submitted to accumulate stats from 8448 documents (354002 virtual)
2024-04-09 23:19:20,473 - gensim.topic_coherence.text_analysis - INFO - 133 batches submitted to accumulate stats from 8512 documents (359278 virtual)
2024-04-09 23:19:20,473 - gensim.topic_coherence.text_analysis - INFO - 134 batches submitted to accumulate stats from 8576 documents (364694 virtual)
2024-04-09 23:19:20,489 - gensim.topic_coherence.text_analysis - INFO - 135 batches submitted to accumulate stats from 8640 documents (370961 virtual)
2024-04-09 23:19:20,505 - gensim.topic_coherence.text_analysis - INFO - 136 batches submitted to accumulate stats from 8704 documents (376061 virtual)
2024-04-09 23:19:20,505 - gensim.topic_coherence.text_analysis - INFO - 137 batches submitted to accumulate stats from 8768 documents (381326 virtual)
2024-04-09 23:19:20,520 - gensim.topic_coherence.text_analysis - INFO - 138 batches submitted to accumulate stats from 8832 documents (387120 virtual)
2024-04-09 23:19:20,520 - gensim.topic_coherence.text_analysis - INFO - 139 batches submitted to accumulate stats from 8896 documents (392939 virtual)
2024-04-09 23:19:20,536 - gensim.topic_coherence.text_analysis - INFO - 140 batches submitted to accumulate stats from 8960 documents (399565 virtual)
2024-04-09 23:19:20,536 - gensim.topic_coherence.text_analysis - INFO - 141 batches submitted to accumulate stats from 9024 documents (406096 virtual)
2024-04-09 23:19:20,536 - gensim.topic_coherence.text_analysis - INFO - 142 batches submitted to accumulate stats from 9088 documents (411367 virtual)
2024-04-09 23:19:20,552 - gensim.topic_coherence.text_analysis - INFO - 143 batches submitted to accumulate stats from 9152 documents (415184 virtual)
2024-04-09 23:19:20,552 - gensim.topic_coherence.text_analysis - INFO - 144 batches submitted to accumulate stats from 9216 documents (418901 virtual)
2024-04-09 23:19:20,552 - gensim.topic_coherence.text_analysis - INFO - 145 batches submitted to accumulate stats from 9280 documents (422980 virtual)
2024-04-09 23:19:20,567 - gensim.topic_coherence.text_analysis - INFO - 146 batches submitted to accumulate stats from 9344 documents (427637 virtual)
2024-04-09 23:19:20,567 - gensim.topic_coherence.text_analysis - INFO - 147 batches submitted to accumulate stats from 9408 documents (431473 virtual)
2024-04-09 23:19:20,583 - gensim.topic_coherence.text_analysis - INFO - 148 batches submitted to accumulate stats from 9472 documents (434867 virtual)
2024-04-09 23:19:20,598 - gensim.topic_coherence.text_analysis - INFO - 149 batches submitted to accumulate stats from 9536 documents (439009 virtual)
2024-04-09 23:19:20,614 - gensim.topic_coherence.text_analysis - INFO - 150 batches submitted to accumulate stats from 9600 documents (443232 virtual)
2024-04-09 23:19:20,614 - gensim.topic_coherence.text_analysis - INFO - 151 batches submitted to accumulate stats from 9664 documents (447789 virtual)
2024-04-09 23:19:20,614 - gensim.topic_coherence.text_analysis - INFO - 152 batches submitted to accumulate stats from 9728 documents (452368 virtual)
2024-04-09 23:19:20,630 - gensim.topic_coherence.text_analysis - INFO - 153 batches submitted to accumulate stats from 9792 documents (456047 virtual)
2024-04-09 23:19:20,630 - gensim.topic_coherence.text_analysis - INFO - 154 batches submitted to accumulate stats from 9856 documents (459948 virtual)
2024-04-09 23:19:20,645 - gensim.topic_coherence.text_analysis - INFO - 155 batches submitted to accumulate stats from 9920 documents (464419 virtual)
2024-04-09 23:19:20,645 - gensim.topic_coherence.text_analysis - INFO - 156 batches submitted to accumulate stats from 9984 documents (468174 virtual)
2024-04-09 23:19:20,661 - gensim.topic_coherence.text_analysis - INFO - 157 batches submitted to accumulate stats from 10048 documents (470212 virtual)
2024-04-09 23:19:20,817 - gensim.topic_coherence.text_analysis - INFO - 11 accumulators retrieved from output queue
2024-04-09 23:19:20,895 - gensim.topic_coherence.text_analysis - INFO - accumulated word occurrence stats for 481518 virtual documents
2024-04-09 23:19:21,005 - gensim.corpora.dictionary - INFO - adding document #0 to Dictionary<0 unique tokens: []>
2024-04-09 23:19:21,255 - gensim.corpora.dictionary - INFO - built Dictionary<8243 unique tokens: ['%', '(', ')', '+', '-']...> from 10000 documents (total 560212 corpus positions)
2024-04-09 23:19:21,255 - gensim.utils - INFO - Dictionary lifecycle event {'msg': "built Dictionary<8243 unique tokens: ['%', '(', ')', '+', '-']...> from 10000 documents (total 560212 corpus positions)", 'datetime': '2024-04-09T23:19:21.255182', 'gensim': '4.3.2', 'python': '3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}
2024-04-09 23:19:21,270 - gensim.topic_coherence.probability_estimation - INFO - using ParallelWordOccurrenceAccumulator<processes=11, batch_size=64> to estimate probabilities from sliding windows
2024-04-09 23:20:17,921 - gensim.topic_coherence.text_analysis - INFO - 49 batches submitted to accumulate stats from 3136 documents (-205485 virtual)
2024-04-09 23:20:17,968 - gensim.topic_coherence.text_analysis - INFO - 77 batches submitted to accumulate stats from 4928 documents (-273925 virtual)
2024-04-09 23:20:17,968 - gensim.topic_coherence.text_analysis - INFO - 78 batches submitted to accumulate stats from 4992 documents (-267808 virtual)
2024-04-09 23:20:18,046 - gensim.topic_coherence.text_analysis - INFO - 140 batches submitted to accumulate stats from 8960 documents (-496435 virtual)
2024-04-09 23:20:18,046 - gensim.topic_coherence.text_analysis - INFO - 141 batches submitted to accumulate stats from 9024 documents (-496304 virtual)
2024-04-09 23:20:18,078 - gensim.topic_coherence.text_analysis - INFO - 157 batches submitted to accumulate stats from 10048 documents (-529788 virtual)
2024-04-09 23:20:18,171 - gensim.topic_coherence.text_analysis - INFO - 11 accumulators retrieved from output queue
2024-04-09 23:20:18,203 - gensim.topic_coherence.text_analysis - INFO - accumulated word occurrence stats for 123467 virtual documents
2024-04-09 23:20:21,298 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=10, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,645,139
Freeze params: 0
2024-04-09 23:29:16,060 - trainer - INFO -     epoch          : 1
2024-04-09 23:29:16,060 - trainer - INFO -     loss           : 0.225795
2024-04-09 23:29:16,060 - trainer - INFO -     accuracy       : 0.9343
2024-04-09 23:29:16,076 - trainer - INFO -     macro_f        : 0.933192
2024-04-09 23:29:16,076 - trainer - INFO -     precision      : 0.948664
2024-04-09 23:29:16,076 - trainer - INFO -     recall         : 0.9343
2024-04-09 23:29:16,076 - trainer - INFO -     doc_entropy    : 4.085825
2024-04-09 23:29:16,076 - trainer - INFO -     val_loss       : 0.180706
2024-04-09 23:29:16,076 - trainer - INFO -     val_accuracy   : 0.948
2024-04-09 23:29:16,076 - trainer - INFO -     val_macro_f    : 0.946813
2024-04-09 23:29:16,076 - trainer - INFO -     val_precision  : 0.961146
2024-04-09 23:29:16,076 - trainer - INFO -     val_recall     : 0.948
2024-04-09 23:29:16,076 - trainer - INFO -     val_doc_entropy: 3.848904
2024-04-09 23:29:16,076 - trainer - INFO -     test_loss      : 0.171639
2024-04-09 23:29:16,076 - trainer - INFO -     test_accuracy  : 0.9451
2024-04-09 23:29:16,076 - trainer - INFO -     test_macro_f   : 0.943674
2024-04-09 23:29:16,076 - trainer - INFO -     test_precision : 0.957437
2024-04-09 23:29:16,076 - trainer - INFO -     test_recall    : 0.9451
2024-04-09 23:29:16,076 - trainer - INFO -     test_doc_entropy: 3.842523
2024-04-09 23:38:12,177 - trainer - INFO -     epoch          : 2
2024-04-09 23:38:12,177 - trainer - INFO -     loss           : 0.082743
2024-04-09 23:38:12,177 - trainer - INFO -     accuracy       : 0.97374
2024-04-09 23:38:12,177 - trainer - INFO -     macro_f        : 0.973668
2024-04-09 23:38:12,177 - trainer - INFO -     precision      : 0.98039
2024-04-09 23:38:12,177 - trainer - INFO -     recall         : 0.97374
2024-04-09 23:38:12,177 - trainer - INFO -     doc_entropy    : 3.617146
2024-04-09 23:38:12,177 - trainer - INFO -     val_loss       : 0.168433
2024-04-09 23:38:12,177 - trainer - INFO -     val_accuracy   : 0.951
2024-04-09 23:38:12,177 - trainer - INFO -     val_macro_f    : 0.950622
2024-04-09 23:38:12,177 - trainer - INFO -     val_precision  : 0.963663
2024-04-09 23:38:12,177 - trainer - INFO -     val_recall     : 0.951
2024-04-09 23:38:12,177 - trainer - INFO -     val_doc_entropy: 3.58848
2024-04-09 23:38:12,177 - trainer - INFO -     test_loss      : 0.135087
2024-04-09 23:38:12,177 - trainer - INFO -     test_accuracy  : 0.9568
2024-04-09 23:38:12,177 - trainer - INFO -     test_macro_f   : 0.9566
2024-04-09 23:38:12,177 - trainer - INFO -     test_precision : 0.967008
2024-04-09 23:38:12,177 - trainer - INFO -     test_recall    : 0.9568
2024-04-09 23:38:12,177 - trainer - INFO -     test_doc_entropy: 3.581709
2024-04-09 23:47:07,592 - trainer - INFO -     epoch          : 3
2024-04-09 23:47:07,592 - trainer - INFO -     loss           : 0.04906
2024-04-09 23:47:07,592 - trainer - INFO -     accuracy       : 0.98472
2024-04-09 23:47:07,592 - trainer - INFO -     macro_f        : 0.984744
2024-04-09 23:47:07,592 - trainer - INFO -     precision      : 0.988945
2024-04-09 23:47:07,592 - trainer - INFO -     recall         : 0.98472
2024-04-09 23:47:07,592 - trainer - INFO -     doc_entropy    : 3.297702
2024-04-09 23:47:07,592 - trainer - INFO -     val_loss       : 0.199347
2024-04-09 23:47:07,592 - trainer - INFO -     val_accuracy   : 0.9446
2024-04-09 23:47:07,592 - trainer - INFO -     val_macro_f    : 0.943296
2024-04-09 23:47:07,592 - trainer - INFO -     val_precision  : 0.957884
2024-04-09 23:47:07,608 - trainer - INFO -     val_recall     : 0.9446
2024-04-09 23:47:07,608 - trainer - INFO -     val_doc_entropy: 3.370378
2024-04-09 23:47:07,608 - trainer - INFO -     test_loss      : 0.181225
2024-04-09 23:47:07,608 - trainer - INFO -     test_accuracy  : 0.9486
2024-04-09 23:47:07,608 - trainer - INFO -     test_macro_f   : 0.947976
2024-04-09 23:47:07,608 - trainer - INFO -     test_precision : 0.961559
2024-04-09 23:47:07,608 - trainer - INFO -     test_recall    : 0.9486
2024-04-09 23:47:07,608 - trainer - INFO -     test_doc_entropy: 3.371788
2024-04-09 23:48:28,015 - gensim.corpora.dictionary - INFO - adding document #0 to Dictionary<0 unique tokens: []>
2024-04-09 23:48:28,249 - gensim.corpora.dictionary - INFO - built Dictionary<8243 unique tokens: ['%', '(', ')', '+', '-']...> from 10000 documents (total 560212 corpus positions)
2024-04-09 23:48:28,249 - gensim.utils - INFO - Dictionary lifecycle event {'msg': "built Dictionary<8243 unique tokens: ['%', '(', ')', '+', '-']...> from 10000 documents (total 560212 corpus positions)", 'datetime': '2024-04-09T23:48:28.249899', 'gensim': '4.3.2', 'python': '3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}
2024-04-23 21:15:16,302 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=10, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (gru): GRU(768, 768, batch_first=True, bidirectional=True)
  (W_k): Linear(in_features=768, out_features=40, bias=False)
  (W_q): Linear(in_features=768, out_features=40, bias=False)
  (droputout): Dropout(p=0.2, inplace=False)
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,793,683
Freeze params: 0
2024-04-23 21:15:55,393 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=10, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (gru): GRU(768, 768, batch_first=True, bidirectional=True)
  (W_k): Linear(in_features=1536, out_features=40, bias=False)
  (W_q): Linear(in_features=768, out_features=40, bias=False)
  (droputout): Dropout(p=0.2, inplace=False)
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,824,403
Freeze params: 0
2024-04-23 21:30:56,130 - trainer - INFO -     epoch          : 1
2024-04-23 21:30:56,130 - trainer - INFO -     loss           : 0.221365
2024-04-23 21:30:56,130 - trainer - INFO -     accuracy       : 0.93594
2024-04-23 21:30:56,130 - trainer - INFO -     macro_f        : 0.934979
2024-04-23 21:30:56,130 - trainer - INFO -     precision      : 0.949583
2024-04-23 21:30:56,130 - trainer - INFO -     recall         : 0.93594
2024-04-23 21:30:56,130 - trainer - INFO -     doc_entropy    : 3.387725
2024-04-23 21:30:56,130 - trainer - INFO -     val_loss       : 0.233644
2024-04-23 21:30:56,130 - trainer - INFO -     val_accuracy   : 0.9278
2024-04-23 21:30:56,130 - trainer - INFO -     val_macro_f    : 0.923466
2024-04-23 21:30:56,130 - trainer - INFO -     val_precision  : 0.944454
2024-04-23 21:30:56,130 - trainer - INFO -     val_recall     : 0.9278
2024-04-23 21:30:56,145 - trainer - INFO -     val_doc_entropy: 4.009089
2024-04-23 21:30:56,145 - trainer - INFO -     test_loss      : 0.22904
2024-04-23 21:30:56,145 - trainer - INFO -     test_accuracy  : 0.9302
2024-04-23 21:30:56,145 - trainer - INFO -     test_macro_f   : 0.927984
2024-04-23 21:30:56,145 - trainer - INFO -     test_precision : 0.948092
2024-04-23 21:30:56,145 - trainer - INFO -     test_recall    : 0.9302
2024-04-23 21:30:56,145 - trainer - INFO -     test_doc_entropy: 4.007785
2024-04-23 21:46:09,447 - trainer - INFO -     epoch          : 2
2024-04-23 21:46:09,463 - trainer - INFO -     loss           : 0.082368
2024-04-23 21:46:09,463 - trainer - INFO -     accuracy       : 0.97386
2024-04-23 21:46:09,463 - trainer - INFO -     macro_f        : 0.97364
2024-04-23 21:46:09,463 - trainer - INFO -     precision      : 0.980381
2024-04-23 21:46:09,463 - trainer - INFO -     recall         : 0.97386
2024-04-23 21:46:09,463 - trainer - INFO -     doc_entropy    : 3.007153
2024-04-23 21:46:09,463 - trainer - INFO -     val_loss       : 0.169219
2024-04-23 21:46:09,463 - trainer - INFO -     val_accuracy   : 0.9514
2024-04-23 21:46:09,463 - trainer - INFO -     val_macro_f    : 0.950041
2024-04-23 21:46:09,463 - trainer - INFO -     val_precision  : 0.962879
2024-04-23 21:46:09,463 - trainer - INFO -     val_recall     : 0.9514
2024-04-23 21:46:09,463 - trainer - INFO -     val_doc_entropy: 3.687605
2024-04-23 21:46:09,463 - trainer - INFO -     test_loss      : 0.156356
2024-04-23 21:46:09,463 - trainer - INFO -     test_accuracy  : 0.9531
2024-04-23 21:46:09,463 - trainer - INFO -     test_macro_f   : 0.952574
2024-04-23 21:46:09,463 - trainer - INFO -     test_precision : 0.965183
2024-04-23 21:46:09,463 - trainer - INFO -     test_recall    : 0.9531
2024-04-23 21:46:09,463 - trainer - INFO -     test_doc_entropy: 3.67217
2024-04-23 21:48:07,078 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=10, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (gru): GRU(768, 768, batch_first=True, bidirectional=True)
  (W_k): Linear(in_features=1536, out_features=40, bias=False)
  (W_q): Linear(in_features=768, out_features=40, bias=False)
  (droputout): Dropout(p=0.2, inplace=False)
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,824,403
Freeze params: 0
2024-04-26 22:29:50,083 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 1024, padding_idx=0)
      (position_embeddings): Embedding(512, 1024)
      (token_type_embeddings): Embedding(2, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=1024, out_features=1024, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=1024, out_features=10, bias=True)
  (final): Linear(in_features=1024, out_features=1024, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=1024, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=1024, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 37,852,499
Freeze params: 0
2024-04-26 22:42:49,505 - trainer - INFO -     epoch          : 1
2024-04-26 22:42:49,505 - trainer - INFO -     loss           : 0.226918
2024-04-26 22:42:49,505 - trainer - INFO -     accuracy       : 0.9332
2024-04-26 22:42:49,505 - trainer - INFO -     macro_f        : 0.931465
2024-04-26 22:42:49,505 - trainer - INFO -     precision      : 0.946675
2024-04-26 22:42:49,505 - trainer - INFO -     recall         : 0.9332
2024-04-26 22:42:49,505 - trainer - INFO -     doc_entropy    : 4.386979
2024-04-26 22:42:49,505 - trainer - INFO -     val_loss       : 0.155609
2024-04-26 22:42:49,505 - trainer - INFO -     val_accuracy   : 0.9518
2024-04-26 22:42:49,505 - trainer - INFO -     val_macro_f    : 0.950435
2024-04-26 22:42:49,505 - trainer - INFO -     val_precision  : 0.961707
2024-04-26 22:42:49,505 - trainer - INFO -     val_recall     : 0.9518
2024-04-26 22:42:49,505 - trainer - INFO -     val_doc_entropy: 3.963328
2024-04-26 22:42:49,505 - trainer - INFO -     test_loss      : 0.146582
2024-04-26 22:42:49,505 - trainer - INFO -     test_accuracy  : 0.954
2024-04-26 22:42:49,505 - trainer - INFO -     test_macro_f   : 0.95351
2024-04-26 22:42:49,505 - trainer - INFO -     test_precision : 0.965133
2024-04-26 22:42:49,505 - trainer - INFO -     test_recall    : 0.954
2024-04-26 22:42:49,505 - trainer - INFO -     test_doc_entropy: 3.977616
2024-04-26 22:55:54,443 - trainer - INFO -     epoch          : 2
2024-04-26 22:55:54,443 - trainer - INFO -     loss           : 0.07506
2024-04-26 22:55:54,443 - trainer - INFO -     accuracy       : 0.97646
2024-04-26 22:55:54,443 - trainer - INFO -     macro_f        : 0.976397
2024-04-26 22:55:54,443 - trainer - INFO -     precision      : 0.982629
2024-04-26 22:55:54,443 - trainer - INFO -     recall         : 0.97646
2024-04-26 22:55:54,459 - trainer - INFO -     doc_entropy    : 4.112123
2024-04-26 22:55:54,459 - trainer - INFO -     val_loss       : 0.236561
2024-04-26 22:55:54,459 - trainer - INFO -     val_accuracy   : 0.9332
2024-04-26 22:55:54,459 - trainer - INFO -     val_macro_f    : 0.931678
2024-04-26 22:55:54,459 - trainer - INFO -     val_precision  : 0.950918
2024-04-26 22:55:54,459 - trainer - INFO -     val_recall     : 0.9332
2024-04-26 22:55:54,459 - trainer - INFO -     val_doc_entropy: 3.831134
2024-04-26 22:55:54,459 - trainer - INFO -     test_loss      : 0.196707
2024-04-26 22:55:54,459 - trainer - INFO -     test_accuracy  : 0.9428
2024-04-26 22:55:54,459 - trainer - INFO -     test_macro_f   : 0.941108
2024-04-26 22:55:54,459 - trainer - INFO -     test_precision : 0.956062
2024-04-26 22:55:54,459 - trainer - INFO -     test_recall    : 0.9428
2024-04-26 22:55:54,459 - trainer - INFO -     test_doc_entropy: 3.859266
2024-04-26 23:09:01,220 - trainer - INFO -     epoch          : 3
2024-04-26 23:09:01,220 - trainer - INFO -     loss           : 0.035032
2024-04-26 23:09:01,220 - trainer - INFO -     accuracy       : 0.9889
2024-04-26 23:09:01,220 - trainer - INFO -     macro_f        : 0.988867
2024-04-26 23:09:01,220 - trainer - INFO -     precision      : 0.991708
2024-04-26 23:09:01,220 - trainer - INFO -     recall         : 0.9889
2024-04-26 23:09:01,220 - trainer - INFO -     doc_entropy    : 3.895246
2024-04-26 23:09:01,220 - trainer - INFO -     val_loss       : 0.203423
2024-04-26 23:09:01,220 - trainer - INFO -     val_accuracy   : 0.9458
2024-04-26 23:09:01,220 - trainer - INFO -     val_macro_f    : 0.944771
2024-04-26 23:09:01,236 - trainer - INFO -     val_precision  : 0.958501
2024-04-26 23:09:01,236 - trainer - INFO -     val_recall     : 0.9458
2024-04-26 23:09:01,236 - trainer - INFO -     val_doc_entropy: 3.895127
2024-04-26 23:09:01,237 - trainer - INFO -     test_loss      : 0.198041
2024-04-26 23:09:01,237 - trainer - INFO -     test_accuracy  : 0.9508
2024-04-26 23:09:01,237 - trainer - INFO -     test_macro_f   : 0.950081
2024-04-26 23:09:01,237 - trainer - INFO -     test_precision : 0.963547
2024-04-26 23:09:01,237 - trainer - INFO -     test_recall    : 0.9508
2024-04-26 23:09:01,237 - trainer - INFO -     test_doc_entropy: 3.904067
2024-04-26 23:10:36,427 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 1024, padding_idx=0)
      (position_embeddings): Embedding(512, 1024)
      (token_type_embeddings): Embedding(2, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=1024, out_features=1024, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=1024, out_features=10, bias=True)
  (final): Linear(in_features=1024, out_features=1024, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=1024, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=1024, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 37,852,499
Freeze params: 0
2024-04-26 23:23:47,959 - trainer - INFO -     epoch          : 1
2024-04-26 23:23:48,831 - trainer - INFO -     loss           : 0.227411
2024-04-26 23:23:48,831 - trainer - INFO -     accuracy       : 0.93388
2024-04-26 23:23:48,831 - trainer - INFO -     macro_f        : 0.932618
2024-04-26 23:23:48,831 - trainer - INFO -     precision      : 0.947112
2024-04-26 23:23:48,831 - trainer - INFO -     recall         : 0.93388
2024-04-26 23:23:48,841 - trainer - INFO -     doc_entropy    : 4.294071
2024-04-26 23:23:48,841 - trainer - INFO -     val_loss       : 0.185649
2024-04-26 23:23:48,841 - trainer - INFO -     val_accuracy   : 0.941
2024-04-26 23:23:48,841 - trainer - INFO -     val_macro_f    : 0.939701
2024-04-26 23:23:48,841 - trainer - INFO -     val_precision  : 0.955346
2024-04-26 23:23:48,841 - trainer - INFO -     val_recall     : 0.941
2024-04-26 23:23:48,841 - trainer - INFO -     val_doc_entropy: 4.258198
2024-04-26 23:23:48,841 - trainer - INFO -     test_loss      : 0.1567
2024-04-26 23:23:48,841 - trainer - INFO -     test_accuracy  : 0.9487
2024-04-26 23:23:48,841 - trainer - INFO -     test_macro_f   : 0.947677
2024-04-26 23:23:48,841 - trainer - INFO -     test_precision : 0.960404
2024-04-26 23:23:48,841 - trainer - INFO -     test_recall    : 0.9487
2024-04-26 23:23:48,841 - trainer - INFO -     test_doc_entropy: 4.244412
2024-04-26 23:36:46,568 - trainer - INFO -     epoch          : 2
2024-04-26 23:36:46,568 - trainer - INFO -     loss           : 0.073207
2024-04-26 23:36:46,568 - trainer - INFO -     accuracy       : 0.97614
2024-04-26 23:36:46,568 - trainer - INFO -     macro_f        : 0.975859
2024-04-26 23:36:46,568 - trainer - INFO -     precision      : 0.981736
2024-04-26 23:36:46,568 - trainer - INFO -     recall         : 0.97614
2024-04-26 23:36:46,568 - trainer - INFO -     doc_entropy    : 4.034162
2024-04-26 23:36:46,568 - trainer - INFO -     val_loss       : 0.291772
2024-04-26 23:36:46,568 - trainer - INFO -     val_accuracy   : 0.9162
2024-04-26 23:36:46,568 - trainer - INFO -     val_macro_f    : 0.910661
2024-04-26 23:36:46,568 - trainer - INFO -     val_precision  : 0.933891
2024-04-26 23:36:46,568 - trainer - INFO -     val_recall     : 0.9162
2024-04-26 23:36:46,568 - trainer - INFO -     val_doc_entropy: 4.045327
2024-04-26 23:36:46,568 - trainer - INFO -     test_loss      : 0.227143
2024-04-26 23:36:46,568 - trainer - INFO -     test_accuracy  : 0.935
2024-04-26 23:36:46,568 - trainer - INFO -     test_macro_f   : 0.932598
2024-04-26 23:36:46,568 - trainer - INFO -     test_precision : 0.950031
2024-04-26 23:36:46,568 - trainer - INFO -     test_recall    : 0.935
2024-04-26 23:36:46,568 - trainer - INFO -     test_doc_entropy: 4.022606
2024-04-26 23:49:50,818 - trainer - INFO -     epoch          : 3
2024-04-26 23:49:50,833 - trainer - INFO -     loss           : 0.034633
2024-04-26 23:49:50,833 - trainer - INFO -     accuracy       : 0.9894
2024-04-26 23:49:50,833 - trainer - INFO -     macro_f        : 0.989386
2024-04-26 23:49:50,833 - trainer - INFO -     precision      : 0.992201
2024-04-26 23:49:50,833 - trainer - INFO -     recall         : 0.9894
2024-04-26 23:49:50,833 - trainer - INFO -     doc_entropy    : 3.835343
2024-04-26 23:49:50,833 - trainer - INFO -     val_loss       : 0.269477
2024-04-26 23:49:50,833 - trainer - INFO -     val_accuracy   : 0.9304
2024-04-26 23:49:50,833 - trainer - INFO -     val_macro_f    : 0.92843
2024-04-26 23:49:50,833 - trainer - INFO -     val_precision  : 0.949002
2024-04-26 23:49:50,833 - trainer - INFO -     val_recall     : 0.9304
2024-04-26 23:49:50,833 - trainer - INFO -     val_doc_entropy: 3.876109
2024-04-26 23:49:50,833 - trainer - INFO -     test_loss      : 0.227459
2024-04-26 23:49:50,833 - trainer - INFO -     test_accuracy  : 0.9435
2024-04-26 23:49:50,833 - trainer - INFO -     test_macro_f   : 0.942794
2024-04-26 23:49:50,833 - trainer - INFO -     test_precision : 0.958449
2024-04-26 23:49:50,833 - trainer - INFO -     test_recall    : 0.9435
2024-04-26 23:49:50,833 - trainer - INFO -     test_doc_entropy: 3.879279
2024-04-26 23:51:26,837 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 1024, padding_idx=0)
      (position_embeddings): Embedding(512, 1024)
      (token_type_embeddings): Embedding(2, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=1024, out_features=1024, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=1024, out_features=10, bias=True)
  (final): Linear(in_features=1024, out_features=1024, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=1024, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=1024, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 37,852,499
Freeze params: 0
2024-04-27 00:04:43,053 - trainer - INFO -     epoch          : 1
2024-04-27 00:04:43,069 - trainer - INFO -     loss           : 0.227394
2024-04-27 00:04:43,069 - trainer - INFO -     accuracy       : 0.93194
2024-04-27 00:04:43,069 - trainer - INFO -     macro_f        : 0.930341
2024-04-27 00:04:43,069 - trainer - INFO -     precision      : 0.945572
2024-04-27 00:04:43,069 - trainer - INFO -     recall         : 0.93194
2024-04-27 00:04:43,069 - trainer - INFO -     doc_entropy    : 4.344347
2024-04-27 00:04:43,069 - trainer - INFO -     val_loss       : 0.188435
2024-04-27 00:04:43,069 - trainer - INFO -     val_accuracy   : 0.9426
2024-04-27 00:04:43,069 - trainer - INFO -     val_macro_f    : 0.941886
2024-04-27 00:04:43,069 - trainer - INFO -     val_precision  : 0.956168
2024-04-27 00:04:43,069 - trainer - INFO -     val_recall     : 0.9426
2024-04-27 00:04:43,069 - trainer - INFO -     val_doc_entropy: 4.118758
2024-04-27 00:04:43,069 - trainer - INFO -     test_loss      : 0.159544
2024-04-27 00:04:43,069 - trainer - INFO -     test_accuracy  : 0.9508
2024-04-27 00:04:43,069 - trainer - INFO -     test_macro_f   : 0.950293
2024-04-27 00:04:43,069 - trainer - INFO -     test_precision : 0.962095
2024-04-27 00:04:43,069 - trainer - INFO -     test_recall    : 0.9508
2024-04-27 00:04:43,069 - trainer - INFO -     test_doc_entropy: 4.135835
2024-04-27 00:17:55,193 - trainer - INFO -     epoch          : 2
2024-04-27 00:17:55,193 - trainer - INFO -     loss           : 0.073326
2024-04-27 00:17:55,193 - trainer - INFO -     accuracy       : 0.97652
2024-04-27 00:17:55,193 - trainer - INFO -     macro_f        : 0.976711
2024-04-27 00:17:55,209 - trainer - INFO -     precision      : 0.982842
2024-04-27 00:17:55,209 - trainer - INFO -     recall         : 0.97652
2024-04-27 00:17:55,209 - trainer - INFO -     doc_entropy    : 4.045732
2024-04-27 00:17:55,209 - trainer - INFO -     val_loss       : 0.250079
2024-04-27 00:17:55,209 - trainer - INFO -     val_accuracy   : 0.9274
2024-04-27 00:17:55,209 - trainer - INFO -     val_macro_f    : 0.924686
2024-04-27 00:17:55,209 - trainer - INFO -     val_precision  : 0.946812
2024-04-27 00:17:55,209 - trainer - INFO -     val_recall     : 0.9274
2024-04-27 00:17:55,209 - trainer - INFO -     val_doc_entropy: 4.033192
2024-04-27 00:17:55,209 - trainer - INFO -     test_loss      : 0.204169
2024-04-27 00:17:55,209 - trainer - INFO -     test_accuracy  : 0.94
2024-04-27 00:17:55,209 - trainer - INFO -     test_macro_f   : 0.938897
2024-04-27 00:17:55,209 - trainer - INFO -     test_precision : 0.956128
2024-04-27 00:17:55,209 - trainer - INFO -     test_recall    : 0.94
2024-04-27 00:17:55,209 - trainer - INFO -     test_doc_entropy: 4.03512
2024-04-27 00:31:01,786 - trainer - INFO -     epoch          : 3
2024-04-27 00:31:01,786 - trainer - INFO -     loss           : 0.034212
2024-04-27 00:31:01,786 - trainer - INFO -     accuracy       : 0.9891
2024-04-27 00:31:01,786 - trainer - INFO -     macro_f        : 0.989078
2024-04-27 00:31:01,786 - trainer - INFO -     precision      : 0.991933
2024-04-27 00:31:01,786 - trainer - INFO -     recall         : 0.9891
2024-04-27 00:31:01,786 - trainer - INFO -     doc_entropy    : 3.861155
2024-04-27 00:31:01,786 - trainer - INFO -     val_loss       : 0.253098
2024-04-27 00:31:01,802 - trainer - INFO -     val_accuracy   : 0.9362
2024-04-27 00:31:01,802 - trainer - INFO -     val_macro_f    : 0.934465
2024-04-27 00:31:01,802 - trainer - INFO -     val_precision  : 0.95158
2024-04-27 00:31:01,802 - trainer - INFO -     val_recall     : 0.9362
2024-04-27 00:31:01,802 - trainer - INFO -     val_doc_entropy: 3.766127
2024-04-27 00:31:01,802 - trainer - INFO -     test_loss      : 0.219473
2024-04-27 00:31:01,802 - trainer - INFO -     test_accuracy  : 0.9468
2024-04-27 00:31:01,802 - trainer - INFO -     test_macro_f   : 0.94606
2024-04-27 00:31:01,802 - trainer - INFO -     test_precision : 0.960473
2024-04-27 00:31:01,802 - trainer - INFO -     test_recall    : 0.9468
2024-04-27 00:31:01,802 - trainer - INFO -     test_doc_entropy: 3.787472
2024-04-27 00:32:39,020 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 1024, padding_idx=0)
      (position_embeddings): Embedding(512, 1024)
      (token_type_embeddings): Embedding(2, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=1024, out_features=1024, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=1024, out_features=10, bias=True)
  (final): Linear(in_features=1024, out_features=1024, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=1024, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=1024, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 37,852,499
Freeze params: 0
2024-04-27 00:45:46,193 - trainer - INFO -     epoch          : 1
2024-04-27 00:45:47,052 - trainer - INFO -     loss           : 0.226424
2024-04-27 00:45:47,052 - trainer - INFO -     accuracy       : 0.93456
2024-04-27 00:45:47,052 - trainer - INFO -     macro_f        : 0.933159
2024-04-27 00:45:47,052 - trainer - INFO -     precision      : 0.947785
2024-04-27 00:45:47,052 - trainer - INFO -     recall         : 0.93456
2024-04-27 00:45:47,052 - trainer - INFO -     doc_entropy    : 4.389369
2024-04-27 00:45:47,052 - trainer - INFO -     val_loss       : 0.167769
2024-04-27 00:45:47,052 - trainer - INFO -     val_accuracy   : 0.9494
2024-04-27 00:45:47,052 - trainer - INFO -     val_macro_f    : 0.94862
2024-04-27 00:45:47,052 - trainer - INFO -     val_precision  : 0.962281
2024-04-27 00:45:47,068 - trainer - INFO -     val_recall     : 0.9494
2024-04-27 00:45:47,068 - trainer - INFO -     val_doc_entropy: 4.26564
2024-04-27 00:45:47,068 - trainer - INFO -     test_loss      : 0.159267
2024-04-27 00:45:47,068 - trainer - INFO -     test_accuracy  : 0.9483
2024-04-27 00:45:47,068 - trainer - INFO -     test_macro_f   : 0.94747
2024-04-27 00:45:47,068 - trainer - INFO -     test_precision : 0.960276
2024-04-27 00:45:47,068 - trainer - INFO -     test_recall    : 0.9483
2024-04-27 00:45:47,068 - trainer - INFO -     test_doc_entropy: 4.275737
2024-04-27 00:58:53,818 - trainer - INFO -     epoch          : 2
2024-04-27 00:58:53,818 - trainer - INFO -     loss           : 0.075235
2024-04-27 00:58:53,818 - trainer - INFO -     accuracy       : 0.97566
2024-04-27 00:58:53,818 - trainer - INFO -     macro_f        : 0.975644
2024-04-27 00:58:53,818 - trainer - INFO -     precision      : 0.981863
2024-04-27 00:58:53,818 - trainer - INFO -     recall         : 0.97566
2024-04-27 00:58:53,818 - trainer - INFO -     doc_entropy    : 4.183692
2024-04-27 00:58:53,818 - trainer - INFO -     val_loss       : 0.202384
2024-04-27 00:58:53,818 - trainer - INFO -     val_accuracy   : 0.9408
2024-04-27 00:58:53,818 - trainer - INFO -     val_macro_f    : 0.939908
2024-04-27 00:58:53,818 - trainer - INFO -     val_precision  : 0.956029
2024-04-27 00:58:53,818 - trainer - INFO -     val_recall     : 0.9408
2024-04-27 00:58:53,818 - trainer - INFO -     val_doc_entropy: 4.211546
2024-04-27 00:58:53,818 - trainer - INFO -     test_loss      : 0.179222
2024-04-27 00:58:53,818 - trainer - INFO -     test_accuracy  : 0.9465
2024-04-27 00:58:53,818 - trainer - INFO -     test_macro_f   : 0.946146
2024-04-27 00:58:53,818 - trainer - INFO -     test_precision : 0.96134
2024-04-27 00:58:53,818 - trainer - INFO -     test_recall    : 0.9465
2024-04-27 00:58:53,818 - trainer - INFO -     test_doc_entropy: 4.218744
2024-04-27 01:12:00,334 - trainer - INFO -     epoch          : 3
2024-04-27 01:12:00,334 - trainer - INFO -     loss           : 0.034509
2024-04-27 01:12:00,334 - trainer - INFO -     accuracy       : 0.98944
2024-04-27 01:12:00,334 - trainer - INFO -     macro_f        : 0.989365
2024-04-27 01:12:00,334 - trainer - INFO -     precision      : 0.992107
2024-04-27 01:12:00,334 - trainer - INFO -     recall         : 0.98944
2024-04-27 01:12:00,334 - trainer - INFO -     doc_entropy    : 3.959905
2024-04-27 01:12:00,334 - trainer - INFO -     val_loss       : 0.231081
2024-04-27 01:12:00,334 - trainer - INFO -     val_accuracy   : 0.9378
2024-04-27 01:12:00,334 - trainer - INFO -     val_macro_f    : 0.936819
2024-04-27 01:12:00,334 - trainer - INFO -     val_precision  : 0.954715
2024-04-27 01:12:00,334 - trainer - INFO -     val_recall     : 0.9378
2024-04-27 01:12:00,334 - trainer - INFO -     val_doc_entropy: 4.028652
2024-04-27 01:12:00,334 - trainer - INFO -     test_loss      : 0.196824
2024-04-27 01:12:00,334 - trainer - INFO -     test_accuracy  : 0.9504
2024-04-27 01:12:00,334 - trainer - INFO -     test_macro_f   : 0.950384
2024-04-27 01:12:00,334 - trainer - INFO -     test_precision : 0.963432
2024-04-27 01:12:00,334 - trainer - INFO -     test_recall    : 0.9504
2024-04-27 01:12:00,334 - trainer - INFO -     test_doc_entropy: 4.026199
2024-04-27 01:13:36,833 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 1024, padding_idx=0)
      (position_embeddings): Embedding(512, 1024)
      (token_type_embeddings): Embedding(2, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=1024, out_features=1024, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=1024, out_features=10, bias=True)
  (final): Linear(in_features=1024, out_features=1024, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=1024, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=1024, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 37,852,499
Freeze params: 0
2024-04-27 01:26:55,632 - trainer - INFO -     epoch          : 1
2024-04-27 01:26:55,635 - trainer - INFO -     loss           : 0.228461
2024-04-27 01:26:55,635 - trainer - INFO -     accuracy       : 0.93344
2024-04-27 01:26:55,635 - trainer - INFO -     macro_f        : 0.931733
2024-04-27 01:26:55,635 - trainer - INFO -     precision      : 0.946478
2024-04-27 01:26:55,635 - trainer - INFO -     recall         : 0.93344
2024-04-27 01:26:55,635 - trainer - INFO -     doc_entropy    : 4.542484
2024-04-27 01:26:55,635 - trainer - INFO -     val_loss       : 0.195403
2024-04-27 01:26:55,635 - trainer - INFO -     val_accuracy   : 0.9368
2024-04-27 01:26:55,635 - trainer - INFO -     val_macro_f    : 0.934742
2024-04-27 01:26:55,635 - trainer - INFO -     val_precision  : 0.949848
2024-04-27 01:26:55,635 - trainer - INFO -     val_recall     : 0.9368
2024-04-27 01:26:55,635 - trainer - INFO -     val_doc_entropy: 4.458607
2024-04-27 01:26:55,635 - trainer - INFO -     test_loss      : 0.164596
2024-04-27 01:26:55,635 - trainer - INFO -     test_accuracy  : 0.9479
2024-04-27 01:26:55,635 - trainer - INFO -     test_macro_f   : 0.947376
2024-04-27 01:26:55,635 - trainer - INFO -     test_precision : 0.961868
2024-04-27 01:26:55,635 - trainer - INFO -     test_recall    : 0.9479
2024-04-27 01:26:55,635 - trainer - INFO -     test_doc_entropy: 4.450859
2024-04-27 01:40:05,787 - trainer - INFO -     epoch          : 2
2024-04-27 01:40:05,787 - trainer - INFO -     loss           : 0.0746
2024-04-27 01:40:05,802 - trainer - INFO -     accuracy       : 0.97628
2024-04-27 01:40:05,802 - trainer - INFO -     macro_f        : 0.976249
2024-04-27 01:40:05,802 - trainer - INFO -     precision      : 0.982414
2024-04-27 01:40:05,802 - trainer - INFO -     recall         : 0.97628
2024-04-27 01:40:05,802 - trainer - INFO -     doc_entropy    : 4.277224
2024-04-27 01:40:05,802 - trainer - INFO -     val_loss       : 0.158142
2024-04-27 01:40:05,802 - trainer - INFO -     val_accuracy   : 0.9516
2024-04-27 01:40:05,802 - trainer - INFO -     val_macro_f    : 0.950705
2024-04-27 01:40:05,802 - trainer - INFO -     val_precision  : 0.962982
2024-04-27 01:40:05,802 - trainer - INFO -     val_recall     : 0.9516
2024-04-27 01:40:05,802 - trainer - INFO -     val_doc_entropy: 4.324777
2024-04-27 01:40:05,802 - trainer - INFO -     test_loss      : 0.155321
2024-04-27 01:40:05,802 - trainer - INFO -     test_accuracy  : 0.9532
2024-04-27 01:40:05,802 - trainer - INFO -     test_macro_f   : 0.953322
2024-04-27 01:40:05,802 - trainer - INFO -     test_precision : 0.965881
2024-04-27 01:40:05,802 - trainer - INFO -     test_recall    : 0.9532
2024-04-27 01:40:05,802 - trainer - INFO -     test_doc_entropy: 4.316093
2024-04-27 01:53:13,786 - trainer - INFO -     epoch          : 3
2024-04-27 01:53:13,786 - trainer - INFO -     loss           : 0.035543
2024-04-27 01:53:13,786 - trainer - INFO -     accuracy       : 0.9889
2024-04-27 01:53:13,786 - trainer - INFO -     macro_f        : 0.988998
2024-04-27 01:53:13,786 - trainer - INFO -     precision      : 0.992067
2024-04-27 01:53:13,786 - trainer - INFO -     recall         : 0.9889
2024-04-27 01:53:13,786 - trainer - INFO -     doc_entropy    : 4.084708
2024-04-27 01:53:13,802 - trainer - INFO -     val_loss       : 0.202524
2024-04-27 01:53:13,802 - trainer - INFO -     val_accuracy   : 0.945
2024-04-27 01:53:13,802 - trainer - INFO -     val_macro_f    : 0.94368
2024-04-27 01:53:13,802 - trainer - INFO -     val_precision  : 0.959538
2024-04-27 01:53:13,802 - trainer - INFO -     val_recall     : 0.945
2024-04-27 01:53:13,802 - trainer - INFO -     val_doc_entropy: 4.196583
2024-04-27 01:53:13,802 - trainer - INFO -     test_loss      : 0.211323
2024-04-27 01:53:13,802 - trainer - INFO -     test_accuracy  : 0.9445
2024-04-27 01:53:13,802 - trainer - INFO -     test_macro_f   : 0.943623
2024-04-27 01:53:13,802 - trainer - INFO -     test_precision : 0.959004
2024-04-27 01:53:13,802 - trainer - INFO -     test_recall    : 0.9445
2024-04-27 01:53:13,802 - trainer - INFO -     test_doc_entropy: 4.191624
2024-05-02 13:35:44,224 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=10, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(768, 768)
    (lstm): LSTM(768, 768)
  )
  (W_k): Linear(in_features=768, out_features=50, bias=False)
  (W_q): Linear(in_features=768, out_features=50, bias=False)
  (W_v): Linear(in_features=768, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 34,176,757
Freeze params: 0
2024-05-02 13:47:35,197 - trainer - INFO -     epoch          : 1
2024-05-02 13:47:35,197 - trainer - INFO -     loss           : 0.239379
2024-05-02 13:47:35,197 - trainer - INFO -     accuracy       : 0.92734
2024-05-02 13:47:35,197 - trainer - INFO -     macro_f        : 0.925935
2024-05-02 13:47:35,197 - trainer - INFO -     precision      : 0.942062
2024-05-02 13:47:35,197 - trainer - INFO -     recall         : 0.92734
2024-05-02 13:47:35,197 - trainer - INFO -     doc_entropy    : 4.476991
2024-05-02 13:47:35,197 - trainer - INFO -     val_loss       : 0.16115
2024-05-02 13:47:35,197 - trainer - INFO -     val_accuracy   : 0.9506
2024-05-02 13:47:35,197 - trainer - INFO -     val_macro_f    : 0.949084
2024-05-02 13:47:35,197 - trainer - INFO -     val_precision  : 0.961137
2024-05-02 13:47:35,197 - trainer - INFO -     val_recall     : 0.9506
2024-05-02 13:47:35,197 - trainer - INFO -     val_doc_entropy: 4.816294
2024-05-02 13:47:35,197 - trainer - INFO -     test_loss      : 0.15338
2024-05-02 13:47:35,197 - trainer - INFO -     test_accuracy  : 0.9497
2024-05-02 13:47:35,197 - trainer - INFO -     test_macro_f   : 0.948724
2024-05-02 13:47:35,197 - trainer - INFO -     test_precision : 0.961759
2024-05-02 13:47:35,197 - trainer - INFO -     test_recall    : 0.9497
2024-05-02 13:47:35,197 - trainer - INFO -     test_doc_entropy: 4.826316
2024-05-02 13:59:30,026 - trainer - INFO -     epoch          : 2
2024-05-02 13:59:30,026 - trainer - INFO -     loss           : 0.087131
2024-05-02 13:59:30,026 - trainer - INFO -     accuracy       : 0.9725
2024-05-02 13:59:30,026 - trainer - INFO -     macro_f        : 0.972464
2024-05-02 13:59:30,026 - trainer - INFO -     precision      : 0.979615
2024-05-02 13:59:30,026 - trainer - INFO -     recall         : 0.9725
2024-05-02 13:59:30,026 - trainer - INFO -     doc_entropy    : 4.406133
2024-05-02 13:59:30,026 - trainer - INFO -     val_loss       : 0.22969
2024-05-02 13:59:30,026 - trainer - INFO -     val_accuracy   : 0.9282
2024-05-02 13:59:30,026 - trainer - INFO -     val_macro_f    : 0.924947
2024-05-02 13:59:30,026 - trainer - INFO -     val_precision  : 0.946279
2024-05-02 13:59:30,026 - trainer - INFO -     val_recall     : 0.9282
2024-05-02 13:59:30,026 - trainer - INFO -     val_doc_entropy: 4.817494
2024-05-02 13:59:30,026 - trainer - INFO -     test_loss      : 0.204819
2024-05-02 13:59:30,026 - trainer - INFO -     test_accuracy  : 0.9386
2024-05-02 13:59:30,026 - trainer - INFO -     test_macro_f   : 0.93651
2024-05-02 13:59:30,026 - trainer - INFO -     test_precision : 0.95377
2024-05-02 13:59:30,026 - trainer - INFO -     test_recall    : 0.9386
2024-05-02 13:59:30,026 - trainer - INFO -     test_doc_entropy: 4.82702
2024-05-02 14:01:04,654 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=10, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(768, 768)
    (lstm): LSTM(768, 768)
  )
  (W_k): Linear(in_features=768, out_features=50, bias=False)
  (W_q): Linear(in_features=768, out_features=50, bias=False)
  (W_v): Linear(in_features=768, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 34,176,757
Freeze params: 0
2024-05-02 14:13:02,670 - trainer - INFO -     epoch          : 1
2024-05-02 14:13:03,531 - trainer - INFO -     loss           : 0.239339
2024-05-02 14:13:03,531 - trainer - INFO -     accuracy       : 0.92678
2024-05-02 14:13:03,531 - trainer - INFO -     macro_f        : 0.925013
2024-05-02 14:13:03,547 - trainer - INFO -     precision      : 0.940107
2024-05-02 14:13:03,547 - trainer - INFO -     recall         : 0.92678
2024-05-02 14:13:03,547 - trainer - INFO -     doc_entropy    : 4.512873
2024-05-02 14:13:03,547 - trainer - INFO -     val_loss       : 0.208898
2024-05-02 14:13:03,547 - trainer - INFO -     val_accuracy   : 0.9352
2024-05-02 14:13:03,547 - trainer - INFO -     val_macro_f    : 0.933376
2024-05-02 14:13:03,547 - trainer - INFO -     val_precision  : 0.951435
2024-05-02 14:13:03,547 - trainer - INFO -     val_recall     : 0.9352
2024-05-02 14:13:03,547 - trainer - INFO -     val_doc_entropy: 4.890333
2024-05-02 14:13:03,547 - trainer - INFO -     test_loss      : 0.191771
2024-05-02 14:13:03,547 - trainer - INFO -     test_accuracy  : 0.9381
2024-05-02 14:13:03,547 - trainer - INFO -     test_macro_f   : 0.93654
2024-05-02 14:13:03,547 - trainer - INFO -     test_precision : 0.953126
2024-05-02 14:13:03,547 - trainer - INFO -     test_recall    : 0.9381
2024-05-02 14:13:03,547 - trainer - INFO -     test_doc_entropy: 4.894043
2024-05-02 14:24:59,626 - trainer - INFO -     epoch          : 2
2024-05-02 14:24:59,626 - trainer - INFO -     loss           : 0.087999
2024-05-02 14:24:59,626 - trainer - INFO -     accuracy       : 0.9718
2024-05-02 14:24:59,626 - trainer - INFO -     macro_f        : 0.971573
2024-05-02 14:24:59,626 - trainer - INFO -     precision      : 0.978728
2024-05-02 14:24:59,626 - trainer - INFO -     recall         : 0.9718
2024-05-02 14:24:59,626 - trainer - INFO -     doc_entropy    : 4.439515
2024-05-02 14:24:59,626 - trainer - INFO -     val_loss       : 0.192821
2024-05-02 14:24:59,626 - trainer - INFO -     val_accuracy   : 0.9406
2024-05-02 14:24:59,626 - trainer - INFO -     val_macro_f    : 0.938567
2024-05-02 14:24:59,626 - trainer - INFO -     val_precision  : 0.954423
2024-05-02 14:24:59,626 - trainer - INFO -     val_recall     : 0.9406
2024-05-02 14:24:59,626 - trainer - INFO -     val_doc_entropy: 4.827015
2024-05-02 14:24:59,626 - trainer - INFO -     test_loss      : 0.180293
2024-05-02 14:24:59,626 - trainer - INFO -     test_accuracy  : 0.9438
2024-05-02 14:24:59,626 - trainer - INFO -     test_macro_f   : 0.942679
2024-05-02 14:24:59,626 - trainer - INFO -     test_precision : 0.957411
2024-05-02 14:24:59,626 - trainer - INFO -     test_recall    : 0.9438
2024-05-02 14:24:59,626 - trainer - INFO -     test_doc_entropy: 4.831449
2024-05-02 14:26:34,431 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=10, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(768, 768)
    (lstm): LSTM(768, 768)
  )
  (W_k): Linear(in_features=768, out_features=50, bias=False)
  (W_q): Linear(in_features=768, out_features=50, bias=False)
  (W_v): Linear(in_features=768, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 34,176,757
Freeze params: 0
2024-05-02 14:38:31,588 - trainer - INFO -     epoch          : 1
2024-05-02 14:38:31,588 - trainer - INFO -     loss           : 0.238433
2024-05-02 14:38:31,588 - trainer - INFO -     accuracy       : 0.92718
2024-05-02 14:38:31,588 - trainer - INFO -     macro_f        : 0.925855
2024-05-02 14:38:31,588 - trainer - INFO -     precision      : 0.942107
2024-05-02 14:38:31,588 - trainer - INFO -     recall         : 0.92718
2024-05-02 14:38:31,588 - trainer - INFO -     doc_entropy    : 4.550478
2024-05-02 14:38:31,588 - trainer - INFO -     val_loss       : 0.162627
2024-05-02 14:38:31,588 - trainer - INFO -     val_accuracy   : 0.9516
2024-05-02 14:38:31,588 - trainer - INFO -     val_macro_f    : 0.95016
2024-05-02 14:38:31,588 - trainer - INFO -     val_precision  : 0.962032
2024-05-02 14:38:31,588 - trainer - INFO -     val_recall     : 0.9516
2024-05-02 14:38:31,588 - trainer - INFO -     val_doc_entropy: 4.878121
2024-05-02 14:38:31,588 - trainer - INFO -     test_loss      : 0.154157
2024-05-02 14:38:31,588 - trainer - INFO -     test_accuracy  : 0.9499
2024-05-02 14:38:31,588 - trainer - INFO -     test_macro_f   : 0.949657
2024-05-02 14:38:31,588 - trainer - INFO -     test_precision : 0.962904
2024-05-02 14:38:31,588 - trainer - INFO -     test_recall    : 0.9499
2024-05-02 14:38:31,588 - trainer - INFO -     test_doc_entropy: 4.8891
2024-05-02 14:50:29,778 - trainer - INFO -     epoch          : 2
2024-05-02 14:50:29,778 - trainer - INFO -     loss           : 0.087914
2024-05-02 14:50:29,778 - trainer - INFO -     accuracy       : 0.97212
2024-05-02 14:50:29,778 - trainer - INFO -     macro_f        : 0.971904
2024-05-02 14:50:29,778 - trainer - INFO -     precision      : 0.978881
2024-05-02 14:50:29,778 - trainer - INFO -     recall         : 0.97212
2024-05-02 14:50:29,778 - trainer - INFO -     doc_entropy    : 4.455848
2024-05-02 14:50:29,778 - trainer - INFO -     val_loss       : 0.149662
2024-05-02 14:50:29,778 - trainer - INFO -     val_accuracy   : 0.9554
2024-05-02 14:50:29,778 - trainer - INFO -     val_macro_f    : 0.954855
2024-05-02 14:50:29,778 - trainer - INFO -     val_precision  : 0.967661
2024-05-02 14:50:29,778 - trainer - INFO -     val_recall     : 0.9554
2024-05-02 14:50:29,778 - trainer - INFO -     val_doc_entropy: 4.839241
2024-05-02 14:50:29,778 - trainer - INFO -     test_loss      : 0.161433
2024-05-02 14:50:29,778 - trainer - INFO -     test_accuracy  : 0.9497
2024-05-02 14:50:29,778 - trainer - INFO -     test_macro_f   : 0.948996
2024-05-02 14:50:29,778 - trainer - INFO -     test_precision : 0.962482
2024-05-02 14:50:29,778 - trainer - INFO -     test_recall    : 0.9497
2024-05-02 14:50:29,778 - trainer - INFO -     test_doc_entropy: 4.845574
2024-05-02 14:52:04,326 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=10, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(768, 768)
    (lstm): LSTM(768, 768)
  )
  (W_k): Linear(in_features=768, out_features=50, bias=False)
  (W_q): Linear(in_features=768, out_features=50, bias=False)
  (W_v): Linear(in_features=768, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 34,176,757
Freeze params: 0
2024-05-02 15:04:02,004 - trainer - INFO -     epoch          : 1
2024-05-02 15:04:02,004 - trainer - INFO -     loss           : 0.23734
2024-05-02 15:04:02,004 - trainer - INFO -     accuracy       : 0.92578
2024-05-02 15:04:02,004 - trainer - INFO -     macro_f        : 0.924382
2024-05-02 15:04:02,004 - trainer - INFO -     precision      : 0.939982
2024-05-02 15:04:02,004 - trainer - INFO -     recall         : 0.92578
2024-05-02 15:04:02,004 - trainer - INFO -     doc_entropy    : 4.482619
2024-05-02 15:04:02,004 - trainer - INFO -     val_loss       : 0.161944
2024-05-02 15:04:02,004 - trainer - INFO -     val_accuracy   : 0.9506
2024-05-02 15:04:02,004 - trainer - INFO -     val_macro_f    : 0.949328
2024-05-02 15:04:02,004 - trainer - INFO -     val_precision  : 0.963049
2024-05-02 15:04:02,004 - trainer - INFO -     val_recall     : 0.9506
2024-05-02 15:04:02,004 - trainer - INFO -     val_doc_entropy: 4.858363
2024-05-02 15:04:02,004 - trainer - INFO -     test_loss      : 0.16542
2024-05-02 15:04:02,004 - trainer - INFO -     test_accuracy  : 0.9474
2024-05-02 15:04:02,004 - trainer - INFO -     test_macro_f   : 0.945592
2024-05-02 15:04:02,020 - trainer - INFO -     test_precision : 0.958001
2024-05-02 15:04:02,020 - trainer - INFO -     test_recall    : 0.9474
2024-05-02 15:04:02,020 - trainer - INFO -     test_doc_entropy: 4.858256
2024-05-02 15:15:59,597 - trainer - INFO -     epoch          : 2
2024-05-02 15:15:59,597 - trainer - INFO -     loss           : 0.085861
2024-05-02 15:15:59,597 - trainer - INFO -     accuracy       : 0.9728
2024-05-02 15:15:59,597 - trainer - INFO -     macro_f        : 0.972764
2024-05-02 15:15:59,597 - trainer - INFO -     precision      : 0.979714
2024-05-02 15:15:59,597 - trainer - INFO -     recall         : 0.9728
2024-05-02 15:15:59,597 - trainer - INFO -     doc_entropy    : 4.406157
2024-05-02 15:15:59,597 - trainer - INFO -     val_loss       : 0.202426
2024-05-02 15:15:59,597 - trainer - INFO -     val_accuracy   : 0.9376
2024-05-02 15:15:59,597 - trainer - INFO -     val_macro_f    : 0.935876
2024-05-02 15:15:59,597 - trainer - INFO -     val_precision  : 0.952515
2024-05-02 15:15:59,597 - trainer - INFO -     val_recall     : 0.9376
2024-05-02 15:15:59,597 - trainer - INFO -     val_doc_entropy: 4.834946
2024-05-02 15:15:59,597 - trainer - INFO -     test_loss      : 0.182597
2024-05-02 15:15:59,597 - trainer - INFO -     test_accuracy  : 0.9414
2024-05-02 15:15:59,597 - trainer - INFO -     test_macro_f   : 0.940065
2024-05-02 15:15:59,597 - trainer - INFO -     test_precision : 0.95578
2024-05-02 15:15:59,597 - trainer - INFO -     test_recall    : 0.9414
2024-05-02 15:15:59,597 - trainer - INFO -     test_doc_entropy: 4.833959
2024-05-02 15:17:34,646 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=10, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(768, 768)
    (lstm): LSTM(768, 768)
  )
  (W_k): Linear(in_features=768, out_features=50, bias=False)
  (W_q): Linear(in_features=768, out_features=50, bias=False)
  (W_v): Linear(in_features=768, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 34,176,757
Freeze params: 0
2024-05-02 15:29:31,319 - trainer - INFO -     epoch          : 1
2024-05-02 15:29:31,319 - trainer - INFO -     loss           : 0.237738
2024-05-02 15:29:31,319 - trainer - INFO -     accuracy       : 0.92698
2024-05-02 15:29:31,319 - trainer - INFO -     macro_f        : 0.925431
2024-05-02 15:29:31,319 - trainer - INFO -     precision      : 0.9408
2024-05-02 15:29:31,319 - trainer - INFO -     recall         : 0.92698
2024-05-02 15:29:31,319 - trainer - INFO -     doc_entropy    : 4.539756
2024-05-02 15:29:31,319 - trainer - INFO -     val_loss       : 0.175779
2024-05-02 15:29:31,319 - trainer - INFO -     val_accuracy   : 0.9488
2024-05-02 15:29:31,319 - trainer - INFO -     val_macro_f    : 0.947897
2024-05-02 15:29:31,319 - trainer - INFO -     val_precision  : 0.962086
2024-05-02 15:29:31,319 - trainer - INFO -     val_recall     : 0.9488
2024-05-02 15:29:31,319 - trainer - INFO -     val_doc_entropy: 4.868601
2024-05-02 15:29:31,319 - trainer - INFO -     test_loss      : 0.18006
2024-05-02 15:29:31,319 - trainer - INFO -     test_accuracy  : 0.945
2024-05-02 15:29:31,319 - trainer - INFO -     test_macro_f   : 0.944244
2024-05-02 15:29:31,319 - trainer - INFO -     test_precision : 0.959048
2024-05-02 15:29:31,319 - trainer - INFO -     test_recall    : 0.945
2024-05-02 15:29:31,319 - trainer - INFO -     test_doc_entropy: 4.874422
2024-05-02 15:41:28,366 - trainer - INFO -     epoch          : 2
2024-05-02 15:41:28,366 - trainer - INFO -     loss           : 0.088485
2024-05-02 15:41:28,382 - trainer - INFO -     accuracy       : 0.97216
2024-05-02 15:41:28,382 - trainer - INFO -     macro_f        : 0.97221
2024-05-02 15:41:28,382 - trainer - INFO -     precision      : 0.97952
2024-05-02 15:41:28,382 - trainer - INFO -     recall         : 0.97216
2024-05-02 15:41:28,382 - trainer - INFO -     doc_entropy    : 4.454499
2024-05-02 15:41:28,382 - trainer - INFO -     val_loss       : 0.207794
2024-05-02 15:41:28,382 - trainer - INFO -     val_accuracy   : 0.9382
2024-05-02 15:41:28,382 - trainer - INFO -     val_macro_f    : 0.936043
2024-05-02 15:41:28,382 - trainer - INFO -     val_precision  : 0.952787
2024-05-02 15:41:28,382 - trainer - INFO -     val_recall     : 0.9382
2024-05-02 15:41:28,382 - trainer - INFO -     val_doc_entropy: 4.863386
2024-05-02 15:41:28,382 - trainer - INFO -     test_loss      : 0.219414
2024-05-02 15:41:28,382 - trainer - INFO -     test_accuracy  : 0.931
2024-05-02 15:41:28,382 - trainer - INFO -     test_macro_f   : 0.928523
2024-05-02 15:41:28,382 - trainer - INFO -     test_precision : 0.946506
2024-05-02 15:41:28,382 - trainer - INFO -     test_recall    : 0.931
2024-05-02 15:41:28,382 - trainer - INFO -     test_doc_entropy: 4.868457
2024-05-03 03:08:49,770 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 1024, padding_idx=0)
      (position_embeddings): Embedding(512, 1024)
      (token_type_embeddings): Embedding(2, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=1024, out_features=1024, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=1024, out_features=10, bias=True)
  (final): Linear(in_features=1024, out_features=1024, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=1024, out_features=1200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1200, out_features=60, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(1024, 1024)
    (lstm): LSTM(1024, 1024)
  )
  (W_k): Linear(in_features=1024, out_features=60, bias=False)
  (W_q): Linear(in_features=1024, out_features=60, bias=False)
  (W_v): Linear(in_features=1024, out_features=60, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((60, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=1024, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 53,146,295
Freeze params: 0
2024-05-03 03:26:36,151 - trainer - INFO -     epoch          : 1
2024-05-03 03:26:36,151 - trainer - INFO -     loss           : 0.236741
2024-05-03 03:26:36,151 - trainer - INFO -     accuracy       : 0.92814
2024-05-03 03:26:36,151 - trainer - INFO -     macro_f        : 0.926703
2024-05-03 03:26:36,151 - trainer - INFO -     precision      : 0.941929
2024-05-03 03:26:36,151 - trainer - INFO -     recall         : 0.92814
2024-05-03 03:26:36,151 - trainer - INFO -     doc_entropy    : 4.34564
2024-05-03 03:26:36,151 - trainer - INFO -     val_loss       : 0.21478
2024-05-03 03:26:36,151 - trainer - INFO -     val_accuracy   : 0.933
2024-05-03 03:26:36,151 - trainer - INFO -     val_macro_f    : 0.930055
2024-05-03 03:26:36,151 - trainer - INFO -     val_precision  : 0.949176
2024-05-03 03:26:36,151 - trainer - INFO -     val_recall     : 0.933
2024-05-03 03:26:36,151 - trainer - INFO -     val_doc_entropy: 4.833257
2024-05-03 03:26:36,151 - trainer - INFO -     test_loss      : 0.19348
2024-05-03 03:26:36,151 - trainer - INFO -     test_accuracy  : 0.9364
2024-05-03 03:26:36,151 - trainer - INFO -     test_macro_f   : 0.933665
2024-05-03 03:26:36,151 - trainer - INFO -     test_precision : 0.951472
2024-05-03 03:26:36,151 - trainer - INFO -     test_recall    : 0.9364
2024-05-03 03:26:36,151 - trainer - INFO -     test_doc_entropy: 4.821995
2024-05-03 03:44:21,765 - trainer - INFO -     epoch          : 2
2024-05-03 03:44:21,765 - trainer - INFO -     loss           : 0.079365
2024-05-03 03:44:21,765 - trainer - INFO -     accuracy       : 0.97462
2024-05-03 03:44:21,765 - trainer - INFO -     macro_f        : 0.974627
2024-05-03 03:44:21,765 - trainer - INFO -     precision      : 0.981402
2024-05-03 03:44:21,765 - trainer - INFO -     recall         : 0.97462
2024-05-03 03:44:21,765 - trainer - INFO -     doc_entropy    : 4.034338
2024-05-03 03:44:21,765 - trainer - INFO -     val_loss       : 0.173494
2024-05-03 03:44:21,765 - trainer - INFO -     val_accuracy   : 0.9468
2024-05-03 03:44:21,765 - trainer - INFO -     val_macro_f    : 0.945339
2024-05-03 03:44:21,765 - trainer - INFO -     val_precision  : 0.958556
2024-05-03 03:44:21,765 - trainer - INFO -     val_recall     : 0.9468
2024-05-03 03:44:21,765 - trainer - INFO -     val_doc_entropy: 4.707082
2024-05-03 03:44:21,765 - trainer - INFO -     test_loss      : 0.142134
2024-05-03 03:44:21,765 - trainer - INFO -     test_accuracy  : 0.9551
2024-05-03 03:44:21,765 - trainer - INFO -     test_macro_f   : 0.954756
2024-05-03 03:44:21,765 - trainer - INFO -     test_precision : 0.965115
2024-05-03 03:44:21,765 - trainer - INFO -     test_recall    : 0.9551
2024-05-03 03:44:21,765 - trainer - INFO -     test_doc_entropy: 4.695599
2024-05-03 03:46:23,661 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 1024, padding_idx=0)
      (position_embeddings): Embedding(512, 1024)
      (token_type_embeddings): Embedding(2, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=1024, out_features=1024, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=1024, out_features=10, bias=True)
  (final): Linear(in_features=1024, out_features=1024, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=1024, out_features=1200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1200, out_features=60, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(1024, 1024)
    (lstm): LSTM(1024, 1024)
  )
  (W_k): Linear(in_features=1024, out_features=60, bias=False)
  (W_q): Linear(in_features=1024, out_features=60, bias=False)
  (W_v): Linear(in_features=1024, out_features=60, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((60, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=1024, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 53,146,295
Freeze params: 0
2024-05-03 04:04:10,330 - trainer - INFO -     epoch          : 1
2024-05-03 04:04:11,191 - trainer - INFO -     loss           : 0.235659
2024-05-03 04:04:11,191 - trainer - INFO -     accuracy       : 0.92816
2024-05-03 04:04:11,191 - trainer - INFO -     macro_f        : 0.926713
2024-05-03 04:04:11,191 - trainer - INFO -     precision      : 0.942481
2024-05-03 04:04:11,191 - trainer - INFO -     recall         : 0.92816
2024-05-03 04:04:11,191 - trainer - INFO -     doc_entropy    : 4.4478
2024-05-03 04:04:11,191 - trainer - INFO -     val_loss       : 0.18847
2024-05-03 04:04:11,191 - trainer - INFO -     val_accuracy   : 0.9426
2024-05-03 04:04:11,191 - trainer - INFO -     val_macro_f    : 0.940411
2024-05-03 04:04:11,191 - trainer - INFO -     val_precision  : 0.955682
2024-05-03 04:04:11,191 - trainer - INFO -     val_recall     : 0.9426
2024-05-03 04:04:11,191 - trainer - INFO -     val_doc_entropy: 4.900403
2024-05-03 04:04:11,191 - trainer - INFO -     test_loss      : 0.182944
2024-05-03 04:04:11,207 - trainer - INFO -     test_accuracy  : 0.9425
2024-05-03 04:04:11,207 - trainer - INFO -     test_macro_f   : 0.940507
2024-05-03 04:04:11,207 - trainer - INFO -     test_precision : 0.955851
2024-05-03 04:04:11,207 - trainer - INFO -     test_recall    : 0.9425
2024-05-03 04:04:11,207 - trainer - INFO -     test_doc_entropy: 4.893961
2024-05-03 04:21:55,199 - trainer - INFO -     epoch          : 2
2024-05-03 04:21:55,199 - trainer - INFO -     loss           : 0.078616
2024-05-03 04:21:55,199 - trainer - INFO -     accuracy       : 0.97498
2024-05-03 04:21:55,199 - trainer - INFO -     macro_f        : 0.975022
2024-05-03 04:21:55,199 - trainer - INFO -     precision      : 0.981812
2024-05-03 04:21:55,199 - trainer - INFO -     recall         : 0.97498
2024-05-03 04:21:55,199 - trainer - INFO -     doc_entropy    : 4.146124
2024-05-03 04:21:55,199 - trainer - INFO -     val_loss       : 0.227345
2024-05-03 04:21:55,199 - trainer - INFO -     val_accuracy   : 0.9348
2024-05-03 04:21:55,199 - trainer - INFO -     val_macro_f    : 0.932201
2024-05-03 04:21:55,199 - trainer - INFO -     val_precision  : 0.951251
2024-05-03 04:21:55,199 - trainer - INFO -     val_recall     : 0.9348
2024-05-03 04:21:55,199 - trainer - INFO -     val_doc_entropy: 4.827314
2024-05-03 04:21:55,199 - trainer - INFO -     test_loss      : 0.216537
2024-05-03 04:21:55,199 - trainer - INFO -     test_accuracy  : 0.9366
2024-05-03 04:21:55,199 - trainer - INFO -     test_macro_f   : 0.934074
2024-05-03 04:21:55,199 - trainer - INFO -     test_precision : 0.951672
2024-05-03 04:21:55,199 - trainer - INFO -     test_recall    : 0.9366
2024-05-03 04:21:55,199 - trainer - INFO -     test_doc_entropy: 4.819316
2024-05-03 04:23:57,231 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 1024, padding_idx=0)
      (position_embeddings): Embedding(512, 1024)
      (token_type_embeddings): Embedding(2, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=1024, out_features=1024, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=1024, out_features=10, bias=True)
  (final): Linear(in_features=1024, out_features=1024, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=1024, out_features=1200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1200, out_features=60, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(1024, 1024)
    (lstm): LSTM(1024, 1024)
  )
  (W_k): Linear(in_features=1024, out_features=60, bias=False)
  (W_q): Linear(in_features=1024, out_features=60, bias=False)
  (W_v): Linear(in_features=1024, out_features=60, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((60, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=1024, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 53,146,295
Freeze params: 0
2024-05-03 09:53:13,626 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 1024, padding_idx=0)
      (position_embeddings): Embedding(512, 1024)
      (token_type_embeddings): Embedding(2, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=1024, out_features=1024, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=1024, out_features=10, bias=True)
  (final): Linear(in_features=1024, out_features=1024, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=1024, out_features=1200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1200, out_features=60, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(1024, 1024)
    (lstm): LSTM(1024, 1024)
  )
  (W_k): Linear(in_features=1024, out_features=60, bias=False)
  (W_q): Linear(in_features=1024, out_features=60, bias=False)
  (W_v): Linear(in_features=1024, out_features=60, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((60, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=1024, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 53,146,295
Freeze params: 0
2024-05-03 10:10:47,252 - trainer - INFO -     epoch          : 1
2024-05-03 10:10:47,252 - trainer - INFO -     loss           : 0.237781
2024-05-03 10:10:47,252 - trainer - INFO -     accuracy       : 0.92678
2024-05-03 10:10:47,252 - trainer - INFO -     macro_f        : 0.925266
2024-05-03 10:10:47,252 - trainer - INFO -     precision      : 0.940873
2024-05-03 10:10:47,252 - trainer - INFO -     recall         : 0.92678
2024-05-03 10:10:47,252 - trainer - INFO -     doc_entropy    : 4.400666
2024-05-03 10:10:47,252 - trainer - INFO -     val_loss       : 0.178106
2024-05-03 10:10:47,252 - trainer - INFO -     val_accuracy   : 0.9438
2024-05-03 10:10:47,252 - trainer - INFO -     val_macro_f    : 0.94224
2024-05-03 10:10:47,252 - trainer - INFO -     val_precision  : 0.956374
2024-05-03 10:10:47,252 - trainer - INFO -     val_recall     : 0.9438
2024-05-03 10:10:47,252 - trainer - INFO -     val_doc_entropy: 4.834486
2024-05-03 10:10:47,252 - trainer - INFO -     test_loss      : 0.174283
2024-05-03 10:10:47,252 - trainer - INFO -     test_accuracy  : 0.9441
2024-05-03 10:10:47,252 - trainer - INFO -     test_macro_f   : 0.942444
2024-05-03 10:10:47,252 - trainer - INFO -     test_precision : 0.956246
2024-05-03 10:10:47,252 - trainer - INFO -     test_recall    : 0.9441
2024-05-03 10:10:47,252 - trainer - INFO -     test_doc_entropy: 4.826038
2024-05-03 10:28:33,325 - trainer - INFO -     epoch          : 2
2024-05-03 10:28:33,325 - trainer - INFO -     loss           : 0.078025
2024-05-03 10:28:33,325 - trainer - INFO -     accuracy       : 0.97474
2024-05-03 10:28:33,325 - trainer - INFO -     macro_f        : 0.974642
2024-05-03 10:28:33,325 - trainer - INFO -     precision      : 0.981354
2024-05-03 10:28:33,325 - trainer - INFO -     recall         : 0.97474
2024-05-03 10:28:33,325 - trainer - INFO -     doc_entropy    : 4.05245
2024-05-03 10:28:33,325 - trainer - INFO -     val_loss       : 0.216904
2024-05-03 10:28:33,325 - trainer - INFO -     val_accuracy   : 0.9336
2024-05-03 10:28:33,325 - trainer - INFO -     val_macro_f    : 0.930898
2024-05-03 10:28:33,325 - trainer - INFO -     val_precision  : 0.950284
2024-05-03 10:28:33,325 - trainer - INFO -     val_recall     : 0.9336
2024-05-03 10:28:33,325 - trainer - INFO -     val_doc_entropy: 4.758399
2024-05-03 10:28:33,325 - trainer - INFO -     test_loss      : 0.208047
2024-05-03 10:28:33,325 - trainer - INFO -     test_accuracy  : 0.9365
2024-05-03 10:28:33,325 - trainer - INFO -     test_macro_f   : 0.935069
2024-05-03 10:28:33,325 - trainer - INFO -     test_precision : 0.953226
2024-05-03 10:28:33,325 - trainer - INFO -     test_recall    : 0.9365
2024-05-03 10:28:33,325 - trainer - INFO -     test_doc_entropy: 4.750786
2024-05-03 10:30:36,443 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 1024, padding_idx=0)
      (position_embeddings): Embedding(512, 1024)
      (token_type_embeddings): Embedding(2, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=1024, out_features=1024, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=1024, out_features=10, bias=True)
  (final): Linear(in_features=1024, out_features=1024, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=1024, out_features=1200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1200, out_features=60, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(1024, 1024)
    (lstm): LSTM(1024, 1024)
  )
  (W_k): Linear(in_features=1024, out_features=60, bias=False)
  (W_q): Linear(in_features=1024, out_features=60, bias=False)
  (W_v): Linear(in_features=1024, out_features=60, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((60, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=1024, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 53,146,295
Freeze params: 0
2024-05-03 10:48:26,912 - trainer - INFO -     epoch          : 1
2024-05-03 10:48:26,912 - trainer - INFO -     loss           : 0.236792
2024-05-03 10:48:26,928 - trainer - INFO -     accuracy       : 0.9268
2024-05-03 10:48:26,928 - trainer - INFO -     macro_f        : 0.92533
2024-05-03 10:48:26,928 - trainer - INFO -     precision      : 0.941101
2024-05-03 10:48:26,928 - trainer - INFO -     recall         : 0.9268
2024-05-03 10:48:26,928 - trainer - INFO -     doc_entropy    : 4.389392
2024-05-03 10:48:26,928 - trainer - INFO -     val_loss       : 0.189533
2024-05-03 10:48:26,928 - trainer - INFO -     val_accuracy   : 0.9424
2024-05-03 10:48:26,928 - trainer - INFO -     val_macro_f    : 0.940288
2024-05-03 10:48:26,928 - trainer - INFO -     val_precision  : 0.956735
2024-05-03 10:48:26,928 - trainer - INFO -     val_recall     : 0.9424
2024-05-03 10:48:26,928 - trainer - INFO -     val_doc_entropy: 4.886734
2024-05-03 10:48:26,928 - trainer - INFO -     test_loss      : 0.181339
2024-05-03 10:48:26,928 - trainer - INFO -     test_accuracy  : 0.9406
2024-05-03 10:48:26,928 - trainer - INFO -     test_macro_f   : 0.93889
2024-05-03 10:48:26,928 - trainer - INFO -     test_precision : 0.955484
2024-05-03 10:48:26,928 - trainer - INFO -     test_recall    : 0.9406
2024-05-03 10:48:26,928 - trainer - INFO -     test_doc_entropy: 4.874313
2024-05-03 11:06:13,089 - trainer - INFO -     epoch          : 2
2024-05-03 11:06:13,089 - trainer - INFO -     loss           : 0.077434
2024-05-03 11:06:13,089 - trainer - INFO -     accuracy       : 0.97504
2024-05-03 11:06:13,089 - trainer - INFO -     macro_f        : 0.975163
2024-05-03 11:06:13,089 - trainer - INFO -     precision      : 0.981964
2024-05-03 11:06:13,089 - trainer - INFO -     recall         : 0.97504
2024-05-03 11:06:13,089 - trainer - INFO -     doc_entropy    : 4.113407
2024-05-03 11:06:13,089 - trainer - INFO -     val_loss       : 0.207384
2024-05-03 11:06:13,089 - trainer - INFO -     val_accuracy   : 0.9354
2024-05-03 11:06:13,089 - trainer - INFO -     val_macro_f    : 0.932348
2024-05-03 11:06:13,089 - trainer - INFO -     val_precision  : 0.946635
2024-05-03 11:06:13,089 - trainer - INFO -     val_recall     : 0.9354
2024-05-03 11:06:13,089 - trainer - INFO -     val_doc_entropy: 4.764842
2024-05-03 11:06:13,089 - trainer - INFO -     test_loss      : 0.170869
2024-05-03 11:06:13,089 - trainer - INFO -     test_accuracy  : 0.9487
2024-05-03 11:06:13,089 - trainer - INFO -     test_macro_f   : 0.948224
2024-05-03 11:06:13,089 - trainer - INFO -     test_precision : 0.960241
2024-05-03 11:06:13,089 - trainer - INFO -     test_recall    : 0.9487
2024-05-03 11:06:13,089 - trainer - INFO -     test_doc_entropy: 4.75622
2024-05-03 11:09:01,295 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 1024, padding_idx=0)
      (position_embeddings): Embedding(512, 1024)
      (token_type_embeddings): Embedding(2, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=1024, out_features=1024, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=1024, out_features=10, bias=True)
  (final): Linear(in_features=1024, out_features=1024, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=1024, out_features=1200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1200, out_features=60, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(1024, 1024)
    (lstm): LSTM(1024, 1024)
  )
  (W_k): Linear(in_features=1024, out_features=60, bias=False)
  (W_q): Linear(in_features=1024, out_features=60, bias=False)
  (W_v): Linear(in_features=1024, out_features=60, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((60, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=1024, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 53,146,295
Freeze params: 0
2024-05-03 11:26:46,986 - trainer - INFO -     epoch          : 1
2024-05-03 11:26:46,986 - trainer - INFO -     loss           : 0.2366
2024-05-03 11:26:46,986 - trainer - INFO -     accuracy       : 0.92794
2024-05-03 11:26:46,986 - trainer - INFO -     macro_f        : 0.926981
2024-05-03 11:26:46,986 - trainer - INFO -     precision      : 0.943434
2024-05-03 11:26:46,986 - trainer - INFO -     recall         : 0.92794
2024-05-03 11:26:46,986 - trainer - INFO -     doc_entropy    : 4.437721
2024-05-03 11:26:46,986 - trainer - INFO -     val_loss       : 0.184963
2024-05-03 11:26:46,986 - trainer - INFO -     val_accuracy   : 0.943
2024-05-03 11:26:46,986 - trainer - INFO -     val_macro_f    : 0.942811
2024-05-03 11:26:46,986 - trainer - INFO -     val_precision  : 0.958481
2024-05-03 11:26:46,986 - trainer - INFO -     val_recall     : 0.943
2024-05-03 11:26:46,986 - trainer - INFO -     val_doc_entropy: 4.888159
2024-05-03 11:26:46,986 - trainer - INFO -     test_loss      : 0.171536
2024-05-03 11:26:46,986 - trainer - INFO -     test_accuracy  : 0.942
2024-05-03 11:26:46,986 - trainer - INFO -     test_macro_f   : 0.940979
2024-05-03 11:26:46,986 - trainer - INFO -     test_precision : 0.955884
2024-05-03 11:26:46,986 - trainer - INFO -     test_recall    : 0.942
2024-05-03 11:26:46,986 - trainer - INFO -     test_doc_entropy: 4.877349
2024-05-03 11:44:31,003 - trainer - INFO -     epoch          : 2
2024-05-03 11:44:31,003 - trainer - INFO -     loss           : 0.079041
2024-05-03 11:44:31,003 - trainer - INFO -     accuracy       : 0.97496
2024-05-03 11:44:31,003 - trainer - INFO -     macro_f        : 0.974971
2024-05-03 11:44:31,003 - trainer - INFO -     precision      : 0.981523
2024-05-03 11:44:31,003 - trainer - INFO -     recall         : 0.97496
2024-05-03 11:44:31,003 - trainer - INFO -     doc_entropy    : 4.059845
2024-05-03 11:44:31,003 - trainer - INFO -     val_loss       : 0.169623
2024-05-03 11:44:31,003 - trainer - INFO -     val_accuracy   : 0.9462
2024-05-03 11:44:31,003 - trainer - INFO -     val_macro_f    : 0.945981
2024-05-03 11:44:31,003 - trainer - INFO -     val_precision  : 0.9614
2024-05-03 11:44:31,003 - trainer - INFO -     val_recall     : 0.9462
2024-05-03 11:44:31,003 - trainer - INFO -     val_doc_entropy: 4.741968
2024-05-03 11:44:31,003 - trainer - INFO -     test_loss      : 0.163551
2024-05-03 11:44:31,003 - trainer - INFO -     test_accuracy  : 0.9489
2024-05-03 11:44:31,003 - trainer - INFO -     test_macro_f   : 0.947982
2024-05-03 11:44:31,003 - trainer - INFO -     test_precision : 0.961123
2024-05-03 11:44:31,003 - trainer - INFO -     test_recall    : 0.9489
2024-05-03 11:44:31,003 - trainer - INFO -     test_doc_entropy: 4.727743
