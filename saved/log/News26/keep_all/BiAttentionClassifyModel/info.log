2024-03-02 19:51:39,020 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=600, bias=True)
    (1): Tanh()
    (2): Linear(in_features=600, out_features=30, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,112,097
Freeze params: 0
2024-03-02 20:01:08,537 - trainer - INFO -     epoch          : 1
2024-03-02 20:01:08,537 - trainer - INFO -     loss           : 1.330269
2024-03-02 20:01:08,537 - trainer - INFO -     accuracy       : 0.631376
2024-03-02 20:01:08,537 - trainer - INFO -     macro_f        : 0.474785
2024-03-02 20:01:08,537 - trainer - INFO -     doc_entropy    : 2.814781
2024-03-02 20:01:08,537 - trainer - INFO -     val_loss       : 1.268964
2024-03-02 20:01:08,537 - trainer - INFO -     val_accuracy   : 0.656527
2024-03-02 20:01:08,537 - trainer - INFO -     val_macro_f    : 0.504108
2024-03-02 20:01:08,537 - trainer - INFO -     val_doc_entropy: 2.771843
2024-03-02 20:10:31,126 - trainer - INFO -     epoch          : 2
2024-03-02 20:10:31,126 - trainer - INFO -     loss           : 1.020176
2024-03-02 20:10:31,126 - trainer - INFO -     accuracy       : 0.703304
2024-03-02 20:10:31,126 - trainer - INFO -     macro_f        : 0.559696
2024-03-02 20:10:31,126 - trainer - INFO -     doc_entropy    : 2.968651
2024-03-02 20:10:31,126 - trainer - INFO -     val_loss       : 1.145242
2024-03-02 20:10:31,126 - trainer - INFO -     val_accuracy   : 0.673902
2024-03-02 20:10:31,126 - trainer - INFO -     val_macro_f    : 0.521942
2024-03-02 20:10:31,126 - trainer - INFO -     val_doc_entropy: 3.042071
2024-03-02 20:19:55,224 - trainer - INFO -     epoch          : 3
2024-03-02 20:19:55,224 - trainer - INFO -     loss           : 0.850126
2024-03-02 20:19:55,224 - trainer - INFO -     accuracy       : 0.747511
2024-03-02 20:19:55,224 - trainer - INFO -     macro_f        : 0.617727
2024-03-02 20:19:55,224 - trainer - INFO -     doc_entropy    : 3.065521
2024-03-02 20:19:55,224 - trainer - INFO -     val_loss       : 1.186033
2024-03-02 20:19:55,224 - trainer - INFO -     val_accuracy   : 0.666833
2024-03-02 20:19:55,224 - trainer - INFO -     val_macro_f    : 0.513615
2024-03-02 20:19:55,224 - trainer - INFO -     val_doc_entropy: 3.020201
2024-03-02 20:20:58,406 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=600, bias=True)
    (1): Tanh()
    (2): Linear(in_features=600, out_features=30, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,112,097
Freeze params: 0
2024-03-02 20:30:26,024 - trainer - INFO -     epoch          : 1
2024-03-02 20:30:26,024 - trainer - INFO -     loss           : 1.325816
2024-03-02 20:30:26,024 - trainer - INFO -     accuracy       : 0.630324
2024-03-02 20:30:26,024 - trainer - INFO -     macro_f        : 0.473162
2024-03-02 20:30:26,024 - trainer - INFO -     doc_entropy    : 2.84979
2024-03-02 20:30:26,024 - trainer - INFO -     val_loss       : 1.183628
2024-03-02 20:30:26,024 - trainer - INFO -     val_accuracy   : 0.666086
2024-03-02 20:30:26,024 - trainer - INFO -     val_macro_f    : 0.514263
2024-03-02 20:30:26,024 - trainer - INFO -     val_doc_entropy: 2.950187
2024-03-02 20:40:06,799 - trainer - INFO -     epoch          : 2
2024-03-02 20:40:06,799 - trainer - INFO -     loss           : 1.004611
2024-03-02 20:40:06,799 - trainer - INFO -     accuracy       : 0.707909
2024-03-02 20:40:06,799 - trainer - INFO -     macro_f        : 0.566501
2024-03-02 20:40:06,799 - trainer - INFO -     doc_entropy    : 2.90513
2024-03-02 20:40:06,799 - trainer - INFO -     val_loss       : 1.147208
2024-03-02 20:40:06,799 - trainer - INFO -     val_accuracy   : 0.674699
2024-03-02 20:40:06,799 - trainer - INFO -     val_macro_f    : 0.520863
2024-03-02 20:40:06,799 - trainer - INFO -     val_doc_entropy: 3.012566
2024-03-02 20:49:37,548 - trainer - INFO -     epoch          : 3
2024-03-02 20:49:37,548 - trainer - INFO -     loss           : 0.842658
2024-03-02 20:49:37,548 - trainer - INFO -     accuracy       : 0.746832
2024-03-02 20:49:37,548 - trainer - INFO -     macro_f        : 0.61765
2024-03-02 20:49:37,548 - trainer - INFO -     doc_entropy    : 2.925442
2024-03-02 20:49:37,548 - trainer - INFO -     val_loss       : 1.179737
2024-03-02 20:49:37,548 - trainer - INFO -     val_accuracy   : 0.67216
2024-03-02 20:49:37,548 - trainer - INFO -     val_macro_f    : 0.523263
2024-03-02 20:49:37,548 - trainer - INFO -     val_doc_entropy: 3.035075
2024-03-02 20:50:42,572 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=600, bias=True)
    (1): Tanh()
    (2): Linear(in_features=600, out_features=30, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,112,097
Freeze params: 0
2024-03-02 21:00:27,637 - trainer - INFO -     epoch          : 1
2024-03-02 21:00:27,641 - trainer - INFO -     loss           : 1.321386
2024-03-02 21:00:27,641 - trainer - INFO -     accuracy       : 0.632254
2024-03-02 21:00:27,641 - trainer - INFO -     macro_f        : 0.474591
2024-03-02 21:00:27,641 - trainer - INFO -     doc_entropy    : 2.947308
2024-03-02 21:00:27,641 - trainer - INFO -     val_loss       : 1.193299
2024-03-02 21:00:27,641 - trainer - INFO -     val_accuracy   : 0.664542
2024-03-02 21:00:27,641 - trainer - INFO -     val_macro_f    : 0.511698
2024-03-02 21:00:27,641 - trainer - INFO -     val_doc_entropy: 3.013545
2024-03-02 21:10:08,025 - trainer - INFO -     epoch          : 2
2024-03-02 21:10:08,025 - trainer - INFO -     loss           : 1.004924
2024-03-02 21:10:08,025 - trainer - INFO -     accuracy       : 0.706291
2024-03-02 21:10:08,025 - trainer - INFO -     macro_f        : 0.563732
2024-03-02 21:10:08,025 - trainer - INFO -     doc_entropy    : 3.046385
2024-03-02 21:10:08,025 - trainer - INFO -     val_loss       : 1.155724
2024-03-02 21:10:08,025 - trainer - INFO -     val_accuracy   : 0.671114
2024-03-02 21:10:08,025 - trainer - INFO -     val_macro_f    : 0.521628
2024-03-02 21:10:08,025 - trainer - INFO -     val_doc_entropy: 3.077518
2024-03-02 21:19:43,298 - trainer - INFO -     epoch          : 3
2024-03-02 21:19:43,298 - trainer - INFO -     loss           : 0.840946
2024-03-02 21:19:43,298 - trainer - INFO -     accuracy       : 0.747803
2024-03-02 21:19:43,298 - trainer - INFO -     macro_f        : 0.618243
2024-03-02 21:19:43,298 - trainer - INFO -     doc_entropy    : 3.013534
2024-03-02 21:19:43,298 - trainer - INFO -     val_loss       : 1.159501
2024-03-02 21:19:43,298 - trainer - INFO -     val_accuracy   : 0.672608
2024-03-02 21:19:43,298 - trainer - INFO -     val_macro_f    : 0.520953
2024-03-02 21:19:43,298 - trainer - INFO -     val_doc_entropy: 3.079878
2024-03-02 21:20:50,314 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=600, bias=True)
    (1): Tanh()
    (2): Linear(in_features=600, out_features=30, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,112,097
Freeze params: 0
2024-03-02 21:30:31,653 - trainer - INFO -     epoch          : 1
2024-03-02 21:30:31,663 - trainer - INFO -     loss           : 1.318405
2024-03-02 21:30:31,663 - trainer - INFO -     accuracy       : 0.63188
2024-03-02 21:30:31,663 - trainer - INFO -     macro_f        : 0.474095
2024-03-02 21:30:31,663 - trainer - INFO -     doc_entropy    : 3.003472
2024-03-02 21:30:31,663 - trainer - INFO -     val_loss       : 1.173498
2024-03-02 21:30:31,663 - trainer - INFO -     val_accuracy   : 0.668924
2024-03-02 21:30:31,663 - trainer - INFO -     val_macro_f    : 0.514642
2024-03-02 21:30:31,663 - trainer - INFO -     val_doc_entropy: 3.072215
2024-03-02 21:40:03,105 - trainer - INFO -     epoch          : 2
2024-03-02 21:40:03,105 - trainer - INFO -     loss           : 0.99688
2024-03-02 21:40:03,105 - trainer - INFO -     accuracy       : 0.709546
2024-03-02 21:40:03,105 - trainer - INFO -     macro_f        : 0.567058
2024-03-02 21:40:03,105 - trainer - INFO -     doc_entropy    : 3.02129
2024-03-02 21:40:03,105 - trainer - INFO -     val_loss       : 1.177571
2024-03-02 21:40:03,105 - trainer - INFO -     val_accuracy   : 0.671064
2024-03-02 21:40:03,105 - trainer - INFO -     val_macro_f    : 0.52047
2024-03-02 21:40:03,105 - trainer - INFO -     val_doc_entropy: 3.054096
2024-03-02 21:49:35,651 - trainer - INFO -     epoch          : 3
2024-03-02 21:49:35,667 - trainer - INFO -     loss           : 0.835488
2024-03-02 21:49:35,667 - trainer - INFO -     accuracy       : 0.749365
2024-03-02 21:49:35,667 - trainer - INFO -     macro_f        : 0.620883
2024-03-02 21:49:35,667 - trainer - INFO -     doc_entropy    : 3.046424
2024-03-02 21:49:35,667 - trainer - INFO -     val_loss       : 1.174908
2024-03-02 21:49:35,667 - trainer - INFO -     val_accuracy   : 0.67435
2024-03-02 21:49:35,667 - trainer - INFO -     val_macro_f    : 0.523995
2024-03-02 21:49:35,667 - trainer - INFO -     val_doc_entropy: 3.22876
2024-03-02 21:50:40,662 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=600, bias=True)
    (1): Tanh()
    (2): Linear(in_features=600, out_features=30, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,112,097
Freeze params: 0
2024-03-02 22:00:12,291 - trainer - INFO -     epoch          : 1
2024-03-02 22:00:12,291 - trainer - INFO -     loss           : 1.313932
2024-03-02 22:00:12,291 - trainer - INFO -     accuracy       : 0.633523
2024-03-02 22:00:12,291 - trainer - INFO -     macro_f        : 0.475267
2024-03-02 22:00:12,291 - trainer - INFO -     doc_entropy    : 3.012874
2024-03-02 22:00:12,291 - trainer - INFO -     val_loss       : 1.203336
2024-03-02 22:00:12,291 - trainer - INFO -     val_accuracy   : 0.65812
2024-03-02 22:00:12,291 - trainer - INFO -     val_macro_f    : 0.501289
2024-03-02 22:00:12,291 - trainer - INFO -     val_doc_entropy: 3.028462
2024-03-02 22:09:44,707 - trainer - INFO -     epoch          : 2
2024-03-02 22:09:44,707 - trainer - INFO -     loss           : 0.993748
2024-03-02 22:09:44,707 - trainer - INFO -     accuracy       : 0.709403
2024-03-02 22:09:44,707 - trainer - INFO -     macro_f        : 0.567343
2024-03-02 22:09:44,707 - trainer - INFO -     doc_entropy    : 2.962237
2024-03-02 22:09:44,707 - trainer - INFO -     val_loss       : 1.139918
2024-03-02 22:09:44,707 - trainer - INFO -     val_accuracy   : 0.678881
2024-03-02 22:09:44,707 - trainer - INFO -     val_macro_f    : 0.531168
2024-03-02 22:09:44,707 - trainer - INFO -     val_doc_entropy: 2.971508
2024-03-02 22:19:14,546 - trainer - INFO -     epoch          : 3
2024-03-02 22:19:14,546 - trainer - INFO -     loss           : 0.828781
2024-03-02 22:19:14,546 - trainer - INFO -     accuracy       : 0.751712
2024-03-02 22:19:14,546 - trainer - INFO -     macro_f        : 0.623297
2024-03-02 22:19:14,546 - trainer - INFO -     doc_entropy    : 2.957893
2024-03-02 22:19:14,546 - trainer - INFO -     val_loss       : 1.206352
2024-03-02 22:19:14,546 - trainer - INFO -     val_accuracy   : 0.664841
2024-03-02 22:19:14,546 - trainer - INFO -     val_macro_f    : 0.51404
2024-03-02 22:19:14,546 - trainer - INFO -     val_doc_entropy: 2.99831
2024-04-01 17:37:52,386 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,760,523
Freeze params: 0
2024-04-01 17:41:00,171 - trainer - INFO -     epoch          : 1
2024-04-01 17:41:00,171 - trainer - INFO -     loss           : 1.217016
2024-04-01 17:41:00,171 - trainer - INFO -     accuracy       : 0.654086
2024-04-01 17:41:00,171 - trainer - INFO -     macro_f        : 0.634204
2024-04-01 17:41:00,171 - trainer - INFO -     precision      : 0.665439
2024-04-01 17:41:00,171 - trainer - INFO -     recall         : 0.654086
2024-04-01 17:41:00,171 - trainer - INFO -     doc_entropy    : 2.404529
2024-04-01 17:41:00,171 - trainer - INFO -     val_loss       : 1.083479
2024-04-01 17:41:00,187 - trainer - INFO -     val_accuracy   : 0.682665
2024-04-01 17:41:00,187 - trainer - INFO -     val_macro_f    : 0.664367
2024-04-01 17:41:00,187 - trainer - INFO -     val_precision  : 0.693945
2024-04-01 17:41:00,187 - trainer - INFO -     val_recall     : 0.682665
2024-04-01 17:41:00,187 - trainer - INFO -     val_doc_entropy: 2.350401
2024-04-01 17:41:00,187 - trainer - INFO -     test_loss      : 1.087034
2024-04-01 17:41:00,187 - trainer - INFO -     test_accuracy  : 0.681569
2024-04-01 17:41:00,187 - trainer - INFO -     test_macro_f   : 0.664828
2024-04-01 17:41:00,187 - trainer - INFO -     test_precision : 0.698296
2024-04-01 17:41:00,187 - trainer - INFO -     test_recall    : 0.681569
2024-04-01 17:41:00,187 - trainer - INFO -     test_doc_entropy: 2.354489
2024-04-01 17:43:58,098 - trainer - INFO -     epoch          : 2
2024-04-01 17:43:58,098 - trainer - INFO -     loss           : 0.892376
2024-04-01 17:43:58,098 - trainer - INFO -     accuracy       : 0.737092
2024-04-01 17:43:58,098 - trainer - INFO -     macro_f        : 0.727365
2024-04-01 17:43:58,098 - trainer - INFO -     precision      : 0.761715
2024-04-01 17:43:58,098 - trainer - INFO -     recall         : 0.737092
2024-04-01 17:43:58,098 - trainer - INFO -     doc_entropy    : 2.16723
2024-04-01 17:43:58,098 - trainer - INFO -     val_loss       : 1.077456
2024-04-01 17:43:58,098 - trainer - INFO -     val_accuracy   : 0.688091
2024-04-01 17:43:58,098 - trainer - INFO -     val_macro_f    : 0.675198
2024-04-01 17:43:58,098 - trainer - INFO -     val_precision  : 0.709773
2024-04-01 17:43:58,098 - trainer - INFO -     val_recall     : 0.688091
2024-04-01 17:43:58,098 - trainer - INFO -     val_doc_entropy: 2.206817
2024-04-01 17:43:58,098 - trainer - INFO -     test_loss      : 1.070374
2024-04-01 17:43:58,098 - trainer - INFO -     test_accuracy  : 0.690879
2024-04-01 17:43:58,098 - trainer - INFO -     test_macro_f   : 0.678761
2024-04-01 17:43:58,098 - trainer - INFO -     test_precision : 0.714361
2024-04-01 17:43:58,098 - trainer - INFO -     test_recall    : 0.690879
2024-04-01 17:43:58,098 - trainer - INFO -     test_doc_entropy: 2.211331
2024-04-01 17:46:53,797 - trainer - INFO -     epoch          : 3
2024-04-01 17:46:53,797 - trainer - INFO -     loss           : 0.678361
2024-04-01 17:46:53,797 - trainer - INFO -     accuracy       : 0.798483
2024-04-01 17:46:53,797 - trainer - INFO -     macro_f        : 0.792609
2024-04-01 17:46:53,797 - trainer - INFO -     precision      : 0.823663
2024-04-01 17:46:53,797 - trainer - INFO -     recall         : 0.798483
2024-04-01 17:46:53,797 - trainer - INFO -     doc_entropy    : 2.022079
2024-04-01 17:46:53,813 - trainer - INFO -     val_loss       : 1.156857
2024-04-01 17:46:53,813 - trainer - INFO -     val_accuracy   : 0.679827
2024-04-01 17:46:53,813 - trainer - INFO -     val_macro_f    : 0.669628
2024-04-01 17:46:53,813 - trainer - INFO -     val_precision  : 0.707053
2024-04-01 17:46:53,813 - trainer - INFO -     val_recall     : 0.679827
2024-04-01 17:46:53,813 - trainer - INFO -     val_doc_entropy: 2.160636
2024-04-01 17:46:53,813 - trainer - INFO -     test_loss      : 1.158578
2024-04-01 17:46:53,813 - trainer - INFO -     test_accuracy  : 0.676889
2024-04-01 17:46:53,813 - trainer - INFO -     test_macro_f   : 0.668558
2024-04-01 17:46:53,813 - trainer - INFO -     test_precision : 0.707668
2024-04-01 17:46:53,813 - trainer - INFO -     test_recall    : 0.676889
2024-04-01 17:46:53,813 - trainer - INFO -     test_doc_entropy: 2.164427
2024-04-01 17:47:31,037 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,760,523
Freeze params: 0
2024-04-01 17:50:25,929 - trainer - INFO -     epoch          : 1
2024-04-01 17:50:25,929 - trainer - INFO -     loss           : 1.223805
2024-04-01 17:50:25,929 - trainer - INFO -     accuracy       : 0.651921
2024-04-01 17:50:25,929 - trainer - INFO -     macro_f        : 0.632565
2024-04-01 17:50:25,929 - trainer - INFO -     precision      : 0.664585
2024-04-01 17:50:25,929 - trainer - INFO -     recall         : 0.651921
2024-04-01 17:50:25,929 - trainer - INFO -     doc_entropy    : 2.433625
2024-04-01 17:50:25,943 - trainer - INFO -     val_loss       : 1.07403
2024-04-01 17:50:25,943 - trainer - INFO -     val_accuracy   : 0.686647
2024-04-01 17:50:25,944 - trainer - INFO -     val_macro_f    : 0.670584
2024-04-01 17:50:25,944 - trainer - INFO -     val_precision  : 0.701792
2024-04-01 17:50:25,944 - trainer - INFO -     val_recall     : 0.686647
2024-04-01 17:50:25,944 - trainer - INFO -     val_doc_entropy: 2.441882
2024-04-01 17:50:25,944 - trainer - INFO -     test_loss      : 1.08282
2024-04-01 17:50:25,945 - trainer - INFO -     test_accuracy  : 0.686448
2024-04-01 17:50:25,945 - trainer - INFO -     test_macro_f   : 0.669914
2024-04-01 17:50:25,945 - trainer - INFO -     test_precision : 0.700753
2024-04-01 17:50:25,945 - trainer - INFO -     test_recall    : 0.686448
2024-04-01 17:50:25,945 - trainer - INFO -     test_doc_entropy: 2.445239
2024-04-01 17:53:28,277 - trainer - INFO -     epoch          : 2
2024-04-01 17:53:28,277 - trainer - INFO -     loss           : 0.893206
2024-04-01 17:53:28,277 - trainer - INFO -     accuracy       : 0.736731
2024-04-01 17:53:28,277 - trainer - INFO -     macro_f        : 0.727199
2024-04-01 17:53:28,277 - trainer - INFO -     precision      : 0.761377
2024-04-01 17:53:28,277 - trainer - INFO -     recall         : 0.736731
2024-04-01 17:53:28,277 - trainer - INFO -     doc_entropy    : 2.160754
2024-04-01 17:53:28,277 - trainer - INFO -     val_loss       : 1.090505
2024-04-01 17:53:28,277 - trainer - INFO -     val_accuracy   : 0.684706
2024-04-01 17:53:28,277 - trainer - INFO -     val_macro_f    : 0.668322
2024-04-01 17:53:28,277 - trainer - INFO -     val_precision  : 0.699558
2024-04-01 17:53:28,277 - trainer - INFO -     val_recall     : 0.684706
2024-04-01 17:53:28,277 - trainer - INFO -     val_doc_entropy: 2.236834
2024-04-01 17:53:28,277 - trainer - INFO -     test_loss      : 1.088338
2024-04-01 17:53:28,277 - trainer - INFO -     test_accuracy  : 0.686299
2024-04-01 17:53:28,277 - trainer - INFO -     test_macro_f   : 0.669797
2024-04-01 17:53:28,277 - trainer - INFO -     test_precision : 0.701228
2024-04-01 17:53:28,277 - trainer - INFO -     test_recall    : 0.686299
2024-04-01 17:53:28,277 - trainer - INFO -     test_doc_entropy: 2.240351
2024-04-01 17:56:25,354 - trainer - INFO -     epoch          : 3
2024-04-01 17:56:25,354 - trainer - INFO -     loss           : 0.679723
2024-04-01 17:56:25,354 - trainer - INFO -     accuracy       : 0.797761
2024-04-01 17:56:25,354 - trainer - INFO -     macro_f        : 0.792203
2024-04-01 17:56:25,354 - trainer - INFO -     precision      : 0.823481
2024-04-01 17:56:25,369 - trainer - INFO -     recall         : 0.797761
2024-04-01 17:56:25,369 - trainer - INFO -     doc_entropy    : 1.987016
2024-04-01 17:56:25,369 - trainer - INFO -     val_loss       : 1.180234
2024-04-01 17:56:25,369 - trainer - INFO -     val_accuracy   : 0.673803
2024-04-01 17:56:25,369 - trainer - INFO -     val_macro_f    : 0.667783
2024-04-01 17:56:25,369 - trainer - INFO -     val_precision  : 0.711165
2024-04-01 17:56:25,369 - trainer - INFO -     val_recall     : 0.673803
2024-04-01 17:56:25,369 - trainer - INFO -     val_doc_entropy: 2.032072
2024-04-01 17:56:25,369 - trainer - INFO -     test_loss      : 1.176167
2024-04-01 17:56:25,369 - trainer - INFO -     test_accuracy  : 0.677387
2024-04-01 17:56:25,369 - trainer - INFO -     test_macro_f   : 0.671822
2024-04-01 17:56:25,369 - trainer - INFO -     test_precision : 0.71513
2024-04-01 17:56:25,369 - trainer - INFO -     test_recall    : 0.677387
2024-04-01 17:56:25,369 - trainer - INFO -     test_doc_entropy: 2.036296
2024-04-01 17:56:59,320 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,760,523
Freeze params: 0
2024-04-01 17:59:55,835 - trainer - INFO -     epoch          : 1
2024-04-01 17:59:55,835 - trainer - INFO -     loss           : 1.224482
2024-04-01 17:59:55,835 - trainer - INFO -     accuracy       : 0.652481
2024-04-01 17:59:55,835 - trainer - INFO -     macro_f        : 0.632539
2024-04-01 17:59:55,835 - trainer - INFO -     precision      : 0.662565
2024-04-01 17:59:55,835 - trainer - INFO -     recall         : 0.652481
2024-04-01 17:59:55,835 - trainer - INFO -     doc_entropy    : 2.502926
2024-04-01 17:59:55,835 - trainer - INFO -     val_loss       : 1.078071
2024-04-01 17:59:55,835 - trainer - INFO -     val_accuracy   : 0.685801
2024-04-01 17:59:55,835 - trainer - INFO -     val_macro_f    : 0.668038
2024-04-01 17:59:55,835 - trainer - INFO -     val_precision  : 0.697706
2024-04-01 17:59:55,835 - trainer - INFO -     val_recall     : 0.685801
2024-04-01 17:59:55,835 - trainer - INFO -     val_doc_entropy: 2.478573
2024-04-01 17:59:55,835 - trainer - INFO -     test_loss      : 1.074956
2024-04-01 17:59:55,835 - trainer - INFO -     test_accuracy  : 0.684756
2024-04-01 17:59:55,835 - trainer - INFO -     test_macro_f   : 0.669156
2024-04-01 17:59:55,835 - trainer - INFO -     test_precision : 0.703271
2024-04-01 17:59:55,835 - trainer - INFO -     test_recall    : 0.684756
2024-04-01 17:59:55,835 - trainer - INFO -     test_doc_entropy: 2.481983
2024-04-01 18:02:55,547 - trainer - INFO -     epoch          : 2
2024-04-01 18:02:55,547 - trainer - INFO -     loss           : 0.893225
2024-04-01 18:02:55,547 - trainer - INFO -     accuracy       : 0.738044
2024-04-01 18:02:55,547 - trainer - INFO -     macro_f        : 0.728462
2024-04-01 18:02:55,547 - trainer - INFO -     precision      : 0.762596
2024-04-01 18:02:55,547 - trainer - INFO -     recall         : 0.738044
2024-04-01 18:02:55,547 - trainer - INFO -     doc_entropy    : 2.218435
2024-04-01 18:02:55,547 - trainer - INFO -     val_loss       : 1.088242
2024-04-01 18:02:55,547 - trainer - INFO -     val_accuracy   : 0.685751
2024-04-01 18:02:55,547 - trainer - INFO -     val_macro_f    : 0.674475
2024-04-01 18:02:55,547 - trainer - INFO -     val_precision  : 0.710931
2024-04-01 18:02:55,547 - trainer - INFO -     val_recall     : 0.685751
2024-04-01 18:02:55,547 - trainer - INFO -     val_doc_entropy: 2.250004
2024-04-01 18:02:55,563 - trainer - INFO -     test_loss      : 1.093917
2024-04-01 18:02:55,563 - trainer - INFO -     test_accuracy  : 0.683959
2024-04-01 18:02:55,563 - trainer - INFO -     test_macro_f   : 0.671877
2024-04-01 18:02:55,563 - trainer - INFO -     test_precision : 0.708939
2024-04-01 18:02:55,563 - trainer - INFO -     test_recall    : 0.683959
2024-04-01 18:02:55,563 - trainer - INFO -     test_doc_entropy: 2.253904
2024-04-01 18:05:51,478 - trainer - INFO -     epoch          : 3
2024-04-01 18:05:51,478 - trainer - INFO -     loss           : 0.681219
2024-04-01 18:05:51,478 - trainer - INFO -     accuracy       : 0.797499
2024-04-01 18:05:51,478 - trainer - INFO -     macro_f        : 0.791384
2024-04-01 18:05:51,478 - trainer - INFO -     precision      : 0.822111
2024-04-01 18:05:51,478 - trainer - INFO -     recall         : 0.797499
2024-04-01 18:05:51,478 - trainer - INFO -     doc_entropy    : 2.023292
2024-04-01 18:05:51,478 - trainer - INFO -     val_loss       : 1.147446
2024-04-01 18:05:51,478 - trainer - INFO -     val_accuracy   : 0.680325
2024-04-01 18:05:51,478 - trainer - INFO -     val_macro_f    : 0.67268
2024-04-01 18:05:51,478 - trainer - INFO -     val_precision  : 0.712869
2024-04-01 18:05:51,478 - trainer - INFO -     val_recall     : 0.680325
2024-04-01 18:05:51,478 - trainer - INFO -     val_doc_entropy: 2.169317
2024-04-01 18:05:51,478 - trainer - INFO -     test_loss      : 1.142523
2024-04-01 18:05:51,478 - trainer - INFO -     test_accuracy  : 0.683461
2024-04-01 18:05:51,478 - trainer - INFO -     test_macro_f   : 0.676096
2024-04-01 18:05:51,478 - trainer - INFO -     test_precision : 0.716544
2024-04-01 18:05:51,478 - trainer - INFO -     test_recall    : 0.683461
2024-04-01 18:05:51,478 - trainer - INFO -     test_doc_entropy: 2.173502
2024-04-01 18:06:28,385 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,760,523
Freeze params: 0
2024-04-01 18:09:23,804 - trainer - INFO -     epoch          : 1
2024-04-01 18:09:23,804 - trainer - INFO -     loss           : 1.22109
2024-04-01 18:09:23,804 - trainer - INFO -     accuracy       : 0.654435
2024-04-01 18:09:23,804 - trainer - INFO -     macro_f        : 0.634356
2024-04-01 18:09:23,804 - trainer - INFO -     precision      : 0.664918
2024-04-01 18:09:23,804 - trainer - INFO -     recall         : 0.654435
2024-04-01 18:09:23,804 - trainer - INFO -     doc_entropy    : 2.484935
2024-04-01 18:09:23,804 - trainer - INFO -     val_loss       : 1.077858
2024-04-01 18:09:23,804 - trainer - INFO -     val_accuracy   : 0.688041
2024-04-01 18:09:23,804 - trainer - INFO -     val_macro_f    : 0.673675
2024-04-01 18:09:23,804 - trainer - INFO -     val_precision  : 0.704834
2024-04-01 18:09:23,804 - trainer - INFO -     val_recall     : 0.688041
2024-04-01 18:09:23,804 - trainer - INFO -     val_doc_entropy: 2.345363
2024-04-01 18:09:23,804 - trainer - INFO -     test_loss      : 1.079651
2024-04-01 18:09:23,804 - trainer - INFO -     test_accuracy  : 0.686598
2024-04-01 18:09:23,820 - trainer - INFO -     test_macro_f   : 0.672211
2024-04-01 18:09:23,820 - trainer - INFO -     test_precision : 0.705736
2024-04-01 18:09:23,820 - trainer - INFO -     test_recall    : 0.686598
2024-04-01 18:09:23,820 - trainer - INFO -     test_doc_entropy: 2.349319
2024-04-01 18:12:21,214 - trainer - INFO -     epoch          : 2
2024-04-01 18:12:21,214 - trainer - INFO -     loss           : 0.889274
2024-04-01 18:12:21,214 - trainer - INFO -     accuracy       : 0.738922
2024-04-01 18:12:21,214 - trainer - INFO -     macro_f        : 0.729894
2024-04-01 18:12:21,214 - trainer - INFO -     precision      : 0.764328
2024-04-01 18:12:21,214 - trainer - INFO -     recall         : 0.738922
2024-04-01 18:12:21,214 - trainer - INFO -     doc_entropy    : 2.220042
2024-04-01 18:12:21,214 - trainer - INFO -     val_loss       : 1.073049
2024-04-01 18:12:21,214 - trainer - INFO -     val_accuracy   : 0.689535
2024-04-01 18:12:21,230 - trainer - INFO -     val_macro_f    : 0.677678
2024-04-01 18:12:21,230 - trainer - INFO -     val_precision  : 0.712141
2024-04-01 18:12:21,230 - trainer - INFO -     val_recall     : 0.689535
2024-04-01 18:12:21,230 - trainer - INFO -     val_doc_entropy: 2.300027
2024-04-01 18:12:21,230 - trainer - INFO -     test_loss      : 1.069598
2024-04-01 18:12:21,230 - trainer - INFO -     test_accuracy  : 0.690232
2024-04-01 18:12:21,230 - trainer - INFO -     test_macro_f   : 0.679103
2024-04-01 18:12:21,230 - trainer - INFO -     test_precision : 0.713641
2024-04-01 18:12:21,230 - trainer - INFO -     test_recall    : 0.690232
2024-04-01 18:12:21,230 - trainer - INFO -     test_doc_entropy: 2.303995
2024-04-01 18:15:17,909 - trainer - INFO -     epoch          : 3
2024-04-01 18:15:17,909 - trainer - INFO -     loss           : 0.679086
2024-04-01 18:15:17,909 - trainer - INFO -     accuracy       : 0.798084
2024-04-01 18:15:17,909 - trainer - INFO -     macro_f        : 0.792259
2024-04-01 18:15:17,909 - trainer - INFO -     precision      : 0.823344
2024-04-01 18:15:17,909 - trainer - INFO -     recall         : 0.798084
2024-04-01 18:15:17,909 - trainer - INFO -     doc_entropy    : 2.046113
2024-04-01 18:15:17,909 - trainer - INFO -     val_loss       : 1.187949
2024-04-01 18:15:17,909 - trainer - INFO -     val_accuracy   : 0.67206
2024-04-01 18:15:17,909 - trainer - INFO -     val_macro_f    : 0.665534
2024-04-01 18:15:17,909 - trainer - INFO -     val_precision  : 0.707611
2024-04-01 18:15:17,909 - trainer - INFO -     val_recall     : 0.67206
2024-04-01 18:15:17,909 - trainer - INFO -     val_doc_entropy: 2.109999
2024-04-01 18:15:17,909 - trainer - INFO -     test_loss      : 1.179901
2024-04-01 18:15:17,909 - trainer - INFO -     test_accuracy  : 0.672857
2024-04-01 18:15:17,909 - trainer - INFO -     test_macro_f   : 0.666196
2024-04-01 18:15:17,909 - trainer - INFO -     test_precision : 0.710164
2024-04-01 18:15:17,909 - trainer - INFO -     test_recall    : 0.672857
2024-04-01 18:15:17,909 - trainer - INFO -     test_doc_entropy: 2.113788
2024-04-01 18:15:52,859 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,760,523
Freeze params: 0
2024-04-01 18:18:46,431 - trainer - INFO -     epoch          : 1
2024-04-01 18:18:46,431 - trainer - INFO -     loss           : 1.223879
2024-04-01 18:18:46,431 - trainer - INFO -     accuracy       : 0.652518
2024-04-01 18:18:46,431 - trainer - INFO -     macro_f        : 0.633123
2024-04-01 18:18:46,431 - trainer - INFO -     precision      : 0.664194
2024-04-01 18:18:46,431 - trainer - INFO -     recall         : 0.652518
2024-04-01 18:18:46,431 - trainer - INFO -     doc_entropy    : 2.511726
2024-04-01 18:18:46,431 - trainer - INFO -     val_loss       : 1.078362
2024-04-01 18:18:46,431 - trainer - INFO -     val_accuracy   : 0.686647
2024-04-01 18:18:46,431 - trainer - INFO -     val_macro_f    : 0.674595
2024-04-01 18:18:46,431 - trainer - INFO -     val_precision  : 0.708565
2024-04-01 18:18:46,431 - trainer - INFO -     val_recall     : 0.686647
2024-04-01 18:18:46,431 - trainer - INFO -     val_doc_entropy: 2.363145
2024-04-01 18:18:46,431 - trainer - INFO -     test_loss      : 1.080859
2024-04-01 18:18:46,431 - trainer - INFO -     test_accuracy  : 0.685154
2024-04-01 18:18:46,431 - trainer - INFO -     test_macro_f   : 0.673484
2024-04-01 18:18:46,431 - trainer - INFO -     test_precision : 0.709298
2024-04-01 18:18:46,431 - trainer - INFO -     test_recall    : 0.685154
2024-04-01 18:18:46,431 - trainer - INFO -     test_doc_entropy: 2.367479
2024-04-01 18:21:40,214 - trainer - INFO -     epoch          : 2
2024-04-01 18:21:40,214 - trainer - INFO -     loss           : 0.891526
2024-04-01 18:21:40,214 - trainer - INFO -     accuracy       : 0.737764
2024-04-01 18:21:40,214 - trainer - INFO -     macro_f        : 0.727926
2024-04-01 18:21:40,214 - trainer - INFO -     precision      : 0.761654
2024-04-01 18:21:40,214 - trainer - INFO -     recall         : 0.737764
2024-04-01 18:21:40,230 - trainer - INFO -     doc_entropy    : 2.201353
2024-04-01 18:21:40,230 - trainer - INFO -     val_loss       : 1.089295
2024-04-01 18:21:40,230 - trainer - INFO -     val_accuracy   : 0.689585
2024-04-01 18:21:40,230 - trainer - INFO -     val_macro_f    : 0.682903
2024-04-01 18:21:40,230 - trainer - INFO -     val_precision  : 0.724616
2024-04-01 18:21:40,230 - trainer - INFO -     val_recall     : 0.689585
2024-04-01 18:21:40,230 - trainer - INFO -     val_doc_entropy: 2.166564
2024-04-01 18:21:40,230 - trainer - INFO -     test_loss      : 1.094202
2024-04-01 18:21:40,230 - trainer - INFO -     test_accuracy  : 0.687195
2024-04-01 18:21:40,230 - trainer - INFO -     test_macro_f   : 0.679993
2024-04-01 18:21:40,230 - trainer - INFO -     test_precision : 0.719274
2024-04-01 18:21:40,230 - trainer - INFO -     test_recall    : 0.687195
2024-04-01 18:21:40,230 - trainer - INFO -     test_doc_entropy: 2.168763
2024-04-01 18:24:39,344 - trainer - INFO -     epoch          : 3
2024-04-01 18:24:39,344 - trainer - INFO -     loss           : 0.676615
2024-04-01 18:24:39,344 - trainer - INFO -     accuracy       : 0.798794
2024-04-01 18:24:39,344 - trainer - INFO -     macro_f        : 0.793124
2024-04-01 18:24:39,344 - trainer - INFO -     precision      : 0.823994
2024-04-01 18:24:39,344 - trainer - INFO -     recall         : 0.798794
2024-04-01 18:24:39,344 - trainer - INFO -     doc_entropy    : 2.03006
2024-04-01 18:24:39,344 - trainer - INFO -     val_loss       : 1.162388
2024-04-01 18:24:39,344 - trainer - INFO -     val_accuracy   : 0.680225
2024-04-01 18:24:39,344 - trainer - INFO -     val_macro_f    : 0.670218
2024-04-01 18:24:39,344 - trainer - INFO -     val_precision  : 0.706782
2024-04-01 18:24:39,360 - trainer - INFO -     val_recall     : 0.680225
2024-04-01 18:24:39,360 - trainer - INFO -     val_doc_entropy: 2.09085
2024-04-01 18:24:39,360 - trainer - INFO -     test_loss      : 1.163438
2024-04-01 18:24:39,360 - trainer - INFO -     test_accuracy  : 0.676989
2024-04-01 18:24:39,360 - trainer - INFO -     test_macro_f   : 0.66643
2024-04-01 18:24:39,360 - trainer - INFO -     test_precision : 0.703183
2024-04-01 18:24:39,360 - trainer - INFO -     test_recall    : 0.676989
2024-04-01 18:24:39,360 - trainer - INFO -     test_doc_entropy: 2.093436
2024-04-01 18:25:28,088 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,760,523
Freeze params: 0
2024-04-01 19:06:42,294 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=400, bias=True)
    (1): Tanh()
    (2): Linear(in_features=400, out_features=20, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,616,103
Freeze params: 0
2024-04-01 19:09:33,801 - trainer - INFO -     epoch          : 1
2024-04-01 19:09:33,801 - trainer - INFO -     loss           : 1.211495
2024-04-01 19:09:33,801 - trainer - INFO -     accuracy       : 0.654454
2024-04-01 19:09:33,801 - trainer - INFO -     macro_f        : 0.635245
2024-04-01 19:09:33,801 - trainer - INFO -     precision      : 0.666275
2024-04-01 19:09:33,801 - trainer - INFO -     recall         : 0.654454
2024-04-01 19:09:33,801 - trainer - INFO -     doc_entropy    : 2.565718
2024-04-01 19:09:33,801 - trainer - INFO -     val_loss       : 1.072575
2024-04-01 19:09:33,801 - trainer - INFO -     val_accuracy   : 0.685801
2024-04-01 19:09:33,801 - trainer - INFO -     val_macro_f    : 0.669438
2024-04-01 19:09:33,801 - trainer - INFO -     val_precision  : 0.700147
2024-04-01 19:09:33,801 - trainer - INFO -     val_recall     : 0.685801
2024-04-01 19:09:33,801 - trainer - INFO -     val_doc_entropy: 2.473997
2024-04-01 19:09:33,801 - trainer - INFO -     test_loss      : 1.071498
2024-04-01 19:09:33,801 - trainer - INFO -     test_accuracy  : 0.687593
2024-04-01 19:09:33,801 - trainer - INFO -     test_macro_f   : 0.672673
2024-04-01 19:09:33,801 - trainer - INFO -     test_precision : 0.705378
2024-04-01 19:09:33,801 - trainer - INFO -     test_recall    : 0.687593
2024-04-01 19:09:33,801 - trainer - INFO -     test_doc_entropy: 2.478766
2024-04-01 19:12:24,708 - trainer - INFO -     epoch          : 2
2024-04-01 19:12:24,708 - trainer - INFO -     loss           : 0.884602
2024-04-01 19:12:24,708 - trainer - INFO -     accuracy       : 0.738467
2024-04-01 19:12:24,708 - trainer - INFO -     macro_f        : 0.729712
2024-04-01 19:12:24,708 - trainer - INFO -     precision      : 0.765228
2024-04-01 19:12:24,708 - trainer - INFO -     recall         : 0.738467
2024-04-01 19:12:24,708 - trainer - INFO -     doc_entropy    : 2.382322
2024-04-01 19:12:24,708 - trainer - INFO -     val_loss       : 1.090728
2024-04-01 19:12:24,708 - trainer - INFO -     val_accuracy   : 0.687643
2024-04-01 19:12:24,708 - trainer - INFO -     val_macro_f    : 0.669956
2024-04-01 19:12:24,708 - trainer - INFO -     val_precision  : 0.700728
2024-04-01 19:12:24,708 - trainer - INFO -     val_recall     : 0.687643
2024-04-01 19:12:24,708 - trainer - INFO -     val_doc_entropy: 2.489526
2024-04-01 19:12:24,708 - trainer - INFO -     test_loss      : 1.086247
2024-04-01 19:12:24,708 - trainer - INFO -     test_accuracy  : 0.68615
2024-04-01 19:12:24,708 - trainer - INFO -     test_macro_f   : 0.670942
2024-04-01 19:12:24,708 - trainer - INFO -     test_precision : 0.704631
2024-04-01 19:12:24,708 - trainer - INFO -     test_recall    : 0.68615
2024-04-01 19:12:24,708 - trainer - INFO -     test_doc_entropy: 2.493934
2024-04-01 19:15:17,227 - trainer - INFO -     epoch          : 3
2024-04-01 19:15:17,227 - trainer - INFO -     loss           : 0.674198
2024-04-01 19:15:17,227 - trainer - INFO -     accuracy       : 0.798956
2024-04-01 19:15:17,227 - trainer - INFO -     macro_f        : 0.792831
2024-04-01 19:15:17,227 - trainer - INFO -     precision      : 0.823088
2024-04-01 19:15:17,227 - trainer - INFO -     recall         : 0.798956
2024-04-01 19:15:17,227 - trainer - INFO -     doc_entropy    : 2.271423
2024-04-01 19:15:17,227 - trainer - INFO -     val_loss       : 1.163577
2024-04-01 19:15:17,227 - trainer - INFO -     val_accuracy   : 0.682416
2024-04-01 19:15:17,227 - trainer - INFO -     val_macro_f    : 0.675606
2024-04-01 19:15:17,227 - trainer - INFO -     val_precision  : 0.716488
2024-04-01 19:15:17,243 - trainer - INFO -     val_recall     : 0.682416
2024-04-01 19:15:17,243 - trainer - INFO -     val_doc_entropy: 2.343915
2024-04-01 19:15:17,243 - trainer - INFO -     test_loss      : 1.166381
2024-04-01 19:15:17,243 - trainer - INFO -     test_accuracy  : 0.684208
2024-04-01 19:15:17,243 - trainer - INFO -     test_macro_f   : 0.677038
2024-04-01 19:15:17,243 - trainer - INFO -     test_precision : 0.717765
2024-04-01 19:15:17,243 - trainer - INFO -     test_recall    : 0.684208
2024-04-01 19:15:17,243 - trainer - INFO -     test_doc_entropy: 2.347682
2024-04-01 19:15:53,169 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=400, bias=True)
    (1): Tanh()
    (2): Linear(in_features=400, out_features=20, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,616,103
Freeze params: 0
2024-04-01 19:18:46,160 - trainer - INFO -     epoch          : 1
2024-04-01 19:18:46,160 - trainer - INFO -     loss           : 1.211641
2024-04-01 19:18:46,160 - trainer - INFO -     accuracy       : 0.655898
2024-04-01 19:18:46,160 - trainer - INFO -     macro_f        : 0.636326
2024-04-01 19:18:46,160 - trainer - INFO -     precision      : 0.667104
2024-04-01 19:18:46,160 - trainer - INFO -     recall         : 0.655898
2024-04-01 19:18:46,160 - trainer - INFO -     doc_entropy    : 2.542366
2024-04-01 19:18:46,160 - trainer - INFO -     val_loss       : 1.068699
2024-04-01 19:18:46,160 - trainer - INFO -     val_accuracy   : 0.687593
2024-04-01 19:18:46,160 - trainer - INFO -     val_macro_f    : 0.67373
2024-04-01 19:18:46,160 - trainer - INFO -     val_precision  : 0.707865
2024-04-01 19:18:46,160 - trainer - INFO -     val_recall     : 0.687593
2024-04-01 19:18:46,160 - trainer - INFO -     val_doc_entropy: 2.457119
2024-04-01 19:18:46,160 - trainer - INFO -     test_loss      : 1.073197
2024-04-01 19:18:46,160 - trainer - INFO -     test_accuracy  : 0.688838
2024-04-01 19:18:46,160 - trainer - INFO -     test_macro_f   : 0.675915
2024-04-01 19:18:46,160 - trainer - INFO -     test_precision : 0.710644
2024-04-01 19:18:46,160 - trainer - INFO -     test_recall    : 0.688838
2024-04-01 19:18:46,160 - trainer - INFO -     test_doc_entropy: 2.459972
2024-04-01 19:21:40,427 - trainer - INFO -     epoch          : 2
2024-04-01 19:21:40,427 - trainer - INFO -     loss           : 0.887603
2024-04-01 19:21:40,427 - trainer - INFO -     accuracy       : 0.738816
2024-04-01 19:21:40,427 - trainer - INFO -     macro_f        : 0.729392
2024-04-01 19:21:40,427 - trainer - INFO -     precision      : 0.763603
2024-04-01 19:21:40,427 - trainer - INFO -     recall         : 0.738816
2024-04-01 19:21:40,427 - trainer - INFO -     doc_entropy    : 2.309324
2024-04-01 19:21:40,427 - trainer - INFO -     val_loss       : 1.090771
2024-04-01 19:21:40,427 - trainer - INFO -     val_accuracy   : 0.68834
2024-04-01 19:21:40,427 - trainer - INFO -     val_macro_f    : 0.679046
2024-04-01 19:21:40,427 - trainer - INFO -     val_precision  : 0.717223
2024-04-01 19:21:40,427 - trainer - INFO -     val_recall     : 0.68834
2024-04-01 19:21:40,427 - trainer - INFO -     val_doc_entropy: 2.354344
2024-04-01 19:21:40,427 - trainer - INFO -     test_loss      : 1.093902
2024-04-01 19:21:40,427 - trainer - INFO -     test_accuracy  : 0.688639
2024-04-01 19:21:40,427 - trainer - INFO -     test_macro_f   : 0.678369
2024-04-01 19:21:40,427 - trainer - INFO -     test_precision : 0.715974
2024-04-01 19:21:40,427 - trainer - INFO -     test_recall    : 0.688639
2024-04-01 19:21:40,427 - trainer - INFO -     test_doc_entropy: 2.357565
2024-04-01 19:24:45,058 - trainer - INFO -     epoch          : 3
2024-04-01 19:24:45,058 - trainer - INFO -     loss           : 0.676932
2024-04-01 19:24:45,058 - trainer - INFO -     accuracy       : 0.799385
2024-04-01 19:24:45,058 - trainer - INFO -     macro_f        : 0.793613
2024-04-01 19:24:45,058 - trainer - INFO -     precision      : 0.824287
2024-04-01 19:24:45,058 - trainer - INFO -     recall         : 0.799385
2024-04-01 19:24:45,058 - trainer - INFO -     doc_entropy    : 2.178514
2024-04-01 19:24:45,058 - trainer - INFO -     val_loss       : 1.161118
2024-04-01 19:24:45,058 - trainer - INFO -     val_accuracy   : 0.678234
2024-04-01 19:24:45,058 - trainer - INFO -     val_macro_f    : 0.671761
2024-04-01 19:24:45,058 - trainer - INFO -     val_precision  : 0.712504
2024-04-01 19:24:45,058 - trainer - INFO -     val_recall     : 0.678234
2024-04-01 19:24:45,058 - trainer - INFO -     val_doc_entropy: 2.293361
2024-04-01 19:24:45,058 - trainer - INFO -     test_loss      : 1.150894
2024-04-01 19:24:45,058 - trainer - INFO -     test_accuracy  : 0.684208
2024-04-01 19:24:45,058 - trainer - INFO -     test_macro_f   : 0.675616
2024-04-01 19:24:45,058 - trainer - INFO -     test_precision : 0.714777
2024-04-01 19:24:45,058 - trainer - INFO -     test_recall    : 0.684208
2024-04-01 19:24:45,058 - trainer - INFO -     test_doc_entropy: 2.297366
2024-04-01 19:25:21,498 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=400, bias=True)
    (1): Tanh()
    (2): Linear(in_features=400, out_features=20, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,616,103
Freeze params: 0
2024-04-01 19:28:25,441 - trainer - INFO -     epoch          : 1
2024-04-01 19:28:25,441 - trainer - INFO -     loss           : 1.212249
2024-04-01 19:28:25,441 - trainer - INFO -     accuracy       : 0.65423
2024-04-01 19:28:25,441 - trainer - INFO -     macro_f        : 0.635026
2024-04-01 19:28:25,441 - trainer - INFO -     precision      : 0.666004
2024-04-01 19:28:25,441 - trainer - INFO -     recall         : 0.65423
2024-04-01 19:28:25,441 - trainer - INFO -     doc_entropy    : 2.614293
2024-04-01 19:28:25,441 - trainer - INFO -     val_loss       : 1.075164
2024-04-01 19:28:25,441 - trainer - INFO -     val_accuracy   : 0.689635
2024-04-01 19:28:25,441 - trainer - INFO -     val_macro_f    : 0.676281
2024-04-01 19:28:25,441 - trainer - INFO -     val_precision  : 0.710688
2024-04-01 19:28:25,441 - trainer - INFO -     val_recall     : 0.689635
2024-04-01 19:28:25,441 - trainer - INFO -     val_doc_entropy: 2.556229
2024-04-01 19:28:25,441 - trainer - INFO -     test_loss      : 1.072172
2024-04-01 19:28:25,441 - trainer - INFO -     test_accuracy  : 0.69073
2024-04-01 19:28:25,441 - trainer - INFO -     test_macro_f   : 0.676089
2024-04-01 19:28:25,441 - trainer - INFO -     test_precision : 0.709308
2024-04-01 19:28:25,441 - trainer - INFO -     test_recall    : 0.69073
2024-04-01 19:28:25,441 - trainer - INFO -     test_doc_entropy: 2.560386
2024-04-01 19:31:37,637 - trainer - INFO -     epoch          : 2
2024-04-01 19:31:37,639 - trainer - INFO -     loss           : 0.884946
2024-04-01 19:31:37,640 - trainer - INFO -     accuracy       : 0.739152
2024-04-01 19:31:37,640 - trainer - INFO -     macro_f        : 0.72979
2024-04-01 19:31:37,640 - trainer - INFO -     precision      : 0.763752
2024-04-01 19:31:37,640 - trainer - INFO -     recall         : 0.739152
2024-04-01 19:31:37,640 - trainer - INFO -     doc_entropy    : 2.378779
2024-04-01 19:31:37,640 - trainer - INFO -     val_loss       : 1.078413
2024-04-01 19:31:37,640 - trainer - INFO -     val_accuracy   : 0.689435
2024-04-01 19:31:37,640 - trainer - INFO -     val_macro_f    : 0.67695
2024-04-01 19:31:37,640 - trainer - INFO -     val_precision  : 0.711534
2024-04-01 19:31:37,641 - trainer - INFO -     val_recall     : 0.689435
2024-04-01 19:31:37,641 - trainer - INFO -     val_doc_entropy: 2.389441
2024-04-01 19:31:37,641 - trainer - INFO -     test_loss      : 1.079054
2024-04-01 19:31:37,641 - trainer - INFO -     test_accuracy  : 0.692423
2024-04-01 19:31:37,641 - trainer - INFO -     test_macro_f   : 0.682181
2024-04-01 19:31:37,641 - trainer - INFO -     test_precision : 0.718684
2024-04-01 19:31:37,642 - trainer - INFO -     test_recall    : 0.692423
2024-04-01 19:31:37,642 - trainer - INFO -     test_doc_entropy: 2.39396
2024-04-01 19:34:40,963 - trainer - INFO -     epoch          : 3
2024-04-01 19:34:40,964 - trainer - INFO -     loss           : 0.673135
2024-04-01 19:34:40,964 - trainer - INFO -     accuracy       : 0.800151
2024-04-01 19:34:40,965 - trainer - INFO -     macro_f        : 0.79395
2024-04-01 19:34:40,965 - trainer - INFO -     precision      : 0.824182
2024-04-01 19:34:40,965 - trainer - INFO -     recall         : 0.800151
2024-04-01 19:34:40,965 - trainer - INFO -     doc_entropy    : 2.227021
2024-04-01 19:34:40,966 - trainer - INFO -     val_loss       : 1.159046
2024-04-01 19:34:40,966 - trainer - INFO -     val_accuracy   : 0.676889
2024-04-01 19:34:40,966 - trainer - INFO -     val_macro_f    : 0.669863
2024-04-01 19:34:40,966 - trainer - INFO -     val_precision  : 0.710953
2024-04-01 19:34:40,967 - trainer - INFO -     val_recall     : 0.676889
2024-04-01 19:34:40,967 - trainer - INFO -     val_doc_entropy: 2.35358
2024-04-01 19:34:40,967 - trainer - INFO -     test_loss      : 1.154203
2024-04-01 19:34:40,967 - trainer - INFO -     test_accuracy  : 0.67903
2024-04-01 19:34:40,968 - trainer - INFO -     test_macro_f   : 0.671798
2024-04-01 19:34:40,968 - trainer - INFO -     test_precision : 0.71293
2024-04-01 19:34:40,968 - trainer - INFO -     test_recall    : 0.67903
2024-04-01 19:34:40,969 - trainer - INFO -     test_doc_entropy: 2.356567
2024-04-01 19:35:21,112 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=400, bias=True)
    (1): Tanh()
    (2): Linear(in_features=400, out_features=20, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,616,103
Freeze params: 0
2024-04-01 19:38:37,258 - trainer - INFO -     epoch          : 1
2024-04-01 19:38:37,261 - trainer - INFO -     loss           : 1.207969
2024-04-01 19:38:37,261 - trainer - INFO -     accuracy       : 0.655854
2024-04-01 19:38:37,261 - trainer - INFO -     macro_f        : 0.636727
2024-04-01 19:38:37,262 - trainer - INFO -     precision      : 0.667364
2024-04-01 19:38:37,262 - trainer - INFO -     recall         : 0.655854
2024-04-01 19:38:37,262 - trainer - INFO -     doc_entropy    : 2.602157
2024-04-01 19:38:37,262 - trainer - INFO -     val_loss       : 1.067384
2024-04-01 19:38:37,262 - trainer - INFO -     val_accuracy   : 0.689535
2024-04-01 19:38:37,263 - trainer - INFO -     val_macro_f    : 0.67706
2024-04-01 19:38:37,263 - trainer - INFO -     val_precision  : 0.710739
2024-04-01 19:38:37,263 - trainer - INFO -     val_recall     : 0.689535
2024-04-01 19:38:37,263 - trainer - INFO -     val_doc_entropy: 2.561695
2024-04-01 19:38:37,263 - trainer - INFO -     test_loss      : 1.07291
2024-04-01 19:38:37,263 - trainer - INFO -     test_accuracy  : 0.687743
2024-04-01 19:38:37,264 - trainer - INFO -     test_macro_f   : 0.675233
2024-04-01 19:38:37,264 - trainer - INFO -     test_precision : 0.709399
2024-04-01 19:38:37,264 - trainer - INFO -     test_recall    : 0.687743
2024-04-01 19:38:37,264 - trainer - INFO -     test_doc_entropy: 2.566154
2024-04-01 19:41:39,746 - trainer - INFO -     epoch          : 2
2024-04-01 19:41:39,746 - trainer - INFO -     loss           : 0.883435
2024-04-01 19:41:39,746 - trainer - INFO -     accuracy       : 0.740129
2024-04-01 19:41:39,746 - trainer - INFO -     macro_f        : 0.730475
2024-04-01 19:41:39,747 - trainer - INFO -     precision      : 0.763914
2024-04-01 19:41:39,747 - trainer - INFO -     recall         : 0.740129
2024-04-01 19:41:39,747 - trainer - INFO -     doc_entropy    : 2.449207
2024-04-01 19:41:39,747 - trainer - INFO -     val_loss       : 1.087737
2024-04-01 19:41:39,747 - trainer - INFO -     val_accuracy   : 0.687394
2024-04-01 19:41:39,747 - trainer - INFO -     val_macro_f    : 0.677864
2024-04-01 19:41:39,749 - trainer - INFO -     val_precision  : 0.715449
2024-04-01 19:41:39,749 - trainer - INFO -     val_recall     : 0.687394
2024-04-01 19:41:39,749 - trainer - INFO -     val_doc_entropy: 2.50967
2024-04-01 19:41:39,749 - trainer - INFO -     test_loss      : 1.093096
2024-04-01 19:41:39,749 - trainer - INFO -     test_accuracy  : 0.684059
2024-04-01 19:41:39,750 - trainer - INFO -     test_macro_f   : 0.674367
2024-04-01 19:41:39,750 - trainer - INFO -     test_precision : 0.712537
2024-04-01 19:41:39,750 - trainer - INFO -     test_recall    : 0.684059
2024-04-01 19:41:39,750 - trainer - INFO -     test_doc_entropy: 2.514036
2024-04-01 19:44:35,840 - trainer - INFO -     epoch          : 3
2024-04-01 19:44:35,840 - trainer - INFO -     loss           : 0.67627
2024-04-01 19:44:35,840 - trainer - INFO -     accuracy       : 0.798414
2024-04-01 19:44:35,840 - trainer - INFO -     macro_f        : 0.792526
2024-04-01 19:44:35,840 - trainer - INFO -     precision      : 0.823227
2024-04-01 19:44:35,840 - trainer - INFO -     recall         : 0.798414
2024-04-01 19:44:35,840 - trainer - INFO -     doc_entropy    : 2.294297
2024-04-01 19:44:35,840 - trainer - INFO -     val_loss       : 1.179101
2024-04-01 19:44:35,840 - trainer - INFO -     val_accuracy   : 0.676939
2024-04-01 19:44:35,840 - trainer - INFO -     val_macro_f    : 0.665414
2024-04-01 19:44:35,840 - trainer - INFO -     val_precision  : 0.701792
2024-04-01 19:44:35,840 - trainer - INFO -     val_recall     : 0.676939
2024-04-01 19:44:35,840 - trainer - INFO -     val_doc_entropy: 2.348465
2024-04-01 19:44:35,840 - trainer - INFO -     test_loss      : 1.173464
2024-04-01 19:44:35,840 - trainer - INFO -     test_accuracy  : 0.678682
2024-04-01 19:44:35,840 - trainer - INFO -     test_macro_f   : 0.667714
2024-04-01 19:44:35,840 - trainer - INFO -     test_precision : 0.707418
2024-04-01 19:44:35,840 - trainer - INFO -     test_recall    : 0.678682
2024-04-01 19:44:35,840 - trainer - INFO -     test_doc_entropy: 2.352733
2024-04-01 19:45:13,434 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=400, bias=True)
    (1): Tanh()
    (2): Linear(in_features=400, out_features=20, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,616,103
Freeze params: 0
2024-04-01 19:48:12,018 - trainer - INFO -     epoch          : 1
2024-04-01 19:48:12,018 - trainer - INFO -     loss           : 1.210756
2024-04-01 19:48:12,018 - trainer - INFO -     accuracy       : 0.655898
2024-04-01 19:48:12,018 - trainer - INFO -     macro_f        : 0.63645
2024-04-01 19:48:12,018 - trainer - INFO -     precision      : 0.66703
2024-04-01 19:48:12,018 - trainer - INFO -     recall         : 0.655898
2024-04-01 19:48:12,018 - trainer - INFO -     doc_entropy    : 2.598814
2024-04-01 19:48:12,018 - trainer - INFO -     val_loss       : 1.068842
2024-04-01 19:48:12,018 - trainer - INFO -     val_accuracy   : 0.689236
2024-04-01 19:48:12,018 - trainer - INFO -     val_macro_f    : 0.678867
2024-04-01 19:48:12,018 - trainer - INFO -     val_precision  : 0.715482
2024-04-01 19:48:12,018 - trainer - INFO -     val_recall     : 0.689236
2024-04-01 19:48:12,018 - trainer - INFO -     val_doc_entropy: 2.53072
2024-04-01 19:48:12,018 - trainer - INFO -     test_loss      : 1.076955
2024-04-01 19:48:12,018 - trainer - INFO -     test_accuracy  : 0.690083
2024-04-01 19:48:12,018 - trainer - INFO -     test_macro_f   : 0.679861
2024-04-01 19:48:12,018 - trainer - INFO -     test_precision : 0.718342
2024-04-01 19:48:12,018 - trainer - INFO -     test_recall    : 0.690083
2024-04-01 19:48:12,018 - trainer - INFO -     test_doc_entropy: 2.535288
2024-04-01 19:51:27,384 - trainer - INFO -     epoch          : 2
2024-04-01 19:51:27,384 - trainer - INFO -     loss           : 0.881995
2024-04-01 19:51:27,384 - trainer - INFO -     accuracy       : 0.73998
2024-04-01 19:51:27,384 - trainer - INFO -     macro_f        : 0.730908
2024-04-01 19:51:27,384 - trainer - INFO -     precision      : 0.76521
2024-04-01 19:51:27,384 - trainer - INFO -     recall         : 0.73998
2024-04-01 19:51:27,384 - trainer - INFO -     doc_entropy    : 2.342184
2024-04-01 19:51:27,384 - trainer - INFO -     val_loss       : 1.07701
2024-04-01 19:51:27,384 - trainer - INFO -     val_accuracy   : 0.686747
2024-04-01 19:51:27,384 - trainer - INFO -     val_macro_f    : 0.677292
2024-04-01 19:51:27,384 - trainer - INFO -     val_precision  : 0.716125
2024-04-01 19:51:27,384 - trainer - INFO -     val_recall     : 0.686747
2024-04-01 19:51:27,384 - trainer - INFO -     val_doc_entropy: 2.347351
2024-04-01 19:51:27,384 - trainer - INFO -     test_loss      : 1.077477
2024-04-01 19:51:27,384 - trainer - INFO -     test_accuracy  : 0.687394
2024-04-01 19:51:27,384 - trainer - INFO -     test_macro_f   : 0.678246
2024-04-01 19:51:27,384 - trainer - INFO -     test_precision : 0.718446
2024-04-01 19:51:27,384 - trainer - INFO -     test_recall    : 0.687394
2024-04-01 19:51:27,384 - trainer - INFO -     test_doc_entropy: 2.350926
2024-04-01 19:54:28,785 - trainer - INFO -     epoch          : 3
2024-04-01 19:54:28,785 - trainer - INFO -     loss           : 0.672207
2024-04-01 19:54:28,785 - trainer - INFO -     accuracy       : 0.800188
2024-04-01 19:54:28,785 - trainer - INFO -     macro_f        : 0.794053
2024-04-01 19:54:28,785 - trainer - INFO -     precision      : 0.824616
2024-04-01 19:54:28,785 - trainer - INFO -     recall         : 0.800188
2024-04-01 19:54:28,785 - trainer - INFO -     doc_entropy    : 2.209253
2024-04-01 19:54:28,785 - trainer - INFO -     val_loss       : 1.168776
2024-04-01 19:54:28,785 - trainer - INFO -     val_accuracy   : 0.673753
2024-04-01 19:54:28,785 - trainer - INFO -     val_macro_f    : 0.668441
2024-04-01 19:54:28,785 - trainer - INFO -     val_precision  : 0.712425
2024-04-01 19:54:28,785 - trainer - INFO -     val_recall     : 0.673753
2024-04-01 19:54:28,785 - trainer - INFO -     val_doc_entropy: 2.321017
2024-04-01 19:54:28,785 - trainer - INFO -     test_loss      : 1.176091
2024-04-01 19:54:28,785 - trainer - INFO -     test_accuracy  : 0.67669
2024-04-01 19:54:28,785 - trainer - INFO -     test_macro_f   : 0.670926
2024-04-01 19:54:28,785 - trainer - INFO -     test_precision : 0.714782
2024-04-01 19:54:28,785 - trainer - INFO -     test_recall    : 0.67669
2024-04-01 19:54:28,785 - trainer - INFO -     test_doc_entropy: 2.323268
2024-04-01 20:06:57,784 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1200, out_features=60, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,920,943
Freeze params: 0
2024-04-01 20:10:00,640 - trainer - INFO -     epoch          : 1
2024-04-01 20:10:00,640 - trainer - INFO -     loss           : 1.222514
2024-04-01 20:10:00,640 - trainer - INFO -     accuracy       : 0.651697
2024-04-01 20:10:00,640 - trainer - INFO -     macro_f        : 0.631997
2024-04-01 20:10:00,640 - trainer - INFO -     precision      : 0.663247
2024-04-01 20:10:00,640 - trainer - INFO -     recall         : 0.651697
2024-04-01 20:10:00,640 - trainer - INFO -     doc_entropy    : 2.403797
2024-04-01 20:10:00,640 - trainer - INFO -     val_loss       : 1.082144
2024-04-01 20:10:00,640 - trainer - INFO -     val_accuracy   : 0.687892
2024-04-01 20:10:00,640 - trainer - INFO -     val_macro_f    : 0.675179
2024-04-01 20:10:00,640 - trainer - INFO -     val_precision  : 0.710667
2024-04-01 20:10:00,640 - trainer - INFO -     val_recall     : 0.687892
2024-04-01 20:10:00,640 - trainer - INFO -     val_doc_entropy: 2.263862
2024-04-01 20:10:00,640 - trainer - INFO -     test_loss      : 1.08538
2024-04-01 20:10:00,640 - trainer - INFO -     test_accuracy  : 0.687743
2024-04-01 20:10:00,640 - trainer - INFO -     test_macro_f   : 0.676178
2024-04-01 20:10:00,640 - trainer - INFO -     test_precision : 0.712095
2024-04-01 20:10:00,640 - trainer - INFO -     test_recall    : 0.687743
2024-04-01 20:10:00,640 - trainer - INFO -     test_doc_entropy: 2.267181
2024-04-01 20:13:07,246 - trainer - INFO -     epoch          : 2
2024-04-01 20:13:07,246 - trainer - INFO -     loss           : 0.891892
2024-04-01 20:13:07,246 - trainer - INFO -     accuracy       : 0.737577
2024-04-01 20:13:07,246 - trainer - INFO -     macro_f        : 0.72828
2024-04-01 20:13:07,246 - trainer - INFO -     precision      : 0.763258
2024-04-01 20:13:07,246 - trainer - INFO -     recall         : 0.737577
2024-04-01 20:13:07,246 - trainer - INFO -     doc_entropy    : 2.069962
2024-04-01 20:13:07,246 - trainer - INFO -     val_loss       : 1.083399
2024-04-01 20:13:07,246 - trainer - INFO -     val_accuracy   : 0.690083
2024-04-01 20:13:07,246 - trainer - INFO -     val_macro_f    : 0.683767
2024-04-01 20:13:07,246 - trainer - INFO -     val_precision  : 0.724127
2024-04-01 20:13:07,246 - trainer - INFO -     val_recall     : 0.690083
2024-04-01 20:13:07,246 - trainer - INFO -     val_doc_entropy: 2.119342
2024-04-01 20:13:07,246 - trainer - INFO -     test_loss      : 1.081289
2024-04-01 20:13:07,246 - trainer - INFO -     test_accuracy  : 0.690581
2024-04-01 20:13:07,246 - trainer - INFO -     test_macro_f   : 0.682821
2024-04-01 20:13:07,246 - trainer - INFO -     test_precision : 0.722799
2024-04-01 20:13:07,246 - trainer - INFO -     test_recall    : 0.690581
2024-04-01 20:13:07,246 - trainer - INFO -     test_doc_entropy: 2.122765
2024-04-01 20:16:18,219 - trainer - INFO -     epoch          : 3
2024-04-01 20:16:18,219 - trainer - INFO -     loss           : 0.680921
2024-04-01 20:16:18,219 - trainer - INFO -     accuracy       : 0.797661
2024-04-01 20:16:18,219 - trainer - INFO -     macro_f        : 0.791691
2024-04-01 20:16:18,219 - trainer - INFO -     precision      : 0.822733
2024-04-01 20:16:18,219 - trainer - INFO -     recall         : 0.797661
2024-04-01 20:16:18,219 - trainer - INFO -     doc_entropy    : 1.886649
2024-04-01 20:16:18,219 - trainer - INFO -     val_loss       : 1.148119
2024-04-01 20:16:18,219 - trainer - INFO -     val_accuracy   : 0.67918
2024-04-01 20:16:18,219 - trainer - INFO -     val_macro_f    : 0.672193
2024-04-01 20:16:18,219 - trainer - INFO -     val_precision  : 0.71212
2024-04-01 20:16:18,219 - trainer - INFO -     val_recall     : 0.67918
2024-04-01 20:16:18,219 - trainer - INFO -     val_doc_entropy: 1.991019
2024-04-01 20:16:18,219 - trainer - INFO -     test_loss      : 1.146041
2024-04-01 20:16:18,219 - trainer - INFO -     test_accuracy  : 0.679727
2024-04-01 20:16:18,219 - trainer - INFO -     test_macro_f   : 0.674754
2024-04-01 20:16:18,219 - trainer - INFO -     test_precision : 0.718502
2024-04-01 20:16:18,219 - trainer - INFO -     test_recall    : 0.679727
2024-04-01 20:16:18,219 - trainer - INFO -     test_doc_entropy: 1.994766
2024-04-01 20:16:52,885 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1200, out_features=60, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,920,943
Freeze params: 0
2024-04-01 20:20:01,067 - trainer - INFO -     epoch          : 1
2024-04-01 20:20:01,067 - trainer - INFO -     loss           : 1.220914
2024-04-01 20:20:01,067 - trainer - INFO -     accuracy       : 0.65329
2024-04-01 20:20:01,067 - trainer - INFO -     macro_f        : 0.634064
2024-04-01 20:20:01,067 - trainer - INFO -     precision      : 0.665491
2024-04-01 20:20:01,067 - trainer - INFO -     recall         : 0.65329
2024-04-01 20:20:01,067 - trainer - INFO -     doc_entropy    : 2.258519
2024-04-01 20:20:01,067 - trainer - INFO -     val_loss       : 1.088279
2024-04-01 20:20:01,067 - trainer - INFO -     val_accuracy   : 0.686448
2024-04-01 20:20:01,067 - trainer - INFO -     val_macro_f    : 0.678182
2024-04-01 20:20:01,067 - trainer - INFO -     val_precision  : 0.718198
2024-04-01 20:20:01,067 - trainer - INFO -     val_recall     : 0.686448
2024-04-01 20:20:01,067 - trainer - INFO -     val_doc_entropy: 2.034874
2024-04-01 20:20:01,067 - trainer - INFO -     test_loss      : 1.091123
2024-04-01 20:20:01,067 - trainer - INFO -     test_accuracy  : 0.68381
2024-04-01 20:20:01,067 - trainer - INFO -     test_macro_f   : 0.675367
2024-04-01 20:20:01,067 - trainer - INFO -     test_precision : 0.714688
2024-04-01 20:20:01,067 - trainer - INFO -     test_recall    : 0.68381
2024-04-01 20:20:01,067 - trainer - INFO -     test_doc_entropy: 2.037844
2024-04-01 20:23:10,279 - trainer - INFO -     epoch          : 2
2024-04-01 20:23:10,279 - trainer - INFO -     loss           : 0.891421
2024-04-01 20:23:10,279 - trainer - INFO -     accuracy       : 0.737646
2024-04-01 20:23:10,279 - trainer - INFO -     macro_f        : 0.728167
2024-04-01 20:23:10,279 - trainer - INFO -     precision      : 0.76276
2024-04-01 20:23:10,279 - trainer - INFO -     recall         : 0.737646
2024-04-01 20:23:10,279 - trainer - INFO -     doc_entropy    : 1.976756
2024-04-01 20:23:10,279 - trainer - INFO -     val_loss       : 1.088807
2024-04-01 20:23:10,279 - trainer - INFO -     val_accuracy   : 0.687444
2024-04-01 20:23:10,279 - trainer - INFO -     val_macro_f    : 0.675261
2024-04-01 20:23:10,279 - trainer - INFO -     val_precision  : 0.709913
2024-04-01 20:23:10,279 - trainer - INFO -     val_recall     : 0.687444
2024-04-01 20:23:10,279 - trainer - INFO -     val_doc_entropy: 2.088467
2024-04-01 20:23:10,279 - trainer - INFO -     test_loss      : 1.084521
2024-04-01 20:23:10,279 - trainer - INFO -     test_accuracy  : 0.687892
2024-04-01 20:23:10,279 - trainer - INFO -     test_macro_f   : 0.675229
2024-04-01 20:23:10,279 - trainer - INFO -     test_precision : 0.710691
2024-04-01 20:23:10,279 - trainer - INFO -     test_recall    : 0.687892
2024-04-01 20:23:10,279 - trainer - INFO -     test_doc_entropy: 2.093508
2024-04-01 20:26:19,753 - trainer - INFO -     epoch          : 3
2024-04-01 20:26:19,753 - trainer - INFO -     loss           : 0.679175
2024-04-01 20:26:19,768 - trainer - INFO -     accuracy       : 0.797742
2024-04-01 20:26:19,768 - trainer - INFO -     macro_f        : 0.79201
2024-04-01 20:26:19,768 - trainer - INFO -     precision      : 0.822945
2024-04-01 20:26:19,768 - trainer - INFO -     recall         : 0.797742
2024-04-01 20:26:19,768 - trainer - INFO -     doc_entropy    : 1.841363
2024-04-01 20:26:19,768 - trainer - INFO -     val_loss       : 1.168475
2024-04-01 20:26:19,768 - trainer - INFO -     val_accuracy   : 0.678283
2024-04-01 20:26:19,768 - trainer - INFO -     val_macro_f    : 0.667834
2024-04-01 20:26:19,768 - trainer - INFO -     val_precision  : 0.704604
2024-04-01 20:26:19,768 - trainer - INFO -     val_recall     : 0.678283
2024-04-01 20:26:19,768 - trainer - INFO -     val_doc_entropy: 2.000683
2024-04-01 20:26:19,768 - trainer - INFO -     test_loss      : 1.162546
2024-04-01 20:26:19,768 - trainer - INFO -     test_accuracy  : 0.678034
2024-04-01 20:26:19,768 - trainer - INFO -     test_macro_f   : 0.667414
2024-04-01 20:26:19,768 - trainer - INFO -     test_precision : 0.705387
2024-04-01 20:26:19,768 - trainer - INFO -     test_recall    : 0.678034
2024-04-01 20:26:19,768 - trainer - INFO -     test_doc_entropy: 2.003146
2024-04-01 20:26:55,647 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1200, out_features=60, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,920,943
Freeze params: 0
2024-04-01 20:30:05,403 - trainer - INFO -     epoch          : 1
2024-04-01 20:30:05,403 - trainer - INFO -     loss           : 1.225072
2024-04-01 20:30:05,403 - trainer - INFO -     accuracy       : 0.652904
2024-04-01 20:30:05,403 - trainer - INFO -     macro_f        : 0.633526
2024-04-01 20:30:05,403 - trainer - INFO -     precision      : 0.66484
2024-04-01 20:30:05,403 - trainer - INFO -     recall         : 0.652904
2024-04-01 20:30:05,403 - trainer - INFO -     doc_entropy    : 2.279861
2024-04-01 20:30:05,403 - trainer - INFO -     val_loss       : 1.08836
2024-04-01 20:30:05,403 - trainer - INFO -     val_accuracy   : 0.684706
2024-04-01 20:30:05,403 - trainer - INFO -     val_macro_f    : 0.672472
2024-04-01 20:30:05,403 - trainer - INFO -     val_precision  : 0.707934
2024-04-01 20:30:05,403 - trainer - INFO -     val_recall     : 0.684706
2024-04-01 20:30:05,403 - trainer - INFO -     val_doc_entropy: 2.066393
2024-04-01 20:30:05,403 - trainer - INFO -     test_loss      : 1.083357
2024-04-01 20:30:05,403 - trainer - INFO -     test_accuracy  : 0.68615
2024-04-01 20:30:05,403 - trainer - INFO -     test_macro_f   : 0.674516
2024-04-01 20:30:05,403 - trainer - INFO -     test_precision : 0.710448
2024-04-01 20:30:05,403 - trainer - INFO -     test_recall    : 0.68615
2024-04-01 20:30:05,403 - trainer - INFO -     test_doc_entropy: 2.070939
2024-04-01 20:33:14,940 - trainer - INFO -     epoch          : 2
2024-04-01 20:33:14,940 - trainer - INFO -     loss           : 0.893402
2024-04-01 20:33:14,940 - trainer - INFO -     accuracy       : 0.737459
2024-04-01 20:33:14,940 - trainer - INFO -     macro_f        : 0.727746
2024-04-01 20:33:14,940 - trainer - INFO -     precision      : 0.761587
2024-04-01 20:33:14,940 - trainer - INFO -     recall         : 0.737459
2024-04-01 20:33:14,940 - trainer - INFO -     doc_entropy    : 1.95436
2024-04-01 20:33:14,940 - trainer - INFO -     val_loss       : 1.077623
2024-04-01 20:33:14,940 - trainer - INFO -     val_accuracy   : 0.688639
2024-04-01 20:33:14,940 - trainer - INFO -     val_macro_f    : 0.675672
2024-04-01 20:33:14,940 - trainer - INFO -     val_precision  : 0.710421
2024-04-01 20:33:14,940 - trainer - INFO -     val_recall     : 0.688639
2024-04-01 20:33:14,940 - trainer - INFO -     val_doc_entropy: 1.9823
2024-04-01 20:33:14,940 - trainer - INFO -     test_loss      : 1.074717
2024-04-01 20:33:14,940 - trainer - INFO -     test_accuracy  : 0.688241
2024-04-01 20:33:14,940 - trainer - INFO -     test_macro_f   : 0.674303
2024-04-01 20:33:14,940 - trainer - INFO -     test_precision : 0.709604
2024-04-01 20:33:14,940 - trainer - INFO -     test_recall    : 0.688241
2024-04-01 20:33:14,940 - trainer - INFO -     test_doc_entropy: 1.986116
2024-04-01 20:36:25,785 - trainer - INFO -     epoch          : 3
2024-04-01 20:36:25,785 - trainer - INFO -     loss           : 0.67804
2024-04-01 20:36:25,785 - trainer - INFO -     accuracy       : 0.797891
2024-04-01 20:36:25,785 - trainer - INFO -     macro_f        : 0.792289
2024-04-01 20:36:25,785 - trainer - INFO -     precision      : 0.823822
2024-04-01 20:36:25,785 - trainer - INFO -     recall         : 0.797891
2024-04-01 20:36:25,785 - trainer - INFO -     doc_entropy    : 1.800944
2024-04-01 20:36:25,785 - trainer - INFO -     val_loss       : 1.16299
2024-04-01 20:36:25,785 - trainer - INFO -     val_accuracy   : 0.680972
2024-04-01 20:36:25,785 - trainer - INFO -     val_macro_f    : 0.66949
2024-04-01 20:36:25,785 - trainer - INFO -     val_precision  : 0.705172
2024-04-01 20:36:25,785 - trainer - INFO -     val_recall     : 0.680972
2024-04-01 20:36:25,785 - trainer - INFO -     val_doc_entropy: 1.929237
2024-04-01 20:36:25,785 - trainer - INFO -     test_loss      : 1.167516
2024-04-01 20:36:25,785 - trainer - INFO -     test_accuracy  : 0.679279
2024-04-01 20:36:25,785 - trainer - INFO -     test_macro_f   : 0.669772
2024-04-01 20:36:25,785 - trainer - INFO -     test_precision : 0.709699
2024-04-01 20:36:25,785 - trainer - INFO -     test_recall    : 0.679279
2024-04-01 20:36:25,785 - trainer - INFO -     test_doc_entropy: 1.931115
2024-04-01 20:36:59,792 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1200, out_features=60, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,920,943
Freeze params: 0
2024-04-01 20:40:08,911 - trainer - INFO -     epoch          : 1
2024-04-01 20:40:08,911 - trainer - INFO -     loss           : 1.225864
2024-04-01 20:40:08,911 - trainer - INFO -     accuracy       : 0.651709
2024-04-01 20:40:08,911 - trainer - INFO -     macro_f        : 0.632068
2024-04-01 20:40:08,911 - trainer - INFO -     precision      : 0.663852
2024-04-01 20:40:08,911 - trainer - INFO -     recall         : 0.651709
2024-04-01 20:40:08,911 - trainer - INFO -     doc_entropy    : 2.40373
2024-04-01 20:40:08,911 - trainer - INFO -     val_loss       : 1.079172
2024-04-01 20:40:08,911 - trainer - INFO -     val_accuracy   : 0.686847
2024-04-01 20:40:08,911 - trainer - INFO -     val_macro_f    : 0.675484
2024-04-01 20:40:08,911 - trainer - INFO -     val_precision  : 0.710128
2024-04-01 20:40:08,911 - trainer - INFO -     val_recall     : 0.686847
2024-04-01 20:40:08,911 - trainer - INFO -     val_doc_entropy: 2.318026
2024-04-01 20:40:08,911 - trainer - INFO -     test_loss      : 1.07624
2024-04-01 20:40:08,911 - trainer - INFO -     test_accuracy  : 0.689037
2024-04-01 20:40:08,911 - trainer - INFO -     test_macro_f   : 0.677077
2024-04-01 20:40:08,911 - trainer - INFO -     test_precision : 0.710409
2024-04-01 20:40:08,911 - trainer - INFO -     test_recall    : 0.689037
2024-04-01 20:40:08,911 - trainer - INFO -     test_doc_entropy: 2.321949
2024-04-01 20:43:46,446 - trainer - INFO -     epoch          : 2
2024-04-01 20:43:46,449 - trainer - INFO -     loss           : 0.894835
2024-04-01 20:43:46,449 - trainer - INFO -     accuracy       : 0.736519
2024-04-01 20:43:46,449 - trainer - INFO -     macro_f        : 0.727137
2024-04-01 20:43:46,450 - trainer - INFO -     precision      : 0.762202
2024-04-01 20:43:46,450 - trainer - INFO -     recall         : 0.736519
2024-04-01 20:43:46,450 - trainer - INFO -     doc_entropy    : 2.139226
2024-04-01 20:43:46,451 - trainer - INFO -     val_loss       : 1.095646
2024-04-01 20:43:46,451 - trainer - INFO -     val_accuracy   : 0.687295
2024-04-01 20:43:46,451 - trainer - INFO -     val_macro_f    : 0.680943
2024-04-01 20:43:46,452 - trainer - INFO -     val_precision  : 0.722129
2024-04-01 20:43:46,452 - trainer - INFO -     val_recall     : 0.687295
2024-04-01 20:43:46,452 - trainer - INFO -     val_doc_entropy: 2.150528
2024-04-01 20:43:46,453 - trainer - INFO -     test_loss      : 1.091431
2024-04-01 20:43:46,453 - trainer - INFO -     test_accuracy  : 0.687942
2024-04-01 20:43:46,453 - trainer - INFO -     test_macro_f   : 0.679195
2024-04-01 20:43:46,453 - trainer - INFO -     test_precision : 0.717016
2024-04-01 20:43:46,453 - trainer - INFO -     test_recall    : 0.687942
2024-04-01 20:43:46,454 - trainer - INFO -     test_doc_entropy: 2.154314
2024-04-01 20:47:43,270 - trainer - INFO -     epoch          : 3
2024-04-01 20:47:43,271 - trainer - INFO -     loss           : 0.683092
2024-04-01 20:47:43,271 - trainer - INFO -     accuracy       : 0.797966
2024-04-01 20:47:43,271 - trainer - INFO -     macro_f        : 0.792009
2024-04-01 20:47:43,272 - trainer - INFO -     precision      : 0.822819
2024-04-01 20:47:43,272 - trainer - INFO -     recall         : 0.797966
2024-04-01 20:47:43,272 - trainer - INFO -     doc_entropy    : 1.920301
2024-04-01 20:47:43,272 - trainer - INFO -     val_loss       : 1.168532
2024-04-01 20:47:43,272 - trainer - INFO -     val_accuracy   : 0.671413
2024-04-01 20:47:43,272 - trainer - INFO -     val_macro_f    : 0.664116
2024-04-01 20:47:43,272 - trainer - INFO -     val_precision  : 0.704247
2024-04-01 20:47:43,272 - trainer - INFO -     val_recall     : 0.671413
2024-04-01 20:47:43,272 - trainer - INFO -     val_doc_entropy: 2.021921
2024-04-01 20:47:43,272 - trainer - INFO -     test_loss      : 1.162787
2024-04-01 20:47:43,272 - trainer - INFO -     test_accuracy  : 0.67435
2024-04-01 20:47:43,272 - trainer - INFO -     test_macro_f   : 0.668149
2024-04-01 20:47:43,272 - trainer - INFO -     test_precision : 0.710672
2024-04-01 20:47:43,272 - trainer - INFO -     test_recall    : 0.67435
2024-04-01 20:47:43,272 - trainer - INFO -     test_doc_entropy: 2.025629
2024-04-01 20:48:28,477 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1200, out_features=60, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,920,943
Freeze params: 0
2024-04-01 20:52:15,323 - trainer - INFO -     epoch          : 1
2024-04-01 20:52:15,325 - trainer - INFO -     loss           : 1.22479
2024-04-01 20:52:15,325 - trainer - INFO -     accuracy       : 0.651298
2024-04-01 20:52:15,325 - trainer - INFO -     macro_f        : 0.631416
2024-04-01 20:52:15,325 - trainer - INFO -     precision      : 0.662917
2024-04-01 20:52:15,325 - trainer - INFO -     recall         : 0.651298
2024-04-01 20:52:15,326 - trainer - INFO -     doc_entropy    : 2.317794
2024-04-01 20:52:15,326 - trainer - INFO -     val_loss       : 1.082827
2024-04-01 20:52:15,326 - trainer - INFO -     val_accuracy   : 0.685502
2024-04-01 20:52:15,326 - trainer - INFO -     val_macro_f    : 0.669341
2024-04-01 20:52:15,326 - trainer - INFO -     val_precision  : 0.702015
2024-04-01 20:52:15,327 - trainer - INFO -     val_recall     : 0.685502
2024-04-01 20:52:15,327 - trainer - INFO -     val_doc_entropy: 2.248197
2024-04-01 20:52:15,327 - trainer - INFO -     test_loss      : 1.090191
2024-04-01 20:52:15,327 - trainer - INFO -     test_accuracy  : 0.682963
2024-04-01 20:52:15,327 - trainer - INFO -     test_macro_f   : 0.6675
2024-04-01 20:52:15,327 - trainer - INFO -     test_precision : 0.699779
2024-04-01 20:52:15,328 - trainer - INFO -     test_recall    : 0.682963
2024-04-01 20:52:15,328 - trainer - INFO -     test_doc_entropy: 2.251609
2024-04-01 20:55:50,123 - trainer - INFO -     epoch          : 2
2024-04-01 20:55:50,126 - trainer - INFO -     loss           : 0.897786
2024-04-01 20:55:50,126 - trainer - INFO -     accuracy       : 0.736463
2024-04-01 20:55:50,128 - trainer - INFO -     macro_f        : 0.726847
2024-04-01 20:55:50,128 - trainer - INFO -     precision      : 0.761017
2024-04-01 20:55:50,128 - trainer - INFO -     recall         : 0.736463
2024-04-01 20:55:50,128 - trainer - INFO -     doc_entropy    : 2.000724
2024-04-01 20:55:50,129 - trainer - INFO -     val_loss       : 1.079433
2024-04-01 20:55:50,129 - trainer - INFO -     val_accuracy   : 0.689186
2024-04-01 20:55:50,129 - trainer - INFO -     val_macro_f    : 0.678496
2024-04-01 20:55:50,129 - trainer - INFO -     val_precision  : 0.715331
2024-04-01 20:55:50,129 - trainer - INFO -     val_recall     : 0.689186
2024-04-01 20:55:50,130 - trainer - INFO -     val_doc_entropy: 2.081055
2024-04-01 20:55:50,130 - trainer - INFO -     test_loss      : 1.083644
2024-04-01 20:55:50,130 - trainer - INFO -     test_accuracy  : 0.685801
2024-04-01 20:55:50,130 - trainer - INFO -     test_macro_f   : 0.675751
2024-04-01 20:55:50,130 - trainer - INFO -     test_precision : 0.713982
2024-04-01 20:55:50,131 - trainer - INFO -     test_recall    : 0.685801
2024-04-01 20:55:50,131 - trainer - INFO -     test_doc_entropy: 2.084548
2024-04-01 21:00:04,003 - trainer - INFO -     epoch          : 3
2024-04-01 21:00:04,004 - trainer - INFO -     loss           : 0.682293
2024-04-01 21:00:04,004 - trainer - INFO -     accuracy       : 0.796734
2024-04-01 21:00:04,004 - trainer - INFO -     macro_f        : 0.790797
2024-04-01 21:00:04,005 - trainer - INFO -     precision      : 0.821883
2024-04-01 21:00:04,005 - trainer - INFO -     recall         : 0.796734
2024-04-01 21:00:04,005 - trainer - INFO -     doc_entropy    : 1.81537
2024-04-01 21:00:04,006 - trainer - INFO -     val_loss       : 1.176304
2024-04-01 21:00:04,006 - trainer - INFO -     val_accuracy   : 0.675894
2024-04-01 21:00:04,006 - trainer - INFO -     val_macro_f    : 0.66837
2024-04-01 21:00:04,007 - trainer - INFO -     val_precision  : 0.709543
2024-04-01 21:00:04,007 - trainer - INFO -     val_recall     : 0.675894
2024-04-01 21:00:04,007 - trainer - INFO -     val_doc_entropy: 1.919921
2024-04-01 21:00:04,007 - trainer - INFO -     test_loss      : 1.169134
2024-04-01 21:00:04,008 - trainer - INFO -     test_accuracy  : 0.671911
2024-04-01 21:00:04,008 - trainer - INFO -     test_macro_f   : 0.663766
2024-04-01 21:00:04,008 - trainer - INFO -     test_precision : 0.706257
2024-04-01 21:00:04,008 - trainer - INFO -     test_recall    : 0.671911
2024-04-01 21:00:04,008 - trainer - INFO -     test_doc_entropy: 1.923122
2024-04-01 21:01:52,384 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1600, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1600, out_features=80, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,097,363
Freeze params: 0
2024-04-01 21:06:02,408 - trainer - INFO -     epoch          : 1
2024-04-01 21:06:02,408 - trainer - INFO -     loss           : 1.232311
2024-04-01 21:06:02,409 - trainer - INFO -     accuracy       : 0.650041
2024-04-01 21:06:02,409 - trainer - INFO -     macro_f        : 0.630389
2024-04-01 21:06:02,409 - trainer - INFO -     precision      : 0.661952
2024-04-01 21:06:02,409 - trainer - INFO -     recall         : 0.650041
2024-04-01 21:06:02,409 - trainer - INFO -     doc_entropy    : 2.063002
2024-04-01 21:06:02,409 - trainer - INFO -     val_loss       : 1.087008
2024-04-01 21:06:02,409 - trainer - INFO -     val_accuracy   : 0.684556
2024-04-01 21:06:02,410 - trainer - INFO -     val_macro_f    : 0.670066
2024-04-01 21:06:02,410 - trainer - INFO -     val_precision  : 0.702554
2024-04-01 21:06:02,410 - trainer - INFO -     val_recall     : 0.684556
2024-04-01 21:06:02,410 - trainer - INFO -     val_doc_entropy: 1.796835
2024-04-01 21:06:02,410 - trainer - INFO -     test_loss      : 1.090364
2024-04-01 21:06:02,410 - trainer - INFO -     test_accuracy  : 0.682665
2024-04-01 21:06:02,410 - trainer - INFO -     test_macro_f   : 0.66825
2024-04-01 21:06:02,410 - trainer - INFO -     test_precision : 0.700408
2024-04-01 21:06:02,410 - trainer - INFO -     test_recall    : 0.682665
2024-04-01 21:06:02,410 - trainer - INFO -     test_doc_entropy: 1.800027
2024-04-01 21:10:22,182 - trainer - INFO -     epoch          : 2
2024-04-01 21:10:22,184 - trainer - INFO -     loss           : 0.904429
2024-04-01 21:10:22,184 - trainer - INFO -     accuracy       : 0.733283
2024-04-01 21:10:22,185 - trainer - INFO -     macro_f        : 0.723289
2024-04-01 21:10:22,185 - trainer - INFO -     precision      : 0.757614
2024-04-01 21:10:22,185 - trainer - INFO -     recall         : 0.733283
2024-04-01 21:10:22,186 - trainer - INFO -     doc_entropy    : 1.65476
2024-04-01 21:10:22,186 - trainer - INFO -     val_loss       : 1.088794
2024-04-01 21:10:22,186 - trainer - INFO -     val_accuracy   : 0.689734
2024-04-01 21:10:22,186 - trainer - INFO -     val_macro_f    : 0.67117
2024-04-01 21:10:22,187 - trainer - INFO -     val_precision  : 0.701954
2024-04-01 21:10:22,187 - trainer - INFO -     val_recall     : 0.689734
2024-04-01 21:10:22,187 - trainer - INFO -     val_doc_entropy: 1.690521
2024-04-01 21:10:22,187 - trainer - INFO -     test_loss      : 1.090683
2024-04-01 21:10:22,187 - trainer - INFO -     test_accuracy  : 0.685453
2024-04-01 21:10:22,188 - trainer - INFO -     test_macro_f   : 0.66757
2024-04-01 21:10:22,188 - trainer - INFO -     test_precision : 0.698912
2024-04-01 21:10:22,188 - trainer - INFO -     test_recall    : 0.685453
2024-04-01 21:10:22,188 - trainer - INFO -     test_doc_entropy: 1.693184
2024-04-01 21:14:39,870 - trainer - INFO -     epoch          : 3
2024-04-01 21:14:39,870 - trainer - INFO -     loss           : 0.688096
2024-04-01 21:14:39,872 - trainer - INFO -     accuracy       : 0.79618
2024-04-01 21:14:39,872 - trainer - INFO -     macro_f        : 0.790123
2024-04-01 21:14:39,872 - trainer - INFO -     precision      : 0.820869
2024-04-01 21:14:39,872 - trainer - INFO -     recall         : 0.79618
2024-04-01 21:14:39,873 - trainer - INFO -     doc_entropy    : 1.532492
2024-04-01 21:14:39,873 - trainer - INFO -     val_loss       : 1.183991
2024-04-01 21:14:39,873 - trainer - INFO -     val_accuracy   : 0.67898
2024-04-01 21:14:39,873 - trainer - INFO -     val_macro_f    : 0.673458
2024-04-01 21:14:39,874 - trainer - INFO -     val_precision  : 0.715758
2024-04-01 21:14:39,874 - trainer - INFO -     val_recall     : 0.67898
2024-04-01 21:14:39,874 - trainer - INFO -     val_doc_entropy: 1.63014
2024-04-01 21:14:39,874 - trainer - INFO -     test_loss      : 1.185022
2024-04-01 21:14:39,874 - trainer - INFO -     test_accuracy  : 0.677138
2024-04-01 21:14:39,875 - trainer - INFO -     test_macro_f   : 0.672249
2024-04-01 21:14:39,875 - trainer - INFO -     test_precision : 0.715171
2024-04-01 21:14:39,875 - trainer - INFO -     test_recall    : 0.677138
2024-04-01 21:14:39,875 - trainer - INFO -     test_doc_entropy: 1.632366
2024-04-01 21:15:32,406 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1600, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1600, out_features=80, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,097,363
Freeze params: 0
2024-04-01 21:19:44,037 - trainer - INFO -     epoch          : 1
2024-04-01 21:19:44,037 - trainer - INFO -     loss           : 1.230906
2024-04-01 21:19:44,037 - trainer - INFO -     accuracy       : 0.650122
2024-04-01 21:19:44,038 - trainer - INFO -     macro_f        : 0.629976
2024-04-01 21:19:44,038 - trainer - INFO -     precision      : 0.661109
2024-04-01 21:19:44,038 - trainer - INFO -     recall         : 0.650122
2024-04-01 21:19:44,038 - trainer - INFO -     doc_entropy    : 2.140983
2024-04-01 21:19:44,039 - trainer - INFO -     val_loss       : 1.081591
2024-04-01 21:19:44,039 - trainer - INFO -     val_accuracy   : 0.688539
2024-04-01 21:19:44,039 - trainer - INFO -     val_macro_f    : 0.676766
2024-04-01 21:19:44,039 - trainer - INFO -     val_precision  : 0.712802
2024-04-01 21:19:44,039 - trainer - INFO -     val_recall     : 0.688539
2024-04-01 21:19:44,040 - trainer - INFO -     val_doc_entropy: 1.85375
2024-04-01 21:19:44,040 - trainer - INFO -     test_loss      : 1.085479
2024-04-01 21:19:44,040 - trainer - INFO -     test_accuracy  : 0.685303
2024-04-01 21:19:44,040 - trainer - INFO -     test_macro_f   : 0.674952
2024-04-01 21:19:44,041 - trainer - INFO -     test_precision : 0.713353
2024-04-01 21:19:44,041 - trainer - INFO -     test_recall    : 0.685303
2024-04-01 21:19:44,041 - trainer - INFO -     test_doc_entropy: 1.856385
2024-04-01 21:24:03,232 - trainer - INFO -     epoch          : 2
2024-04-01 21:24:03,232 - trainer - INFO -     loss           : 0.90223
2024-04-01 21:24:03,233 - trainer - INFO -     accuracy       : 0.735144
2024-04-01 21:24:03,233 - trainer - INFO -     macro_f        : 0.724764
2024-04-01 21:24:03,233 - trainer - INFO -     precision      : 0.758381
2024-04-01 21:24:03,234 - trainer - INFO -     recall         : 0.735144
2024-04-01 21:24:03,234 - trainer - INFO -     doc_entropy    : 1.713691
2024-04-01 21:24:03,234 - trainer - INFO -     val_loss       : 1.081341
2024-04-01 21:24:03,234 - trainer - INFO -     val_accuracy   : 0.68834
2024-04-01 21:24:03,234 - trainer - INFO -     val_macro_f    : 0.676154
2024-04-01 21:24:03,235 - trainer - INFO -     val_precision  : 0.712875
2024-04-01 21:24:03,235 - trainer - INFO -     val_recall     : 0.68834
2024-04-01 21:24:03,235 - trainer - INFO -     val_doc_entropy: 1.718131
2024-04-01 21:24:03,235 - trainer - INFO -     test_loss      : 1.083949
2024-04-01 21:24:03,236 - trainer - INFO -     test_accuracy  : 0.688141
2024-04-01 21:24:03,236 - trainer - INFO -     test_macro_f   : 0.675782
2024-04-01 21:24:03,236 - trainer - INFO -     test_precision : 0.710536
2024-04-01 21:24:03,236 - trainer - INFO -     test_recall    : 0.688141
2024-04-01 21:24:03,236 - trainer - INFO -     test_doc_entropy: 1.722739
2024-04-01 21:28:19,653 - trainer - INFO -     epoch          : 3
2024-04-01 21:28:19,654 - trainer - INFO -     loss           : 0.688318
2024-04-01 21:28:19,654 - trainer - INFO -     accuracy       : 0.79608
2024-04-01 21:28:19,654 - trainer - INFO -     macro_f        : 0.789476
2024-04-01 21:28:19,654 - trainer - INFO -     precision      : 0.819671
2024-04-01 21:28:19,656 - trainer - INFO -     recall         : 0.79608
2024-04-01 21:28:19,656 - trainer - INFO -     doc_entropy    : 1.579025
2024-04-01 21:28:19,656 - trainer - INFO -     val_loss       : 1.185391
2024-04-01 21:28:19,656 - trainer - INFO -     val_accuracy   : 0.671961
2024-04-01 21:28:19,657 - trainer - INFO -     val_macro_f    : 0.663612
2024-04-01 21:28:19,657 - trainer - INFO -     val_precision  : 0.703299
2024-04-01 21:28:19,657 - trainer - INFO -     val_recall     : 0.671961
2024-04-01 21:28:19,658 - trainer - INFO -     val_doc_entropy: 1.607309
2024-04-01 21:28:19,658 - trainer - INFO -     test_loss      : 1.180222
2024-04-01 21:28:19,658 - trainer - INFO -     test_accuracy  : 0.675943
2024-04-01 21:28:19,658 - trainer - INFO -     test_macro_f   : 0.668581
2024-04-01 21:28:19,659 - trainer - INFO -     test_precision : 0.711578
2024-04-01 21:28:19,659 - trainer - INFO -     test_recall    : 0.675943
2024-04-01 21:28:19,659 - trainer - INFO -     test_doc_entropy: 1.609352
2024-04-01 21:29:12,352 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1600, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1600, out_features=80, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,097,363
Freeze params: 0
2024-04-01 21:33:25,950 - trainer - INFO -     epoch          : 1
2024-04-01 21:33:25,954 - trainer - INFO -     loss           : 1.228663
2024-04-01 21:33:25,956 - trainer - INFO -     accuracy       : 0.651553
2024-04-01 21:33:25,957 - trainer - INFO -     macro_f        : 0.63197
2024-04-01 21:33:25,958 - trainer - INFO -     precision      : 0.663673
2024-04-01 21:33:25,959 - trainer - INFO -     recall         : 0.651553
2024-04-01 21:33:25,960 - trainer - INFO -     doc_entropy    : 2.05852
2024-04-01 21:33:25,960 - trainer - INFO -     val_loss       : 1.109793
2024-04-01 21:33:25,960 - trainer - INFO -     val_accuracy   : 0.67903
2024-04-01 21:33:25,961 - trainer - INFO -     val_macro_f    : 0.667777
2024-04-01 21:33:25,961 - trainer - INFO -     val_precision  : 0.704456
2024-04-01 21:33:25,961 - trainer - INFO -     val_recall     : 0.67903
2024-04-01 21:33:25,961 - trainer - INFO -     val_doc_entropy: 1.835576
2024-04-01 21:33:25,962 - trainer - INFO -     test_loss      : 1.118706
2024-04-01 21:33:25,962 - trainer - INFO -     test_accuracy  : 0.677835
2024-04-01 21:33:25,962 - trainer - INFO -     test_macro_f   : 0.665926
2024-04-01 21:33:25,962 - trainer - INFO -     test_precision : 0.701294
2024-04-01 21:33:25,962 - trainer - INFO -     test_recall    : 0.677835
2024-04-01 21:33:25,963 - trainer - INFO -     test_doc_entropy: 1.839209
2024-04-01 21:37:04,304 - trainer - INFO -     epoch          : 2
2024-04-01 21:37:04,304 - trainer - INFO -     loss           : 0.904377
2024-04-01 21:37:04,304 - trainer - INFO -     accuracy       : 0.734298
2024-04-01 21:37:04,304 - trainer - INFO -     macro_f        : 0.72405
2024-04-01 21:37:04,304 - trainer - INFO -     precision      : 0.757787
2024-04-01 21:37:04,304 - trainer - INFO -     recall         : 0.734298
2024-04-01 21:37:04,304 - trainer - INFO -     doc_entropy    : 1.624187
2024-04-01 21:37:04,304 - trainer - INFO -     val_loss       : 1.091109
2024-04-01 21:37:04,304 - trainer - INFO -     val_accuracy   : 0.687095
2024-04-01 21:37:04,304 - trainer - INFO -     val_macro_f    : 0.680197
2024-04-01 21:37:04,304 - trainer - INFO -     val_precision  : 0.720767
2024-04-01 21:37:04,304 - trainer - INFO -     val_recall     : 0.687095
2024-04-01 21:37:04,304 - trainer - INFO -     val_doc_entropy: 1.654973
2024-04-01 21:37:04,304 - trainer - INFO -     test_loss      : 1.08831
2024-04-01 21:37:04,304 - trainer - INFO -     test_accuracy  : 0.686498
2024-04-01 21:37:04,304 - trainer - INFO -     test_macro_f   : 0.68214
2024-04-01 21:37:04,304 - trainer - INFO -     test_precision : 0.726652
2024-04-01 21:37:04,304 - trainer - INFO -     test_recall    : 0.686498
2024-04-01 21:37:04,304 - trainer - INFO -     test_doc_entropy: 1.656745
2024-04-01 21:40:25,491 - trainer - INFO -     epoch          : 3
2024-04-01 21:40:25,491 - trainer - INFO -     loss           : 0.687811
2024-04-01 21:40:25,491 - trainer - INFO -     accuracy       : 0.796068
2024-04-01 21:40:25,491 - trainer - INFO -     macro_f        : 0.789881
2024-04-01 21:40:25,491 - trainer - INFO -     precision      : 0.820735
2024-04-01 21:40:25,491 - trainer - INFO -     recall         : 0.796068
2024-04-01 21:40:25,491 - trainer - INFO -     doc_entropy    : 1.463174
2024-04-01 21:40:25,491 - trainer - INFO -     val_loss       : 1.175723
2024-04-01 21:40:25,491 - trainer - INFO -     val_accuracy   : 0.678582
2024-04-01 21:40:25,491 - trainer - INFO -     val_macro_f    : 0.673724
2024-04-01 21:40:25,491 - trainer - INFO -     val_precision  : 0.717839
2024-04-01 21:40:25,491 - trainer - INFO -     val_recall     : 0.678582
2024-04-01 21:40:25,491 - trainer - INFO -     val_doc_entropy: 1.60146
2024-04-01 21:40:25,491 - trainer - INFO -     test_loss      : 1.171019
2024-04-01 21:40:25,491 - trainer - INFO -     test_accuracy  : 0.678333
2024-04-01 21:40:25,491 - trainer - INFO -     test_macro_f   : 0.671128
2024-04-01 21:40:25,491 - trainer - INFO -     test_precision : 0.714608
2024-04-01 21:40:25,491 - trainer - INFO -     test_recall    : 0.678333
2024-04-01 21:40:25,491 - trainer - INFO -     test_doc_entropy: 1.603204
2024-04-01 21:41:01,059 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1600, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1600, out_features=80, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,097,363
Freeze params: 0
2024-04-01 21:44:20,275 - trainer - INFO -     epoch          : 1
2024-04-01 21:44:20,275 - trainer - INFO -     loss           : 1.231903
2024-04-01 21:44:20,275 - trainer - INFO -     accuracy       : 0.651678
2024-04-01 21:44:20,275 - trainer - INFO -     macro_f        : 0.631545
2024-04-01 21:44:20,275 - trainer - INFO -     precision      : 0.661967
2024-04-01 21:44:20,275 - trainer - INFO -     recall         : 0.651678
2024-04-01 21:44:20,275 - trainer - INFO -     doc_entropy    : 2.162957
2024-04-01 21:44:20,275 - trainer - INFO -     val_loss       : 1.108129
2024-04-01 21:44:20,275 - trainer - INFO -     val_accuracy   : 0.676491
2024-04-01 21:44:20,275 - trainer - INFO -     val_macro_f    : 0.660576
2024-04-01 21:44:20,275 - trainer - INFO -     val_precision  : 0.698106
2024-04-01 21:44:20,275 - trainer - INFO -     val_recall     : 0.676491
2024-04-01 21:44:20,275 - trainer - INFO -     val_doc_entropy: 1.902843
2024-04-01 21:44:20,275 - trainer - INFO -     test_loss      : 1.104397
2024-04-01 21:44:20,275 - trainer - INFO -     test_accuracy  : 0.677138
2024-04-01 21:44:20,275 - trainer - INFO -     test_macro_f   : 0.663307
2024-04-01 21:44:20,275 - trainer - INFO -     test_precision : 0.702287
2024-04-01 21:44:20,275 - trainer - INFO -     test_recall    : 0.677138
2024-04-01 21:44:20,275 - trainer - INFO -     test_doc_entropy: 1.906486
2024-04-01 21:47:41,514 - trainer - INFO -     epoch          : 2
2024-04-01 21:47:41,514 - trainer - INFO -     loss           : 0.901012
2024-04-01 21:47:41,514 - trainer - INFO -     accuracy       : 0.7352
2024-04-01 21:47:41,514 - trainer - INFO -     macro_f        : 0.725299
2024-04-01 21:47:41,514 - trainer - INFO -     precision      : 0.759193
2024-04-01 21:47:41,514 - trainer - INFO -     recall         : 0.7352
2024-04-01 21:47:41,514 - trainer - INFO -     doc_entropy    : 1.755208
2024-04-01 21:47:41,514 - trainer - INFO -     val_loss       : 1.113329
2024-04-01 21:47:41,514 - trainer - INFO -     val_accuracy   : 0.68132
2024-04-01 21:47:41,514 - trainer - INFO -     val_macro_f    : 0.669203
2024-04-01 21:47:41,514 - trainer - INFO -     val_precision  : 0.703967
2024-04-01 21:47:41,514 - trainer - INFO -     val_recall     : 0.68132
2024-04-01 21:47:41,514 - trainer - INFO -     val_doc_entropy: 1.780107
2024-04-01 21:47:41,514 - trainer - INFO -     test_loss      : 1.117117
2024-04-01 21:47:41,514 - trainer - INFO -     test_accuracy  : 0.682167
2024-04-01 21:47:41,514 - trainer - INFO -     test_macro_f   : 0.670034
2024-04-01 21:47:41,514 - trainer - INFO -     test_precision : 0.706065
2024-04-01 21:47:41,514 - trainer - INFO -     test_recall    : 0.682167
2024-04-01 21:47:41,514 - trainer - INFO -     test_doc_entropy: 1.783746
2024-04-01 21:51:02,154 - trainer - INFO -     epoch          : 3
2024-04-01 21:51:02,154 - trainer - INFO -     loss           : 0.684547
2024-04-01 21:51:02,154 - trainer - INFO -     accuracy       : 0.798532
2024-04-01 21:51:02,154 - trainer - INFO -     macro_f        : 0.79256
2024-04-01 21:51:02,154 - trainer - INFO -     precision      : 0.823534
2024-04-01 21:51:02,154 - trainer - INFO -     recall         : 0.798532
2024-04-01 21:51:02,154 - trainer - INFO -     doc_entropy    : 1.590793
2024-04-01 21:51:02,154 - trainer - INFO -     val_loss       : 1.17063
2024-04-01 21:51:02,154 - trainer - INFO -     val_accuracy   : 0.676939
2024-04-01 21:51:02,154 - trainer - INFO -     val_macro_f    : 0.665486
2024-04-01 21:51:02,154 - trainer - INFO -     val_precision  : 0.701703
2024-04-01 21:51:02,154 - trainer - INFO -     val_recall     : 0.676939
2024-04-01 21:51:02,154 - trainer - INFO -     val_doc_entropy: 1.635735
2024-04-01 21:51:02,154 - trainer - INFO -     test_loss      : 1.166102
2024-04-01 21:51:02,154 - trainer - INFO -     test_accuracy  : 0.676043
2024-04-01 21:51:02,154 - trainer - INFO -     test_macro_f   : 0.666224
2024-04-01 21:51:02,154 - trainer - INFO -     test_precision : 0.706332
2024-04-01 21:51:02,154 - trainer - INFO -     test_recall    : 0.676043
2024-04-01 21:51:02,154 - trainer - INFO -     test_doc_entropy: 1.639221
2024-04-01 21:51:37,114 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1600, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1600, out_features=80, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,097,363
Freeze params: 0
2024-04-01 21:54:56,804 - trainer - INFO -     epoch          : 1
2024-04-01 21:54:56,804 - trainer - INFO -     loss           : 1.232809
2024-04-01 21:54:56,804 - trainer - INFO -     accuracy       : 0.649313
2024-04-01 21:54:56,804 - trainer - INFO -     macro_f        : 0.629658
2024-04-01 21:54:56,804 - trainer - INFO -     precision      : 0.661483
2024-04-01 21:54:56,804 - trainer - INFO -     recall         : 0.649313
2024-04-01 21:54:56,804 - trainer - INFO -     doc_entropy    : 2.007417
2024-04-01 21:54:56,804 - trainer - INFO -     val_loss       : 1.083324
2024-04-01 21:54:56,804 - trainer - INFO -     val_accuracy   : 0.683063
2024-04-01 21:54:56,804 - trainer - INFO -     val_macro_f    : 0.669903
2024-04-01 21:54:56,804 - trainer - INFO -     val_precision  : 0.705225
2024-04-01 21:54:56,804 - trainer - INFO -     val_recall     : 0.683063
2024-04-01 21:54:56,804 - trainer - INFO -     val_doc_entropy: 1.787616
2024-04-01 21:54:56,804 - trainer - INFO -     test_loss      : 1.08804
2024-04-01 21:54:56,804 - trainer - INFO -     test_accuracy  : 0.682416
2024-04-01 21:54:56,804 - trainer - INFO -     test_macro_f   : 0.668013
2024-04-01 21:54:56,804 - trainer - INFO -     test_precision : 0.702204
2024-04-01 21:54:56,804 - trainer - INFO -     test_recall    : 0.682416
2024-04-01 21:54:56,804 - trainer - INFO -     test_doc_entropy: 1.789976
2024-04-01 21:58:18,022 - trainer - INFO -     epoch          : 2
2024-04-01 21:58:18,022 - trainer - INFO -     loss           : 0.906447
2024-04-01 21:58:18,022 - trainer - INFO -     accuracy       : 0.732823
2024-04-01 21:58:18,022 - trainer - INFO -     macro_f        : 0.722685
2024-04-01 21:58:18,022 - trainer - INFO -     precision      : 0.75652
2024-04-01 21:58:18,022 - trainer - INFO -     recall         : 0.732823
2024-04-01 21:58:18,022 - trainer - INFO -     doc_entropy    : 1.528836
2024-04-01 21:58:18,022 - trainer - INFO -     val_loss       : 1.083993
2024-04-01 21:58:18,022 - trainer - INFO -     val_accuracy   : 0.686199
2024-04-01 21:58:18,022 - trainer - INFO -     val_macro_f    : 0.676496
2024-04-01 21:58:18,022 - trainer - INFO -     val_precision  : 0.713779
2024-04-01 21:58:18,022 - trainer - INFO -     val_recall     : 0.686199
2024-04-01 21:58:18,022 - trainer - INFO -     val_doc_entropy: 1.521503
2024-04-01 21:58:18,022 - trainer - INFO -     test_loss      : 1.088875
2024-04-01 21:58:18,022 - trainer - INFO -     test_accuracy  : 0.683909
2024-04-01 21:58:18,022 - trainer - INFO -     test_macro_f   : 0.674085
2024-04-01 21:58:18,022 - trainer - INFO -     test_precision : 0.713298
2024-04-01 21:58:18,022 - trainer - INFO -     test_recall    : 0.683909
2024-04-01 21:58:18,022 - trainer - INFO -     test_doc_entropy: 1.52287
2024-04-01 22:01:38,804 - trainer - INFO -     epoch          : 3
2024-04-01 22:01:38,804 - trainer - INFO -     loss           : 0.690901
2024-04-01 22:01:38,804 - trainer - INFO -     accuracy       : 0.794686
2024-04-01 22:01:38,804 - trainer - INFO -     macro_f        : 0.788147
2024-04-01 22:01:38,804 - trainer - INFO -     precision      : 0.819031
2024-04-01 22:01:38,804 - trainer - INFO -     recall         : 0.794686
2024-04-01 22:01:38,804 - trainer - INFO -     doc_entropy    : 1.387669
2024-04-01 22:01:38,804 - trainer - INFO -     val_loss       : 1.195
2024-04-01 22:01:38,804 - trainer - INFO -     val_accuracy   : 0.670815
2024-04-01 22:01:38,804 - trainer - INFO -     val_macro_f    : 0.663885
2024-04-01 22:01:38,804 - trainer - INFO -     val_precision  : 0.706262
2024-04-01 22:01:38,804 - trainer - INFO -     val_recall     : 0.670815
2024-04-01 22:01:38,804 - trainer - INFO -     val_doc_entropy: 1.460119
2024-04-01 22:01:38,804 - trainer - INFO -     test_loss      : 1.185093
2024-04-01 22:01:38,804 - trainer - INFO -     test_accuracy  : 0.672857
2024-04-01 22:01:38,804 - trainer - INFO -     test_macro_f   : 0.664274
2024-04-01 22:01:38,804 - trainer - INFO -     test_precision : 0.703557
2024-04-01 22:01:38,804 - trainer - INFO -     test_recall    : 0.672857
2024-04-01 22:01:38,804 - trainer - INFO -     test_doc_entropy: 1.46299
2024-04-01 22:02:38,327 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=2000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=2000, out_features=100, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,289,783
Freeze params: 0
2024-04-01 22:07:03,565 - trainer - INFO -     epoch          : 1
2024-04-01 22:07:03,566 - trainer - INFO -     loss           : 1.239433
2024-04-01 22:07:03,566 - trainer - INFO -     accuracy       : 0.647564
2024-04-01 22:07:03,566 - trainer - INFO -     macro_f        : 0.62838
2024-04-01 22:07:03,566 - trainer - INFO -     precision      : 0.659814
2024-04-01 22:07:03,567 - trainer - INFO -     recall         : 0.647564
2024-04-01 22:07:03,567 - trainer - INFO -     doc_entropy    : 1.576671
2024-04-01 22:07:03,567 - trainer - INFO -     val_loss       : 1.111826
2024-04-01 22:07:03,567 - trainer - INFO -     val_accuracy   : 0.67918
2024-04-01 22:07:03,568 - trainer - INFO -     val_macro_f    : 0.665117
2024-04-01 22:07:03,568 - trainer - INFO -     val_precision  : 0.700092
2024-04-01 22:07:03,568 - trainer - INFO -     val_recall     : 0.67918
2024-04-01 22:07:03,568 - trainer - INFO -     val_doc_entropy: 1.073628
2024-04-01 22:07:03,568 - trainer - INFO -     test_loss      : 1.108538
2024-04-01 22:07:03,568 - trainer - INFO -     test_accuracy  : 0.679877
2024-04-01 22:07:03,569 - trainer - INFO -     test_macro_f   : 0.666829
2024-04-01 22:07:03,569 - trainer - INFO -     test_precision : 0.701963
2024-04-01 22:07:03,569 - trainer - INFO -     test_recall    : 0.679877
2024-04-01 22:07:03,570 - trainer - INFO -     test_doc_entropy: 1.073692
2024-04-01 22:11:48,277 - trainer - INFO -     epoch          : 2
2024-04-01 22:11:48,279 - trainer - INFO -     loss           : 0.925579
2024-04-01 22:11:48,280 - trainer - INFO -     accuracy       : 0.727906
2024-04-01 22:11:48,280 - trainer - INFO -     macro_f        : 0.717585
2024-04-01 22:11:48,281 - trainer - INFO -     precision      : 0.752186
2024-04-01 22:11:48,281 - trainer - INFO -     recall         : 0.727906
2024-04-01 22:11:48,282 - trainer - INFO -     doc_entropy    : 0.844942
2024-04-01 22:11:48,282 - trainer - INFO -     val_loss       : 1.104223
2024-04-01 22:11:48,282 - trainer - INFO -     val_accuracy   : 0.68147
2024-04-01 22:11:48,282 - trainer - INFO -     val_macro_f    : 0.672812
2024-04-01 22:11:48,283 - trainer - INFO -     val_precision  : 0.71162
2024-04-01 22:11:48,283 - trainer - INFO -     val_recall     : 0.68147
2024-04-01 22:11:48,283 - trainer - INFO -     val_doc_entropy: 0.712235
2024-04-01 22:11:48,283 - trainer - INFO -     test_loss      : 1.105615
2024-04-01 22:11:48,283 - trainer - INFO -     test_accuracy  : 0.680424
2024-04-01 22:11:48,284 - trainer - INFO -     test_macro_f   : 0.672087
2024-04-01 22:11:48,284 - trainer - INFO -     test_precision : 0.71338
2024-04-01 22:11:48,284 - trainer - INFO -     test_recall    : 0.680424
2024-04-01 22:11:48,284 - trainer - INFO -     test_doc_entropy: 0.71249
2024-04-01 22:16:26,897 - trainer - INFO -     epoch          : 3
2024-04-01 22:16:26,897 - trainer - INFO -     loss           : 0.722327
2024-04-01 22:16:26,898 - trainer - INFO -     accuracy       : 0.786539
2024-04-01 22:16:26,898 - trainer - INFO -     macro_f        : 0.779853
2024-04-01 22:16:26,898 - trainer - INFO -     precision      : 0.811656
2024-04-01 22:16:26,899 - trainer - INFO -     recall         : 0.786539
2024-04-01 22:16:26,899 - trainer - INFO -     doc_entropy    : 0.632979
2024-04-01 22:16:26,899 - trainer - INFO -     val_loss       : 1.18955
2024-04-01 22:16:26,899 - trainer - INFO -     val_accuracy   : 0.669421
2024-04-01 22:16:26,899 - trainer - INFO -     val_macro_f    : 0.659956
2024-04-01 22:16:26,900 - trainer - INFO -     val_precision  : 0.698835
2024-04-01 22:16:26,900 - trainer - INFO -     val_recall     : 0.669421
2024-04-01 22:16:26,900 - trainer - INFO -     val_doc_entropy: 0.611051
2024-04-01 22:16:26,901 - trainer - INFO -     test_loss      : 1.1868
2024-04-01 22:16:26,901 - trainer - INFO -     test_accuracy  : 0.668973
2024-04-01 22:16:26,901 - trainer - INFO -     test_macro_f   : 0.659168
2024-04-01 22:16:26,902 - trainer - INFO -     test_precision : 0.697275
2024-04-01 22:16:26,902 - trainer - INFO -     test_recall    : 0.668973
2024-04-01 22:16:26,902 - trainer - INFO -     test_doc_entropy: 0.610448
2024-04-01 22:17:22,230 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=2000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=2000, out_features=100, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,289,783
Freeze params: 0
2024-04-01 22:21:53,900 - trainer - INFO -     epoch          : 1
2024-04-01 22:21:53,900 - trainer - INFO -     loss           : 1.236323
2024-04-01 22:21:53,900 - trainer - INFO -     accuracy       : 0.648871
2024-04-01 22:21:53,901 - trainer - INFO -     macro_f        : 0.628384
2024-04-01 22:21:53,901 - trainer - INFO -     precision      : 0.659047
2024-04-01 22:21:53,901 - trainer - INFO -     recall         : 0.648871
2024-04-01 22:21:53,901 - trainer - INFO -     doc_entropy    : 1.746354
2024-04-01 22:21:53,902 - trainer - INFO -     val_loss       : 1.089067
2024-04-01 22:21:53,902 - trainer - INFO -     val_accuracy   : 0.680524
2024-04-01 22:21:53,902 - trainer - INFO -     val_macro_f    : 0.664529
2024-04-01 22:21:53,902 - trainer - INFO -     val_precision  : 0.699419
2024-04-01 22:21:53,903 - trainer - INFO -     val_recall     : 0.680524
2024-04-01 22:21:53,903 - trainer - INFO -     val_doc_entropy: 1.417468
2024-04-01 22:21:53,903 - trainer - INFO -     test_loss      : 1.091216
2024-04-01 22:21:53,903 - trainer - INFO -     test_accuracy  : 0.682017
2024-04-01 22:21:53,903 - trainer - INFO -     test_macro_f   : 0.667395
2024-04-01 22:21:53,904 - trainer - INFO -     test_precision : 0.70362
2024-04-01 22:21:53,904 - trainer - INFO -     test_recall    : 0.682017
2024-04-01 22:21:53,904 - trainer - INFO -     test_doc_entropy: 1.418906
2024-04-01 22:26:31,087 - trainer - INFO -     epoch          : 2
2024-04-01 22:26:31,087 - trainer - INFO -     loss           : 0.916774
2024-04-01 22:26:31,087 - trainer - INFO -     accuracy       : 0.729263
2024-04-01 22:26:31,088 - trainer - INFO -     macro_f        : 0.718698
2024-04-01 22:26:31,088 - trainer - INFO -     precision      : 0.752718
2024-04-01 22:26:31,089 - trainer - INFO -     recall         : 0.729263
2024-04-01 22:26:31,089 - trainer - INFO -     doc_entropy    : 1.082457
2024-04-01 22:26:31,089 - trainer - INFO -     val_loss       : 1.101801
2024-04-01 22:26:31,089 - trainer - INFO -     val_accuracy   : 0.687942
2024-04-01 22:26:31,089 - trainer - INFO -     val_macro_f    : 0.677456
2024-04-01 22:26:31,090 - trainer - INFO -     val_precision  : 0.713773
2024-04-01 22:26:31,090 - trainer - INFO -     val_recall     : 0.687942
2024-04-01 22:26:31,090 - trainer - INFO -     val_doc_entropy: 0.967392
2024-04-01 22:26:31,090 - trainer - INFO -     test_loss      : 1.101516
2024-04-01 22:26:31,090 - trainer - INFO -     test_accuracy  : 0.685652
2024-04-01 22:26:31,090 - trainer - INFO -     test_macro_f   : 0.672764
2024-04-01 22:26:31,091 - trainer - INFO -     test_precision : 0.707244
2024-04-01 22:26:31,091 - trainer - INFO -     test_recall    : 0.685652
2024-04-01 22:26:31,091 - trainer - INFO -     test_doc_entropy: 0.9679
2024-04-01 22:31:06,709 - trainer - INFO -     epoch          : 3
2024-04-01 22:31:06,709 - trainer - INFO -     loss           : 0.708755
2024-04-01 22:31:06,710 - trainer - INFO -     accuracy       : 0.790118
2024-04-01 22:31:06,710 - trainer - INFO -     macro_f        : 0.783193
2024-04-01 22:31:06,710 - trainer - INFO -     precision      : 0.814203
2024-04-01 22:31:06,710 - trainer - INFO -     recall         : 0.790118
2024-04-01 22:31:06,710 - trainer - INFO -     doc_entropy    : 0.818178
2024-04-01 22:31:06,711 - trainer - INFO -     val_loss       : 1.195065
2024-04-01 22:31:06,711 - trainer - INFO -     val_accuracy   : 0.66982
2024-04-01 22:31:06,711 - trainer - INFO -     val_macro_f    : 0.659486
2024-04-01 22:31:06,711 - trainer - INFO -     val_precision  : 0.699019
2024-04-01 22:31:06,712 - trainer - INFO -     val_recall     : 0.66982
2024-04-01 22:31:06,712 - trainer - INFO -     val_doc_entropy: 0.820209
2024-04-01 22:31:06,712 - trainer - INFO -     test_loss      : 1.19063
2024-04-01 22:31:06,712 - trainer - INFO -     test_accuracy  : 0.673653
2024-04-01 22:31:06,712 - trainer - INFO -     test_macro_f   : 0.663482
2024-04-01 22:31:06,713 - trainer - INFO -     test_precision : 0.702938
2024-04-01 22:31:06,713 - trainer - INFO -     test_recall    : 0.673653
2024-04-01 22:31:06,713 - trainer - INFO -     test_doc_entropy: 0.820088
2024-04-01 22:32:02,130 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=2000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=2000, out_features=100, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,289,783
Freeze params: 0
2024-04-01 22:36:36,084 - trainer - INFO -     epoch          : 1
2024-04-01 22:36:36,084 - trainer - INFO -     loss           : 1.24166
2024-04-01 22:36:36,084 - trainer - INFO -     accuracy       : 0.646867
2024-04-01 22:36:36,084 - trainer - INFO -     macro_f        : 0.627001
2024-04-01 22:36:36,086 - trainer - INFO -     precision      : 0.658968
2024-04-01 22:36:36,086 - trainer - INFO -     recall         : 0.646867
2024-04-01 22:36:36,086 - trainer - INFO -     doc_entropy    : 1.579769
2024-04-01 22:36:36,086 - trainer - INFO -     val_loss       : 1.097414
2024-04-01 22:36:36,086 - trainer - INFO -     val_accuracy   : 0.682565
2024-04-01 22:36:36,087 - trainer - INFO -     val_macro_f    : 0.668025
2024-04-01 22:36:36,087 - trainer - INFO -     val_precision  : 0.700384
2024-04-01 22:36:36,087 - trainer - INFO -     val_recall     : 0.682565
2024-04-01 22:36:36,087 - trainer - INFO -     val_doc_entropy: 1.064273
2024-04-01 22:36:36,087 - trainer - INFO -     test_loss      : 1.097007
2024-04-01 22:36:36,087 - trainer - INFO -     test_accuracy  : 0.682017
2024-04-01 22:36:36,088 - trainer - INFO -     test_macro_f   : 0.669976
2024-04-01 22:36:36,088 - trainer - INFO -     test_precision : 0.708157
2024-04-01 22:36:36,088 - trainer - INFO -     test_recall    : 0.682017
2024-04-01 22:36:36,088 - trainer - INFO -     test_doc_entropy: 1.065824
2024-04-01 22:41:11,653 - trainer - INFO -     epoch          : 2
2024-04-01 22:41:11,653 - trainer - INFO -     loss           : 0.924813
2024-04-01 22:41:11,653 - trainer - INFO -     accuracy       : 0.728068
2024-04-01 22:41:11,653 - trainer - INFO -     macro_f        : 0.717419
2024-04-01 22:41:11,653 - trainer - INFO -     precision      : 0.751402
2024-04-01 22:41:11,655 - trainer - INFO -     recall         : 0.728068
2024-04-01 22:41:11,655 - trainer - INFO -     doc_entropy    : 0.839579
2024-04-01 22:41:11,655 - trainer - INFO -     val_loss       : 1.104968
2024-04-01 22:41:11,655 - trainer - INFO -     val_accuracy   : 0.682764
2024-04-01 22:41:11,656 - trainer - INFO -     val_macro_f    : 0.672861
2024-04-01 22:41:11,656 - trainer - INFO -     val_precision  : 0.710315
2024-04-01 22:41:11,656 - trainer - INFO -     val_recall     : 0.682764
2024-04-01 22:41:11,656 - trainer - INFO -     val_doc_entropy: 0.720341
2024-04-01 22:41:11,657 - trainer - INFO -     test_loss      : 1.11665
2024-04-01 22:41:11,657 - trainer - INFO -     test_accuracy  : 0.678084
2024-04-01 22:41:11,657 - trainer - INFO -     test_macro_f   : 0.667869
2024-04-01 22:41:11,657 - trainer - INFO -     test_precision : 0.70619
2024-04-01 22:41:11,658 - trainer - INFO -     test_recall    : 0.678084
2024-04-01 22:41:11,658 - trainer - INFO -     test_doc_entropy: 0.720966
2024-04-01 22:45:42,579 - trainer - INFO -     epoch          : 3
2024-04-01 22:45:42,586 - trainer - INFO -     loss           : 0.723149
2024-04-01 22:45:42,587 - trainer - INFO -     accuracy       : 0.785195
2024-04-01 22:45:42,588 - trainer - INFO -     macro_f        : 0.778205
2024-04-01 22:45:42,588 - trainer - INFO -     precision      : 0.809868
2024-04-01 22:45:42,589 - trainer - INFO -     recall         : 0.785195
2024-04-01 22:45:42,589 - trainer - INFO -     doc_entropy    : 0.612487
2024-04-01 22:45:42,589 - trainer - INFO -     val_loss       : 1.175643
2024-04-01 22:45:42,590 - trainer - INFO -     val_accuracy   : 0.671712
2024-04-01 22:45:42,590 - trainer - INFO -     val_macro_f    : 0.664277
2024-04-01 22:45:42,590 - trainer - INFO -     val_precision  : 0.705682
2024-04-01 22:45:42,590 - trainer - INFO -     val_recall     : 0.671712
2024-04-01 22:45:42,591 - trainer - INFO -     val_doc_entropy: 0.597325
2024-04-01 22:45:42,591 - trainer - INFO -     test_loss      : 1.180198
2024-04-01 22:45:42,591 - trainer - INFO -     test_accuracy  : 0.671463
2024-04-01 22:45:42,592 - trainer - INFO -     test_macro_f   : 0.665666
2024-04-01 22:45:42,592 - trainer - INFO -     test_precision : 0.709724
2024-04-01 22:45:42,592 - trainer - INFO -     test_recall    : 0.671463
2024-04-01 22:45:42,592 - trainer - INFO -     test_doc_entropy: 0.598031
2024-04-01 22:46:36,559 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=2000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=2000, out_features=100, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,289,783
Freeze params: 0
2024-04-01 22:50:53,131 - trainer - INFO -     epoch          : 1
2024-04-01 22:50:53,131 - trainer - INFO -     loss           : 1.238439
2024-04-01 22:50:53,131 - trainer - INFO -     accuracy       : 0.648106
2024-04-01 22:50:53,131 - trainer - INFO -     macro_f        : 0.628312
2024-04-01 22:50:53,131 - trainer - INFO -     precision      : 0.660131
2024-04-01 22:50:53,131 - trainer - INFO -     recall         : 0.648106
2024-04-01 22:50:53,131 - trainer - INFO -     doc_entropy    : 1.712051
2024-04-01 22:50:53,131 - trainer - INFO -     val_loss       : 1.103428
2024-04-01 22:50:53,131 - trainer - INFO -     val_accuracy   : 0.680026
2024-04-01 22:50:53,131 - trainer - INFO -     val_macro_f    : 0.666005
2024-04-01 22:50:53,131 - trainer - INFO -     val_precision  : 0.70284
2024-04-01 22:50:53,131 - trainer - INFO -     val_recall     : 0.680026
2024-04-01 22:50:53,131 - trainer - INFO -     val_doc_entropy: 1.319926
2024-04-01 22:50:53,131 - trainer - INFO -     test_loss      : 1.104006
2024-04-01 22:50:53,131 - trainer - INFO -     test_accuracy  : 0.679877
2024-04-01 22:50:53,131 - trainer - INFO -     test_macro_f   : 0.667684
2024-04-01 22:50:53,131 - trainer - INFO -     test_precision : 0.70549
2024-04-01 22:50:53,131 - trainer - INFO -     test_recall    : 0.679877
2024-04-01 22:50:53,131 - trainer - INFO -     test_doc_entropy: 1.321738
2024-04-01 22:55:01,722 - trainer - INFO -     epoch          : 2
2024-04-01 22:55:01,725 - trainer - INFO -     loss           : 0.919818
2024-04-01 22:55:01,725 - trainer - INFO -     accuracy       : 0.730215
2024-04-01 22:55:01,725 - trainer - INFO -     macro_f        : 0.720045
2024-04-01 22:55:01,725 - trainer - INFO -     precision      : 0.754713
2024-04-01 22:55:01,725 - trainer - INFO -     recall         : 0.730215
2024-04-01 22:55:01,725 - trainer - INFO -     doc_entropy    : 1.032259
2024-04-01 22:55:01,725 - trainer - INFO -     val_loss       : 1.083483
2024-04-01 22:55:01,725 - trainer - INFO -     val_accuracy   : 0.687544
2024-04-01 22:55:01,725 - trainer - INFO -     val_macro_f    : 0.671971
2024-04-01 22:55:01,725 - trainer - INFO -     val_precision  : 0.704515
2024-04-01 22:55:01,725 - trainer - INFO -     val_recall     : 0.687544
2024-04-01 22:55:01,725 - trainer - INFO -     val_doc_entropy: 0.973045
2024-04-01 22:55:01,725 - trainer - INFO -     test_loss      : 1.087186
2024-04-01 22:55:01,725 - trainer - INFO -     test_accuracy  : 0.687444
2024-04-01 22:55:01,725 - trainer - INFO -     test_macro_f   : 0.672439
2024-04-01 22:55:01,725 - trainer - INFO -     test_precision : 0.704848
2024-04-01 22:55:01,725 - trainer - INFO -     test_recall    : 0.687444
2024-04-01 22:55:01,725 - trainer - INFO -     test_doc_entropy: 0.973983
2024-04-01 22:59:20,547 - trainer - INFO -     epoch          : 3
2024-04-01 22:59:20,547 - trainer - INFO -     loss           : 0.715505
2024-04-01 22:59:20,547 - trainer - INFO -     accuracy       : 0.787778
2024-04-01 22:59:20,547 - trainer - INFO -     macro_f        : 0.781126
2024-04-01 22:59:20,547 - trainer - INFO -     precision      : 0.812889
2024-04-01 22:59:20,547 - trainer - INFO -     recall         : 0.787778
2024-04-01 22:59:20,547 - trainer - INFO -     doc_entropy    : 0.795723
2024-04-01 22:59:20,547 - trainer - INFO -     val_loss       : 1.182057
2024-04-01 22:59:20,551 - trainer - INFO -     val_accuracy   : 0.673305
2024-04-01 22:59:20,551 - trainer - INFO -     val_macro_f    : 0.664334
2024-04-01 22:59:20,551 - trainer - INFO -     val_precision  : 0.702703
2024-04-01 22:59:20,551 - trainer - INFO -     val_recall     : 0.673305
2024-04-01 22:59:20,551 - trainer - INFO -     val_doc_entropy: 0.75964
2024-04-01 22:59:20,551 - trainer - INFO -     test_loss      : 1.180969
2024-04-01 22:59:20,551 - trainer - INFO -     test_accuracy  : 0.670766
2024-04-01 22:59:20,551 - trainer - INFO -     test_macro_f   : 0.661475
2024-04-01 22:59:20,551 - trainer - INFO -     test_precision : 0.700056
2024-04-01 22:59:20,551 - trainer - INFO -     test_recall    : 0.670766
2024-04-01 22:59:20,551 - trainer - INFO -     test_doc_entropy: 0.76033
2024-04-01 23:00:07,871 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=2000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=2000, out_features=100, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,289,783
Freeze params: 0
2024-04-01 23:04:28,990 - trainer - INFO -     epoch          : 1
2024-04-01 23:04:28,990 - trainer - INFO -     loss           : 1.239112
2024-04-01 23:04:28,990 - trainer - INFO -     accuracy       : 0.648361
2024-04-01 23:04:28,990 - trainer - INFO -     macro_f        : 0.62855
2024-04-01 23:04:28,990 - trainer - INFO -     precision      : 0.659791
2024-04-01 23:04:28,990 - trainer - INFO -     recall         : 0.648361
2024-04-01 23:04:28,990 - trainer - INFO -     doc_entropy    : 1.669505
2024-04-01 23:04:28,990 - trainer - INFO -     val_loss       : 1.087834
2024-04-01 23:04:28,990 - trainer - INFO -     val_accuracy   : 0.682266
2024-04-01 23:04:28,990 - trainer - INFO -     val_macro_f    : 0.668961
2024-04-01 23:04:28,990 - trainer - INFO -     val_precision  : 0.704041
2024-04-01 23:04:28,990 - trainer - INFO -     val_recall     : 0.682266
2024-04-01 23:04:28,990 - trainer - INFO -     val_doc_entropy: 1.182226
2024-04-01 23:04:28,990 - trainer - INFO -     test_loss      : 1.098659
2024-04-01 23:04:28,990 - trainer - INFO -     test_accuracy  : 0.679428
2024-04-01 23:04:28,990 - trainer - INFO -     test_macro_f   : 0.667316
2024-04-01 23:04:28,990 - trainer - INFO -     test_precision : 0.70471
2024-04-01 23:04:28,990 - trainer - INFO -     test_recall    : 0.679428
2024-04-01 23:04:28,990 - trainer - INFO -     test_doc_entropy: 1.182388
2024-04-01 23:08:49,685 - trainer - INFO -     epoch          : 2
2024-04-01 23:08:49,685 - trainer - INFO -     loss           : 0.923254
2024-04-01 23:08:49,685 - trainer - INFO -     accuracy       : 0.7277
2024-04-01 23:08:49,685 - trainer - INFO -     macro_f        : 0.717067
2024-04-01 23:08:49,685 - trainer - INFO -     precision      : 0.751511
2024-04-01 23:08:49,685 - trainer - INFO -     recall         : 0.7277
2024-04-01 23:08:49,685 - trainer - INFO -     doc_entropy    : 0.969913
2024-04-01 23:08:49,685 - trainer - INFO -     val_loss       : 1.096586
2024-04-01 23:08:49,688 - trainer - INFO -     val_accuracy   : 0.683461
2024-04-01 23:08:49,688 - trainer - INFO -     val_macro_f    : 0.678481
2024-04-01 23:08:49,688 - trainer - INFO -     val_precision  : 0.7228
2024-04-01 23:08:49,688 - trainer - INFO -     val_recall     : 0.683461
2024-04-01 23:08:49,688 - trainer - INFO -     val_doc_entropy: 0.852633
2024-04-01 23:08:49,688 - trainer - INFO -     test_loss      : 1.095301
2024-04-01 23:08:49,688 - trainer - INFO -     test_accuracy  : 0.683561
2024-04-01 23:08:49,690 - trainer - INFO -     test_macro_f   : 0.678584
2024-04-01 23:08:49,690 - trainer - INFO -     test_precision : 0.723131
2024-04-01 23:08:49,690 - trainer - INFO -     test_recall    : 0.683561
2024-04-01 23:08:49,690 - trainer - INFO -     test_doc_entropy: 0.85414
2024-04-01 23:13:11,320 - trainer - INFO -     epoch          : 3
2024-04-01 23:13:11,320 - trainer - INFO -     loss           : 0.714301
2024-04-01 23:13:11,320 - trainer - INFO -     accuracy       : 0.788357
2024-04-01 23:13:11,320 - trainer - INFO -     macro_f        : 0.781906
2024-04-01 23:13:11,320 - trainer - INFO -     precision      : 0.813148
2024-04-01 23:13:11,320 - trainer - INFO -     recall         : 0.788357
2024-04-01 23:13:11,320 - trainer - INFO -     doc_entropy    : 0.721055
2024-04-01 23:13:11,320 - trainer - INFO -     val_loss       : 1.196942
2024-04-01 23:13:11,320 - trainer - INFO -     val_accuracy   : 0.668625
2024-04-01 23:13:11,320 - trainer - INFO -     val_macro_f    : 0.658807
2024-04-01 23:13:11,320 - trainer - INFO -     val_precision  : 0.697249
2024-04-01 23:13:11,320 - trainer - INFO -     val_recall     : 0.668625
2024-04-01 23:13:11,320 - trainer - INFO -     val_doc_entropy: 0.721123
2024-04-01 23:13:11,320 - trainer - INFO -     test_loss      : 1.194037
2024-04-01 23:13:11,336 - trainer - INFO -     test_accuracy  : 0.671313
2024-04-01 23:13:11,336 - trainer - INFO -     test_macro_f   : 0.661973
2024-04-01 23:13:11,336 - trainer - INFO -     test_precision : 0.70321
2024-04-01 23:13:11,336 - trainer - INFO -     test_recall    : 0.671313
2024-04-01 23:13:11,336 - trainer - INFO -     test_doc_entropy: 0.722005
2024-04-01 23:57:59,668 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=2400, bias=True)
    (1): Tanh()
    (2): Linear(in_features=2400, out_features=120, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,498,203
Freeze params: 0
2024-04-02 00:01:44,443 - trainer - INFO -     epoch          : 1
2024-04-02 00:01:44,443 - trainer - INFO -     loss           : 1.250872
2024-04-02 00:01:44,443 - trainer - INFO -     accuracy       : 0.645772
2024-04-02 00:01:44,443 - trainer - INFO -     macro_f        : 0.625237
2024-04-02 00:01:44,443 - trainer - INFO -     precision      : 0.656027
2024-04-02 00:01:44,443 - trainer - INFO -     recall         : 0.645772
2024-04-02 00:01:44,443 - trainer - INFO -     doc_entropy    : 1.132519
2024-04-02 00:01:44,443 - trainer - INFO -     val_loss       : 1.104061
2024-04-02 00:01:44,443 - trainer - INFO -     val_accuracy   : 0.681022
2024-04-02 00:01:44,443 - trainer - INFO -     val_macro_f    : 0.668487
2024-04-02 00:01:44,443 - trainer - INFO -     val_precision  : 0.702278
2024-04-02 00:01:44,443 - trainer - INFO -     val_recall     : 0.681022
2024-04-02 00:01:44,443 - trainer - INFO -     val_doc_entropy: 0.499199
2024-04-02 00:01:44,443 - trainer - INFO -     test_loss      : 1.105832
2024-04-02 00:01:44,443 - trainer - INFO -     test_accuracy  : 0.678582
2024-04-02 00:01:44,443 - trainer - INFO -     test_macro_f   : 0.665913
2024-04-02 00:01:44,443 - trainer - INFO -     test_precision : 0.70168
2024-04-02 00:01:44,443 - trainer - INFO -     test_recall    : 0.678582
2024-04-02 00:01:44,443 - trainer - INFO -     test_doc_entropy: 0.498262
2024-04-02 00:05:34,047 - trainer - INFO -     epoch          : 2
2024-04-02 00:05:34,047 - trainer - INFO -     loss           : 0.95056
2024-04-02 00:05:34,047 - trainer - INFO -     accuracy       : 0.720338
2024-04-02 00:05:34,047 - trainer - INFO -     macro_f        : 0.70978
2024-04-02 00:05:34,047 - trainer - INFO -     precision      : 0.745002
2024-04-02 00:05:34,047 - trainer - INFO -     recall         : 0.720338
2024-04-02 00:05:34,047 - trainer - INFO -     doc_entropy    : 0.371666
2024-04-02 00:05:34,047 - trainer - INFO -     val_loss       : 1.104649
2024-04-02 00:05:34,047 - trainer - INFO -     val_accuracy   : 0.682714
2024-04-02 00:05:34,047 - trainer - INFO -     val_macro_f    : 0.671567
2024-04-02 00:05:34,047 - trainer - INFO -     val_precision  : 0.707989
2024-04-02 00:05:34,047 - trainer - INFO -     val_recall     : 0.682714
2024-04-02 00:05:34,047 - trainer - INFO -     val_doc_entropy: 0.286633
2024-04-02 00:05:34,047 - trainer - INFO -     test_loss      : 1.105812
2024-04-02 00:05:34,047 - trainer - INFO -     test_accuracy  : 0.677238
2024-04-02 00:05:34,047 - trainer - INFO -     test_macro_f   : 0.666784
2024-04-02 00:05:34,047 - trainer - INFO -     test_precision : 0.703704
2024-04-02 00:05:34,047 - trainer - INFO -     test_recall    : 0.677238
2024-04-02 00:05:34,047 - trainer - INFO -     test_doc_entropy: 0.286883
2024-04-02 00:09:23,030 - trainer - INFO -     epoch          : 3
2024-04-02 00:09:23,030 - trainer - INFO -     loss           : 0.760578
2024-04-02 00:09:23,030 - trainer - INFO -     accuracy       : 0.773588
2024-04-02 00:09:23,030 - trainer - INFO -     macro_f        : 0.766293
2024-04-02 00:09:23,030 - trainer - INFO -     precision      : 0.799304
2024-04-02 00:09:23,030 - trainer - INFO -     recall         : 0.773588
2024-04-02 00:09:23,030 - trainer - INFO -     doc_entropy    : 0.258885
2024-04-02 00:09:23,030 - trainer - INFO -     val_loss       : 1.19772
2024-04-02 00:09:23,046 - trainer - INFO -     val_accuracy   : 0.668575
2024-04-02 00:09:23,046 - trainer - INFO -     val_macro_f    : 0.662533
2024-04-02 00:09:23,046 - trainer - INFO -     val_precision  : 0.705696
2024-04-02 00:09:23,046 - trainer - INFO -     val_recall     : 0.668575
2024-04-02 00:09:23,046 - trainer - INFO -     val_doc_entropy: 0.230388
2024-04-02 00:09:23,046 - trainer - INFO -     test_loss      : 1.190891
2024-04-02 00:09:23,046 - trainer - INFO -     test_accuracy  : 0.666584
2024-04-02 00:09:23,046 - trainer - INFO -     test_macro_f   : 0.661404
2024-04-02 00:09:23,046 - trainer - INFO -     test_precision : 0.705063
2024-04-02 00:09:23,046 - trainer - INFO -     test_recall    : 0.666584
2024-04-02 00:09:23,046 - trainer - INFO -     test_doc_entropy: 0.23104
2024-04-02 00:09:58,735 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=2400, bias=True)
    (1): Tanh()
    (2): Linear(in_features=2400, out_features=120, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,498,203
Freeze params: 0
2024-04-02 00:13:45,882 - trainer - INFO -     epoch          : 1
2024-04-02 00:13:45,882 - trainer - INFO -     loss           : 1.24663
2024-04-02 00:13:45,882 - trainer - INFO -     accuracy       : 0.645921
2024-04-02 00:13:45,882 - trainer - INFO -     macro_f        : 0.625924
2024-04-02 00:13:45,882 - trainer - INFO -     precision      : 0.657536
2024-04-02 00:13:45,882 - trainer - INFO -     recall         : 0.645921
2024-04-02 00:13:45,882 - trainer - INFO -     doc_entropy    : 1.194889
2024-04-02 00:13:45,882 - trainer - INFO -     val_loss       : 1.115889
2024-04-02 00:13:45,882 - trainer - INFO -     val_accuracy   : 0.675197
2024-04-02 00:13:45,882 - trainer - INFO -     val_macro_f    : 0.669758
2024-04-02 00:13:45,882 - trainer - INFO -     val_precision  : 0.713003
2024-04-02 00:13:45,882 - trainer - INFO -     val_recall     : 0.675197
2024-04-02 00:13:45,882 - trainer - INFO -     val_doc_entropy: 0.570354
2024-04-02 00:13:45,882 - trainer - INFO -     test_loss      : 1.123409
2024-04-02 00:13:45,882 - trainer - INFO -     test_accuracy  : 0.676192
2024-04-02 00:13:45,882 - trainer - INFO -     test_macro_f   : 0.670808
2024-04-02 00:13:45,882 - trainer - INFO -     test_precision : 0.713906
2024-04-02 00:13:45,882 - trainer - INFO -     test_recall    : 0.676192
2024-04-02 00:13:45,882 - trainer - INFO -     test_doc_entropy: 0.570502
2024-04-02 00:17:35,067 - trainer - INFO -     epoch          : 2
2024-04-02 00:17:35,067 - trainer - INFO -     loss           : 0.945701
2024-04-02 00:17:35,067 - trainer - INFO -     accuracy       : 0.722448
2024-04-02 00:17:35,067 - trainer - INFO -     macro_f        : 0.711142
2024-04-02 00:17:35,067 - trainer - INFO -     precision      : 0.745338
2024-04-02 00:17:35,067 - trainer - INFO -     recall         : 0.722448
2024-04-02 00:17:35,067 - trainer - INFO -     doc_entropy    : 0.408265
2024-04-02 00:17:35,067 - trainer - INFO -     val_loss       : 1.123455
2024-04-02 00:17:35,067 - trainer - INFO -     val_accuracy   : 0.679578
2024-04-02 00:17:35,067 - trainer - INFO -     val_macro_f    : 0.671445
2024-04-02 00:17:35,067 - trainer - INFO -     val_precision  : 0.712521
2024-04-02 00:17:35,067 - trainer - INFO -     val_recall     : 0.679578
2024-04-02 00:17:35,067 - trainer - INFO -     val_doc_entropy: 0.322821
2024-04-02 00:17:35,067 - trainer - INFO -     test_loss      : 1.125777
2024-04-02 00:17:35,067 - trainer - INFO -     test_accuracy  : 0.675894
2024-04-02 00:17:35,067 - trainer - INFO -     test_macro_f   : 0.666171
2024-04-02 00:17:35,067 - trainer - INFO -     test_precision : 0.705677
2024-04-02 00:17:35,067 - trainer - INFO -     test_recall    : 0.675894
2024-04-02 00:17:35,067 - trainer - INFO -     test_doc_entropy: 0.322511
2024-04-02 00:21:25,318 - trainer - INFO -     epoch          : 3
2024-04-02 00:21:25,318 - trainer - INFO -     loss           : 0.754084
2024-04-02 00:21:25,318 - trainer - INFO -     accuracy       : 0.775947
2024-04-02 00:21:25,318 - trainer - INFO -     macro_f        : 0.768892
2024-04-02 00:21:25,318 - trainer - INFO -     precision      : 0.80108
2024-04-02 00:21:25,318 - trainer - INFO -     recall         : 0.775947
2024-04-02 00:21:25,318 - trainer - INFO -     doc_entropy    : 0.280264
2024-04-02 00:21:25,318 - trainer - INFO -     val_loss       : 1.174295
2024-04-02 00:21:25,318 - trainer - INFO -     val_accuracy   : 0.67216
2024-04-02 00:21:25,318 - trainer - INFO -     val_macro_f    : 0.662099
2024-04-02 00:21:25,318 - trainer - INFO -     val_precision  : 0.699603
2024-04-02 00:21:25,318 - trainer - INFO -     val_recall     : 0.67216
2024-04-02 00:21:25,318 - trainer - INFO -     val_doc_entropy: 0.244563
2024-04-02 00:21:25,318 - trainer - INFO -     test_loss      : 1.16736
2024-04-02 00:21:25,318 - trainer - INFO -     test_accuracy  : 0.673106
2024-04-02 00:21:25,318 - trainer - INFO -     test_macro_f   : 0.663772
2024-04-02 00:21:25,318 - trainer - INFO -     test_precision : 0.703788
2024-04-02 00:21:25,318 - trainer - INFO -     test_recall    : 0.673106
2024-04-02 00:21:25,318 - trainer - INFO -     test_doc_entropy: 0.244558
2024-04-02 00:22:00,631 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=2400, bias=True)
    (1): Tanh()
    (2): Linear(in_features=2400, out_features=120, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,498,203
Freeze params: 0
2024-04-02 00:25:48,385 - trainer - INFO -     epoch          : 1
2024-04-02 00:25:48,401 - trainer - INFO -     loss           : 1.252732
2024-04-02 00:25:48,401 - trainer - INFO -     accuracy       : 0.642654
2024-04-02 00:25:48,401 - trainer - INFO -     macro_f        : 0.622934
2024-04-02 00:25:48,401 - trainer - INFO -     precision      : 0.65427
2024-04-02 00:25:48,401 - trainer - INFO -     recall         : 0.642654
2024-04-02 00:25:48,401 - trainer - INFO -     doc_entropy    : 1.089144
2024-04-02 00:25:48,401 - trainer - INFO -     val_loss       : 1.126493
2024-04-02 00:25:48,401 - trainer - INFO -     val_accuracy   : 0.670815
2024-04-02 00:25:48,401 - trainer - INFO -     val_macro_f    : 0.649602
2024-04-02 00:25:48,401 - trainer - INFO -     val_precision  : 0.67764
2024-04-02 00:25:48,401 - trainer - INFO -     val_recall     : 0.670815
2024-04-02 00:25:48,401 - trainer - INFO -     val_doc_entropy: 0.492868
2024-04-02 00:25:48,401 - trainer - INFO -     test_loss      : 1.128
2024-04-02 00:25:48,401 - trainer - INFO -     test_accuracy  : 0.67206
2024-04-02 00:25:48,401 - trainer - INFO -     test_macro_f   : 0.652861
2024-04-02 00:25:48,401 - trainer - INFO -     test_precision : 0.684331
2024-04-02 00:25:48,401 - trainer - INFO -     test_recall    : 0.67206
2024-04-02 00:25:48,401 - trainer - INFO -     test_doc_entropy: 0.49262
2024-04-02 00:29:37,252 - trainer - INFO -     epoch          : 2
2024-04-02 00:29:37,252 - trainer - INFO -     loss           : 0.951168
2024-04-02 00:29:37,252 - trainer - INFO -     accuracy       : 0.71956
2024-04-02 00:29:37,252 - trainer - INFO -     macro_f        : 0.708618
2024-04-02 00:29:37,252 - trainer - INFO -     precision      : 0.743438
2024-04-02 00:29:37,252 - trainer - INFO -     recall         : 0.71956
2024-04-02 00:29:37,252 - trainer - INFO -     doc_entropy    : 0.352943
2024-04-02 00:29:37,252 - trainer - INFO -     val_loss       : 1.103681
2024-04-02 00:29:37,252 - trainer - INFO -     val_accuracy   : 0.68361
2024-04-02 00:29:37,252 - trainer - INFO -     val_macro_f    : 0.673186
2024-04-02 00:29:37,252 - trainer - INFO -     val_precision  : 0.710682
2024-04-02 00:29:37,252 - trainer - INFO -     val_recall     : 0.68361
2024-04-02 00:29:37,252 - trainer - INFO -     val_doc_entropy: 0.279002
2024-04-02 00:29:37,252 - trainer - INFO -     test_loss      : 1.109969
2024-04-02 00:29:37,252 - trainer - INFO -     test_accuracy  : 0.67898
2024-04-02 00:29:37,252 - trainer - INFO -     test_macro_f   : 0.667928
2024-04-02 00:29:37,252 - trainer - INFO -     test_precision : 0.705736
2024-04-02 00:29:37,268 - trainer - INFO -     test_recall    : 0.67898
2024-04-02 00:29:37,268 - trainer - INFO -     test_doc_entropy: 0.278937
2024-04-02 00:33:27,615 - trainer - INFO -     epoch          : 3
2024-04-02 00:33:27,615 - trainer - INFO -     loss           : 0.762556
2024-04-02 00:33:27,615 - trainer - INFO -     accuracy       : 0.772175
2024-04-02 00:33:27,615 - trainer - INFO -     macro_f        : 0.765138
2024-04-02 00:33:27,615 - trainer - INFO -     precision      : 0.798559
2024-04-02 00:33:27,615 - trainer - INFO -     recall         : 0.772175
2024-04-02 00:33:27,615 - trainer - INFO -     doc_entropy    : 0.252354
2024-04-02 00:33:27,615 - trainer - INFO -     val_loss       : 1.166388
2024-04-02 00:33:27,615 - trainer - INFO -     val_accuracy   : 0.674898
2024-04-02 00:33:27,615 - trainer - INFO -     val_macro_f    : 0.665921
2024-04-02 00:33:27,615 - trainer - INFO -     val_precision  : 0.704291
2024-04-02 00:33:27,615 - trainer - INFO -     val_recall     : 0.674898
2024-04-02 00:33:27,615 - trainer - INFO -     val_doc_entropy: 0.226196
2024-04-02 00:33:27,615 - trainer - INFO -     test_loss      : 1.168362
2024-04-02 00:33:27,615 - trainer - INFO -     test_accuracy  : 0.6744
2024-04-02 00:33:27,615 - trainer - INFO -     test_macro_f   : 0.664103
2024-04-02 00:33:27,615 - trainer - INFO -     test_precision : 0.702125
2024-04-02 00:33:27,615 - trainer - INFO -     test_recall    : 0.6744
2024-04-02 00:33:27,615 - trainer - INFO -     test_doc_entropy: 0.226041
2024-04-02 00:34:03,569 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=2400, bias=True)
    (1): Tanh()
    (2): Linear(in_features=2400, out_features=120, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,498,203
Freeze params: 0
2024-04-02 00:37:50,751 - trainer - INFO -     epoch          : 1
2024-04-02 00:37:50,751 - trainer - INFO -     loss           : 1.254517
2024-04-02 00:37:50,751 - trainer - INFO -     accuracy       : 0.644682
2024-04-02 00:37:50,751 - trainer - INFO -     macro_f        : 0.624518
2024-04-02 00:37:50,751 - trainer - INFO -     precision      : 0.655283
2024-04-02 00:37:50,751 - trainer - INFO -     recall         : 0.644682
2024-04-02 00:37:50,751 - trainer - INFO -     doc_entropy    : 0.929537
2024-04-02 00:37:50,751 - trainer - INFO -     val_loss       : 1.116767
2024-04-02 00:37:50,751 - trainer - INFO -     val_accuracy   : 0.675894
2024-04-02 00:37:50,751 - trainer - INFO -     val_macro_f    : 0.664819
2024-04-02 00:37:50,751 - trainer - INFO -     val_precision  : 0.701851
2024-04-02 00:37:50,751 - trainer - INFO -     val_recall     : 0.675894
2024-04-02 00:37:50,751 - trainer - INFO -     val_doc_entropy: 0.412344
2024-04-02 00:37:50,751 - trainer - INFO -     test_loss      : 1.115987
2024-04-02 00:37:50,751 - trainer - INFO -     test_accuracy  : 0.67898
2024-04-02 00:37:50,751 - trainer - INFO -     test_macro_f   : 0.66724
2024-04-02 00:37:50,751 - trainer - INFO -     test_precision : 0.704434
2024-04-02 00:37:50,751 - trainer - INFO -     test_recall    : 0.67898
2024-04-02 00:37:50,751 - trainer - INFO -     test_doc_entropy: 0.413587
2024-04-02 00:41:42,054 - trainer - INFO -     epoch          : 2
2024-04-02 00:41:42,054 - trainer - INFO -     loss           : 0.951973
2024-04-02 00:41:42,054 - trainer - INFO -     accuracy       : 0.720518
2024-04-02 00:41:42,054 - trainer - INFO -     macro_f        : 0.710031
2024-04-02 00:41:42,069 - trainer - INFO -     precision      : 0.745451
2024-04-02 00:41:42,069 - trainer - INFO -     recall         : 0.720518
2024-04-02 00:41:42,069 - trainer - INFO -     doc_entropy    : 0.303455
2024-04-02 00:41:42,069 - trainer - INFO -     val_loss       : 1.122107
2024-04-02 00:41:42,069 - trainer - INFO -     val_accuracy   : 0.680026
2024-04-02 00:41:42,069 - trainer - INFO -     val_macro_f    : 0.669483
2024-04-02 00:41:42,069 - trainer - INFO -     val_precision  : 0.707033
2024-04-02 00:41:42,069 - trainer - INFO -     val_recall     : 0.680026
2024-04-02 00:41:42,069 - trainer - INFO -     val_doc_entropy: 0.244778
2024-04-02 00:41:42,069 - trainer - INFO -     test_loss      : 1.107611
2024-04-02 00:41:42,069 - trainer - INFO -     test_accuracy  : 0.684059
2024-04-02 00:41:42,069 - trainer - INFO -     test_macro_f   : 0.673403
2024-04-02 00:41:42,069 - trainer - INFO -     test_precision : 0.711939
2024-04-02 00:41:42,069 - trainer - INFO -     test_recall    : 0.684059
2024-04-02 00:41:42,069 - trainer - INFO -     test_doc_entropy: 0.244893
2024-04-02 00:45:34,577 - trainer - INFO -     epoch          : 3
2024-04-02 00:45:34,577 - trainer - INFO -     loss           : 0.765304
2024-04-02 00:45:34,577 - trainer - INFO -     accuracy       : 0.772436
2024-04-02 00:45:34,577 - trainer - INFO -     macro_f        : 0.764857
2024-04-02 00:45:34,577 - trainer - INFO -     precision      : 0.797643
2024-04-02 00:45:34,577 - trainer - INFO -     recall         : 0.772436
2024-04-02 00:45:34,577 - trainer - INFO -     doc_entropy    : 0.228811
2024-04-02 00:45:34,577 - trainer - INFO -     val_loss       : 1.179598
2024-04-02 00:45:34,577 - trainer - INFO -     val_accuracy   : 0.670417
2024-04-02 00:45:34,577 - trainer - INFO -     val_macro_f    : 0.664134
2024-04-02 00:45:34,577 - trainer - INFO -     val_precision  : 0.707727
2024-04-02 00:45:34,577 - trainer - INFO -     val_recall     : 0.670417
2024-04-02 00:45:34,592 - trainer - INFO -     val_doc_entropy: 0.209205
2024-04-02 00:45:34,592 - trainer - INFO -     test_loss      : 1.171746
2024-04-02 00:45:34,592 - trainer - INFO -     test_accuracy  : 0.670517
2024-04-02 00:45:34,592 - trainer - INFO -     test_macro_f   : 0.661618
2024-04-02 00:45:34,592 - trainer - INFO -     test_precision : 0.703348
2024-04-02 00:45:34,593 - trainer - INFO -     test_recall    : 0.670517
2024-04-02 00:45:34,593 - trainer - INFO -     test_doc_entropy: 0.209239
2024-04-02 00:46:10,449 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=2400, bias=True)
    (1): Tanh()
    (2): Linear(in_features=2400, out_features=120, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,498,203
Freeze params: 0
2024-04-02 00:49:58,404 - trainer - INFO -     epoch          : 1
2024-04-02 00:49:58,404 - trainer - INFO -     loss           : 1.247838
2024-04-02 00:49:58,404 - trainer - INFO -     accuracy       : 0.645691
2024-04-02 00:49:58,420 - trainer - INFO -     macro_f        : 0.625641
2024-04-02 00:49:58,420 - trainer - INFO -     precision      : 0.657567
2024-04-02 00:49:58,420 - trainer - INFO -     recall         : 0.645691
2024-04-02 00:49:58,420 - trainer - INFO -     doc_entropy    : 1.28725
2024-04-02 00:49:58,420 - trainer - INFO -     val_loss       : 1.103434
2024-04-02 00:49:58,420 - trainer - INFO -     val_accuracy   : 0.677935
2024-04-02 00:49:58,420 - trainer - INFO -     val_macro_f    : 0.663828
2024-04-02 00:49:58,420 - trainer - INFO -     val_precision  : 0.696255
2024-04-02 00:49:58,420 - trainer - INFO -     val_recall     : 0.677935
2024-04-02 00:49:58,420 - trainer - INFO -     val_doc_entropy: 0.622223
2024-04-02 00:49:58,420 - trainer - INFO -     test_loss      : 1.11097
2024-04-02 00:49:58,420 - trainer - INFO -     test_accuracy  : 0.679976
2024-04-02 00:49:58,420 - trainer - INFO -     test_macro_f   : 0.666342
2024-04-02 00:49:58,420 - trainer - INFO -     test_precision : 0.700523
2024-04-02 00:49:58,420 - trainer - INFO -     test_recall    : 0.679976
2024-04-02 00:49:58,420 - trainer - INFO -     test_doc_entropy: 0.622097
2024-04-02 00:53:48,273 - trainer - INFO -     epoch          : 2
2024-04-02 00:53:48,273 - trainer - INFO -     loss           : 0.944483
2024-04-02 00:53:48,273 - trainer - INFO -     accuracy       : 0.720338
2024-04-02 00:53:48,273 - trainer - INFO -     macro_f        : 0.709442
2024-04-02 00:53:48,273 - trainer - INFO -     precision      : 0.74412
2024-04-02 00:53:48,273 - trainer - INFO -     recall         : 0.720338
2024-04-02 00:53:48,273 - trainer - INFO -     doc_entropy    : 0.434335
2024-04-02 00:53:48,273 - trainer - INFO -     val_loss       : 1.112023
2024-04-02 00:53:48,273 - trainer - INFO -     val_accuracy   : 0.683511
2024-04-02 00:53:48,273 - trainer - INFO -     val_macro_f    : 0.672746
2024-04-02 00:53:48,273 - trainer - INFO -     val_precision  : 0.71006
2024-04-02 00:53:48,273 - trainer - INFO -     val_recall     : 0.683511
2024-04-02 00:53:48,273 - trainer - INFO -     val_doc_entropy: 0.3252
2024-04-02 00:53:48,289 - trainer - INFO -     test_loss      : 1.106762
2024-04-02 00:53:48,289 - trainer - INFO -     test_accuracy  : 0.682216
2024-04-02 00:53:48,289 - trainer - INFO -     test_macro_f   : 0.670641
2024-04-02 00:53:48,289 - trainer - INFO -     test_precision : 0.707179
2024-04-02 00:53:48,289 - trainer - INFO -     test_recall    : 0.682216
2024-04-02 00:53:48,289 - trainer - INFO -     test_doc_entropy: 0.324606
2024-04-02 00:57:38,526 - trainer - INFO -     epoch          : 3
2024-04-02 00:57:38,526 - trainer - INFO -     loss           : 0.752898
2024-04-02 00:57:38,526 - trainer - INFO -     accuracy       : 0.776164
2024-04-02 00:57:38,526 - trainer - INFO -     macro_f        : 0.768395
2024-04-02 00:57:38,526 - trainer - INFO -     precision      : 0.799819
2024-04-02 00:57:38,526 - trainer - INFO -     recall         : 0.776164
2024-04-02 00:57:38,526 - trainer - INFO -     doc_entropy    : 0.280054
2024-04-02 00:57:38,526 - trainer - INFO -     val_loss       : 1.190231
2024-04-02 00:57:38,526 - trainer - INFO -     val_accuracy   : 0.66738
2024-04-02 00:57:38,526 - trainer - INFO -     val_macro_f    : 0.66038
2024-04-02 00:57:38,526 - trainer - INFO -     val_precision  : 0.703698
2024-04-02 00:57:38,526 - trainer - INFO -     val_recall     : 0.66738
2024-04-02 00:57:38,526 - trainer - INFO -     val_doc_entropy: 0.245089
2024-04-02 00:57:38,526 - trainer - INFO -     test_loss      : 1.175571
2024-04-02 00:57:38,526 - trainer - INFO -     test_accuracy  : 0.671662
2024-04-02 00:57:38,526 - trainer - INFO -     test_macro_f   : 0.665663
2024-04-02 00:57:38,526 - trainer - INFO -     test_precision : 0.709632
2024-04-02 00:57:38,526 - trainer - INFO -     test_recall    : 0.671662
2024-04-02 00:57:38,526 - trainer - INFO -     test_doc_entropy: 0.244896
2024-04-02 00:59:03,277 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=2800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=2800, out_features=140, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,722,623
Freeze params: 0
2024-04-02 01:03:02,063 - trainer - INFO -     epoch          : 1
2024-04-02 01:03:02,063 - trainer - INFO -     loss           : 1.262138
2024-04-02 01:03:02,063 - trainer - INFO -     accuracy       : 0.641359
2024-04-02 01:03:02,063 - trainer - INFO -     macro_f        : 0.621539
2024-04-02 01:03:02,063 - trainer - INFO -     precision      : 0.653359
2024-04-02 01:03:02,063 - trainer - INFO -     recall         : 0.641359
2024-04-02 01:03:02,063 - trainer - INFO -     doc_entropy    : 0.700906
2024-04-02 01:03:02,063 - trainer - INFO -     val_loss       : 1.124564
2024-04-02 01:03:02,063 - trainer - INFO -     val_accuracy   : 0.675894
2024-04-02 01:03:02,063 - trainer - INFO -     val_macro_f    : 0.665609
2024-04-02 01:03:02,063 - trainer - INFO -     val_precision  : 0.70458
2024-04-02 01:03:02,063 - trainer - INFO -     val_recall     : 0.675894
2024-04-02 01:03:02,063 - trainer - INFO -     val_doc_entropy: 0.288762
2024-04-02 01:03:02,063 - trainer - INFO -     test_loss      : 1.134848
2024-04-02 01:03:02,063 - trainer - INFO -     test_accuracy  : 0.676143
2024-04-02 01:03:02,063 - trainer - INFO -     test_macro_f   : 0.667771
2024-04-02 01:03:02,063 - trainer - INFO -     test_precision : 0.707662
2024-04-02 01:03:02,063 - trainer - INFO -     test_recall    : 0.676143
2024-04-02 01:03:02,063 - trainer - INFO -     test_doc_entropy: 0.288295
2024-04-02 01:07:06,181 - trainer - INFO -     epoch          : 2
2024-04-02 01:07:06,181 - trainer - INFO -     loss           : 0.969417
2024-04-02 01:07:06,181 - trainer - INFO -     accuracy       : 0.71511
2024-04-02 01:07:06,181 - trainer - INFO -     macro_f        : 0.703476
2024-04-02 01:07:06,181 - trainer - INFO -     precision      : 0.737865
2024-04-02 01:07:06,181 - trainer - INFO -     recall         : 0.71511
2024-04-02 01:07:06,181 - trainer - INFO -     doc_entropy    : 0.226827
2024-04-02 01:07:06,181 - trainer - INFO -     val_loss       : 1.126841
2024-04-02 01:07:06,181 - trainer - INFO -     val_accuracy   : 0.674749
2024-04-02 01:07:06,181 - trainer - INFO -     val_macro_f    : 0.661626
2024-04-02 01:07:06,181 - trainer - INFO -     val_precision  : 0.697732
2024-04-02 01:07:06,181 - trainer - INFO -     val_recall     : 0.674749
2024-04-02 01:07:06,181 - trainer - INFO -     val_doc_entropy: 0.191524
2024-04-02 01:07:06,181 - trainer - INFO -     test_loss      : 1.119812
2024-04-02 01:07:06,185 - trainer - INFO -     test_accuracy  : 0.677686
2024-04-02 01:07:06,185 - trainer - INFO -     test_macro_f   : 0.66705
2024-04-02 01:07:06,185 - trainer - INFO -     test_precision : 0.706062
2024-04-02 01:07:06,185 - trainer - INFO -     test_recall    : 0.677686
2024-04-02 01:07:06,185 - trainer - INFO -     test_doc_entropy: 0.191569
2024-04-02 01:11:10,306 - trainer - INFO -     epoch          : 3
2024-04-02 01:11:10,306 - trainer - INFO -     loss           : 0.789998
2024-04-02 01:11:10,306 - trainer - INFO -     accuracy       : 0.763362
2024-04-02 01:11:10,306 - trainer - INFO -     macro_f        : 0.755607
2024-04-02 01:11:10,306 - trainer - INFO -     precision      : 0.789219
2024-04-02 01:11:10,306 - trainer - INFO -     recall         : 0.763362
2024-04-02 01:11:10,306 - trainer - INFO -     doc_entropy    : 0.186588
2024-04-02 01:11:10,306 - trainer - INFO -     val_loss       : 1.176351
2024-04-02 01:11:10,306 - trainer - INFO -     val_accuracy   : 0.671015
2024-04-02 01:11:10,306 - trainer - INFO -     val_macro_f    : 0.664604
2024-04-02 01:11:10,306 - trainer - INFO -     val_precision  : 0.705272
2024-04-02 01:11:10,306 - trainer - INFO -     val_recall     : 0.671015
2024-04-02 01:11:10,306 - trainer - INFO -     val_doc_entropy: 0.1744
2024-04-02 01:11:10,306 - trainer - INFO -     test_loss      : 1.174104
2024-04-02 01:11:10,306 - trainer - INFO -     test_accuracy  : 0.671015
2024-04-02 01:11:10,306 - trainer - INFO -     test_macro_f   : 0.665309
2024-04-02 01:11:10,306 - trainer - INFO -     test_precision : 0.709715
2024-04-02 01:11:10,306 - trainer - INFO -     test_recall    : 0.671015
2024-04-02 01:11:10,306 - trainer - INFO -     test_doc_entropy: 0.17352
2024-04-02 01:11:46,008 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=2800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=2800, out_features=140, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,722,623
Freeze params: 0
2024-04-02 01:15:47,394 - trainer - INFO -     epoch          : 1
2024-04-02 01:15:47,394 - trainer - INFO -     loss           : 1.254173
2024-04-02 01:15:47,394 - trainer - INFO -     accuracy       : 0.644334
2024-04-02 01:15:47,394 - trainer - INFO -     macro_f        : 0.624526
2024-04-02 01:15:47,394 - trainer - INFO -     precision      : 0.655053
2024-04-02 01:15:47,394 - trainer - INFO -     recall         : 0.644334
2024-04-02 01:15:47,394 - trainer - INFO -     doc_entropy    : 0.837995
2024-04-02 01:15:47,394 - trainer - INFO -     val_loss       : 1.117676
2024-04-02 01:15:47,394 - trainer - INFO -     val_accuracy   : 0.676342
2024-04-02 01:15:47,394 - trainer - INFO -     val_macro_f    : 0.660067
2024-04-02 01:15:47,394 - trainer - INFO -     val_precision  : 0.694018
2024-04-02 01:15:47,394 - trainer - INFO -     val_recall     : 0.676342
2024-04-02 01:15:47,394 - trainer - INFO -     val_doc_entropy: 0.333235
2024-04-02 01:15:47,394 - trainer - INFO -     test_loss      : 1.117313
2024-04-02 01:15:47,409 - trainer - INFO -     test_accuracy  : 0.673604
2024-04-02 01:15:47,409 - trainer - INFO -     test_macro_f   : 0.657424
2024-04-02 01:15:47,409 - trainer - INFO -     test_precision : 0.690954
2024-04-02 01:15:47,409 - trainer - INFO -     test_recall    : 0.673604
2024-04-02 01:15:47,409 - trainer - INFO -     test_doc_entropy: 0.332825
2024-04-02 01:19:49,880 - trainer - INFO -     epoch          : 2
2024-04-02 01:19:49,880 - trainer - INFO -     loss           : 0.957983
2024-04-02 01:19:49,880 - trainer - INFO -     accuracy       : 0.717792
2024-04-02 01:19:49,880 - trainer - INFO -     macro_f        : 0.706805
2024-04-02 01:19:49,880 - trainer - INFO -     precision      : 0.7415
2024-04-02 01:19:49,880 - trainer - INFO -     recall         : 0.717792
2024-04-02 01:19:49,880 - trainer - INFO -     doc_entropy    : 0.256913
2024-04-02 01:19:49,880 - trainer - INFO -     val_loss       : 1.124761
2024-04-02 01:19:49,880 - trainer - INFO -     val_accuracy   : 0.674848
2024-04-02 01:19:49,880 - trainer - INFO -     val_macro_f    : 0.667316
2024-04-02 01:19:49,880 - trainer - INFO -     val_precision  : 0.709638
2024-04-02 01:19:49,880 - trainer - INFO -     val_recall     : 0.674848
2024-04-02 01:19:49,880 - trainer - INFO -     val_doc_entropy: 0.204592
2024-04-02 01:19:49,880 - trainer - INFO -     test_loss      : 1.118351
2024-04-02 01:19:49,896 - trainer - INFO -     test_accuracy  : 0.677288
2024-04-02 01:19:49,896 - trainer - INFO -     test_macro_f   : 0.667711
2024-04-02 01:19:49,896 - trainer - INFO -     test_precision : 0.70746
2024-04-02 01:19:49,896 - trainer - INFO -     test_recall    : 0.677288
2024-04-02 01:19:49,896 - trainer - INFO -     test_doc_entropy: 0.204682
2024-04-02 01:23:52,961 - trainer - INFO -     epoch          : 3
2024-04-02 01:23:52,961 - trainer - INFO -     loss           : 0.778934
2024-04-02 01:23:52,961 - trainer - INFO -     accuracy       : 0.768267
2024-04-02 01:23:52,961 - trainer - INFO -     macro_f        : 0.760471
2024-04-02 01:23:52,961 - trainer - INFO -     precision      : 0.793457
2024-04-02 01:23:52,961 - trainer - INFO -     recall         : 0.768267
2024-04-02 01:23:52,961 - trainer - INFO -     doc_entropy    : 0.191476
2024-04-02 01:23:52,961 - trainer - INFO -     val_loss       : 1.173534
2024-04-02 01:23:52,961 - trainer - INFO -     val_accuracy   : 0.673255
2024-04-02 01:23:52,961 - trainer - INFO -     val_macro_f    : 0.658742
2024-04-02 01:23:52,961 - trainer - INFO -     val_precision  : 0.693708
2024-04-02 01:23:52,961 - trainer - INFO -     val_recall     : 0.673255
2024-04-02 01:23:52,961 - trainer - INFO -     val_doc_entropy: 0.171679
2024-04-02 01:23:52,961 - trainer - INFO -     test_loss      : 1.17056
2024-04-02 01:23:52,961 - trainer - INFO -     test_accuracy  : 0.670367
2024-04-02 01:23:52,961 - trainer - INFO -     test_macro_f   : 0.655911
2024-04-02 01:23:52,961 - trainer - INFO -     test_precision : 0.692586
2024-04-02 01:23:52,961 - trainer - INFO -     test_recall    : 0.670367
2024-04-02 01:23:52,961 - trainer - INFO -     test_doc_entropy: 0.171243
2024-04-02 01:24:28,211 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=2800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=2800, out_features=140, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,722,623
Freeze params: 0
2024-04-02 01:28:29,412 - trainer - INFO -     epoch          : 1
2024-04-02 01:28:29,412 - trainer - INFO -     loss           : 1.262598
2024-04-02 01:28:29,412 - trainer - INFO -     accuracy       : 0.642131
2024-04-02 01:28:29,412 - trainer - INFO -     macro_f        : 0.622147
2024-04-02 01:28:29,412 - trainer - INFO -     precision      : 0.653593
2024-04-02 01:28:29,412 - trainer - INFO -     recall         : 0.642131
2024-04-02 01:28:29,412 - trainer - INFO -     doc_entropy    : 0.78116
2024-04-02 01:28:29,412 - trainer - INFO -     val_loss       : 1.112029
2024-04-02 01:28:29,412 - trainer - INFO -     val_accuracy   : 0.674998
2024-04-02 01:28:29,412 - trainer - INFO -     val_macro_f    : 0.657226
2024-04-02 01:28:29,412 - trainer - INFO -     val_precision  : 0.688567
2024-04-02 01:28:29,412 - trainer - INFO -     val_recall     : 0.674998
2024-04-02 01:28:29,412 - trainer - INFO -     val_doc_entropy: 0.314058
2024-04-02 01:28:29,412 - trainer - INFO -     test_loss      : 1.112851
2024-04-02 01:28:29,412 - trainer - INFO -     test_accuracy  : 0.674599
2024-04-02 01:28:29,412 - trainer - INFO -     test_macro_f   : 0.655587
2024-04-02 01:28:29,412 - trainer - INFO -     test_precision : 0.686724
2024-04-02 01:28:29,412 - trainer - INFO -     test_recall    : 0.674599
2024-04-02 01:28:29,412 - trainer - INFO -     test_doc_entropy: 0.3143
2024-04-02 01:32:32,923 - trainer - INFO -     epoch          : 2
2024-04-02 01:32:32,923 - trainer - INFO -     loss           : 0.962538
2024-04-02 01:32:32,923 - trainer - INFO -     accuracy       : 0.716865
2024-04-02 01:32:32,923 - trainer - INFO -     macro_f        : 0.70534
2024-04-02 01:32:32,923 - trainer - INFO -     precision      : 0.739363
2024-04-02 01:32:32,923 - trainer - INFO -     recall         : 0.716865
2024-04-02 01:32:32,923 - trainer - INFO -     doc_entropy    : 0.245628
2024-04-02 01:32:32,923 - trainer - INFO -     val_loss       : 1.115272
2024-04-02 01:32:32,923 - trainer - INFO -     val_accuracy   : 0.677089
2024-04-02 01:32:32,923 - trainer - INFO -     val_macro_f    : 0.661226
2024-04-02 01:32:32,923 - trainer - INFO -     val_precision  : 0.694046
2024-04-02 01:32:32,923 - trainer - INFO -     val_recall     : 0.677089
2024-04-02 01:32:32,923 - trainer - INFO -     val_doc_entropy: 0.20024
2024-04-02 01:32:32,923 - trainer - INFO -     test_loss      : 1.112493
2024-04-02 01:32:32,923 - trainer - INFO -     test_accuracy  : 0.675446
2024-04-02 01:32:32,923 - trainer - INFO -     test_macro_f   : 0.661083
2024-04-02 01:32:32,923 - trainer - INFO -     test_precision : 0.694645
2024-04-02 01:32:32,923 - trainer - INFO -     test_recall    : 0.675446
2024-04-02 01:32:32,923 - trainer - INFO -     test_doc_entropy: 0.200572
2024-04-02 01:36:35,753 - trainer - INFO -     epoch          : 3
2024-04-02 01:36:35,753 - trainer - INFO -     loss           : 0.781578
2024-04-02 01:36:35,768 - trainer - INFO -     accuracy       : 0.768086
2024-04-02 01:36:35,768 - trainer - INFO -     macro_f        : 0.760549
2024-04-02 01:36:35,768 - trainer - INFO -     precision      : 0.793805
2024-04-02 01:36:35,768 - trainer - INFO -     recall         : 0.768086
2024-04-02 01:36:35,768 - trainer - INFO -     doc_entropy    : 0.190476
2024-04-02 01:36:35,768 - trainer - INFO -     val_loss       : 1.174341
2024-04-02 01:36:35,768 - trainer - INFO -     val_accuracy   : 0.668227
2024-04-02 01:36:35,768 - trainer - INFO -     val_macro_f    : 0.657627
2024-04-02 01:36:35,768 - trainer - INFO -     val_precision  : 0.697941
2024-04-02 01:36:35,768 - trainer - INFO -     val_recall     : 0.668227
2024-04-02 01:36:35,768 - trainer - INFO -     val_doc_entropy: 0.182605
2024-04-02 01:36:35,768 - trainer - INFO -     test_loss      : 1.164248
2024-04-02 01:36:35,768 - trainer - INFO -     test_accuracy  : 0.668774
2024-04-02 01:36:35,768 - trainer - INFO -     test_macro_f   : 0.65985
2024-04-02 01:36:35,768 - trainer - INFO -     test_precision : 0.701553
2024-04-02 01:36:35,768 - trainer - INFO -     test_recall    : 0.668774
2024-04-02 01:36:35,768 - trainer - INFO -     test_doc_entropy: 0.182032
2024-04-02 01:37:10,316 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=2800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=2800, out_features=140, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,722,623
Freeze params: 0
2024-04-02 01:41:11,046 - trainer - INFO -     epoch          : 1
2024-04-02 01:41:11,046 - trainer - INFO -     loss           : 1.256726
2024-04-02 01:41:11,046 - trainer - INFO -     accuracy       : 0.641334
2024-04-02 01:41:11,046 - trainer - INFO -     macro_f        : 0.621628
2024-04-02 01:41:11,046 - trainer - INFO -     precision      : 0.653149
2024-04-02 01:41:11,046 - trainer - INFO -     recall         : 0.641334
2024-04-02 01:41:11,046 - trainer - INFO -     doc_entropy    : 0.819979
2024-04-02 01:41:11,046 - trainer - INFO -     val_loss       : 1.122984
2024-04-02 01:41:11,046 - trainer - INFO -     val_accuracy   : 0.672409
2024-04-02 01:41:11,046 - trainer - INFO -     val_macro_f    : 0.657862
2024-04-02 01:41:11,046 - trainer - INFO -     val_precision  : 0.692135
2024-04-02 01:41:11,061 - trainer - INFO -     val_recall     : 0.672409
2024-04-02 01:41:11,061 - trainer - INFO -     val_doc_entropy: 0.319974
2024-04-02 01:41:11,061 - trainer - INFO -     test_loss      : 1.124416
2024-04-02 01:41:11,061 - trainer - INFO -     test_accuracy  : 0.672608
2024-04-02 01:41:11,061 - trainer - INFO -     test_macro_f   : 0.657533
2024-04-02 01:41:11,061 - trainer - INFO -     test_precision : 0.691611
2024-04-02 01:41:11,061 - trainer - INFO -     test_recall    : 0.672608
2024-04-02 01:41:11,061 - trainer - INFO -     test_doc_entropy: 0.319926
2024-04-02 01:45:13,886 - trainer - INFO -     epoch          : 2
2024-04-02 01:45:13,886 - trainer - INFO -     loss           : 0.965406
2024-04-02 01:45:13,886 - trainer - INFO -     accuracy       : 0.715427
2024-04-02 01:45:13,886 - trainer - INFO -     macro_f        : 0.704435
2024-04-02 01:45:13,886 - trainer - INFO -     precision      : 0.739459
2024-04-02 01:45:13,886 - trainer - INFO -     recall         : 0.715427
2024-04-02 01:45:13,886 - trainer - INFO -     doc_entropy    : 0.244061
2024-04-02 01:45:13,886 - trainer - INFO -     val_loss       : 1.108093
2024-04-02 01:45:13,886 - trainer - INFO -     val_accuracy   : 0.678184
2024-04-02 01:45:13,886 - trainer - INFO -     val_macro_f    : 0.666944
2024-04-02 01:45:13,886 - trainer - INFO -     val_precision  : 0.704975
2024-04-02 01:45:13,886 - trainer - INFO -     val_recall     : 0.678184
2024-04-02 01:45:13,886 - trainer - INFO -     val_doc_entropy: 0.193471
2024-04-02 01:45:13,886 - trainer - INFO -     test_loss      : 1.099661
2024-04-02 01:45:13,886 - trainer - INFO -     test_accuracy  : 0.681121
2024-04-02 01:45:13,886 - trainer - INFO -     test_macro_f   : 0.670006
2024-04-02 01:45:13,886 - trainer - INFO -     test_precision : 0.70731
2024-04-02 01:45:13,886 - trainer - INFO -     test_recall    : 0.681121
2024-04-02 01:45:13,886 - trainer - INFO -     test_doc_entropy: 0.193473
2024-04-02 01:49:17,146 - trainer - INFO -     epoch          : 3
2024-04-02 01:49:17,146 - trainer - INFO -     loss           : 0.786128
2024-04-02 01:49:17,146 - trainer - INFO -     accuracy       : 0.76579
2024-04-02 01:49:17,146 - trainer - INFO -     macro_f        : 0.758453
2024-04-02 01:49:17,146 - trainer - INFO -     precision      : 0.792003
2024-04-02 01:49:17,146 - trainer - INFO -     recall         : 0.76579
2024-04-02 01:49:17,146 - trainer - INFO -     doc_entropy    : 0.183893
2024-04-02 01:49:17,146 - trainer - INFO -     val_loss       : 1.164274
2024-04-02 01:49:17,146 - trainer - INFO -     val_accuracy   : 0.674948
2024-04-02 01:49:17,146 - trainer - INFO -     val_macro_f    : 0.664756
2024-04-02 01:49:17,146 - trainer - INFO -     val_precision  : 0.702805
2024-04-02 01:49:17,146 - trainer - INFO -     val_recall     : 0.674948
2024-04-02 01:49:17,146 - trainer - INFO -     val_doc_entropy: 0.171858
2024-04-02 01:49:17,146 - trainer - INFO -     test_loss      : 1.161124
2024-04-02 01:49:17,146 - trainer - INFO -     test_accuracy  : 0.6744
2024-04-02 01:49:17,146 - trainer - INFO -     test_macro_f   : 0.664544
2024-04-02 01:49:17,146 - trainer - INFO -     test_precision : 0.704217
2024-04-02 01:49:17,146 - trainer - INFO -     test_recall    : 0.6744
2024-04-02 01:49:17,146 - trainer - INFO -     test_doc_entropy: 0.170756
2024-04-02 01:49:52,225 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=2800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=2800, out_features=140, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,722,623
Freeze params: 0
2024-04-02 01:53:53,001 - trainer - INFO -     epoch          : 1
2024-04-02 01:53:53,001 - trainer - INFO -     loss           : 1.257699
2024-04-02 01:53:53,001 - trainer - INFO -     accuracy       : 0.642697
2024-04-02 01:53:53,001 - trainer - INFO -     macro_f        : 0.622178
2024-04-02 01:53:53,001 - trainer - INFO -     precision      : 0.652815
2024-04-02 01:53:53,001 - trainer - INFO -     recall         : 0.642697
2024-04-02 01:53:53,001 - trainer - INFO -     doc_entropy    : 0.819701
2024-04-02 01:53:53,001 - trainer - INFO -     val_loss       : 1.125499
2024-04-02 01:53:53,001 - trainer - INFO -     val_accuracy   : 0.673753
2024-04-02 01:53:53,001 - trainer - INFO -     val_macro_f    : 0.662348
2024-04-02 01:53:53,001 - trainer - INFO -     val_precision  : 0.699102
2024-04-02 01:53:53,001 - trainer - INFO -     val_recall     : 0.673753
2024-04-02 01:53:53,001 - trainer - INFO -     val_doc_entropy: 0.32848
2024-04-02 01:53:53,001 - trainer - INFO -     test_loss      : 1.129932
2024-04-02 01:53:53,001 - trainer - INFO -     test_accuracy  : 0.673355
2024-04-02 01:53:53,001 - trainer - INFO -     test_macro_f   : 0.660309
2024-04-02 01:53:53,001 - trainer - INFO -     test_precision : 0.695514
2024-04-02 01:53:53,001 - trainer - INFO -     test_recall    : 0.673355
2024-04-02 01:53:53,001 - trainer - INFO -     test_doc_entropy: 0.328249
2024-04-02 01:57:55,797 - trainer - INFO -     epoch          : 2
2024-04-02 01:57:55,797 - trainer - INFO -     loss           : 0.960583
2024-04-02 01:57:55,797 - trainer - INFO -     accuracy       : 0.717095
2024-04-02 01:57:55,797 - trainer - INFO -     macro_f        : 0.705887
2024-04-02 01:57:55,797 - trainer - INFO -     precision      : 0.740263
2024-04-02 01:57:55,797 - trainer - INFO -     recall         : 0.717095
2024-04-02 01:57:55,797 - trainer - INFO -     doc_entropy    : 0.24892
2024-04-02 01:57:55,797 - trainer - INFO -     val_loss       : 1.122254
2024-04-02 01:57:55,797 - trainer - INFO -     val_accuracy   : 0.67918
2024-04-02 01:57:55,797 - trainer - INFO -     val_macro_f    : 0.665803
2024-04-02 01:57:55,797 - trainer - INFO -     val_precision  : 0.700588
2024-04-02 01:57:55,797 - trainer - INFO -     val_recall     : 0.67918
2024-04-02 01:57:55,813 - trainer - INFO -     val_doc_entropy: 0.205591
2024-04-02 01:57:55,813 - trainer - INFO -     test_loss      : 1.121603
2024-04-02 01:57:55,813 - trainer - INFO -     test_accuracy  : 0.676541
2024-04-02 01:57:55,813 - trainer - INFO -     test_macro_f   : 0.663412
2024-04-02 01:57:55,813 - trainer - INFO -     test_precision : 0.70046
2024-04-02 01:57:55,813 - trainer - INFO -     test_recall    : 0.676541
2024-04-02 01:57:55,813 - trainer - INFO -     test_doc_entropy: 0.204981
2024-04-02 02:01:59,166 - trainer - INFO -     epoch          : 3
2024-04-02 02:01:59,166 - trainer - INFO -     loss           : 0.776705
2024-04-02 02:01:59,166 - trainer - INFO -     accuracy       : 0.767713
2024-04-02 02:01:59,166 - trainer - INFO -     macro_f        : 0.760099
2024-04-02 02:01:59,166 - trainer - INFO -     precision      : 0.793638
2024-04-02 02:01:59,166 - trainer - INFO -     recall         : 0.767713
2024-04-02 02:01:59,166 - trainer - INFO -     doc_entropy    : 0.187897
2024-04-02 02:01:59,166 - trainer - INFO -     val_loss       : 1.190059
2024-04-02 02:01:59,166 - trainer - INFO -     val_accuracy   : 0.669621
2024-04-02 02:01:59,166 - trainer - INFO -     val_macro_f    : 0.660103
2024-04-02 02:01:59,166 - trainer - INFO -     val_precision  : 0.699391
2024-04-02 02:01:59,166 - trainer - INFO -     val_recall     : 0.669621
2024-04-02 02:01:59,166 - trainer - INFO -     val_doc_entropy: 0.170165
2024-04-02 02:01:59,166 - trainer - INFO -     test_loss      : 1.17767
2024-04-02 02:01:59,166 - trainer - INFO -     test_accuracy  : 0.671562
2024-04-02 02:01:59,166 - trainer - INFO -     test_macro_f   : 0.66122
2024-04-02 02:01:59,166 - trainer - INFO -     test_precision : 0.701651
2024-04-02 02:01:59,166 - trainer - INFO -     test_recall    : 0.671562
2024-04-02 02:01:59,166 - trainer - INFO -     test_doc_entropy: 0.169616
2024-04-02 08:15:15,947 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=3200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=3200, out_features=160, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,963,043
Freeze params: 0
2024-04-02 08:19:31,775 - trainer - INFO -     epoch          : 1
2024-04-02 08:19:31,775 - trainer - INFO -     loss           : 1.268316
2024-04-02 08:19:31,775 - trainer - INFO -     accuracy       : 0.638807
2024-04-02 08:19:31,775 - trainer - INFO -     macro_f        : 0.618796
2024-04-02 08:19:31,775 - trainer - INFO -     precision      : 0.650134
2024-04-02 08:19:31,775 - trainer - INFO -     recall         : 0.638807
2024-04-02 08:19:31,775 - trainer - INFO -     doc_entropy    : 0.583638
2024-04-02 08:19:31,775 - trainer - INFO -     val_loss       : 1.122397
2024-04-02 08:19:31,775 - trainer - INFO -     val_accuracy   : 0.673803
2024-04-02 08:19:31,775 - trainer - INFO -     val_macro_f    : 0.659294
2024-04-02 08:19:31,775 - trainer - INFO -     val_precision  : 0.693698
2024-04-02 08:19:31,775 - trainer - INFO -     val_recall     : 0.673803
2024-04-02 08:19:31,775 - trainer - INFO -     val_doc_entropy: 0.246151
2024-04-02 08:19:31,775 - trainer - INFO -     test_loss      : 1.131022
2024-04-02 08:19:31,775 - trainer - INFO -     test_accuracy  : 0.667928
2024-04-02 08:19:31,775 - trainer - INFO -     test_macro_f   : 0.654744
2024-04-02 08:19:31,775 - trainer - INFO -     test_precision : 0.691558
2024-04-02 08:19:31,775 - trainer - INFO -     test_recall    : 0.667928
2024-04-02 08:19:31,775 - trainer - INFO -     test_doc_entropy: 0.246633
2024-04-02 08:23:56,939 - trainer - INFO -     epoch          : 2
2024-04-02 08:23:56,939 - trainer - INFO -     loss           : 0.970655
2024-04-02 08:23:56,939 - trainer - INFO -     accuracy       : 0.713672
2024-04-02 08:23:56,939 - trainer - INFO -     macro_f        : 0.702739
2024-04-02 08:23:56,939 - trainer - INFO -     precision      : 0.737794
2024-04-02 08:23:56,939 - trainer - INFO -     recall         : 0.713672
2024-04-02 08:23:56,939 - trainer - INFO -     doc_entropy    : 0.19641
2024-04-02 08:23:56,939 - trainer - INFO -     val_loss       : 1.118389
2024-04-02 08:23:56,939 - trainer - INFO -     val_accuracy   : 0.678532
2024-04-02 08:23:56,939 - trainer - INFO -     val_macro_f    : 0.666392
2024-04-02 08:23:56,939 - trainer - INFO -     val_precision  : 0.701307
2024-04-02 08:23:56,939 - trainer - INFO -     val_recall     : 0.678532
2024-04-02 08:23:56,939 - trainer - INFO -     val_doc_entropy: 0.171233
2024-04-02 08:23:56,939 - trainer - INFO -     test_loss      : 1.117061
2024-04-02 08:23:56,939 - trainer - INFO -     test_accuracy  : 0.680325
2024-04-02 08:23:56,939 - trainer - INFO -     test_macro_f   : 0.667235
2024-04-02 08:23:56,939 - trainer - INFO -     test_precision : 0.703185
2024-04-02 08:23:56,939 - trainer - INFO -     test_recall    : 0.680325
2024-04-02 08:23:56,939 - trainer - INFO -     test_doc_entropy: 0.170975
2024-04-02 08:28:23,465 - trainer - INFO -     epoch          : 3
2024-04-02 08:28:23,480 - trainer - INFO -     loss           : 0.790643
2024-04-02 08:28:23,480 - trainer - INFO -     accuracy       : 0.765024
2024-04-02 08:28:23,481 - trainer - INFO -     macro_f        : 0.757699
2024-04-02 08:28:23,481 - trainer - INFO -     precision      : 0.791418
2024-04-02 08:28:23,482 - trainer - INFO -     recall         : 0.765024
2024-04-02 08:28:23,482 - trainer - INFO -     doc_entropy    : 0.165019
2024-04-02 08:28:23,482 - trainer - INFO -     val_loss       : 1.171121
2024-04-02 08:28:23,482 - trainer - INFO -     val_accuracy   : 0.669421
2024-04-02 08:28:23,483 - trainer - INFO -     val_macro_f    : 0.656196
2024-04-02 08:28:23,483 - trainer - INFO -     val_precision  : 0.691754
2024-04-02 08:28:23,483 - trainer - INFO -     val_recall     : 0.669421
2024-04-02 08:28:23,483 - trainer - INFO -     val_doc_entropy: 0.157017
2024-04-02 08:28:23,483 - trainer - INFO -     test_loss      : 1.160062
2024-04-02 08:28:23,483 - trainer - INFO -     test_accuracy  : 0.671512
2024-04-02 08:28:23,483 - trainer - INFO -     test_macro_f   : 0.659184
2024-04-02 08:28:23,483 - trainer - INFO -     test_precision : 0.694464
2024-04-02 08:28:23,483 - trainer - INFO -     test_recall    : 0.671512
2024-04-02 08:28:23,483 - trainer - INFO -     test_doc_entropy: 0.157403
2024-04-02 08:29:02,253 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=3200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=3200, out_features=160, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,963,043
Freeze params: 0
2024-04-02 08:33:28,092 - trainer - INFO -     epoch          : 1
2024-04-02 08:33:28,092 - trainer - INFO -     loss           : 1.265988
2024-04-02 08:33:28,092 - trainer - INFO -     accuracy       : 0.639816
2024-04-02 08:33:28,092 - trainer - INFO -     macro_f        : 0.619427
2024-04-02 08:33:28,092 - trainer - INFO -     precision      : 0.651056
2024-04-02 08:33:28,092 - trainer - INFO -     recall         : 0.639816
2024-04-02 08:33:28,092 - trainer - INFO -     doc_entropy    : 0.582005
2024-04-02 08:33:28,092 - trainer - INFO -     val_loss       : 1.121605
2024-04-02 08:33:28,092 - trainer - INFO -     val_accuracy   : 0.675894
2024-04-02 08:33:28,092 - trainer - INFO -     val_macro_f    : 0.662774
2024-04-02 08:33:28,092 - trainer - INFO -     val_precision  : 0.698505
2024-04-02 08:33:28,092 - trainer - INFO -     val_recall     : 0.675894
2024-04-02 08:33:28,092 - trainer - INFO -     val_doc_entropy: 0.239249
2024-04-02 08:33:28,092 - trainer - INFO -     test_loss      : 1.126847
2024-04-02 08:33:28,092 - trainer - INFO -     test_accuracy  : 0.674101
2024-04-02 08:33:28,092 - trainer - INFO -     test_macro_f   : 0.660972
2024-04-02 08:33:28,092 - trainer - INFO -     test_precision : 0.69531
2024-04-02 08:33:28,092 - trainer - INFO -     test_recall    : 0.674101
2024-04-02 08:33:28,092 - trainer - INFO -     test_doc_entropy: 0.239228
2024-04-02 08:37:55,758 - trainer - INFO -     epoch          : 2
2024-04-02 08:37:55,758 - trainer - INFO -     loss           : 0.973033
2024-04-02 08:37:55,758 - trainer - INFO -     accuracy       : 0.714388
2024-04-02 08:37:55,758 - trainer - INFO -     macro_f        : 0.703046
2024-04-02 08:37:55,758 - trainer - INFO -     precision      : 0.738024
2024-04-02 08:37:55,758 - trainer - INFO -     recall         : 0.714388
2024-04-02 08:37:55,758 - trainer - INFO -     doc_entropy    : 0.189491
2024-04-02 08:37:55,758 - trainer - INFO -     val_loss       : 1.119934
2024-04-02 08:37:55,758 - trainer - INFO -     val_accuracy   : 0.676591
2024-04-02 08:37:55,758 - trainer - INFO -     val_macro_f    : 0.667237
2024-04-02 08:37:55,758 - trainer - INFO -     val_precision  : 0.707772
2024-04-02 08:37:55,758 - trainer - INFO -     val_recall     : 0.676591
2024-04-02 08:37:55,758 - trainer - INFO -     val_doc_entropy: 0.165568
2024-04-02 08:37:55,758 - trainer - INFO -     test_loss      : 1.112122
2024-04-02 08:37:55,758 - trainer - INFO -     test_accuracy  : 0.678483
2024-04-02 08:37:55,758 - trainer - INFO -     test_macro_f   : 0.666904
2024-04-02 08:37:55,758 - trainer - INFO -     test_precision : 0.705812
2024-04-02 08:37:55,758 - trainer - INFO -     test_recall    : 0.678483
2024-04-02 08:37:55,758 - trainer - INFO -     test_doc_entropy: 0.165139
2024-04-02 08:42:25,622 - trainer - INFO -     epoch          : 3
2024-04-02 08:42:25,622 - trainer - INFO -     loss           : 0.790496
2024-04-02 08:42:25,622 - trainer - INFO -     accuracy       : 0.765509
2024-04-02 08:42:25,622 - trainer - INFO -     macro_f        : 0.758078
2024-04-02 08:42:25,622 - trainer - INFO -     precision      : 0.791644
2024-04-02 08:42:25,622 - trainer - INFO -     recall         : 0.765509
2024-04-02 08:42:25,622 - trainer - INFO -     doc_entropy    : 0.155852
2024-04-02 08:42:25,622 - trainer - INFO -     val_loss       : 1.15705
2024-04-02 08:42:25,622 - trainer - INFO -     val_accuracy   : 0.676392
2024-04-02 08:42:25,622 - trainer - INFO -     val_macro_f    : 0.664369
2024-04-02 08:42:25,622 - trainer - INFO -     val_precision  : 0.701292
2024-04-02 08:42:25,622 - trainer - INFO -     val_recall     : 0.676392
2024-04-02 08:42:25,638 - trainer - INFO -     val_doc_entropy: 0.146027
2024-04-02 08:42:25,638 - trainer - INFO -     test_loss      : 1.157779
2024-04-02 08:42:25,638 - trainer - INFO -     test_accuracy  : 0.673106
2024-04-02 08:42:25,638 - trainer - INFO -     test_macro_f   : 0.661633
2024-04-02 08:42:25,638 - trainer - INFO -     test_precision : 0.697828
2024-04-02 08:42:25,638 - trainer - INFO -     test_recall    : 0.673106
2024-04-02 08:42:25,638 - trainer - INFO -     test_doc_entropy: 0.146083
2024-04-02 08:43:05,397 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=3200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=3200, out_features=160, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,963,043
Freeze params: 0
2024-04-02 08:47:31,982 - trainer - INFO -     epoch          : 1
2024-04-02 08:47:31,982 - trainer - INFO -     loss           : 1.259784
2024-04-02 08:47:31,982 - trainer - INFO -     accuracy       : 0.641913
2024-04-02 08:47:31,982 - trainer - INFO -     macro_f        : 0.621969
2024-04-02 08:47:31,982 - trainer - INFO -     precision      : 0.653878
2024-04-02 08:47:31,982 - trainer - INFO -     recall         : 0.641913
2024-04-02 08:47:31,982 - trainer - INFO -     doc_entropy    : 0.612114
2024-04-02 08:47:31,982 - trainer - INFO -     val_loss       : 1.124744
2024-04-02 08:47:31,982 - trainer - INFO -     val_accuracy   : 0.675595
2024-04-02 08:47:31,982 - trainer - INFO -     val_macro_f    : 0.661217
2024-04-02 08:47:31,982 - trainer - INFO -     val_precision  : 0.696109
2024-04-02 08:47:31,982 - trainer - INFO -     val_recall     : 0.675595
2024-04-02 08:47:31,982 - trainer - INFO -     val_doc_entropy: 0.235803
2024-04-02 08:47:31,982 - trainer - INFO -     test_loss      : 1.127591
2024-04-02 08:47:31,982 - trainer - INFO -     test_accuracy  : 0.670318
2024-04-02 08:47:31,982 - trainer - INFO -     test_macro_f   : 0.656382
2024-04-02 08:47:31,982 - trainer - INFO -     test_precision : 0.692133
2024-04-02 08:47:31,982 - trainer - INFO -     test_recall    : 0.670318
2024-04-02 08:47:31,982 - trainer - INFO -     test_doc_entropy: 0.235064
2024-04-02 08:52:00,321 - trainer - INFO -     epoch          : 2
2024-04-02 08:52:00,321 - trainer - INFO -     loss           : 0.967613
2024-04-02 08:52:00,321 - trainer - INFO -     accuracy       : 0.714992
2024-04-02 08:52:00,321 - trainer - INFO -     macro_f        : 0.703687
2024-04-02 08:52:00,321 - trainer - INFO -     precision      : 0.739242
2024-04-02 08:52:00,321 - trainer - INFO -     recall         : 0.714992
2024-04-02 08:52:00,337 - trainer - INFO -     doc_entropy    : 0.19143
2024-04-02 08:52:00,337 - trainer - INFO -     val_loss       : 1.125755
2024-04-02 08:52:00,337 - trainer - INFO -     val_accuracy   : 0.680225
2024-04-02 08:52:00,337 - trainer - INFO -     val_macro_f    : 0.665327
2024-04-02 08:52:00,337 - trainer - INFO -     val_precision  : 0.701321
2024-04-02 08:52:00,337 - trainer - INFO -     val_recall     : 0.680225
2024-04-02 08:52:00,337 - trainer - INFO -     val_doc_entropy: 0.164152
2024-04-02 08:52:00,337 - trainer - INFO -     test_loss      : 1.124886
2024-04-02 08:52:00,337 - trainer - INFO -     test_accuracy  : 0.676143
2024-04-02 08:52:00,337 - trainer - INFO -     test_macro_f   : 0.662089
2024-04-02 08:52:00,337 - trainer - INFO -     test_precision : 0.698715
2024-04-02 08:52:00,337 - trainer - INFO -     test_recall    : 0.676143
2024-04-02 08:52:00,337 - trainer - INFO -     test_doc_entropy: 0.164215
2024-04-02 08:56:28,095 - trainer - INFO -     epoch          : 3
2024-04-02 08:56:28,095 - trainer - INFO -     loss           : 0.784725
2024-04-02 08:56:28,095 - trainer - INFO -     accuracy       : 0.766076
2024-04-02 08:56:28,095 - trainer - INFO -     macro_f        : 0.758541
2024-04-02 08:56:28,095 - trainer - INFO -     precision      : 0.792306
2024-04-02 08:56:28,095 - trainer - INFO -     recall         : 0.766076
2024-04-02 08:56:28,095 - trainer - INFO -     doc_entropy    : 0.16014
2024-04-02 08:56:28,095 - trainer - INFO -     val_loss       : 1.16417
2024-04-02 08:56:28,095 - trainer - INFO -     val_accuracy   : 0.672857
2024-04-02 08:56:28,095 - trainer - INFO -     val_macro_f    : 0.662392
2024-04-02 08:56:28,095 - trainer - INFO -     val_precision  : 0.701991
2024-04-02 08:56:28,095 - trainer - INFO -     val_recall     : 0.672857
2024-04-02 08:56:28,095 - trainer - INFO -     val_doc_entropy: 0.14969
2024-04-02 08:56:28,095 - trainer - INFO -     test_loss      : 1.158134
2024-04-02 08:56:28,095 - trainer - INFO -     test_accuracy  : 0.674649
2024-04-02 08:56:28,095 - trainer - INFO -     test_macro_f   : 0.663863
2024-04-02 08:56:28,095 - trainer - INFO -     test_precision : 0.700108
2024-04-02 08:56:28,095 - trainer - INFO -     test_recall    : 0.674649
2024-04-02 08:56:28,095 - trainer - INFO -     test_doc_entropy: 0.14949
2024-04-02 08:57:07,400 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=3200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=3200, out_features=160, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,963,043
Freeze params: 0
2024-04-02 09:01:34,875 - trainer - INFO -     epoch          : 1
2024-04-02 09:01:34,875 - trainer - INFO -     loss           : 1.263967
2024-04-02 09:01:34,875 - trainer - INFO -     accuracy       : 0.640245
2024-04-02 09:01:34,875 - trainer - INFO -     macro_f        : 0.620193
2024-04-02 09:01:34,875 - trainer - INFO -     precision      : 0.652292
2024-04-02 09:01:34,875 - trainer - INFO -     recall         : 0.640245
2024-04-02 09:01:34,875 - trainer - INFO -     doc_entropy    : 0.578508
2024-04-02 09:01:34,875 - trainer - INFO -     val_loss       : 1.126975
2024-04-02 09:01:34,875 - trainer - INFO -     val_accuracy   : 0.671861
2024-04-02 09:01:34,875 - trainer - INFO -     val_macro_f    : 0.657787
2024-04-02 09:01:34,875 - trainer - INFO -     val_precision  : 0.692997
2024-04-02 09:01:34,875 - trainer - INFO -     val_recall     : 0.671861
2024-04-02 09:01:34,875 - trainer - INFO -     val_doc_entropy: 0.23196
2024-04-02 09:01:34,875 - trainer - INFO -     test_loss      : 1.123576
2024-04-02 09:01:34,875 - trainer - INFO -     test_accuracy  : 0.672707
2024-04-02 09:01:34,875 - trainer - INFO -     test_macro_f   : 0.6595
2024-04-02 09:01:34,875 - trainer - INFO -     test_precision : 0.695906
2024-04-02 09:01:34,875 - trainer - INFO -     test_recall    : 0.672707
2024-04-02 09:01:34,875 - trainer - INFO -     test_doc_entropy: 0.231672
2024-04-02 09:06:05,099 - trainer - INFO -     epoch          : 2
2024-04-02 09:06:05,099 - trainer - INFO -     loss           : 0.970619
2024-04-02 09:06:05,099 - trainer - INFO -     accuracy       : 0.714905
2024-04-02 09:06:05,099 - trainer - INFO -     macro_f        : 0.703492
2024-04-02 09:06:05,099 - trainer - INFO -     precision      : 0.737936
2024-04-02 09:06:05,099 - trainer - INFO -     recall         : 0.714905
2024-04-02 09:06:05,099 - trainer - INFO -     doc_entropy    : 0.185186
2024-04-02 09:06:05,099 - trainer - INFO -     val_loss       : 1.119652
2024-04-02 09:06:05,099 - trainer - INFO -     val_accuracy   : 0.681271
2024-04-02 09:06:05,099 - trainer - INFO -     val_macro_f    : 0.665907
2024-04-02 09:06:05,099 - trainer - INFO -     val_precision  : 0.700275
2024-04-02 09:06:05,099 - trainer - INFO -     val_recall     : 0.681271
2024-04-02 09:06:05,099 - trainer - INFO -     val_doc_entropy: 0.157906
2024-04-02 09:06:05,099 - trainer - INFO -     test_loss      : 1.113269
2024-04-02 09:06:05,099 - trainer - INFO -     test_accuracy  : 0.677537
2024-04-02 09:06:05,099 - trainer - INFO -     test_macro_f   : 0.663191
2024-04-02 09:06:05,099 - trainer - INFO -     test_precision : 0.698671
2024-04-02 09:06:05,099 - trainer - INFO -     test_recall    : 0.677537
2024-04-02 09:06:05,099 - trainer - INFO -     test_doc_entropy: 0.157697
2024-04-02 09:10:33,381 - trainer - INFO -     epoch          : 3
2024-04-02 09:10:33,381 - trainer - INFO -     loss           : 0.795496
2024-04-02 09:10:33,381 - trainer - INFO -     accuracy       : 0.763804
2024-04-02 09:10:33,381 - trainer - INFO -     macro_f        : 0.755716
2024-04-02 09:10:33,381 - trainer - INFO -     precision      : 0.788659
2024-04-02 09:10:33,381 - trainer - INFO -     recall         : 0.763804
2024-04-02 09:10:33,381 - trainer - INFO -     doc_entropy    : 0.157985
2024-04-02 09:10:33,381 - trainer - INFO -     val_loss       : 1.166511
2024-04-02 09:10:33,381 - trainer - INFO -     val_accuracy   : 0.672707
2024-04-02 09:10:33,381 - trainer - INFO -     val_macro_f    : 0.663672
2024-04-02 09:10:33,381 - trainer - INFO -     val_precision  : 0.703436
2024-04-02 09:10:33,381 - trainer - INFO -     val_recall     : 0.672707
2024-04-02 09:10:33,381 - trainer - INFO -     val_doc_entropy: 0.151795
2024-04-02 09:10:33,381 - trainer - INFO -     test_loss      : 1.159192
2024-04-02 09:10:33,381 - trainer - INFO -     test_accuracy  : 0.669023
2024-04-02 09:10:33,381 - trainer - INFO -     test_macro_f   : 0.66047
2024-04-02 09:10:33,381 - trainer - INFO -     test_precision : 0.699875
2024-04-02 09:10:33,381 - trainer - INFO -     test_recall    : 0.669023
2024-04-02 09:10:33,381 - trainer - INFO -     test_doc_entropy: 0.151506
2024-04-02 09:11:14,140 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=3200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=3200, out_features=160, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,963,043
Freeze params: 0
2024-04-02 09:15:41,967 - trainer - INFO -     epoch          : 1
2024-04-02 09:15:41,967 - trainer - INFO -     loss           : 1.260248
2024-04-02 09:15:41,967 - trainer - INFO -     accuracy       : 0.642871
2024-04-02 09:15:41,967 - trainer - INFO -     macro_f        : 0.622596
2024-04-02 09:15:41,967 - trainer - INFO -     precision      : 0.654044
2024-04-02 09:15:41,967 - trainer - INFO -     recall         : 0.642871
2024-04-02 09:15:41,967 - trainer - INFO -     doc_entropy    : 0.703702
2024-04-02 09:15:41,967 - trainer - INFO -     val_loss       : 1.116553
2024-04-02 09:15:41,967 - trainer - INFO -     val_accuracy   : 0.673952
2024-04-02 09:15:41,967 - trainer - INFO -     val_macro_f    : 0.658927
2024-04-02 09:15:41,967 - trainer - INFO -     val_precision  : 0.692888
2024-04-02 09:15:41,967 - trainer - INFO -     val_recall     : 0.673952
2024-04-02 09:15:41,967 - trainer - INFO -     val_doc_entropy: 0.265457
2024-04-02 09:15:41,967 - trainer - INFO -     test_loss      : 1.11415
2024-04-02 09:15:41,967 - trainer - INFO -     test_accuracy  : 0.675197
2024-04-02 09:15:41,967 - trainer - INFO -     test_macro_f   : 0.661313
2024-04-02 09:15:41,967 - trainer - INFO -     test_precision : 0.698526
2024-04-02 09:15:41,967 - trainer - INFO -     test_recall    : 0.675197
2024-04-02 09:15:41,967 - trainer - INFO -     test_doc_entropy: 0.265084
2024-04-02 09:20:12,975 - trainer - INFO -     epoch          : 2
2024-04-02 09:20:12,975 - trainer - INFO -     loss           : 0.970571
2024-04-02 09:20:12,975 - trainer - INFO -     accuracy       : 0.713635
2024-04-02 09:20:12,975 - trainer - INFO -     macro_f        : 0.702177
2024-04-02 09:20:12,975 - trainer - INFO -     precision      : 0.73705
2024-04-02 09:20:12,975 - trainer - INFO -     recall         : 0.713635
2024-04-02 09:20:12,975 - trainer - INFO -     doc_entropy    : 0.213215
2024-04-02 09:20:12,975 - trainer - INFO -     val_loss       : 1.115412
2024-04-02 09:20:12,990 - trainer - INFO -     val_accuracy   : 0.682316
2024-04-02 09:20:12,990 - trainer - INFO -     val_macro_f    : 0.671299
2024-04-02 09:20:12,990 - trainer - INFO -     val_precision  : 0.708718
2024-04-02 09:20:12,990 - trainer - INFO -     val_recall     : 0.682316
2024-04-02 09:20:12,990 - trainer - INFO -     val_doc_entropy: 0.176652
2024-04-02 09:20:12,990 - trainer - INFO -     test_loss      : 1.10583
2024-04-02 09:20:12,990 - trainer - INFO -     test_accuracy  : 0.677786
2024-04-02 09:20:12,990 - trainer - INFO -     test_macro_f   : 0.667152
2024-04-02 09:20:12,990 - trainer - INFO -     test_precision : 0.704864
2024-04-02 09:20:12,990 - trainer - INFO -     test_recall    : 0.677786
2024-04-02 09:20:12,990 - trainer - INFO -     test_doc_entropy: 0.176722
2024-04-02 09:24:42,282 - trainer - INFO -     epoch          : 3
2024-04-02 09:24:42,282 - trainer - INFO -     loss           : 0.788953
2024-04-02 09:24:42,282 - trainer - INFO -     accuracy       : 0.765379
2024-04-02 09:24:42,282 - trainer - INFO -     macro_f        : 0.757182
2024-04-02 09:24:42,282 - trainer - INFO -     precision      : 0.789749
2024-04-02 09:24:42,282 - trainer - INFO -     recall         : 0.765379
2024-04-02 09:24:42,282 - trainer - INFO -     doc_entropy    : 0.167239
2024-04-02 09:24:42,282 - trainer - INFO -     val_loss       : 1.159718
2024-04-02 09:24:42,282 - trainer - INFO -     val_accuracy   : 0.677039
2024-04-02 09:24:42,282 - trainer - INFO -     val_macro_f    : 0.668841
2024-04-02 09:24:42,282 - trainer - INFO -     val_precision  : 0.709369
2024-04-02 09:24:42,282 - trainer - INFO -     val_recall     : 0.677039
2024-04-02 09:24:42,282 - trainer - INFO -     val_doc_entropy: 0.154695
2024-04-02 09:24:42,297 - trainer - INFO -     test_loss      : 1.154479
2024-04-02 09:24:42,297 - trainer - INFO -     test_accuracy  : 0.676192
2024-04-02 09:24:42,297 - trainer - INFO -     test_macro_f   : 0.666517
2024-04-02 09:24:42,297 - trainer - INFO -     test_precision : 0.705487
2024-04-02 09:24:42,297 - trainer - INFO -     test_recall    : 0.676192
2024-04-02 09:24:42,297 - trainer - INFO -     test_doc_entropy: 0.154784
2024-04-02 09:26:07,151 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=3600, bias=True)
    (1): Tanh()
    (2): Linear(in_features=3600, out_features=180, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,219,463
Freeze params: 0
2024-04-02 09:30:50,209 - trainer - INFO -     epoch          : 1
2024-04-02 09:30:50,209 - trainer - INFO -     loss           : 1.263997
2024-04-02 09:30:50,209 - trainer - INFO -     accuracy       : 0.640656
2024-04-02 09:30:50,209 - trainer - INFO -     macro_f        : 0.620756
2024-04-02 09:30:50,209 - trainer - INFO -     precision      : 0.652733
2024-04-02 09:30:50,209 - trainer - INFO -     recall         : 0.640656
2024-04-02 09:30:50,209 - trainer - INFO -     doc_entropy    : 0.522202
2024-04-02 09:30:50,209 - trainer - INFO -     val_loss       : 1.124017
2024-04-02 09:30:50,209 - trainer - INFO -     val_accuracy   : 0.672409
2024-04-02 09:30:50,209 - trainer - INFO -     val_macro_f    : 0.659361
2024-04-02 09:30:50,209 - trainer - INFO -     val_precision  : 0.694538
2024-04-02 09:30:50,209 - trainer - INFO -     val_recall     : 0.672409
2024-04-02 09:30:50,209 - trainer - INFO -     val_doc_entropy: 0.208541
2024-04-02 09:30:50,209 - trainer - INFO -     test_loss      : 1.126634
2024-04-02 09:30:50,209 - trainer - INFO -     test_accuracy  : 0.673355
2024-04-02 09:30:50,209 - trainer - INFO -     test_macro_f   : 0.658599
2024-04-02 09:30:50,209 - trainer - INFO -     test_precision : 0.693426
2024-04-02 09:30:50,209 - trainer - INFO -     test_recall    : 0.673355
2024-04-02 09:30:50,209 - trainer - INFO -     test_doc_entropy: 0.208458
2024-04-02 09:35:37,403 - trainer - INFO -     epoch          : 2
2024-04-02 09:35:37,403 - trainer - INFO -     loss           : 0.975559
2024-04-02 09:35:37,403 - trainer - INFO -     accuracy       : 0.712558
2024-04-02 09:35:37,403 - trainer - INFO -     macro_f        : 0.700913
2024-04-02 09:35:37,403 - trainer - INFO -     precision      : 0.735606
2024-04-02 09:35:37,403 - trainer - INFO -     recall         : 0.712558
2024-04-02 09:35:37,403 - trainer - INFO -     doc_entropy    : 0.170964
2024-04-02 09:35:37,403 - trainer - INFO -     val_loss       : 1.122453
2024-04-02 09:35:37,403 - trainer - INFO -     val_accuracy   : 0.680076
2024-04-02 09:35:37,403 - trainer - INFO -     val_macro_f    : 0.670966
2024-04-02 09:35:37,403 - trainer - INFO -     val_precision  : 0.710808
2024-04-02 09:35:37,403 - trainer - INFO -     val_recall     : 0.680076
2024-04-02 09:35:37,403 - trainer - INFO -     val_doc_entropy: 0.148859
2024-04-02 09:35:37,403 - trainer - INFO -     test_loss      : 1.116186
2024-04-02 09:35:37,403 - trainer - INFO -     test_accuracy  : 0.678134
2024-04-02 09:35:37,403 - trainer - INFO -     test_macro_f   : 0.669874
2024-04-02 09:35:37,403 - trainer - INFO -     test_precision : 0.710297
2024-04-02 09:35:37,403 - trainer - INFO -     test_recall    : 0.678134
2024-04-02 09:35:37,403 - trainer - INFO -     test_doc_entropy: 0.148909
2024-04-02 09:40:25,512 - trainer - INFO -     epoch          : 3
2024-04-02 09:40:25,512 - trainer - INFO -     loss           : 0.79022
2024-04-02 09:40:25,512 - trainer - INFO -     accuracy       : 0.765186
2024-04-02 09:40:25,512 - trainer - INFO -     macro_f        : 0.757148
2024-04-02 09:40:25,527 - trainer - INFO -     precision      : 0.790161
2024-04-02 09:40:25,527 - trainer - INFO -     recall         : 0.765186
2024-04-02 09:40:25,527 - trainer - INFO -     doc_entropy    : 0.145219
2024-04-02 09:40:25,527 - trainer - INFO -     val_loss       : 1.16385
2024-04-02 09:40:25,527 - trainer - INFO -     val_accuracy   : 0.67206
2024-04-02 09:40:25,527 - trainer - INFO -     val_macro_f    : 0.660366
2024-04-02 09:40:25,527 - trainer - INFO -     val_precision  : 0.696472
2024-04-02 09:40:25,527 - trainer - INFO -     val_recall     : 0.67206
2024-04-02 09:40:25,527 - trainer - INFO -     val_doc_entropy: 0.137746
2024-04-02 09:40:25,527 - trainer - INFO -     test_loss      : 1.163373
2024-04-02 09:40:25,527 - trainer - INFO -     test_accuracy  : 0.673952
2024-04-02 09:40:25,527 - trainer - INFO -     test_macro_f   : 0.660999
2024-04-02 09:40:25,527 - trainer - INFO -     test_precision : 0.695673
2024-04-02 09:40:25,527 - trainer - INFO -     test_recall    : 0.673952
2024-04-02 09:40:25,527 - trainer - INFO -     test_doc_entropy: 0.137525
2024-04-02 09:41:08,364 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=3600, bias=True)
    (1): Tanh()
    (2): Linear(in_features=3600, out_features=180, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,219,463
Freeze params: 0
2024-04-02 09:45:54,623 - trainer - INFO -     epoch          : 1
2024-04-02 09:45:54,623 - trainer - INFO -     loss           : 1.269956
2024-04-02 09:45:54,623 - trainer - INFO -     accuracy       : 0.639872
2024-04-02 09:45:54,623 - trainer - INFO -     macro_f        : 0.619703
2024-04-02 09:45:54,623 - trainer - INFO -     precision      : 0.651286
2024-04-02 09:45:54,623 - trainer - INFO -     recall         : 0.639872
2024-04-02 09:45:54,623 - trainer - INFO -     doc_entropy    : 0.477785
2024-04-02 09:45:54,623 - trainer - INFO -     val_loss       : 1.134647
2024-04-02 09:45:54,623 - trainer - INFO -     val_accuracy   : 0.673305
2024-04-02 09:45:54,623 - trainer - INFO -     val_macro_f    : 0.659913
2024-04-02 09:45:54,623 - trainer - INFO -     val_precision  : 0.695815
2024-04-02 09:45:54,623 - trainer - INFO -     val_recall     : 0.673305
2024-04-02 09:45:54,623 - trainer - INFO -     val_doc_entropy: 0.196404
2024-04-02 09:45:54,623 - trainer - INFO -     test_loss      : 1.133345
2024-04-02 09:45:54,623 - trainer - INFO -     test_accuracy  : 0.672409
2024-04-02 09:45:54,623 - trainer - INFO -     test_macro_f   : 0.659583
2024-04-02 09:45:54,623 - trainer - INFO -     test_precision : 0.696645
2024-04-02 09:45:54,639 - trainer - INFO -     test_recall    : 0.672409
2024-04-02 09:45:54,639 - trainer - INFO -     test_doc_entropy: 0.195775
2024-04-02 09:50:41,830 - trainer - INFO -     epoch          : 2
2024-04-02 09:50:41,830 - trainer - INFO -     loss           : 0.980179
2024-04-02 09:50:41,830 - trainer - INFO -     accuracy       : 0.711631
2024-04-02 09:50:41,830 - trainer - INFO -     macro_f        : 0.700286
2024-04-02 09:50:41,830 - trainer - INFO -     precision      : 0.7352
2024-04-02 09:50:41,830 - trainer - INFO -     recall         : 0.711631
2024-04-02 09:50:41,830 - trainer - INFO -     doc_entropy    : 0.164402
2024-04-02 09:50:41,830 - trainer - INFO -     val_loss       : 1.112544
2024-04-02 09:50:41,830 - trainer - INFO -     val_accuracy   : 0.679329
2024-04-02 09:50:41,830 - trainer - INFO -     val_macro_f    : 0.668211
2024-04-02 09:50:41,830 - trainer - INFO -     val_precision  : 0.705271
2024-04-02 09:50:41,830 - trainer - INFO -     val_recall     : 0.679329
2024-04-02 09:50:41,830 - trainer - INFO -     val_doc_entropy: 0.147359
2024-04-02 09:50:41,830 - trainer - INFO -     test_loss      : 1.109044
2024-04-02 09:50:41,830 - trainer - INFO -     test_accuracy  : 0.684258
2024-04-02 09:50:41,830 - trainer - INFO -     test_macro_f   : 0.674319
2024-04-02 09:50:41,830 - trainer - INFO -     test_precision : 0.712796
2024-04-02 09:50:41,830 - trainer - INFO -     test_recall    : 0.684258
2024-04-02 09:50:41,830 - trainer - INFO -     test_doc_entropy: 0.147544
2024-04-02 09:55:28,514 - trainer - INFO -     epoch          : 3
2024-04-02 09:55:28,514 - trainer - INFO -     loss           : 0.799417
2024-04-02 09:55:28,514 - trainer - INFO -     accuracy       : 0.761172
2024-04-02 09:55:28,514 - trainer - INFO -     macro_f        : 0.752836
2024-04-02 09:55:28,514 - trainer - INFO -     precision      : 0.786282
2024-04-02 09:55:28,514 - trainer - INFO -     recall         : 0.761172
2024-04-02 09:55:28,514 - trainer - INFO -     doc_entropy    : 0.143712
2024-04-02 09:55:28,514 - trainer - INFO -     val_loss       : 1.174235
2024-04-02 09:55:28,514 - trainer - INFO -     val_accuracy   : 0.669969
2024-04-02 09:55:28,514 - trainer - INFO -     val_macro_f    : 0.662982
2024-04-02 09:55:28,514 - trainer - INFO -     val_precision  : 0.704508
2024-04-02 09:55:28,514 - trainer - INFO -     val_recall     : 0.669969
2024-04-02 09:55:28,514 - trainer - INFO -     val_doc_entropy: 0.137062
2024-04-02 09:55:28,514 - trainer - INFO -     test_loss      : 1.157322
2024-04-02 09:55:28,514 - trainer - INFO -     test_accuracy  : 0.673255
2024-04-02 09:55:28,514 - trainer - INFO -     test_macro_f   : 0.665217
2024-04-02 09:55:28,514 - trainer - INFO -     test_precision : 0.705839
2024-04-02 09:55:28,514 - trainer - INFO -     test_recall    : 0.673255
2024-04-02 09:55:28,514 - trainer - INFO -     test_doc_entropy: 0.137016
2024-04-02 09:56:10,714 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=3600, bias=True)
    (1): Tanh()
    (2): Linear(in_features=3600, out_features=180, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,219,463
Freeze params: 0
2024-04-02 10:00:57,606 - trainer - INFO -     epoch          : 1
2024-04-02 10:00:57,606 - trainer - INFO -     loss           : 1.267704
2024-04-02 10:00:57,606 - trainer - INFO -     accuracy       : 0.639585
2024-04-02 10:00:57,622 - trainer - INFO -     macro_f        : 0.620158
2024-04-02 10:00:57,622 - trainer - INFO -     precision      : 0.653265
2024-04-02 10:00:57,622 - trainer - INFO -     recall         : 0.639585
2024-04-02 10:00:57,622 - trainer - INFO -     doc_entropy    : 0.48206
2024-04-02 10:00:57,622 - trainer - INFO -     val_loss       : 1.119786
2024-04-02 10:00:57,622 - trainer - INFO -     val_accuracy   : 0.673703
2024-04-02 10:00:57,622 - trainer - INFO -     val_macro_f    : 0.659641
2024-04-02 10:00:57,622 - trainer - INFO -     val_precision  : 0.694899
2024-04-02 10:00:57,622 - trainer - INFO -     val_recall     : 0.673703
2024-04-02 10:00:57,622 - trainer - INFO -     val_doc_entropy: 0.206202
2024-04-02 10:00:57,622 - trainer - INFO -     test_loss      : 1.121097
2024-04-02 10:00:57,622 - trainer - INFO -     test_accuracy  : 0.674948
2024-04-02 10:00:57,622 - trainer - INFO -     test_macro_f   : 0.661585
2024-04-02 10:00:57,622 - trainer - INFO -     test_precision : 0.696995
2024-04-02 10:00:57,622 - trainer - INFO -     test_recall    : 0.674948
2024-04-02 10:00:57,622 - trainer - INFO -     test_doc_entropy: 0.206069
2024-04-02 10:05:45,612 - trainer - INFO -     epoch          : 2
2024-04-02 10:05:45,612 - trainer - INFO -     loss           : 0.978876
2024-04-02 10:05:45,612 - trainer - INFO -     accuracy       : 0.71211
2024-04-02 10:05:45,612 - trainer - INFO -     macro_f        : 0.700833
2024-04-02 10:05:45,612 - trainer - INFO -     precision      : 0.736424
2024-04-02 10:05:45,612 - trainer - INFO -     recall         : 0.71211
2024-04-02 10:05:45,612 - trainer - INFO -     doc_entropy    : 0.166296
2024-04-02 10:05:45,612 - trainer - INFO -     val_loss       : 1.116988
2024-04-02 10:05:45,612 - trainer - INFO -     val_accuracy   : 0.675645
2024-04-02 10:05:45,612 - trainer - INFO -     val_macro_f    : 0.668731
2024-04-02 10:05:45,612 - trainer - INFO -     val_precision  : 0.712103
2024-04-02 10:05:45,612 - trainer - INFO -     val_recall     : 0.675645
2024-04-02 10:05:45,612 - trainer - INFO -     val_doc_entropy: 0.145788
2024-04-02 10:05:45,612 - trainer - INFO -     test_loss      : 1.105899
2024-04-02 10:05:45,612 - trainer - INFO -     test_accuracy  : 0.680175
2024-04-02 10:05:45,612 - trainer - INFO -     test_macro_f   : 0.671774
2024-04-02 10:05:45,612 - trainer - INFO -     test_precision : 0.714419
2024-04-02 10:05:45,612 - trainer - INFO -     test_recall    : 0.680175
2024-04-02 10:05:45,612 - trainer - INFO -     test_doc_entropy: 0.145127
2024-04-02 10:10:33,771 - trainer - INFO -     epoch          : 3
2024-04-02 10:10:33,771 - trainer - INFO -     loss           : 0.800366
2024-04-02 10:10:33,771 - trainer - INFO -     accuracy       : 0.761427
2024-04-02 10:10:33,771 - trainer - INFO -     macro_f        : 0.753836
2024-04-02 10:10:33,771 - trainer - INFO -     precision      : 0.78791
2024-04-02 10:10:33,771 - trainer - INFO -     recall         : 0.761427
2024-04-02 10:10:33,771 - trainer - INFO -     doc_entropy    : 0.140988
2024-04-02 10:10:33,771 - trainer - INFO -     val_loss       : 1.168377
2024-04-02 10:10:33,771 - trainer - INFO -     val_accuracy   : 0.670069
2024-04-02 10:10:33,771 - trainer - INFO -     val_macro_f    : 0.662656
2024-04-02 10:10:33,771 - trainer - INFO -     val_precision  : 0.706828
2024-04-02 10:10:33,771 - trainer - INFO -     val_recall     : 0.670069
2024-04-02 10:10:33,771 - trainer - INFO -     val_doc_entropy: 0.131497
2024-04-02 10:10:33,771 - trainer - INFO -     test_loss      : 1.161319
2024-04-02 10:10:33,771 - trainer - INFO -     test_accuracy  : 0.670218
2024-04-02 10:10:33,771 - trainer - INFO -     test_macro_f   : 0.660651
2024-04-02 10:10:33,771 - trainer - INFO -     test_precision : 0.701673
2024-04-02 10:10:33,771 - trainer - INFO -     test_recall    : 0.670218
2024-04-02 10:10:33,771 - trainer - INFO -     test_doc_entropy: 0.130809
2024-04-02 10:11:16,828 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=3600, bias=True)
    (1): Tanh()
    (2): Linear(in_features=3600, out_features=180, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,219,463
Freeze params: 0
2024-04-02 10:16:03,452 - trainer - INFO -     epoch          : 1
2024-04-02 10:16:03,452 - trainer - INFO -     loss           : 1.26799
2024-04-02 10:16:03,452 - trainer - INFO -     accuracy       : 0.639187
2024-04-02 10:16:03,452 - trainer - INFO -     macro_f        : 0.619228
2024-04-02 10:16:03,452 - trainer - INFO -     precision      : 0.651164
2024-04-02 10:16:03,452 - trainer - INFO -     recall         : 0.639187
2024-04-02 10:16:03,452 - trainer - INFO -     doc_entropy    : 0.484109
2024-04-02 10:16:03,452 - trainer - INFO -     val_loss       : 1.125986
2024-04-02 10:16:03,452 - trainer - INFO -     val_accuracy   : 0.675047
2024-04-02 10:16:03,452 - trainer - INFO -     val_macro_f    : 0.657466
2024-04-02 10:16:03,452 - trainer - INFO -     val_precision  : 0.688872
2024-04-02 10:16:03,452 - trainer - INFO -     val_recall     : 0.675047
2024-04-02 10:16:03,452 - trainer - INFO -     val_doc_entropy: 0.2001
2024-04-02 10:16:03,452 - trainer - INFO -     test_loss      : 1.126131
2024-04-02 10:16:03,452 - trainer - INFO -     test_accuracy  : 0.671064
2024-04-02 10:16:03,452 - trainer - INFO -     test_macro_f   : 0.65616
2024-04-02 10:16:03,452 - trainer - INFO -     test_precision : 0.689838
2024-04-02 10:16:03,452 - trainer - INFO -     test_recall    : 0.671064
2024-04-02 10:16:03,452 - trainer - INFO -     test_doc_entropy: 0.199492
2024-04-02 10:20:53,794 - trainer - INFO -     epoch          : 2
2024-04-02 10:20:53,794 - trainer - INFO -     loss           : 0.972406
2024-04-02 10:20:53,794 - trainer - INFO -     accuracy       : 0.712894
2024-04-02 10:20:53,794 - trainer - INFO -     macro_f        : 0.701688
2024-04-02 10:20:53,794 - trainer - INFO -     precision      : 0.736797
2024-04-02 10:20:53,809 - trainer - INFO -     recall         : 0.712894
2024-04-02 10:20:53,809 - trainer - INFO -     doc_entropy    : 0.167319
2024-04-02 10:20:53,809 - trainer - INFO -     val_loss       : 1.115263
2024-04-02 10:20:53,809 - trainer - INFO -     val_accuracy   : 0.681519
2024-04-02 10:20:53,809 - trainer - INFO -     val_macro_f    : 0.66983
2024-04-02 10:20:53,809 - trainer - INFO -     val_precision  : 0.705999
2024-04-02 10:20:53,809 - trainer - INFO -     val_recall     : 0.681519
2024-04-02 10:20:53,809 - trainer - INFO -     val_doc_entropy: 0.142046
2024-04-02 10:20:53,809 - trainer - INFO -     test_loss      : 1.114008
2024-04-02 10:20:53,809 - trainer - INFO -     test_accuracy  : 0.680424
2024-04-02 10:20:53,809 - trainer - INFO -     test_macro_f   : 0.670958
2024-04-02 10:20:53,809 - trainer - INFO -     test_precision : 0.710489
2024-04-02 10:20:53,809 - trainer - INFO -     test_recall    : 0.680424
2024-04-02 10:20:53,809 - trainer - INFO -     test_doc_entropy: 0.141978
2024-04-02 10:25:41,473 - trainer - INFO -     epoch          : 3
2024-04-02 10:25:41,473 - trainer - INFO -     loss           : 0.79709
2024-04-02 10:25:41,473 - trainer - INFO -     accuracy       : 0.762304
2024-04-02 10:25:41,473 - trainer - INFO -     macro_f        : 0.754366
2024-04-02 10:25:41,473 - trainer - INFO -     precision      : 0.787299
2024-04-02 10:25:41,473 - trainer - INFO -     recall         : 0.762304
2024-04-02 10:25:41,473 - trainer - INFO -     doc_entropy    : 0.143648
2024-04-02 10:25:41,473 - trainer - INFO -     val_loss       : 1.172054
2024-04-02 10:25:41,473 - trainer - INFO -     val_accuracy   : 0.671512
2024-04-02 10:25:41,473 - trainer - INFO -     val_macro_f    : 0.661841
2024-04-02 10:25:41,473 - trainer - INFO -     val_precision  : 0.702006
2024-04-02 10:25:41,473 - trainer - INFO -     val_recall     : 0.671512
2024-04-02 10:25:41,473 - trainer - INFO -     val_doc_entropy: 0.138812
2024-04-02 10:25:41,473 - trainer - INFO -     test_loss      : 1.16649
2024-04-02 10:25:41,473 - trainer - INFO -     test_accuracy  : 0.672359
2024-04-02 10:25:41,473 - trainer - INFO -     test_macro_f   : 0.662501
2024-04-02 10:25:41,473 - trainer - INFO -     test_precision : 0.700702
2024-04-02 10:25:41,473 - trainer - INFO -     test_recall    : 0.672359
2024-04-02 10:25:41,473 - trainer - INFO -     test_doc_entropy: 0.138321
2024-04-02 10:26:24,577 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=3600, bias=True)
    (1): Tanh()
    (2): Linear(in_features=3600, out_features=180, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,219,463
Freeze params: 0
2024-04-02 10:31:10,073 - trainer - INFO -     epoch          : 1
2024-04-02 10:31:10,073 - trainer - INFO -     loss           : 1.277595
2024-04-02 10:31:10,073 - trainer - INFO -     accuracy       : 0.63765
2024-04-02 10:31:10,073 - trainer - INFO -     macro_f        : 0.616794
2024-04-02 10:31:10,073 - trainer - INFO -     precision      : 0.647886
2024-04-02 10:31:10,073 - trainer - INFO -     recall         : 0.63765
2024-04-02 10:31:10,073 - trainer - INFO -     doc_entropy    : 0.394875
2024-04-02 10:31:10,073 - trainer - INFO -     val_loss       : 1.139814
2024-04-02 10:31:10,073 - trainer - INFO -     val_accuracy   : 0.67211
2024-04-02 10:31:10,073 - trainer - INFO -     val_macro_f    : 0.655448
2024-04-02 10:31:10,073 - trainer - INFO -     val_precision  : 0.689485
2024-04-02 10:31:10,073 - trainer - INFO -     val_recall     : 0.67211
2024-04-02 10:31:10,073 - trainer - INFO -     val_doc_entropy: 0.185425
2024-04-02 10:31:10,073 - trainer - INFO -     test_loss      : 1.137372
2024-04-02 10:31:10,073 - trainer - INFO -     test_accuracy  : 0.67216
2024-04-02 10:31:10,073 - trainer - INFO -     test_macro_f   : 0.65806
2024-04-02 10:31:10,073 - trainer - INFO -     test_precision : 0.694046
2024-04-02 10:31:10,073 - trainer - INFO -     test_recall    : 0.67216
2024-04-02 10:31:10,073 - trainer - INFO -     test_doc_entropy: 0.186001
2024-04-02 10:35:58,860 - trainer - INFO -     epoch          : 2
2024-04-02 10:35:58,860 - trainer - INFO -     loss           : 0.984833
2024-04-02 10:35:58,860 - trainer - INFO -     accuracy       : 0.711096
2024-04-02 10:35:58,860 - trainer - INFO -     macro_f        : 0.699711
2024-04-02 10:35:58,860 - trainer - INFO -     precision      : 0.734281
2024-04-02 10:35:58,860 - trainer - INFO -     recall         : 0.711096
2024-04-02 10:35:58,860 - trainer - INFO -     doc_entropy    : 0.151451
2024-04-02 10:35:58,860 - trainer - INFO -     val_loss       : 1.132539
2024-04-02 10:35:58,860 - trainer - INFO -     val_accuracy   : 0.673554
2024-04-02 10:35:58,860 - trainer - INFO -     val_macro_f    : 0.66463
2024-04-02 10:35:58,860 - trainer - INFO -     val_precision  : 0.704047
2024-04-02 10:35:58,860 - trainer - INFO -     val_recall     : 0.673554
2024-04-02 10:35:58,860 - trainer - INFO -     val_doc_entropy: 0.135736
2024-04-02 10:35:58,860 - trainer - INFO -     test_loss      : 1.127411
2024-04-02 10:35:58,860 - trainer - INFO -     test_accuracy  : 0.673006
2024-04-02 10:35:58,860 - trainer - INFO -     test_macro_f   : 0.666082
2024-04-02 10:35:58,860 - trainer - INFO -     test_precision : 0.710194
2024-04-02 10:35:58,860 - trainer - INFO -     test_recall    : 0.673006
2024-04-02 10:35:58,860 - trainer - INFO -     test_doc_entropy: 0.135738
2024-04-02 10:40:45,000 - trainer - INFO -     epoch          : 3
2024-04-02 10:40:45,000 - trainer - INFO -     loss           : 0.813324
2024-04-02 10:40:45,000 - trainer - INFO -     accuracy       : 0.758582
2024-04-02 10:40:45,000 - trainer - INFO -     macro_f        : 0.750791
2024-04-02 10:40:45,000 - trainer - INFO -     precision      : 0.78537
2024-04-02 10:40:45,000 - trainer - INFO -     recall         : 0.758582
2024-04-02 10:40:45,000 - trainer - INFO -     doc_entropy    : 0.133791
2024-04-02 10:40:45,000 - trainer - INFO -     val_loss       : 1.171954
2024-04-02 10:40:45,000 - trainer - INFO -     val_accuracy   : 0.670865
2024-04-02 10:40:45,000 - trainer - INFO -     val_macro_f    : 0.664942
2024-04-02 10:40:45,015 - trainer - INFO -     val_precision  : 0.706688
2024-04-02 10:40:45,015 - trainer - INFO -     val_recall     : 0.670865
2024-04-02 10:40:45,015 - trainer - INFO -     val_doc_entropy: 0.128993
2024-04-02 10:40:45,015 - trainer - INFO -     test_loss      : 1.160981
2024-04-02 10:40:45,015 - trainer - INFO -     test_accuracy  : 0.66987
2024-04-02 10:40:45,015 - trainer - INFO -     test_macro_f   : 0.664114
2024-04-02 10:40:45,015 - trainer - INFO -     test_precision : 0.707697
2024-04-02 10:40:45,015 - trainer - INFO -     test_recall    : 0.66987
2024-04-02 10:40:45,015 - trainer - INFO -     test_doc_entropy: 0.128799
2024-04-02 10:42:50,549 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=4000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=4000, out_features=200, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,491,883
Freeze params: 0
2024-04-02 10:47:38,221 - trainer - INFO -     epoch          : 1
2024-04-02 10:47:38,221 - trainer - INFO -     loss           : 1.271369
2024-04-02 10:47:38,221 - trainer - INFO -     accuracy       : 0.638621
2024-04-02 10:47:38,221 - trainer - INFO -     macro_f        : 0.619034
2024-04-02 10:47:38,221 - trainer - INFO -     precision      : 0.650787
2024-04-02 10:47:38,221 - trainer - INFO -     recall         : 0.638621
2024-04-02 10:47:38,221 - trainer - INFO -     doc_entropy    : 0.435696
2024-04-02 10:47:38,221 - trainer - INFO -     val_loss       : 1.119257
2024-04-02 10:47:38,221 - trainer - INFO -     val_accuracy   : 0.676392
2024-04-02 10:47:38,221 - trainer - INFO -     val_macro_f    : 0.660415
2024-04-02 10:47:38,221 - trainer - INFO -     val_precision  : 0.694818
2024-04-02 10:47:38,221 - trainer - INFO -     val_recall     : 0.676392
2024-04-02 10:47:38,221 - trainer - INFO -     val_doc_entropy: 0.18581
2024-04-02 10:47:38,221 - trainer - INFO -     test_loss      : 1.121003
2024-04-02 10:47:38,221 - trainer - INFO -     test_accuracy  : 0.673056
2024-04-02 10:47:38,221 - trainer - INFO -     test_macro_f   : 0.657624
2024-04-02 10:47:38,221 - trainer - INFO -     test_precision : 0.692237
2024-04-02 10:47:38,221 - trainer - INFO -     test_recall    : 0.673056
2024-04-02 10:47:38,221 - trainer - INFO -     test_doc_entropy: 0.186137
2024-04-02 10:52:30,702 - trainer - INFO -     epoch          : 2
2024-04-02 10:52:30,702 - trainer - INFO -     loss           : 0.979801
2024-04-02 10:52:30,702 - trainer - INFO -     accuracy       : 0.711295
2024-04-02 10:52:30,702 - trainer - INFO -     macro_f        : 0.699841
2024-04-02 10:52:30,702 - trainer - INFO -     precision      : 0.73491
2024-04-02 10:52:30,702 - trainer - INFO -     recall         : 0.711295
2024-04-02 10:52:30,702 - trainer - INFO -     doc_entropy    : 0.15232
2024-04-02 10:52:30,702 - trainer - INFO -     val_loss       : 1.119785
2024-04-02 10:52:30,702 - trainer - INFO -     val_accuracy   : 0.678831
2024-04-02 10:52:30,702 - trainer - INFO -     val_macro_f    : 0.662588
2024-04-02 10:52:30,702 - trainer - INFO -     val_precision  : 0.694744
2024-04-02 10:52:30,702 - trainer - INFO -     val_recall     : 0.678831
2024-04-02 10:52:30,702 - trainer - INFO -     val_doc_entropy: 0.13276
2024-04-02 10:52:30,702 - trainer - INFO -     test_loss      : 1.114101
2024-04-02 10:52:30,702 - trainer - INFO -     test_accuracy  : 0.679727
2024-04-02 10:52:30,702 - trainer - INFO -     test_macro_f   : 0.6647
2024-04-02 10:52:30,702 - trainer - INFO -     test_precision : 0.700024
2024-04-02 10:52:30,702 - trainer - INFO -     test_recall    : 0.679727
2024-04-02 10:52:30,702 - trainer - INFO -     test_doc_entropy: 0.132699
2024-04-02 10:57:24,469 - trainer - INFO -     epoch          : 3
2024-04-02 10:57:24,469 - trainer - INFO -     loss           : 0.802581
2024-04-02 10:57:24,469 - trainer - INFO -     accuracy       : 0.760294
2024-04-02 10:57:24,469 - trainer - INFO -     macro_f        : 0.752301
2024-04-02 10:57:24,469 - trainer - INFO -     precision      : 0.785969
2024-04-02 10:57:24,469 - trainer - INFO -     recall         : 0.760294
2024-04-02 10:57:24,469 - trainer - INFO -     doc_entropy    : 0.131972
2024-04-02 10:57:24,469 - trainer - INFO -     val_loss       : 1.168143
2024-04-02 10:57:24,469 - trainer - INFO -     val_accuracy   : 0.673056
2024-04-02 10:57:24,469 - trainer - INFO -     val_macro_f    : 0.659829
2024-04-02 10:57:24,469 - trainer - INFO -     val_precision  : 0.697557
2024-04-02 10:57:24,469 - trainer - INFO -     val_recall     : 0.673056
2024-04-02 10:57:24,469 - trainer - INFO -     val_doc_entropy: 0.125752
2024-04-02 10:57:24,469 - trainer - INFO -     test_loss      : 1.153259
2024-04-02 10:57:24,469 - trainer - INFO -     test_accuracy  : 0.673106
2024-04-02 10:57:24,469 - trainer - INFO -     test_macro_f   : 0.659388
2024-04-02 10:57:24,469 - trainer - INFO -     test_precision : 0.695637
2024-04-02 10:57:24,469 - trainer - INFO -     test_recall    : 0.673106
2024-04-02 10:57:24,469 - trainer - INFO -     test_doc_entropy: 0.125544
2024-04-02 10:58:07,118 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=4000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=4000, out_features=200, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,491,883
Freeze params: 0
2024-04-02 11:02:58,682 - trainer - INFO -     epoch          : 1
2024-04-02 11:02:58,682 - trainer - INFO -     loss           : 1.263651
2024-04-02 11:02:58,682 - trainer - INFO -     accuracy       : 0.642411
2024-04-02 11:02:58,682 - trainer - INFO -     macro_f        : 0.622194
2024-04-02 11:02:58,682 - trainer - INFO -     precision      : 0.653307
2024-04-02 11:02:58,682 - trainer - INFO -     recall         : 0.642411
2024-04-02 11:02:58,682 - trainer - INFO -     doc_entropy    : 0.468477
2024-04-02 11:02:58,682 - trainer - INFO -     val_loss       : 1.118701
2024-04-02 11:02:58,682 - trainer - INFO -     val_accuracy   : 0.677835
2024-04-02 11:02:58,682 - trainer - INFO -     val_macro_f    : 0.66508
2024-04-02 11:02:58,682 - trainer - INFO -     val_precision  : 0.701147
2024-04-02 11:02:58,682 - trainer - INFO -     val_recall     : 0.677835
2024-04-02 11:02:58,682 - trainer - INFO -     val_doc_entropy: 0.180509
2024-04-02 11:02:58,682 - trainer - INFO -     test_loss      : 1.125686
2024-04-02 11:02:58,682 - trainer - INFO -     test_accuracy  : 0.672707
2024-04-02 11:02:58,682 - trainer - INFO -     test_macro_f   : 0.661304
2024-04-02 11:02:58,682 - trainer - INFO -     test_precision : 0.69721
2024-04-02 11:02:58,682 - trainer - INFO -     test_recall    : 0.672707
2024-04-02 11:02:58,682 - trainer - INFO -     test_doc_entropy: 0.179918
2024-04-02 11:07:54,060 - trainer - INFO -     epoch          : 2
2024-04-02 11:07:54,060 - trainer - INFO -     loss           : 0.976661
2024-04-02 11:07:54,060 - trainer - INFO -     accuracy       : 0.711662
2024-04-02 11:07:54,060 - trainer - INFO -     macro_f        : 0.700372
2024-04-02 11:07:54,060 - trainer - INFO -     precision      : 0.735292
2024-04-02 11:07:54,060 - trainer - INFO -     recall         : 0.711662
2024-04-02 11:07:54,060 - trainer - INFO -     doc_entropy    : 0.149329
2024-04-02 11:07:54,060 - trainer - INFO -     val_loss       : 1.107591
2024-04-02 11:07:54,076 - trainer - INFO -     val_accuracy   : 0.680076
2024-04-02 11:07:54,076 - trainer - INFO -     val_macro_f    : 0.663538
2024-04-02 11:07:54,076 - trainer - INFO -     val_precision  : 0.694574
2024-04-02 11:07:54,076 - trainer - INFO -     val_recall     : 0.680076
2024-04-02 11:07:54,076 - trainer - INFO -     val_doc_entropy: 0.128444
2024-04-02 11:07:54,076 - trainer - INFO -     test_loss      : 1.12026
2024-04-02 11:07:54,076 - trainer - INFO -     test_accuracy  : 0.676541
2024-04-02 11:07:54,076 - trainer - INFO -     test_macro_f   : 0.663129
2024-04-02 11:07:54,076 - trainer - INFO -     test_precision : 0.697844
2024-04-02 11:07:54,076 - trainer - INFO -     test_recall    : 0.676541
2024-04-02 11:07:54,076 - trainer - INFO -     test_doc_entropy: 0.128359
2024-04-02 11:12:47,436 - trainer - INFO -     epoch          : 3
2024-04-02 11:12:47,436 - trainer - INFO -     loss           : 0.809021
2024-04-02 11:12:47,436 - trainer - INFO -     accuracy       : 0.759186
2024-04-02 11:12:47,436 - trainer - INFO -     macro_f        : 0.751047
2024-04-02 11:12:47,436 - trainer - INFO -     precision      : 0.784871
2024-04-02 11:12:47,436 - trainer - INFO -     recall         : 0.759186
2024-04-02 11:12:47,436 - trainer - INFO -     doc_entropy    : 0.127857
2024-04-02 11:12:47,436 - trainer - INFO -     val_loss       : 1.177644
2024-04-02 11:12:47,436 - trainer - INFO -     val_accuracy   : 0.667928
2024-04-02 11:12:47,436 - trainer - INFO -     val_macro_f    : 0.654988
2024-04-02 11:12:47,436 - trainer - INFO -     val_precision  : 0.691282
2024-04-02 11:12:47,436 - trainer - INFO -     val_recall     : 0.667928
2024-04-02 11:12:47,436 - trainer - INFO -     val_doc_entropy: 0.123049
2024-04-02 11:12:47,436 - trainer - INFO -     test_loss      : 1.173673
2024-04-02 11:12:47,436 - trainer - INFO -     test_accuracy  : 0.668127
2024-04-02 11:12:47,436 - trainer - INFO -     test_macro_f   : 0.656098
2024-04-02 11:12:47,436 - trainer - INFO -     test_precision : 0.695147
2024-04-02 11:12:47,436 - trainer - INFO -     test_recall    : 0.668127
2024-04-02 11:12:47,436 - trainer - INFO -     test_doc_entropy: 0.12261
2024-04-02 11:13:28,507 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=4000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=4000, out_features=200, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,491,883
Freeze params: 0
2024-04-02 11:18:21,369 - trainer - INFO -     epoch          : 1
2024-04-02 11:18:21,369 - trainer - INFO -     loss           : 1.271658
2024-04-02 11:18:21,369 - trainer - INFO -     accuracy       : 0.640064
2024-04-02 11:18:21,369 - trainer - INFO -     macro_f        : 0.61982
2024-04-02 11:18:21,369 - trainer - INFO -     precision      : 0.65109
2024-04-02 11:18:21,369 - trainer - INFO -     recall         : 0.640064
2024-04-02 11:18:21,385 - trainer - INFO -     doc_entropy    : 0.438399
2024-04-02 11:18:21,385 - trainer - INFO -     val_loss       : 1.126615
2024-04-02 11:18:21,385 - trainer - INFO -     val_accuracy   : 0.672309
2024-04-02 11:18:21,385 - trainer - INFO -     val_macro_f    : 0.655412
2024-04-02 11:18:21,385 - trainer - INFO -     val_precision  : 0.686367
2024-04-02 11:18:21,385 - trainer - INFO -     val_recall     : 0.672309
2024-04-02 11:18:21,385 - trainer - INFO -     val_doc_entropy: 0.185913
2024-04-02 11:18:21,385 - trainer - INFO -     test_loss      : 1.138229
2024-04-02 11:18:21,385 - trainer - INFO -     test_accuracy  : 0.668476
2024-04-02 11:18:21,385 - trainer - INFO -     test_macro_f   : 0.65154
2024-04-02 11:18:21,385 - trainer - INFO -     test_precision : 0.681606
2024-04-02 11:18:21,385 - trainer - INFO -     test_recall    : 0.668476
2024-04-02 11:18:21,385 - trainer - INFO -     test_doc_entropy: 0.185461
2024-04-02 11:23:15,104 - trainer - INFO -     epoch          : 2
2024-04-02 11:23:15,104 - trainer - INFO -     loss           : 0.976185
2024-04-02 11:23:15,104 - trainer - INFO -     accuracy       : 0.713398
2024-04-02 11:23:15,104 - trainer - INFO -     macro_f        : 0.701889
2024-04-02 11:23:15,104 - trainer - INFO -     precision      : 0.736717
2024-04-02 11:23:15,104 - trainer - INFO -     recall         : 0.713398
2024-04-02 11:23:15,104 - trainer - INFO -     doc_entropy    : 0.151774
2024-04-02 11:23:15,104 - trainer - INFO -     val_loss       : 1.118812
2024-04-02 11:23:15,104 - trainer - INFO -     val_accuracy   : 0.678682
2024-04-02 11:23:15,104 - trainer - INFO -     val_macro_f    : 0.667047
2024-04-02 11:23:15,104 - trainer - INFO -     val_precision  : 0.702566
2024-04-02 11:23:15,104 - trainer - INFO -     val_recall     : 0.678682
2024-04-02 11:23:15,104 - trainer - INFO -     val_doc_entropy: 0.131348
2024-04-02 11:23:15,104 - trainer - INFO -     test_loss      : 1.117884
2024-04-02 11:23:15,104 - trainer - INFO -     test_accuracy  : 0.677337
2024-04-02 11:23:15,104 - trainer - INFO -     test_macro_f   : 0.668202
2024-04-02 11:23:15,104 - trainer - INFO -     test_precision : 0.709073
2024-04-02 11:23:15,120 - trainer - INFO -     test_recall    : 0.677337
2024-04-02 11:23:15,120 - trainer - INFO -     test_doc_entropy: 0.131416
2024-04-02 11:28:09,770 - trainer - INFO -     epoch          : 3
2024-04-02 11:28:09,770 - trainer - INFO -     loss           : 0.793583
2024-04-02 11:28:09,770 - trainer - INFO -     accuracy       : 0.765211
2024-04-02 11:28:09,770 - trainer - INFO -     macro_f        : 0.757093
2024-04-02 11:28:09,770 - trainer - INFO -     precision      : 0.789867
2024-04-02 11:28:09,770 - trainer - INFO -     recall         : 0.765211
2024-04-02 11:28:09,770 - trainer - INFO -     doc_entropy    : 0.128747
2024-04-02 11:28:09,770 - trainer - INFO -     val_loss       : 1.179249
2024-04-02 11:28:09,770 - trainer - INFO -     val_accuracy   : 0.671811
2024-04-02 11:28:09,770 - trainer - INFO -     val_macro_f    : 0.661577
2024-04-02 11:28:09,770 - trainer - INFO -     val_precision  : 0.69954
2024-04-02 11:28:09,770 - trainer - INFO -     val_recall     : 0.671811
2024-04-02 11:28:09,785 - trainer - INFO -     val_doc_entropy: 0.12271
2024-04-02 11:28:09,785 - trainer - INFO -     test_loss      : 1.167898
2024-04-02 11:28:09,785 - trainer - INFO -     test_accuracy  : 0.67216
2024-04-02 11:28:09,785 - trainer - INFO -     test_macro_f   : 0.66147
2024-04-02 11:28:09,785 - trainer - INFO -     test_precision : 0.699904
2024-04-02 11:28:09,785 - trainer - INFO -     test_recall    : 0.67216
2024-04-02 11:28:09,785 - trainer - INFO -     test_doc_entropy: 0.1221
2024-04-02 11:28:50,495 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=4000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=4000, out_features=200, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,491,883
Freeze params: 0
2024-04-02 11:33:41,795 - trainer - INFO -     epoch          : 1
2024-04-02 11:33:41,810 - trainer - INFO -     loss           : 1.274912
2024-04-02 11:33:41,810 - trainer - INFO -     accuracy       : 0.638085
2024-04-02 11:33:41,810 - trainer - INFO -     macro_f        : 0.617706
2024-04-02 11:33:41,810 - trainer - INFO -     precision      : 0.649029
2024-04-02 11:33:41,810 - trainer - INFO -     recall         : 0.638085
2024-04-02 11:33:41,810 - trainer - INFO -     doc_entropy    : 0.42026
2024-04-02 11:33:41,810 - trainer - INFO -     val_loss       : 1.130841
2024-04-02 11:33:41,810 - trainer - INFO -     val_accuracy   : 0.676591
2024-04-02 11:33:41,810 - trainer - INFO -     val_macro_f    : 0.662484
2024-04-02 11:33:41,810 - trainer - INFO -     val_precision  : 0.69716
2024-04-02 11:33:41,810 - trainer - INFO -     val_recall     : 0.676591
2024-04-02 11:33:41,810 - trainer - INFO -     val_doc_entropy: 0.175691
2024-04-02 11:33:41,810 - trainer - INFO -     test_loss      : 1.131751
2024-04-02 11:33:41,810 - trainer - INFO -     test_accuracy  : 0.671562
2024-04-02 11:33:41,810 - trainer - INFO -     test_macro_f   : 0.659431
2024-04-02 11:33:41,810 - trainer - INFO -     test_precision : 0.69801
2024-04-02 11:33:41,810 - trainer - INFO -     test_recall    : 0.671562
2024-04-02 11:33:41,810 - trainer - INFO -     test_doc_entropy: 0.175667
2024-04-02 11:38:34,690 - trainer - INFO -     epoch          : 2
2024-04-02 11:38:34,690 - trainer - INFO -     loss           : 0.980564
2024-04-02 11:38:34,690 - trainer - INFO -     accuracy       : 0.71239
2024-04-02 11:38:34,690 - trainer - INFO -     macro_f        : 0.70103
2024-04-02 11:38:34,690 - trainer - INFO -     precision      : 0.735945
2024-04-02 11:38:34,690 - trainer - INFO -     recall         : 0.71239
2024-04-02 11:38:34,706 - trainer - INFO -     doc_entropy    : 0.147379
2024-04-02 11:38:34,706 - trainer - INFO -     val_loss       : 1.11372
2024-04-02 11:38:34,706 - trainer - INFO -     val_accuracy   : 0.677487
2024-04-02 11:38:34,706 - trainer - INFO -     val_macro_f    : 0.663838
2024-04-02 11:38:34,706 - trainer - INFO -     val_precision  : 0.698134
2024-04-02 11:38:34,706 - trainer - INFO -     val_recall     : 0.677487
2024-04-02 11:38:34,706 - trainer - INFO -     val_doc_entropy: 0.133387
2024-04-02 11:38:34,706 - trainer - INFO -     test_loss      : 1.113467
2024-04-02 11:38:34,706 - trainer - INFO -     test_accuracy  : 0.679926
2024-04-02 11:38:34,706 - trainer - INFO -     test_macro_f   : 0.665184
2024-04-02 11:38:34,706 - trainer - INFO -     test_precision : 0.698934
2024-04-02 11:38:34,706 - trainer - INFO -     test_recall    : 0.679926
2024-04-02 11:38:34,706 - trainer - INFO -     test_doc_entropy: 0.133454
2024-04-02 11:43:26,953 - trainer - INFO -     epoch          : 3
2024-04-02 11:43:26,953 - trainer - INFO -     loss           : 0.80212
2024-04-02 11:43:26,953 - trainer - INFO -     accuracy       : 0.762566
2024-04-02 11:43:26,953 - trainer - INFO -     macro_f        : 0.754805
2024-04-02 11:43:26,953 - trainer - INFO -     precision      : 0.788546
2024-04-02 11:43:26,953 - trainer - INFO -     recall         : 0.762566
2024-04-02 11:43:26,953 - trainer - INFO -     doc_entropy    : 0.13107
2024-04-02 11:43:26,953 - trainer - INFO -     val_loss       : 1.159259
2024-04-02 11:43:26,953 - trainer - INFO -     val_accuracy   : 0.672956
2024-04-02 11:43:26,953 - trainer - INFO -     val_macro_f    : 0.664019
2024-04-02 11:43:26,953 - trainer - INFO -     val_precision  : 0.706451
2024-04-02 11:43:26,953 - trainer - INFO -     val_recall     : 0.672956
2024-04-02 11:43:26,953 - trainer - INFO -     val_doc_entropy: 0.124606
2024-04-02 11:43:26,953 - trainer - INFO -     test_loss      : 1.163527
2024-04-02 11:43:26,953 - trainer - INFO -     test_accuracy  : 0.67201
2024-04-02 11:43:26,953 - trainer - INFO -     test_macro_f   : 0.664652
2024-04-02 11:43:26,953 - trainer - INFO -     test_precision : 0.708903
2024-04-02 11:43:26,953 - trainer - INFO -     test_recall    : 0.67201
2024-04-02 11:43:26,953 - trainer - INFO -     test_doc_entropy: 0.124435
2024-04-02 11:44:06,029 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=4000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=4000, out_features=200, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,491,883
Freeze params: 0
2024-04-02 11:48:56,753 - trainer - INFO -     epoch          : 1
2024-04-02 11:48:56,753 - trainer - INFO -     loss           : 1.270414
2024-04-02 11:48:56,753 - trainer - INFO -     accuracy       : 0.639535
2024-04-02 11:48:56,753 - trainer - INFO -     macro_f        : 0.619292
2024-04-02 11:48:56,753 - trainer - INFO -     precision      : 0.650811
2024-04-02 11:48:56,753 - trainer - INFO -     recall         : 0.639535
2024-04-02 11:48:56,753 - trainer - INFO -     doc_entropy    : 0.42659
2024-04-02 11:48:56,753 - trainer - INFO -     val_loss       : 1.14104
2024-04-02 11:48:56,753 - trainer - INFO -     val_accuracy   : 0.672757
2024-04-02 11:48:56,753 - trainer - INFO -     val_macro_f    : 0.657406
2024-04-02 11:48:56,753 - trainer - INFO -     val_precision  : 0.689494
2024-04-02 11:48:56,753 - trainer - INFO -     val_recall     : 0.672757
2024-04-02 11:48:56,753 - trainer - INFO -     val_doc_entropy: 0.183457
2024-04-02 11:48:56,753 - trainer - INFO -     test_loss      : 1.135442
2024-04-02 11:48:56,753 - trainer - INFO -     test_accuracy  : 0.671214
2024-04-02 11:48:56,753 - trainer - INFO -     test_macro_f   : 0.656118
2024-04-02 11:48:56,753 - trainer - INFO -     test_precision : 0.687381
2024-04-02 11:48:56,753 - trainer - INFO -     test_recall    : 0.671214
2024-04-02 11:48:56,753 - trainer - INFO -     test_doc_entropy: 0.183221
2024-04-02 11:53:48,356 - trainer - INFO -     epoch          : 2
2024-04-02 11:53:48,356 - trainer - INFO -     loss           : 0.979169
2024-04-02 11:53:48,356 - trainer - INFO -     accuracy       : 0.712011
2024-04-02 11:53:48,356 - trainer - INFO -     macro_f        : 0.700644
2024-04-02 11:53:48,356 - trainer - INFO -     precision      : 0.735375
2024-04-02 11:53:48,356 - trainer - INFO -     recall         : 0.712011
2024-04-02 11:53:48,356 - trainer - INFO -     doc_entropy    : 0.151231
2024-04-02 11:53:48,356 - trainer - INFO -     val_loss       : 1.122722
2024-04-02 11:53:48,356 - trainer - INFO -     val_accuracy   : 0.675246
2024-04-02 11:53:48,356 - trainer - INFO -     val_macro_f    : 0.666004
2024-04-02 11:53:48,356 - trainer - INFO -     val_precision  : 0.705998
2024-04-02 11:53:48,356 - trainer - INFO -     val_recall     : 0.675246
2024-04-02 11:53:48,356 - trainer - INFO -     val_doc_entropy: 0.132533
2024-04-02 11:53:48,356 - trainer - INFO -     test_loss      : 1.118903
2024-04-02 11:53:48,356 - trainer - INFO -     test_accuracy  : 0.677337
2024-04-02 11:53:48,356 - trainer - INFO -     test_macro_f   : 0.668998
2024-04-02 11:53:48,356 - trainer - INFO -     test_precision : 0.709754
2024-04-02 11:53:48,356 - trainer - INFO -     test_recall    : 0.677337
2024-04-02 11:53:48,356 - trainer - INFO -     test_doc_entropy: 0.13197
2024-04-02 11:58:39,630 - trainer - INFO -     epoch          : 3
2024-04-02 11:58:39,630 - trainer - INFO -     loss           : 0.805267
2024-04-02 11:58:39,630 - trainer - INFO -     accuracy       : 0.759983
2024-04-02 11:58:39,630 - trainer - INFO -     macro_f        : 0.751876
2024-04-02 11:58:39,630 - trainer - INFO -     precision      : 0.785486
2024-04-02 11:58:39,630 - trainer - INFO -     recall         : 0.759983
2024-04-02 11:58:39,630 - trainer - INFO -     doc_entropy    : 0.129745
2024-04-02 11:58:39,630 - trainer - INFO -     val_loss       : 1.176112
2024-04-02 11:58:39,630 - trainer - INFO -     val_accuracy   : 0.672458
2024-04-02 11:58:39,630 - trainer - INFO -     val_macro_f    : 0.666113
2024-04-02 11:58:39,630 - trainer - INFO -     val_precision  : 0.709216
2024-04-02 11:58:39,630 - trainer - INFO -     val_recall     : 0.672458
2024-04-02 11:58:39,630 - trainer - INFO -     val_doc_entropy: 0.121171
2024-04-02 11:58:39,630 - trainer - INFO -     test_loss      : 1.166764
2024-04-02 11:58:39,630 - trainer - INFO -     test_accuracy  : 0.673554
2024-04-02 11:58:39,630 - trainer - INFO -     test_macro_f   : 0.665088
2024-04-02 11:58:39,630 - trainer - INFO -     test_precision : 0.705445
2024-04-02 11:58:39,630 - trainer - INFO -     test_recall    : 0.673554
2024-04-02 11:58:39,630 - trainer - INFO -     test_doc_entropy: 0.120945
2024-04-06 10:00:43,986 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,760,523
Freeze params: 0
2024-04-06 10:03:47,974 - trainer - INFO -     epoch          : 1
2024-04-06 10:03:47,974 - trainer - INFO -     loss           : 1.217688
2024-04-06 10:03:47,974 - trainer - INFO -     accuracy       : 0.653196
2024-04-06 10:03:47,974 - trainer - INFO -     macro_f        : 0.633445
2024-04-06 10:03:47,974 - trainer - INFO -     precision      : 0.664651
2024-04-06 10:03:47,976 - trainer - INFO -     recall         : 0.653196
2024-04-06 10:03:47,976 - trainer - INFO -     doc_entropy    : 2.405499
2024-04-06 10:03:47,976 - trainer - INFO -     val_loss       : 1.084049
2024-04-06 10:03:47,976 - trainer - INFO -     val_accuracy   : 0.685004
2024-04-06 10:03:47,976 - trainer - INFO -     val_macro_f    : 0.669041
2024-04-06 10:03:47,976 - trainer - INFO -     val_precision  : 0.701875
2024-04-06 10:03:47,976 - trainer - INFO -     val_recall     : 0.685004
2024-04-06 10:03:47,976 - trainer - INFO -     val_doc_entropy: 2.283218
2024-04-06 10:03:47,976 - trainer - INFO -     test_loss      : 1.086722
2024-04-06 10:03:47,976 - trainer - INFO -     test_accuracy  : 0.685403
2024-04-06 10:03:47,976 - trainer - INFO -     test_macro_f   : 0.671089
2024-04-06 10:03:47,976 - trainer - INFO -     test_precision : 0.70546
2024-04-06 10:03:47,976 - trainer - INFO -     test_recall    : 0.685403
2024-04-06 10:03:47,976 - trainer - INFO -     test_doc_entropy: 2.28673
2024-04-06 10:06:56,779 - trainer - INFO -     epoch          : 2
2024-04-06 10:06:56,779 - trainer - INFO -     loss           : 0.892268
2024-04-06 10:06:56,779 - trainer - INFO -     accuracy       : 0.737546
2024-04-06 10:06:56,779 - trainer - INFO -     macro_f        : 0.727431
2024-04-06 10:06:56,779 - trainer - INFO -     precision      : 0.761048
2024-04-06 10:06:56,779 - trainer - INFO -     recall         : 0.737546
2024-04-06 10:06:56,779 - trainer - INFO -     doc_entropy    : 2.09622
2024-04-06 10:06:56,779 - trainer - INFO -     val_loss       : 1.079539
2024-04-06 10:06:56,779 - trainer - INFO -     val_accuracy   : 0.688041
2024-04-06 10:06:56,779 - trainer - INFO -     val_macro_f    : 0.674691
2024-04-06 10:06:56,779 - trainer - INFO -     val_precision  : 0.70801
2024-04-06 10:06:56,779 - trainer - INFO -     val_recall     : 0.688041
2024-04-06 10:06:56,779 - trainer - INFO -     val_doc_entropy: 2.20576
2024-04-06 10:06:56,779 - trainer - INFO -     test_loss      : 1.07506
2024-04-06 10:06:56,779 - trainer - INFO -     test_accuracy  : 0.69068
2024-04-06 10:06:56,779 - trainer - INFO -     test_macro_f   : 0.677713
2024-04-06 10:06:56,779 - trainer - INFO -     test_precision : 0.712152
2024-04-06 10:06:56,779 - trainer - INFO -     test_recall    : 0.69068
2024-04-06 10:06:56,779 - trainer - INFO -     test_doc_entropy: 2.21019
2024-04-06 10:10:06,268 - trainer - INFO -     epoch          : 3
2024-04-06 10:10:06,268 - trainer - INFO -     loss           : 0.675988
2024-04-06 10:10:06,268 - trainer - INFO -     accuracy       : 0.800138
2024-04-06 10:10:06,268 - trainer - INFO -     macro_f        : 0.794358
2024-04-06 10:10:06,268 - trainer - INFO -     precision      : 0.825181
2024-04-06 10:10:06,268 - trainer - INFO -     recall         : 0.800138
2024-04-06 10:10:06,268 - trainer - INFO -     doc_entropy    : 1.972842
2024-04-06 10:10:06,268 - trainer - INFO -     val_loss       : 1.165205
2024-04-06 10:10:06,268 - trainer - INFO -     val_accuracy   : 0.677238
2024-04-06 10:10:06,268 - trainer - INFO -     val_macro_f    : 0.667882
2024-04-06 10:10:06,268 - trainer - INFO -     val_precision  : 0.706268
2024-04-06 10:10:06,268 - trainer - INFO -     val_recall     : 0.677238
2024-04-06 10:10:06,268 - trainer - INFO -     val_doc_entropy: 2.089062
2024-04-06 10:10:06,268 - trainer - INFO -     test_loss      : 1.159165
2024-04-06 10:10:06,268 - trainer - INFO -     test_accuracy  : 0.675943
2024-04-06 10:10:06,268 - trainer - INFO -     test_macro_f   : 0.667127
2024-04-06 10:10:06,268 - trainer - INFO -     test_precision : 0.704865
2024-04-06 10:10:06,268 - trainer - INFO -     test_recall    : 0.675943
2024-04-06 10:10:06,268 - trainer - INFO -     test_doc_entropy: 2.092544
2024-04-06 10:10:48,763 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,760,523
Freeze params: 0
2024-04-06 10:13:51,489 - trainer - INFO -     epoch          : 1
2024-04-06 10:13:51,489 - trainer - INFO -     loss           : 1.222028
2024-04-06 10:13:51,489 - trainer - INFO -     accuracy       : 0.652723
2024-04-06 10:13:51,489 - trainer - INFO -     macro_f        : 0.633592
2024-04-06 10:13:51,489 - trainer - INFO -     precision      : 0.665743
2024-04-06 10:13:51,489 - trainer - INFO -     recall         : 0.652723
2024-04-06 10:13:51,489 - trainer - INFO -     doc_entropy    : 2.399858
2024-04-06 10:13:51,489 - trainer - INFO -     val_loss       : 1.081422
2024-04-06 10:13:51,489 - trainer - INFO -     val_accuracy   : 0.684905
2024-04-06 10:13:51,489 - trainer - INFO -     val_macro_f    : 0.669337
2024-04-06 10:13:51,489 - trainer - INFO -     val_precision  : 0.700426
2024-04-06 10:13:51,489 - trainer - INFO -     val_recall     : 0.684905
2024-04-06 10:13:51,489 - trainer - INFO -     val_doc_entropy: 2.34588
2024-04-06 10:13:51,489 - trainer - INFO -     test_loss      : 1.084945
2024-04-06 10:13:51,489 - trainer - INFO -     test_accuracy  : 0.683859
2024-04-06 10:13:51,489 - trainer - INFO -     test_macro_f   : 0.667596
2024-04-06 10:13:51,489 - trainer - INFO -     test_precision : 0.698551
2024-04-06 10:13:51,489 - trainer - INFO -     test_recall    : 0.683859
2024-04-06 10:13:51,489 - trainer - INFO -     test_doc_entropy: 2.348562
2024-04-06 10:16:52,804 - trainer - INFO -     epoch          : 2
2024-04-06 10:16:52,804 - trainer - INFO -     loss           : 0.889101
2024-04-06 10:16:52,804 - trainer - INFO -     accuracy       : 0.7381
2024-04-06 10:16:52,804 - trainer - INFO -     macro_f        : 0.728404
2024-04-06 10:16:52,804 - trainer - INFO -     precision      : 0.762123
2024-04-06 10:16:52,804 - trainer - INFO -     recall         : 0.7381
2024-04-06 10:16:52,804 - trainer - INFO -     doc_entropy    : 2.134537
2024-04-06 10:16:52,804 - trainer - INFO -     val_loss       : 1.082305
2024-04-06 10:16:52,804 - trainer - INFO -     val_accuracy   : 0.686
2024-04-06 10:16:52,804 - trainer - INFO -     val_macro_f    : 0.672606
2024-04-06 10:16:52,804 - trainer - INFO -     val_precision  : 0.706593
2024-04-06 10:16:52,804 - trainer - INFO -     val_recall     : 0.686
2024-04-06 10:16:52,804 - trainer - INFO -     val_doc_entropy: 2.257243
2024-04-06 10:16:52,804 - trainer - INFO -     test_loss      : 1.086786
2024-04-06 10:16:52,804 - trainer - INFO -     test_accuracy  : 0.687145
2024-04-06 10:16:52,804 - trainer - INFO -     test_macro_f   : 0.674239
2024-04-06 10:16:52,804 - trainer - INFO -     test_precision : 0.707543
2024-04-06 10:16:52,804 - trainer - INFO -     test_recall    : 0.687145
2024-04-06 10:16:52,804 - trainer - INFO -     test_doc_entropy: 2.261617
2024-04-06 10:19:54,979 - trainer - INFO -     epoch          : 3
2024-04-06 10:19:54,979 - trainer - INFO -     loss           : 0.675842
2024-04-06 10:19:54,979 - trainer - INFO -     accuracy       : 0.798607
2024-04-06 10:19:54,979 - trainer - INFO -     macro_f        : 0.792835
2024-04-06 10:19:54,979 - trainer - INFO -     precision      : 0.823993
2024-04-06 10:19:54,979 - trainer - INFO -     recall         : 0.798607
2024-04-06 10:19:54,979 - trainer - INFO -     doc_entropy    : 1.959965
2024-04-06 10:19:54,979 - trainer - INFO -     val_loss       : 1.175308
2024-04-06 10:19:54,979 - trainer - INFO -     val_accuracy   : 0.674848
2024-04-06 10:19:54,979 - trainer - INFO -     val_macro_f    : 0.667957
2024-04-06 10:19:54,979 - trainer - INFO -     val_precision  : 0.71093
2024-04-06 10:19:54,979 - trainer - INFO -     val_recall     : 0.674848
2024-04-06 10:19:54,979 - trainer - INFO -     val_doc_entropy: 2.053795
2024-04-06 10:19:54,979 - trainer - INFO -     test_loss      : 1.168254
2024-04-06 10:19:54,979 - trainer - INFO -     test_accuracy  : 0.679229
2024-04-06 10:19:54,979 - trainer - INFO -     test_macro_f   : 0.673214
2024-04-06 10:19:54,979 - trainer - INFO -     test_precision : 0.715843
2024-04-06 10:19:54,979 - trainer - INFO -     test_recall    : 0.679229
2024-04-06 10:19:54,979 - trainer - INFO -     test_doc_entropy: 2.057935
2024-04-06 10:20:32,351 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,760,523
Freeze params: 0
2024-04-06 10:23:39,083 - trainer - INFO -     epoch          : 1
2024-04-06 10:23:39,083 - trainer - INFO -     loss           : 1.221213
2024-04-06 10:23:39,083 - trainer - INFO -     accuracy       : 0.653383
2024-04-06 10:23:39,083 - trainer - INFO -     macro_f        : 0.633704
2024-04-06 10:23:39,083 - trainer - INFO -     precision      : 0.66379
2024-04-06 10:23:39,083 - trainer - INFO -     recall         : 0.653383
2024-04-06 10:23:39,083 - trainer - INFO -     doc_entropy    : 2.388754
2024-04-06 10:23:39,083 - trainer - INFO -     val_loss       : 1.070491
2024-04-06 10:23:39,083 - trainer - INFO -     val_accuracy   : 0.688938
2024-04-06 10:23:39,083 - trainer - INFO -     val_macro_f    : 0.672653
2024-04-06 10:23:39,083 - trainer - INFO -     val_precision  : 0.702744
2024-04-06 10:23:39,083 - trainer - INFO -     val_recall     : 0.688938
2024-04-06 10:23:39,083 - trainer - INFO -     val_doc_entropy: 2.251231
2024-04-06 10:23:39,083 - trainer - INFO -     test_loss      : 1.073121
2024-04-06 10:23:39,083 - trainer - INFO -     test_accuracy  : 0.687494
2024-04-06 10:23:39,083 - trainer - INFO -     test_macro_f   : 0.672822
2024-04-06 10:23:39,083 - trainer - INFO -     test_precision : 0.705956
2024-04-06 10:23:39,083 - trainer - INFO -     test_recall    : 0.687494
2024-04-06 10:23:39,083 - trainer - INFO -     test_doc_entropy: 2.25235
2024-04-06 10:26:39,747 - trainer - INFO -     epoch          : 2
2024-04-06 10:26:39,747 - trainer - INFO -     loss           : 0.888259
2024-04-06 10:26:39,747 - trainer - INFO -     accuracy       : 0.739389
2024-04-06 10:26:39,747 - trainer - INFO -     macro_f        : 0.729724
2024-04-06 10:26:39,747 - trainer - INFO -     precision      : 0.763594
2024-04-06 10:26:39,747 - trainer - INFO -     recall         : 0.739389
2024-04-06 10:26:39,747 - trainer - INFO -     doc_entropy    : 2.114986
2024-04-06 10:26:39,747 - trainer - INFO -     val_loss       : 1.089343
2024-04-06 10:26:39,747 - trainer - INFO -     val_accuracy   : 0.685253
2024-04-06 10:26:39,747 - trainer - INFO -     val_macro_f    : 0.674817
2024-04-06 10:26:39,747 - trainer - INFO -     val_precision  : 0.713417
2024-04-06 10:26:39,747 - trainer - INFO -     val_recall     : 0.685253
2024-04-06 10:26:39,747 - trainer - INFO -     val_doc_entropy: 2.164296
2024-04-06 10:26:39,747 - trainer - INFO -     test_loss      : 1.101493
2024-04-06 10:26:39,747 - trainer - INFO -     test_accuracy  : 0.686299
2024-04-06 10:26:39,747 - trainer - INFO -     test_macro_f   : 0.675734
2024-04-06 10:26:39,747 - trainer - INFO -     test_precision : 0.713825
2024-04-06 10:26:39,747 - trainer - INFO -     test_recall    : 0.686299
2024-04-06 10:26:39,747 - trainer - INFO -     test_doc_entropy: 2.168895
2024-04-06 10:29:43,624 - trainer - INFO -     epoch          : 3
2024-04-06 10:29:43,624 - trainer - INFO -     loss           : 0.677929
2024-04-06 10:29:43,624 - trainer - INFO -     accuracy       : 0.798377
2024-04-06 10:29:43,624 - trainer - INFO -     macro_f        : 0.792456
2024-04-06 10:29:43,624 - trainer - INFO -     precision      : 0.823359
2024-04-06 10:29:43,624 - trainer - INFO -     recall         : 0.798377
2024-04-06 10:29:43,624 - trainer - INFO -     doc_entropy    : 1.968729
2024-04-06 10:29:43,624 - trainer - INFO -     val_loss       : 1.138945
2024-04-06 10:29:43,624 - trainer - INFO -     val_accuracy   : 0.68366
2024-04-06 10:29:43,624 - trainer - INFO -     val_macro_f    : 0.676389
2024-04-06 10:29:43,624 - trainer - INFO -     val_precision  : 0.716879
2024-04-06 10:29:43,624 - trainer - INFO -     val_recall     : 0.68366
2024-04-06 10:29:43,624 - trainer - INFO -     val_doc_entropy: 2.07013
2024-04-06 10:29:43,624 - trainer - INFO -     test_loss      : 1.14075
2024-04-06 10:29:43,624 - trainer - INFO -     test_accuracy  : 0.68376
2024-04-06 10:29:43,624 - trainer - INFO -     test_macro_f   : 0.676675
2024-04-06 10:29:43,624 - trainer - INFO -     test_precision : 0.71775
2024-04-06 10:29:43,624 - trainer - INFO -     test_recall    : 0.68376
2024-04-06 10:29:43,624 - trainer - INFO -     test_doc_entropy: 2.073339
2024-04-06 10:30:23,849 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,760,523
Freeze params: 0
2024-04-06 10:33:26,101 - trainer - INFO -     epoch          : 1
2024-04-06 10:33:26,101 - trainer - INFO -     loss           : 1.219051
2024-04-06 10:33:26,101 - trainer - INFO -     accuracy       : 0.655089
2024-04-06 10:33:26,101 - trainer - INFO -     macro_f        : 0.635153
2024-04-06 10:33:26,101 - trainer - INFO -     precision      : 0.66615
2024-04-06 10:33:26,101 - trainer - INFO -     recall         : 0.655089
2024-04-06 10:33:26,101 - trainer - INFO -     doc_entropy    : 2.439351
2024-04-06 10:33:26,101 - trainer - INFO -     val_loss       : 1.082439
2024-04-06 10:33:26,101 - trainer - INFO -     val_accuracy   : 0.686797
2024-04-06 10:33:26,101 - trainer - INFO -     val_macro_f    : 0.672616
2024-04-06 10:33:26,101 - trainer - INFO -     val_precision  : 0.706023
2024-04-06 10:33:26,101 - trainer - INFO -     val_recall     : 0.686797
2024-04-06 10:33:26,101 - trainer - INFO -     val_doc_entropy: 2.274273
2024-04-06 10:33:26,101 - trainer - INFO -     test_loss      : 1.081935
2024-04-06 10:33:26,101 - trainer - INFO -     test_accuracy  : 0.684556
2024-04-06 10:33:26,101 - trainer - INFO -     test_macro_f   : 0.669343
2024-04-06 10:33:26,101 - trainer - INFO -     test_precision : 0.702563
2024-04-06 10:33:26,101 - trainer - INFO -     test_recall    : 0.684556
2024-04-06 10:33:26,101 - trainer - INFO -     test_doc_entropy: 2.278581
2024-04-06 10:36:25,621 - trainer - INFO -     epoch          : 2
2024-04-06 10:36:25,621 - trainer - INFO -     loss           : 0.883787
2024-04-06 10:36:25,621 - trainer - INFO -     accuracy       : 0.740391
2024-04-06 10:36:25,621 - trainer - INFO -     macro_f        : 0.731508
2024-04-06 10:36:25,621 - trainer - INFO -     precision      : 0.765967
2024-04-06 10:36:25,621 - trainer - INFO -     recall         : 0.740391
2024-04-06 10:36:25,621 - trainer - INFO -     doc_entropy    : 2.119075
2024-04-06 10:36:25,621 - trainer - INFO -     val_loss       : 1.072561
2024-04-06 10:36:25,621 - trainer - INFO -     val_accuracy   : 0.687942
2024-04-06 10:36:25,621 - trainer - INFO -     val_macro_f    : 0.678871
2024-04-06 10:36:25,621 - trainer - INFO -     val_precision  : 0.71701
2024-04-06 10:36:25,621 - trainer - INFO -     val_recall     : 0.687942
2024-04-06 10:36:25,621 - trainer - INFO -     val_doc_entropy: 2.189495
2024-04-06 10:36:25,621 - trainer - INFO -     test_loss      : 1.068714
2024-04-06 10:36:25,621 - trainer - INFO -     test_accuracy  : 0.690083
2024-04-06 10:36:25,621 - trainer - INFO -     test_macro_f   : 0.680939
2024-04-06 10:36:25,621 - trainer - INFO -     test_precision : 0.718135
2024-04-06 10:36:25,621 - trainer - INFO -     test_recall    : 0.690083
2024-04-06 10:36:25,621 - trainer - INFO -     test_doc_entropy: 2.194255
2024-04-06 10:39:27,167 - trainer - INFO -     epoch          : 3
2024-04-06 10:39:27,167 - trainer - INFO -     loss           : 0.675502
2024-04-06 10:39:27,167 - trainer - INFO -     accuracy       : 0.798738
2024-04-06 10:39:27,167 - trainer - INFO -     macro_f        : 0.792817
2024-04-06 10:39:27,167 - trainer - INFO -     precision      : 0.82376
2024-04-06 10:39:27,167 - trainer - INFO -     recall         : 0.798738
2024-04-06 10:39:27,167 - trainer - INFO -     doc_entropy    : 1.977638
2024-04-06 10:39:27,167 - trainer - INFO -     val_loss       : 1.170693
2024-04-06 10:39:27,167 - trainer - INFO -     val_accuracy   : 0.675197
2024-04-06 10:39:27,167 - trainer - INFO -     val_macro_f    : 0.668215
2024-04-06 10:39:27,167 - trainer - INFO -     val_precision  : 0.709304
2024-04-06 10:39:27,167 - trainer - INFO -     val_recall     : 0.675197
2024-04-06 10:39:27,167 - trainer - INFO -     val_doc_entropy: 2.096718
2024-04-06 10:39:27,167 - trainer - INFO -     test_loss      : 1.164307
2024-04-06 10:39:27,167 - trainer - INFO -     test_accuracy  : 0.675744
2024-04-06 10:39:27,167 - trainer - INFO -     test_macro_f   : 0.66974
2024-04-06 10:39:27,167 - trainer - INFO -     test_precision : 0.713505
2024-04-06 10:39:27,167 - trainer - INFO -     test_recall    : 0.675744
2024-04-06 10:39:27,167 - trainer - INFO -     test_doc_entropy: 2.100969
2024-04-06 10:40:04,141 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,760,523
Freeze params: 0
2024-04-06 10:43:03,068 - trainer - INFO -     epoch          : 1
2024-04-06 10:43:03,068 - trainer - INFO -     loss           : 1.222617
2024-04-06 10:43:03,068 - trainer - INFO -     accuracy       : 0.653533
2024-04-06 10:43:03,068 - trainer - INFO -     macro_f        : 0.634198
2024-04-06 10:43:03,068 - trainer - INFO -     precision      : 0.665315
2024-04-06 10:43:03,068 - trainer - INFO -     recall         : 0.653533
2024-04-06 10:43:03,068 - trainer - INFO -     doc_entropy    : 2.489995
2024-04-06 10:43:03,068 - trainer - INFO -     val_loss       : 1.086935
2024-04-06 10:43:03,068 - trainer - INFO -     val_accuracy   : 0.682465
2024-04-06 10:43:03,068 - trainer - INFO -     val_macro_f    : 0.671855
2024-04-06 10:43:03,068 - trainer - INFO -     val_precision  : 0.70843
2024-04-06 10:43:03,068 - trainer - INFO -     val_recall     : 0.682465
2024-04-06 10:43:03,068 - trainer - INFO -     val_doc_entropy: 2.350665
2024-04-06 10:43:03,068 - trainer - INFO -     test_loss      : 1.088173
2024-04-06 10:43:03,068 - trainer - INFO -     test_accuracy  : 0.684208
2024-04-06 10:43:03,068 - trainer - INFO -     test_macro_f   : 0.673226
2024-04-06 10:43:03,068 - trainer - INFO -     test_precision : 0.711346
2024-04-06 10:43:03,068 - trainer - INFO -     test_recall    : 0.684208
2024-04-06 10:43:03,068 - trainer - INFO -     test_doc_entropy: 2.355052
2024-04-06 10:46:04,260 - trainer - INFO -     epoch          : 2
2024-04-06 10:46:04,260 - trainer - INFO -     loss           : 0.891405
2024-04-06 10:46:04,260 - trainer - INFO -     accuracy       : 0.737577
2024-04-06 10:46:04,260 - trainer - INFO -     macro_f        : 0.727586
2024-04-06 10:46:04,260 - trainer - INFO -     precision      : 0.76102
2024-04-06 10:46:04,260 - trainer - INFO -     recall         : 0.737577
2024-04-06 10:46:04,260 - trainer - INFO -     doc_entropy    : 2.16499
2024-04-06 10:46:04,260 - trainer - INFO -     val_loss       : 1.095823
2024-04-06 10:46:04,260 - trainer - INFO -     val_accuracy   : 0.687743
2024-04-06 10:46:04,260 - trainer - INFO -     val_macro_f    : 0.680405
2024-04-06 10:46:04,260 - trainer - INFO -     val_precision  : 0.721114
2024-04-06 10:46:04,260 - trainer - INFO -     val_recall     : 0.687743
2024-04-06 10:46:04,260 - trainer - INFO -     val_doc_entropy: 2.141547
2024-04-06 10:46:04,260 - trainer - INFO -     test_loss      : 1.09647
2024-04-06 10:46:04,260 - trainer - INFO -     test_accuracy  : 0.68381
2024-04-06 10:46:04,260 - trainer - INFO -     test_macro_f   : 0.67653
2024-04-06 10:46:04,260 - trainer - INFO -     test_precision : 0.71624
2024-04-06 10:46:04,260 - trainer - INFO -     test_recall    : 0.68381
2024-04-06 10:46:04,260 - trainer - INFO -     test_doc_entropy: 2.146445
2024-04-06 10:49:05,396 - trainer - INFO -     epoch          : 3
2024-04-06 10:49:05,396 - trainer - INFO -     loss           : 0.676849
2024-04-06 10:49:05,396 - trainer - INFO -     accuracy       : 0.799068
2024-04-06 10:49:05,396 - trainer - INFO -     macro_f        : 0.793114
2024-04-06 10:49:05,396 - trainer - INFO -     precision      : 0.82395
2024-04-06 10:49:05,396 - trainer - INFO -     recall         : 0.799068
2024-04-06 10:49:05,396 - trainer - INFO -     doc_entropy    : 2.008246
2024-04-06 10:49:05,396 - trainer - INFO -     val_loss       : 1.163309
2024-04-06 10:49:05,396 - trainer - INFO -     val_accuracy   : 0.679279
2024-04-06 10:49:05,396 - trainer - INFO -     val_macro_f    : 0.667344
2024-04-06 10:49:05,396 - trainer - INFO -     val_precision  : 0.701377
2024-04-06 10:49:05,396 - trainer - INFO -     val_recall     : 0.679279
2024-04-06 10:49:05,396 - trainer - INFO -     val_doc_entropy: 2.103833
2024-04-06 10:49:05,396 - trainer - INFO -     test_loss      : 1.162443
2024-04-06 10:49:05,396 - trainer - INFO -     test_accuracy  : 0.678483
2024-04-06 10:49:05,396 - trainer - INFO -     test_macro_f   : 0.666766
2024-04-06 10:49:05,396 - trainer - INFO -     test_precision : 0.70257
2024-04-06 10:49:05,396 - trainer - INFO -     test_recall    : 0.678483
2024-04-06 10:49:05,396 - trainer - INFO -     test_doc_entropy: 2.108864
2024-04-06 12:37:26,142 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,279,907
Freeze params: 0
2024-04-06 12:48:22,479 - trainer - INFO -     epoch          : 1
2024-04-06 12:48:22,479 - trainer - INFO -     loss           : 1.354896
2024-04-06 12:48:22,479 - trainer - INFO -     accuracy       : 0.624231
2024-04-06 12:48:22,479 - trainer - INFO -     macro_f        : 0.603163
2024-04-06 12:48:22,479 - trainer - INFO -     precision      : 0.637218
2024-04-06 12:48:22,479 - trainer - INFO -     recall         : 0.624231
2024-04-06 12:48:22,479 - trainer - INFO -     doc_entropy    : 2.953926
2024-04-06 12:48:22,479 - trainer - INFO -     val_loss       : 1.21203
2024-04-06 12:48:22,479 - trainer - INFO -     val_accuracy   : 0.662601
2024-04-06 12:48:22,479 - trainer - INFO -     val_macro_f    : 0.648785
2024-04-06 12:48:22,479 - trainer - INFO -     val_precision  : 0.687427
2024-04-06 12:48:22,479 - trainer - INFO -     val_recall     : 0.662601
2024-04-06 12:48:22,479 - trainer - INFO -     val_doc_entropy: 3.087522
2024-04-06 12:48:22,479 - trainer - INFO -     test_loss      : 1.219972
2024-04-06 12:48:22,479 - trainer - INFO -     test_accuracy  : 0.653739
2024-04-06 12:48:22,479 - trainer - INFO -     test_macro_f   : 0.638753
2024-04-06 12:48:22,479 - trainer - INFO -     test_precision : 0.676121
2024-04-06 12:48:22,479 - trainer - INFO -     test_recall    : 0.653739
2024-04-06 12:48:22,479 - trainer - INFO -     test_doc_entropy: 3.092045
2024-04-06 12:59:31,055 - trainer - INFO -     epoch          : 2
2024-04-06 12:59:31,055 - trainer - INFO -     loss           : 1.033241
2024-04-06 12:59:31,055 - trainer - INFO -     accuracy       : 0.701418
2024-04-06 12:59:31,055 - trainer - INFO -     macro_f        : 0.688326
2024-04-06 12:59:31,055 - trainer - INFO -     precision      : 0.72354
2024-04-06 12:59:31,055 - trainer - INFO -     recall         : 0.701418
2024-04-06 12:59:31,055 - trainer - INFO -     doc_entropy    : 3.160735
2024-04-06 12:59:31,055 - trainer - INFO -     val_loss       : 1.18861
2024-04-06 12:59:31,055 - trainer - INFO -     val_accuracy   : 0.670367
2024-04-06 12:59:31,055 - trainer - INFO -     val_macro_f    : 0.648376
2024-04-06 12:59:31,055 - trainer - INFO -     val_precision  : 0.677197
2024-04-06 12:59:31,055 - trainer - INFO -     val_recall     : 0.670367
2024-04-06 12:59:31,055 - trainer - INFO -     val_doc_entropy: 3.305429
2024-04-06 12:59:31,055 - trainer - INFO -     test_loss      : 1.181672
2024-04-06 12:59:31,055 - trainer - INFO -     test_accuracy  : 0.669471
2024-04-06 12:59:31,055 - trainer - INFO -     test_macro_f   : 0.648281
2024-04-06 12:59:31,055 - trainer - INFO -     test_precision : 0.677183
2024-04-06 12:59:31,055 - trainer - INFO -     test_recall    : 0.669471
2024-04-06 12:59:31,055 - trainer - INFO -     test_doc_entropy: 3.309101
2024-04-06 13:10:39,245 - trainer - INFO -     epoch          : 3
2024-04-06 13:10:39,245 - trainer - INFO -     loss           : 0.896509
2024-04-06 13:10:39,245 - trainer - INFO -     accuracy       : 0.732841
2024-04-06 13:10:39,245 - trainer - INFO -     macro_f        : 0.722471
2024-04-06 13:10:39,245 - trainer - INFO -     precision      : 0.757136
2024-04-06 13:10:39,245 - trainer - INFO -     recall         : 0.732841
2024-04-06 13:10:39,245 - trainer - INFO -     doc_entropy    : 3.368652
2024-04-06 13:10:39,245 - trainer - INFO -     val_loss       : 1.161496
2024-04-06 13:10:39,245 - trainer - INFO -     val_accuracy   : 0.671413
2024-04-06 13:10:39,245 - trainer - INFO -     val_macro_f    : 0.651334
2024-04-06 13:10:39,245 - trainer - INFO -     val_precision  : 0.683436
2024-04-06 13:10:39,245 - trainer - INFO -     val_recall     : 0.671413
2024-04-06 13:10:39,245 - trainer - INFO -     val_doc_entropy: 3.437256
2024-04-06 13:10:39,245 - trainer - INFO -     test_loss      : 1.163121
2024-04-06 13:10:39,245 - trainer - INFO -     test_accuracy  : 0.669023
2024-04-06 13:10:39,245 - trainer - INFO -     test_macro_f   : 0.650167
2024-04-06 13:10:39,245 - trainer - INFO -     test_precision : 0.682771
2024-04-06 13:10:39,245 - trainer - INFO -     test_recall    : 0.669023
2024-04-06 13:10:39,245 - trainer - INFO -     test_doc_entropy: 3.441601
2024-04-06 13:11:56,671 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,279,907
Freeze params: 0
2024-04-06 13:23:08,108 - trainer - INFO -     epoch          : 1
2024-04-06 13:23:08,108 - trainer - INFO -     loss           : 1.371347
2024-04-06 13:23:08,108 - trainer - INFO -     accuracy       : 0.620715
2024-04-06 13:23:08,108 - trainer - INFO -     macro_f        : 0.599656
2024-04-06 13:23:08,108 - trainer - INFO -     precision      : 0.63299
2024-04-06 13:23:08,108 - trainer - INFO -     recall         : 0.620715
2024-04-06 13:23:08,108 - trainer - INFO -     doc_entropy    : 2.698619
2024-04-06 13:23:08,108 - trainer - INFO -     val_loss       : 1.23119
2024-04-06 13:23:08,108 - trainer - INFO -     val_accuracy   : 0.657722
2024-04-06 13:23:08,108 - trainer - INFO -     val_macro_f    : 0.649839
2024-04-06 13:23:08,108 - trainer - INFO -     val_precision  : 0.692413
2024-04-06 13:23:08,108 - trainer - INFO -     val_recall     : 0.657722
2024-04-06 13:23:08,108 - trainer - INFO -     val_doc_entropy: 2.882026
2024-04-06 13:23:08,108 - trainer - INFO -     test_loss      : 1.240759
2024-04-06 13:23:08,108 - trainer - INFO -     test_accuracy  : 0.65344
2024-04-06 13:23:08,108 - trainer - INFO -     test_macro_f   : 0.64754
2024-04-06 13:23:08,108 - trainer - INFO -     test_precision : 0.69553
2024-04-06 13:23:08,108 - trainer - INFO -     test_recall    : 0.65344
2024-04-06 13:23:08,108 - trainer - INFO -     test_doc_entropy: 2.884677
2024-04-06 13:34:17,137 - trainer - INFO -     epoch          : 2
2024-04-06 13:34:17,137 - trainer - INFO -     loss           : 1.053738
2024-04-06 13:34:17,137 - trainer - INFO -     accuracy       : 0.694572
2024-04-06 13:34:17,137 - trainer - INFO -     macro_f        : 0.680987
2024-04-06 13:34:17,152 - trainer - INFO -     precision      : 0.716839
2024-04-06 13:34:17,152 - trainer - INFO -     recall         : 0.694572
2024-04-06 13:34:17,152 - trainer - INFO -     doc_entropy    : 2.911601
2024-04-06 13:34:17,152 - trainer - INFO -     val_loss       : 1.1663
2024-04-06 13:34:17,152 - trainer - INFO -     val_accuracy   : 0.672907
2024-04-06 13:34:17,152 - trainer - INFO -     val_macro_f    : 0.659534
2024-04-06 13:34:17,152 - trainer - INFO -     val_precision  : 0.695033
2024-04-06 13:34:17,152 - trainer - INFO -     val_recall     : 0.672907
2024-04-06 13:34:17,152 - trainer - INFO -     val_doc_entropy: 3.213271
2024-04-06 13:34:17,152 - trainer - INFO -     test_loss      : 1.165905
2024-04-06 13:34:17,152 - trainer - INFO -     test_accuracy  : 0.66738
2024-04-06 13:34:17,152 - trainer - INFO -     test_macro_f   : 0.653654
2024-04-06 13:34:17,152 - trainer - INFO -     test_precision : 0.68955
2024-04-06 13:34:17,152 - trainer - INFO -     test_recall    : 0.66738
2024-04-06 13:34:17,152 - trainer - INFO -     test_doc_entropy: 3.217074
2024-04-06 13:45:26,471 - trainer - INFO -     epoch          : 3
2024-04-06 13:45:26,471 - trainer - INFO -     loss           : 0.917712
2024-04-06 13:45:26,471 - trainer - INFO -     accuracy       : 0.728964
2024-04-06 13:45:26,471 - trainer - INFO -     macro_f        : 0.718489
2024-04-06 13:45:26,471 - trainer - INFO -     precision      : 0.753257
2024-04-06 13:45:26,471 - trainer - INFO -     recall         : 0.728964
2024-04-06 13:45:26,471 - trainer - INFO -     doc_entropy    : 3.157581
2024-04-06 13:45:26,471 - trainer - INFO -     val_loss       : 1.154748
2024-04-06 13:45:26,471 - trainer - INFO -     val_accuracy   : 0.6745
2024-04-06 13:45:26,471 - trainer - INFO -     val_macro_f    : 0.659308
2024-04-06 13:45:26,471 - trainer - INFO -     val_precision  : 0.692939
2024-04-06 13:45:26,471 - trainer - INFO -     val_recall     : 0.6745
2024-04-06 13:45:26,471 - trainer - INFO -     val_doc_entropy: 3.360728
2024-04-06 13:45:26,471 - trainer - INFO -     test_loss      : 1.156732
2024-04-06 13:45:26,471 - trainer - INFO -     test_accuracy  : 0.669571
2024-04-06 13:45:26,471 - trainer - INFO -     test_macro_f   : 0.654366
2024-04-06 13:45:26,471 - trainer - INFO -     test_precision : 0.689166
2024-04-06 13:45:26,471 - trainer - INFO -     test_recall    : 0.669571
2024-04-06 13:45:26,471 - trainer - INFO -     test_doc_entropy: 3.364848
2024-04-06 13:46:44,637 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,279,907
Freeze params: 0
2024-04-06 13:57:56,486 - trainer - INFO -     epoch          : 1
2024-04-06 13:57:56,486 - trainer - INFO -     loss           : 1.360931
2024-04-06 13:57:56,486 - trainer - INFO -     accuracy       : 0.623099
2024-04-06 13:57:56,486 - trainer - INFO -     macro_f        : 0.602499
2024-04-06 13:57:56,486 - trainer - INFO -     precision      : 0.63675
2024-04-06 13:57:56,486 - trainer - INFO -     recall         : 0.623099
2024-04-06 13:57:56,486 - trainer - INFO -     doc_entropy    : 2.860186
2024-04-06 13:57:56,486 - trainer - INFO -     val_loss       : 1.198685
2024-04-06 13:57:56,486 - trainer - INFO -     val_accuracy   : 0.663646
2024-04-06 13:57:56,486 - trainer - INFO -     val_macro_f    : 0.643212
2024-04-06 13:57:56,486 - trainer - INFO -     val_precision  : 0.672295
2024-04-06 13:57:56,486 - trainer - INFO -     val_recall     : 0.663646
2024-04-06 13:57:56,502 - trainer - INFO -     val_doc_entropy: 2.90495
2024-04-06 13:57:56,502 - trainer - INFO -     test_loss      : 1.207999
2024-04-06 13:57:56,502 - trainer - INFO -     test_accuracy  : 0.660858
2024-04-06 13:57:56,502 - trainer - INFO -     test_macro_f   : 0.640192
2024-04-06 13:57:56,502 - trainer - INFO -     test_precision : 0.670469
2024-04-06 13:57:56,502 - trainer - INFO -     test_recall    : 0.660858
2024-04-06 13:57:56,502 - trainer - INFO -     test_doc_entropy: 2.909488
2024-04-06 14:09:06,391 - trainer - INFO -     epoch          : 2
2024-04-06 14:09:06,391 - trainer - INFO -     loss           : 1.045718
2024-04-06 14:09:06,391 - trainer - INFO -     accuracy       : 0.697279
2024-04-06 14:09:06,391 - trainer - INFO -     macro_f        : 0.683853
2024-04-06 14:09:06,391 - trainer - INFO -     precision      : 0.718694
2024-04-06 14:09:06,391 - trainer - INFO -     recall         : 0.697279
2024-04-06 14:09:06,391 - trainer - INFO -     doc_entropy    : 3.056717
2024-04-06 14:09:06,391 - trainer - INFO -     val_loss       : 1.166932
2024-04-06 14:09:06,391 - trainer - INFO -     val_accuracy   : 0.670965
2024-04-06 14:09:06,391 - trainer - INFO -     val_macro_f    : 0.656008
2024-04-06 14:09:06,391 - trainer - INFO -     val_precision  : 0.69263
2024-04-06 14:09:06,391 - trainer - INFO -     val_recall     : 0.670965
2024-04-06 14:09:06,391 - trainer - INFO -     val_doc_entropy: 3.207049
2024-04-06 14:09:06,391 - trainer - INFO -     test_loss      : 1.172344
2024-04-06 14:09:06,391 - trainer - INFO -     test_accuracy  : 0.666335
2024-04-06 14:09:06,391 - trainer - INFO -     test_macro_f   : 0.653339
2024-04-06 14:09:06,391 - trainer - INFO -     test_precision : 0.692061
2024-04-06 14:09:06,391 - trainer - INFO -     test_recall    : 0.666335
2024-04-06 14:09:06,391 - trainer - INFO -     test_doc_entropy: 3.211615
2024-04-06 14:20:14,648 - trainer - INFO -     epoch          : 3
2024-04-06 14:20:14,648 - trainer - INFO -     loss           : 0.911131
2024-04-06 14:20:14,648 - trainer - INFO -     accuracy       : 0.730377
2024-04-06 14:20:14,648 - trainer - INFO -     macro_f        : 0.719809
2024-04-06 14:20:14,648 - trainer - INFO -     precision      : 0.754959
2024-04-06 14:20:14,664 - trainer - INFO -     recall         : 0.730377
2024-04-06 14:20:14,664 - trainer - INFO -     doc_entropy    : 3.269153
2024-04-06 14:20:14,664 - trainer - INFO -     val_loss       : 1.183134
2024-04-06 14:20:14,664 - trainer - INFO -     val_accuracy   : 0.669521
2024-04-06 14:20:14,664 - trainer - INFO -     val_macro_f    : 0.653904
2024-04-06 14:20:14,664 - trainer - INFO -     val_precision  : 0.688334
2024-04-06 14:20:14,664 - trainer - INFO -     val_recall     : 0.669521
2024-04-06 14:20:14,664 - trainer - INFO -     val_doc_entropy: 3.374905
2024-04-06 14:20:14,664 - trainer - INFO -     test_loss      : 1.172385
2024-04-06 14:20:14,664 - trainer - INFO -     test_accuracy  : 0.672508
2024-04-06 14:20:14,664 - trainer - INFO -     test_macro_f   : 0.657081
2024-04-06 14:20:14,664 - trainer - INFO -     test_precision : 0.690786
2024-04-06 14:20:14,664 - trainer - INFO -     test_recall    : 0.672508
2024-04-06 14:20:14,664 - trainer - INFO -     test_doc_entropy: 3.380191
2024-04-06 14:21:32,576 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,279,907
Freeze params: 0
2024-04-06 14:32:43,104 - trainer - INFO -     epoch          : 1
2024-04-06 14:32:43,104 - trainer - INFO -     loss           : 1.351921
2024-04-06 14:32:43,104 - trainer - INFO -     accuracy       : 0.623951
2024-04-06 14:32:43,104 - trainer - INFO -     macro_f        : 0.603476
2024-04-06 14:32:43,104 - trainer - INFO -     precision      : 0.637409
2024-04-06 14:32:43,104 - trainer - INFO -     recall         : 0.623951
2024-04-06 14:32:43,104 - trainer - INFO -     doc_entropy    : 2.839467
2024-04-06 14:32:43,104 - trainer - INFO -     val_loss       : 1.174143
2024-04-06 14:32:43,104 - trainer - INFO -     val_accuracy   : 0.671662
2024-04-06 14:32:43,104 - trainer - INFO -     val_macro_f    : 0.6609
2024-04-06 14:32:43,104 - trainer - INFO -     val_precision  : 0.696267
2024-04-06 14:32:43,104 - trainer - INFO -     val_recall     : 0.671662
2024-04-06 14:32:43,104 - trainer - INFO -     val_doc_entropy: 2.911323
2024-04-06 14:32:43,104 - trainer - INFO -     test_loss      : 1.162656
2024-04-06 14:32:43,104 - trainer - INFO -     test_accuracy  : 0.666882
2024-04-06 14:32:43,104 - trainer - INFO -     test_macro_f   : 0.657194
2024-04-06 14:32:43,104 - trainer - INFO -     test_precision : 0.697612
2024-04-06 14:32:43,104 - trainer - INFO -     test_recall    : 0.666882
2024-04-06 14:32:43,104 - trainer - INFO -     test_doc_entropy: 2.915744
2024-04-06 14:43:52,658 - trainer - INFO -     epoch          : 2
2024-04-06 14:43:52,658 - trainer - INFO -     loss           : 1.03169
2024-04-06 14:43:52,658 - trainer - INFO -     accuracy       : 0.70115
2024-04-06 14:43:52,658 - trainer - INFO -     macro_f        : 0.688417
2024-04-06 14:43:52,658 - trainer - INFO -     precision      : 0.72385
2024-04-06 14:43:52,658 - trainer - INFO -     recall         : 0.70115
2024-04-06 14:43:52,658 - trainer - INFO -     doc_entropy    : 3.039731
2024-04-06 14:43:52,658 - trainer - INFO -     val_loss       : 1.152409
2024-04-06 14:43:52,658 - trainer - INFO -     val_accuracy   : 0.672757
2024-04-06 14:43:52,658 - trainer - INFO -     val_macro_f    : 0.657247
2024-04-06 14:43:52,658 - trainer - INFO -     val_precision  : 0.689733
2024-04-06 14:43:52,658 - trainer - INFO -     val_recall     : 0.672757
2024-04-06 14:43:52,658 - trainer - INFO -     val_doc_entropy: 3.282045
2024-04-06 14:43:52,658 - trainer - INFO -     test_loss      : 1.166436
2024-04-06 14:43:52,658 - trainer - INFO -     test_accuracy  : 0.665588
2024-04-06 14:43:52,658 - trainer - INFO -     test_macro_f   : 0.650349
2024-04-06 14:43:52,658 - trainer - INFO -     test_precision : 0.683314
2024-04-06 14:43:52,658 - trainer - INFO -     test_recall    : 0.665588
2024-04-06 14:43:52,658 - trainer - INFO -     test_doc_entropy: 3.28684
2024-04-06 14:55:03,512 - trainer - INFO -     epoch          : 3
2024-04-06 14:55:03,512 - trainer - INFO -     loss           : 0.900056
2024-04-06 14:55:03,512 - trainer - INFO -     accuracy       : 0.732107
2024-04-06 14:55:03,512 - trainer - INFO -     macro_f        : 0.721726
2024-04-06 14:55:03,512 - trainer - INFO -     precision      : 0.75681
2024-04-06 14:55:03,512 - trainer - INFO -     recall         : 0.732107
2024-04-06 14:55:03,512 - trainer - INFO -     doc_entropy    : 3.23649
2024-04-06 14:55:03,512 - trainer - INFO -     val_loss       : 1.153841
2024-04-06 14:55:03,512 - trainer - INFO -     val_accuracy   : 0.675446
2024-04-06 14:55:03,512 - trainer - INFO -     val_macro_f    : 0.662321
2024-04-06 14:55:03,512 - trainer - INFO -     val_precision  : 0.69914
2024-04-06 14:55:03,512 - trainer - INFO -     val_recall     : 0.675446
2024-04-06 14:55:03,512 - trainer - INFO -     val_doc_entropy: 3.396211
2024-04-06 14:55:03,512 - trainer - INFO -     test_loss      : 1.146047
2024-04-06 14:55:03,512 - trainer - INFO -     test_accuracy  : 0.677835
2024-04-06 14:55:03,512 - trainer - INFO -     test_macro_f   : 0.666599
2024-04-06 14:55:03,512 - trainer - INFO -     test_precision : 0.704903
2024-04-06 14:55:03,512 - trainer - INFO -     test_recall    : 0.677835
2024-04-06 14:55:03,512 - trainer - INFO -     test_doc_entropy: 3.400563
2024-04-06 14:56:20,319 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,279,907
Freeze params: 0
2024-04-06 15:07:36,954 - trainer - INFO -     epoch          : 1
2024-04-06 15:07:36,960 - trainer - INFO -     loss           : 1.35682
2024-04-06 15:07:36,960 - trainer - INFO -     accuracy       : 0.622526
2024-04-06 15:07:36,960 - trainer - INFO -     macro_f        : 0.60108
2024-04-06 15:07:36,960 - trainer - INFO -     precision      : 0.635118
2024-04-06 15:07:36,960 - trainer - INFO -     recall         : 0.622526
2024-04-06 15:07:36,960 - trainer - INFO -     doc_entropy    : 2.793542
2024-04-06 15:07:36,960 - trainer - INFO -     val_loss       : 1.2282
2024-04-06 15:07:36,960 - trainer - INFO -     val_accuracy   : 0.653341
2024-04-06 15:07:36,960 - trainer - INFO -     val_macro_f    : 0.631731
2024-04-06 15:07:36,960 - trainer - INFO -     val_precision  : 0.664782
2024-04-06 15:07:36,960 - trainer - INFO -     val_recall     : 0.653341
2024-04-06 15:07:36,960 - trainer - INFO -     val_doc_entropy: 2.94712
2024-04-06 15:07:36,960 - trainer - INFO -     test_loss      : 1.21695
2024-04-06 15:07:36,960 - trainer - INFO -     test_accuracy  : 0.653141
2024-04-06 15:07:36,960 - trainer - INFO -     test_macro_f   : 0.632402
2024-04-06 15:07:36,960 - trainer - INFO -     test_precision : 0.666267
2024-04-06 15:07:36,960 - trainer - INFO -     test_recall    : 0.653141
2024-04-06 15:07:36,960 - trainer - INFO -     test_doc_entropy: 2.949205
2024-04-06 15:18:56,762 - trainer - INFO -     epoch          : 2
2024-04-06 15:18:56,762 - trainer - INFO -     loss           : 1.041411
2024-04-06 15:18:56,762 - trainer - INFO -     accuracy       : 0.697852
2024-04-06 15:18:56,762 - trainer - INFO -     macro_f        : 0.684605
2024-04-06 15:18:56,762 - trainer - INFO -     precision      : 0.720615
2024-04-06 15:18:56,762 - trainer - INFO -     recall         : 0.697852
2024-04-06 15:18:56,762 - trainer - INFO -     doc_entropy    : 3.013674
2024-04-06 15:18:56,762 - trainer - INFO -     val_loss       : 1.163555
2024-04-06 15:18:56,762 - trainer - INFO -     val_accuracy   : 0.669372
2024-04-06 15:18:56,762 - trainer - INFO -     val_macro_f    : 0.662339
2024-04-06 15:18:56,777 - trainer - INFO -     val_precision  : 0.706578
2024-04-06 15:18:56,777 - trainer - INFO -     val_recall     : 0.669372
2024-04-06 15:18:56,777 - trainer - INFO -     val_doc_entropy: 3.158892
2024-04-06 15:18:56,777 - trainer - INFO -     test_loss      : 1.161675
2024-04-06 15:18:56,777 - trainer - INFO -     test_accuracy  : 0.668973
2024-04-06 15:18:56,777 - trainer - INFO -     test_macro_f   : 0.66444
2024-04-06 15:18:56,777 - trainer - INFO -     test_precision : 0.70997
2024-04-06 15:18:56,777 - trainer - INFO -     test_recall    : 0.668973
2024-04-06 15:18:56,777 - trainer - INFO -     test_doc_entropy: 3.163668
2024-04-06 15:30:13,907 - trainer - INFO -     epoch          : 3
2024-04-06 15:30:13,907 - trainer - INFO -     loss           : 0.901603
2024-04-06 15:30:13,907 - trainer - INFO -     accuracy       : 0.732468
2024-04-06 15:30:13,907 - trainer - INFO -     macro_f        : 0.721567
2024-04-06 15:30:13,907 - trainer - INFO -     precision      : 0.756135
2024-04-06 15:30:13,907 - trainer - INFO -     recall         : 0.732468
2024-04-06 15:30:13,907 - trainer - INFO -     doc_entropy    : 3.222849
2024-04-06 15:30:13,907 - trainer - INFO -     val_loss       : 1.179009
2024-04-06 15:30:13,907 - trainer - INFO -     val_accuracy   : 0.66748
2024-04-06 15:30:13,907 - trainer - INFO -     val_macro_f    : 0.659503
2024-04-06 15:30:13,907 - trainer - INFO -     val_precision  : 0.701897
2024-04-06 15:30:13,907 - trainer - INFO -     val_recall     : 0.66748
2024-04-06 15:30:13,907 - trainer - INFO -     val_doc_entropy: 3.347811
2024-04-06 15:30:13,907 - trainer - INFO -     test_loss      : 1.172726
2024-04-06 15:30:13,907 - trainer - INFO -     test_accuracy  : 0.668675
2024-04-06 15:30:13,907 - trainer - INFO -     test_macro_f   : 0.659612
2024-04-06 15:30:13,907 - trainer - INFO -     test_precision : 0.701336
2024-04-06 15:30:13,907 - trainer - INFO -     test_recall    : 0.668675
2024-04-06 15:30:13,907 - trainer - INFO -     test_doc_entropy: 3.352638
2024-04-06 16:06:12,373 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,279,907
Freeze params: 0
2024-04-06 16:17:28,921 - trainer - INFO -     epoch          : 1
2024-04-06 16:17:28,921 - trainer - INFO -     loss           : 1.140085
2024-04-06 16:17:28,921 - trainer - INFO -     accuracy       : 0.672253
2024-04-06 16:17:28,921 - trainer - INFO -     macro_f        : 0.654119
2024-04-06 16:17:28,921 - trainer - INFO -     precision      : 0.684931
2024-04-06 16:17:28,921 - trainer - INFO -     recall         : 0.672253
2024-04-06 16:17:28,921 - trainer - INFO -     doc_entropy    : 2.297729
2024-04-06 16:17:28,921 - trainer - INFO -     val_loss       : 0.953078
2024-04-06 16:17:28,921 - trainer - INFO -     val_accuracy   : 0.72095
2024-04-06 16:17:28,921 - trainer - INFO -     val_macro_f    : 0.710242
2024-04-06 16:17:28,921 - trainer - INFO -     val_precision  : 0.744293
2024-04-06 16:17:28,921 - trainer - INFO -     val_recall     : 0.72095
2024-04-06 16:17:28,921 - trainer - INFO -     val_doc_entropy: 2.393861
2024-04-06 16:17:28,921 - trainer - INFO -     test_loss      : 0.944548
2024-04-06 16:17:28,921 - trainer - INFO -     test_accuracy  : 0.716967
2024-04-06 16:17:28,921 - trainer - INFO -     test_macro_f   : 0.705519
2024-04-06 16:17:28,921 - trainer - INFO -     test_precision : 0.738426
2024-04-06 16:17:28,921 - trainer - INFO -     test_recall    : 0.716967
2024-04-06 16:17:28,921 - trainer - INFO -     test_doc_entropy: 2.389267
2024-04-06 16:28:49,212 - trainer - INFO -     epoch          : 2
2024-04-06 16:28:49,212 - trainer - INFO -     loss           : 0.769263
2024-04-06 16:28:49,212 - trainer - INFO -     accuracy       : 0.768273
2024-04-06 16:28:49,212 - trainer - INFO -     macro_f        : 0.76056
2024-04-06 16:28:49,212 - trainer - INFO -     precision      : 0.792956
2024-04-06 16:28:49,212 - trainer - INFO -     recall         : 0.768273
2024-04-06 16:28:49,212 - trainer - INFO -     doc_entropy    : 2.251162
2024-04-06 16:28:49,212 - trainer - INFO -     val_loss       : 0.941949
2024-04-06 16:28:49,212 - trainer - INFO -     val_accuracy   : 0.727372
2024-04-06 16:28:49,212 - trainer - INFO -     val_macro_f    : 0.719023
2024-04-06 16:28:49,212 - trainer - INFO -     val_precision  : 0.754522
2024-04-06 16:28:49,212 - trainer - INFO -     val_recall     : 0.727372
2024-04-06 16:28:49,212 - trainer - INFO -     val_doc_entropy: 2.251475
2024-04-06 16:28:49,212 - trainer - INFO -     test_loss      : 0.931821
2024-04-06 16:28:49,212 - trainer - INFO -     test_accuracy  : 0.727472
2024-04-06 16:28:49,212 - trainer - INFO -     test_macro_f   : 0.721206
2024-04-06 16:28:49,212 - trainer - INFO -     test_precision : 0.75924
2024-04-06 16:28:49,212 - trainer - INFO -     test_recall    : 0.727472
2024-04-06 16:28:49,212 - trainer - INFO -     test_doc_entropy: 2.249489
2024-04-06 16:40:08,242 - trainer - INFO -     epoch          : 3
2024-04-06 16:40:08,242 - trainer - INFO -     loss           : 0.534689
2024-04-06 16:40:08,242 - trainer - INFO -     accuracy       : 0.835881
2024-04-06 16:40:08,242 - trainer - INFO -     macro_f        : 0.830763
2024-04-06 16:40:08,242 - trainer - INFO -     precision      : 0.856465
2024-04-06 16:40:08,242 - trainer - INFO -     recall         : 0.835881
2024-04-06 16:40:08,242 - trainer - INFO -     doc_entropy    : 2.003836
2024-04-06 16:40:08,242 - trainer - INFO -     val_loss       : 1.028503
2024-04-06 16:40:08,242 - trainer - INFO -     val_accuracy   : 0.717714
2024-04-06 16:40:08,242 - trainer - INFO -     val_macro_f    : 0.70885
2024-04-06 16:40:08,242 - trainer - INFO -     val_precision  : 0.744247
2024-04-06 16:40:08,242 - trainer - INFO -     val_recall     : 0.717714
2024-04-06 16:40:08,242 - trainer - INFO -     val_doc_entropy: 2.11144
2024-04-06 16:40:08,242 - trainer - INFO -     test_loss      : 1.017472
2024-04-06 16:40:08,242 - trainer - INFO -     test_accuracy  : 0.717913
2024-04-06 16:40:08,242 - trainer - INFO -     test_macro_f   : 0.70998
2024-04-06 16:40:08,242 - trainer - INFO -     test_precision : 0.746882
2024-04-06 16:40:08,242 - trainer - INFO -     test_recall    : 0.717913
2024-04-06 16:40:08,242 - trainer - INFO -     test_doc_entropy: 2.109957
2024-04-06 16:41:28,735 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,279,907
Freeze params: 0
2024-04-06 16:52:49,069 - trainer - INFO -     epoch          : 1
2024-04-06 16:52:49,069 - trainer - INFO -     loss           : 1.138517
2024-04-06 16:52:49,069 - trainer - INFO -     accuracy       : 0.671227
2024-04-06 16:52:49,069 - trainer - INFO -     macro_f        : 0.653428
2024-04-06 16:52:49,069 - trainer - INFO -     precision      : 0.684789
2024-04-06 16:52:49,069 - trainer - INFO -     recall         : 0.671227
2024-04-06 16:52:49,069 - trainer - INFO -     doc_entropy    : 2.300005
2024-04-06 16:52:49,069 - trainer - INFO -     val_loss       : 0.962992
2024-04-06 16:52:49,069 - trainer - INFO -     val_accuracy   : 0.718162
2024-04-06 16:52:49,069 - trainer - INFO -     val_macro_f    : 0.712202
2024-04-06 16:52:49,069 - trainer - INFO -     val_precision  : 0.74954
2024-04-06 16:52:49,069 - trainer - INFO -     val_recall     : 0.718162
2024-04-06 16:52:49,069 - trainer - INFO -     val_doc_entropy: 2.232688
2024-04-06 16:52:49,069 - trainer - INFO -     test_loss      : 0.956017
2024-04-06 16:52:49,069 - trainer - INFO -     test_accuracy  : 0.716668
2024-04-06 16:52:49,069 - trainer - INFO -     test_macro_f   : 0.710301
2024-04-06 16:52:49,069 - trainer - INFO -     test_precision : 0.75
2024-04-06 16:52:49,069 - trainer - INFO -     test_recall    : 0.716668
2024-04-06 16:52:49,069 - trainer - INFO -     test_doc_entropy: 2.230654
2024-04-06 17:04:03,685 - trainer - INFO -     epoch          : 2
2024-04-06 17:04:03,685 - trainer - INFO -     loss           : 0.768556
2024-04-06 17:04:03,685 - trainer - INFO -     accuracy       : 0.769038
2024-04-06 17:04:03,685 - trainer - INFO -     macro_f        : 0.760853
2024-04-06 17:04:03,685 - trainer - INFO -     precision      : 0.792869
2024-04-06 17:04:03,685 - trainer - INFO -     recall         : 0.769038
2024-04-06 17:04:03,685 - trainer - INFO -     doc_entropy    : 2.250386
2024-04-06 17:04:03,685 - trainer - INFO -     val_loss       : 0.949863
2024-04-06 17:04:03,685 - trainer - INFO -     val_accuracy   : 0.719506
2024-04-06 17:04:03,685 - trainer - INFO -     val_macro_f    : 0.710873
2024-04-06 17:04:03,685 - trainer - INFO -     val_precision  : 0.746149
2024-04-06 17:04:03,685 - trainer - INFO -     val_recall     : 0.719506
2024-04-06 17:04:03,685 - trainer - INFO -     val_doc_entropy: 2.240663
2024-04-06 17:04:03,685 - trainer - INFO -     test_loss      : 0.945222
2024-04-06 17:04:03,685 - trainer - INFO -     test_accuracy  : 0.720751
2024-04-06 17:04:03,685 - trainer - INFO -     test_macro_f   : 0.712184
2024-04-06 17:04:03,685 - trainer - INFO -     test_precision : 0.750655
2024-04-06 17:04:03,685 - trainer - INFO -     test_recall    : 0.720751
2024-04-06 17:04:03,685 - trainer - INFO -     test_doc_entropy: 2.237702
2024-04-06 17:15:17,395 - trainer - INFO -     epoch          : 3
2024-04-06 17:15:17,395 - trainer - INFO -     loss           : 0.534832
2024-04-06 17:15:17,395 - trainer - INFO -     accuracy       : 0.836622
2024-04-06 17:15:17,395 - trainer - INFO -     macro_f        : 0.831287
2024-04-06 17:15:17,395 - trainer - INFO -     precision      : 0.857024
2024-04-06 17:15:17,395 - trainer - INFO -     recall         : 0.836622
2024-04-06 17:15:17,395 - trainer - INFO -     doc_entropy    : 1.996762
2024-04-06 17:15:17,395 - trainer - INFO -     val_loss       : 1.048915
2024-04-06 17:15:17,395 - trainer - INFO -     val_accuracy   : 0.717465
2024-04-06 17:15:17,395 - trainer - INFO -     val_macro_f    : 0.71654
2024-04-06 17:15:17,395 - trainer - INFO -     val_precision  : 0.762165
2024-04-06 17:15:17,395 - trainer - INFO -     val_recall     : 0.717465
2024-04-06 17:15:17,395 - trainer - INFO -     val_doc_entropy: 1.993755
2024-04-06 17:15:17,395 - trainer - INFO -     test_loss      : 1.048184
2024-04-06 17:15:17,395 - trainer - INFO -     test_accuracy  : 0.715573
2024-04-06 17:15:17,395 - trainer - INFO -     test_macro_f   : 0.713521
2024-04-06 17:15:17,395 - trainer - INFO -     test_precision : 0.757596
2024-04-06 17:15:17,395 - trainer - INFO -     test_recall    : 0.715573
2024-04-06 17:15:17,395 - trainer - INFO -     test_doc_entropy: 1.990649
2024-04-06 17:16:35,628 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,279,907
Freeze params: 0
2024-04-06 17:27:49,997 - trainer - INFO -     epoch          : 1
2024-04-06 17:27:49,997 - trainer - INFO -     loss           : 1.13901
2024-04-06 17:27:49,997 - trainer - INFO -     accuracy       : 0.6716
2024-04-06 17:27:49,997 - trainer - INFO -     macro_f        : 0.653296
2024-04-06 17:27:49,997 - trainer - INFO -     precision      : 0.683726
2024-04-06 17:27:49,997 - trainer - INFO -     recall         : 0.6716
2024-04-06 17:27:49,997 - trainer - INFO -     doc_entropy    : 2.279177
2024-04-06 17:27:49,997 - trainer - INFO -     val_loss       : 0.963485
2024-04-06 17:27:49,997 - trainer - INFO -     val_accuracy   : 0.718162
2024-04-06 17:27:49,997 - trainer - INFO -     val_macro_f    : 0.711343
2024-04-06 17:27:49,997 - trainer - INFO -     val_precision  : 0.750107
2024-04-06 17:27:49,997 - trainer - INFO -     val_recall     : 0.718162
2024-04-06 17:27:49,997 - trainer - INFO -     val_doc_entropy: 2.318519
2024-04-06 17:27:49,997 - trainer - INFO -     test_loss      : 0.963684
2024-04-06 17:27:49,997 - trainer - INFO -     test_accuracy  : 0.714776
2024-04-06 17:27:49,997 - trainer - INFO -     test_macro_f   : 0.708139
2024-04-06 17:27:49,997 - trainer - INFO -     test_precision : 0.746156
2024-04-06 17:27:49,997 - trainer - INFO -     test_recall    : 0.714776
2024-04-06 17:27:49,997 - trainer - INFO -     test_doc_entropy: 2.316385
2024-04-06 17:39:02,521 - trainer - INFO -     epoch          : 2
2024-04-06 17:39:02,521 - trainer - INFO -     loss           : 0.766913
2024-04-06 17:39:02,521 - trainer - INFO -     accuracy       : 0.769667
2024-04-06 17:39:02,521 - trainer - INFO -     macro_f        : 0.761421
2024-04-06 17:39:02,521 - trainer - INFO -     precision      : 0.792742
2024-04-06 17:39:02,521 - trainer - INFO -     recall         : 0.769667
2024-04-06 17:39:02,521 - trainer - INFO -     doc_entropy    : 2.216899
2024-04-06 17:39:02,521 - trainer - INFO -     val_loss       : 0.943386
2024-04-06 17:39:02,521 - trainer - INFO -     val_accuracy   : 0.72548
2024-04-06 17:39:02,521 - trainer - INFO -     val_macro_f    : 0.71944
2024-04-06 17:39:02,521 - trainer - INFO -     val_precision  : 0.758025
2024-04-06 17:39:02,521 - trainer - INFO -     val_recall     : 0.72548
2024-04-06 17:39:02,521 - trainer - INFO -     val_doc_entropy: 2.240547
2024-04-06 17:39:02,521 - trainer - INFO -     test_loss      : 0.937688
2024-04-06 17:39:02,521 - trainer - INFO -     test_accuracy  : 0.724535
2024-04-06 17:39:02,521 - trainer - INFO -     test_macro_f   : 0.720972
2024-04-06 17:39:02,521 - trainer - INFO -     test_precision : 0.763304
2024-04-06 17:39:02,521 - trainer - INFO -     test_recall    : 0.724535
2024-04-06 17:39:02,521 - trainer - INFO -     test_doc_entropy: 2.238506
2024-04-06 17:50:14,194 - trainer - INFO -     epoch          : 3
2024-04-06 17:50:14,194 - trainer - INFO -     loss           : 0.534034
2024-04-06 17:50:14,194 - trainer - INFO -     accuracy       : 0.83641
2024-04-06 17:50:14,194 - trainer - INFO -     macro_f        : 0.83162
2024-04-06 17:50:14,194 - trainer - INFO -     precision      : 0.858001
2024-04-06 17:50:14,194 - trainer - INFO -     recall         : 0.83641
2024-04-06 17:50:14,194 - trainer - INFO -     doc_entropy    : 1.985812
2024-04-06 17:50:14,194 - trainer - INFO -     val_loss       : 1.02818
2024-04-06 17:50:14,194 - trainer - INFO -     val_accuracy   : 0.718909
2024-04-06 17:50:14,194 - trainer - INFO -     val_macro_f    : 0.717982
2024-04-06 17:50:14,194 - trainer - INFO -     val_precision  : 0.761932
2024-04-06 17:50:14,194 - trainer - INFO -     val_recall     : 0.718909
2024-04-06 17:50:14,194 - trainer - INFO -     val_doc_entropy: 2.064854
2024-04-06 17:50:14,194 - trainer - INFO -     test_loss      : 1.015035
2024-04-06 17:50:14,194 - trainer - INFO -     test_accuracy  : 0.719805
2024-04-06 17:50:14,194 - trainer - INFO -     test_macro_f   : 0.717346
2024-04-06 17:50:14,194 - trainer - INFO -     test_precision : 0.761076
2024-04-06 17:50:14,194 - trainer - INFO -     test_recall    : 0.719805
2024-04-06 17:50:14,194 - trainer - INFO -     test_doc_entropy: 2.063289
2024-04-06 17:51:32,099 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,279,907
Freeze params: 0
2024-04-06 18:02:42,680 - trainer - INFO -     epoch          : 1
2024-04-06 18:02:42,680 - trainer - INFO -     loss           : 1.14043
2024-04-06 18:02:42,680 - trainer - INFO -     accuracy       : 0.671569
2024-04-06 18:02:42,680 - trainer - INFO -     macro_f        : 0.653191
2024-04-06 18:02:42,680 - trainer - INFO -     precision      : 0.68376
2024-04-06 18:02:42,680 - trainer - INFO -     recall         : 0.671569
2024-04-06 18:02:42,680 - trainer - INFO -     doc_entropy    : 2.31128
2024-04-06 18:02:42,680 - trainer - INFO -     val_loss       : 0.961411
2024-04-06 18:02:42,680 - trainer - INFO -     val_accuracy   : 0.715324
2024-04-06 18:02:42,680 - trainer - INFO -     val_macro_f    : 0.70771
2024-04-06 18:02:42,680 - trainer - INFO -     val_precision  : 0.743602
2024-04-06 18:02:42,680 - trainer - INFO -     val_recall     : 0.715324
2024-04-06 18:02:42,680 - trainer - INFO -     val_doc_entropy: 2.382098
2024-04-06 18:02:42,680 - trainer - INFO -     test_loss      : 0.947679
2024-04-06 18:02:42,680 - trainer - INFO -     test_accuracy  : 0.718013
2024-04-06 18:02:42,680 - trainer - INFO -     test_macro_f   : 0.71013
2024-04-06 18:02:42,680 - trainer - INFO -     test_precision : 0.748291
2024-04-06 18:02:42,680 - trainer - INFO -     test_recall    : 0.718013
2024-04-06 18:02:42,680 - trainer - INFO -     test_doc_entropy: 2.379895
2024-04-06 18:13:56,824 - trainer - INFO -     epoch          : 2
2024-04-06 18:13:56,824 - trainer - INFO -     loss           : 0.765882
2024-04-06 18:13:56,824 - trainer - INFO -     accuracy       : 0.76981
2024-04-06 18:13:56,824 - trainer - INFO -     macro_f        : 0.761687
2024-04-06 18:13:56,824 - trainer - INFO -     precision      : 0.793652
2024-04-06 18:13:56,824 - trainer - INFO -     recall         : 0.76981
2024-04-06 18:13:56,824 - trainer - INFO -     doc_entropy    : 2.255973
2024-04-06 18:13:56,824 - trainer - INFO -     val_loss       : 0.936562
2024-04-06 18:13:56,824 - trainer - INFO -     val_accuracy   : 0.726476
2024-04-06 18:13:56,824 - trainer - INFO -     val_macro_f    : 0.72147
2024-04-06 18:13:56,824 - trainer - INFO -     val_precision  : 0.760493
2024-04-06 18:13:56,824 - trainer - INFO -     val_recall     : 0.726476
2024-04-06 18:13:56,824 - trainer - INFO -     val_doc_entropy: 2.279506
2024-04-06 18:13:56,824 - trainer - INFO -     test_loss      : 0.924755
2024-04-06 18:13:56,824 - trainer - INFO -     test_accuracy  : 0.729762
2024-04-06 18:13:56,824 - trainer - INFO -     test_macro_f   : 0.72396
2024-04-06 18:13:56,824 - trainer - INFO -     test_precision : 0.761475
2024-04-06 18:13:56,824 - trainer - INFO -     test_recall    : 0.729762
2024-04-06 18:13:56,824 - trainer - INFO -     test_doc_entropy: 2.276154
2024-04-06 18:25:09,294 - trainer - INFO -     epoch          : 3
2024-04-06 18:25:09,294 - trainer - INFO -     loss           : 0.536586
2024-04-06 18:25:09,294 - trainer - INFO -     accuracy       : 0.836422
2024-04-06 18:25:09,294 - trainer - INFO -     macro_f        : 0.830856
2024-04-06 18:25:09,294 - trainer - INFO -     precision      : 0.856231
2024-04-06 18:25:09,294 - trainer - INFO -     recall         : 0.836422
2024-04-06 18:25:09,294 - trainer - INFO -     doc_entropy    : 2.02495
2024-04-06 18:25:09,294 - trainer - INFO -     val_loss       : 1.04217
2024-04-06 18:25:09,294 - trainer - INFO -     val_accuracy   : 0.716668
2024-04-06 18:25:09,294 - trainer - INFO -     val_macro_f    : 0.711317
2024-04-06 18:25:09,294 - trainer - INFO -     val_precision  : 0.750476
2024-04-06 18:25:09,294 - trainer - INFO -     val_recall     : 0.716668
2024-04-06 18:25:09,294 - trainer - INFO -     val_doc_entropy: 2.071804
2024-04-06 18:25:09,294 - trainer - INFO -     test_loss      : 1.027429
2024-04-06 18:25:09,294 - trainer - INFO -     test_accuracy  : 0.719805
2024-04-06 18:25:09,294 - trainer - INFO -     test_macro_f   : 0.714116
2024-04-06 18:25:09,294 - trainer - INFO -     test_precision : 0.751883
2024-04-06 18:25:09,294 - trainer - INFO -     test_recall    : 0.719805
2024-04-06 18:25:09,294 - trainer - INFO -     test_doc_entropy: 2.071149
2024-04-06 18:26:25,523 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,279,907
Freeze params: 0
2024-04-06 18:37:36,328 - trainer - INFO -     epoch          : 1
2024-04-06 18:37:36,328 - trainer - INFO -     loss           : 1.141373
2024-04-06 18:37:36,328 - trainer - INFO -     accuracy       : 0.671239
2024-04-06 18:37:36,328 - trainer - INFO -     macro_f        : 0.653324
2024-04-06 18:37:36,328 - trainer - INFO -     precision      : 0.684524
2024-04-06 18:37:36,328 - trainer - INFO -     recall         : 0.671239
2024-04-06 18:37:36,328 - trainer - INFO -     doc_entropy    : 2.298029
2024-04-06 18:37:36,328 - trainer - INFO -     val_loss       : 0.963537
2024-04-06 18:37:36,328 - trainer - INFO -     val_accuracy   : 0.717216
2024-04-06 18:37:36,328 - trainer - INFO -     val_macro_f    : 0.706213
2024-04-06 18:37:36,328 - trainer - INFO -     val_precision  : 0.742336
2024-04-06 18:37:36,328 - trainer - INFO -     val_recall     : 0.717216
2024-04-06 18:37:36,328 - trainer - INFO -     val_doc_entropy: 2.356259
2024-04-06 18:37:36,328 - trainer - INFO -     test_loss      : 0.959866
2024-04-06 18:37:36,328 - trainer - INFO -     test_accuracy  : 0.715125
2024-04-06 18:37:36,328 - trainer - INFO -     test_macro_f   : 0.706948
2024-04-06 18:37:36,328 - trainer - INFO -     test_precision : 0.747779
2024-04-06 18:37:36,328 - trainer - INFO -     test_recall    : 0.715125
2024-04-06 18:37:36,328 - trainer - INFO -     test_doc_entropy: 2.352001
2024-04-06 18:48:52,383 - trainer - INFO -     epoch          : 2
2024-04-06 18:48:52,383 - trainer - INFO -     loss           : 0.767995
2024-04-06 18:48:52,383 - trainer - INFO -     accuracy       : 0.768509
2024-04-06 18:48:52,383 - trainer - INFO -     macro_f        : 0.760261
2024-04-06 18:48:52,383 - trainer - INFO -     precision      : 0.792385
2024-04-06 18:48:52,383 - trainer - INFO -     recall         : 0.768509
2024-04-06 18:48:52,383 - trainer - INFO -     doc_entropy    : 2.237592
2024-04-06 18:48:52,383 - trainer - INFO -     val_loss       : 0.940978
2024-04-06 18:48:52,383 - trainer - INFO -     val_accuracy   : 0.721647
2024-04-06 18:48:52,383 - trainer - INFO -     val_macro_f    : 0.71252
2024-04-06 18:48:52,383 - trainer - INFO -     val_precision  : 0.749454
2024-04-06 18:48:52,383 - trainer - INFO -     val_recall     : 0.721647
2024-04-06 18:48:52,383 - trainer - INFO -     val_doc_entropy: 2.283735
2024-04-06 18:48:52,383 - trainer - INFO -     test_loss      : 0.932425
2024-04-06 18:48:52,383 - trainer - INFO -     test_accuracy  : 0.72334
2024-04-06 18:48:52,383 - trainer - INFO -     test_macro_f   : 0.716077
2024-04-06 18:48:52,383 - trainer - INFO -     test_precision : 0.753848
2024-04-06 18:48:52,383 - trainer - INFO -     test_recall    : 0.72334
2024-04-06 18:48:52,396 - trainer - INFO -     test_doc_entropy: 2.28113
2024-04-06 19:00:07,878 - trainer - INFO -     epoch          : 3
2024-04-06 19:00:07,878 - trainer - INFO -     loss           : 0.535775
2024-04-06 19:00:07,878 - trainer - INFO -     accuracy       : 0.835557
2024-04-06 19:00:07,878 - trainer - INFO -     macro_f        : 0.829943
2024-04-06 19:00:07,878 - trainer - INFO -     precision      : 0.856075
2024-04-06 19:00:07,878 - trainer - INFO -     recall         : 0.835557
2024-04-06 19:00:07,878 - trainer - INFO -     doc_entropy    : 1.991917
2024-04-06 19:00:07,878 - trainer - INFO -     val_loss       : 1.046899
2024-04-06 19:00:07,878 - trainer - INFO -     val_accuracy   : 0.714677
2024-04-06 19:00:07,878 - trainer - INFO -     val_macro_f    : 0.713464
2024-04-06 19:00:07,878 - trainer - INFO -     val_precision  : 0.757848
2024-04-06 19:00:07,878 - trainer - INFO -     val_recall     : 0.714677
2024-04-06 19:00:07,878 - trainer - INFO -     val_doc_entropy: 2.015901
2024-04-06 19:00:07,878 - trainer - INFO -     test_loss      : 1.021068
2024-04-06 19:00:07,878 - trainer - INFO -     test_accuracy  : 0.718958
2024-04-06 19:00:07,878 - trainer - INFO -     test_macro_f   : 0.718501
2024-04-06 19:00:07,878 - trainer - INFO -     test_precision : 0.763729
2024-04-06 19:00:07,878 - trainer - INFO -     test_recall    : 0.718958
2024-04-06 19:00:07,878 - trainer - INFO -     test_doc_entropy: 2.011156
2024-04-06 19:04:02,938 - train - INFO - BiAttentionClassifyModel(
  (embedding): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 48,035,427
Freeze params: 0
2024-04-09 22:05:09,114 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,760,523
Freeze params: 0
2024-04-09 22:08:25,668 - trainer - INFO -     epoch          : 1
2024-04-09 22:08:25,668 - trainer - INFO -     loss           : 1.218709
2024-04-09 22:08:25,668 - trainer - INFO -     accuracy       : 0.653315
2024-04-09 22:08:25,668 - trainer - INFO -     macro_f        : 0.633676
2024-04-09 22:08:25,668 - trainer - INFO -     precision      : 0.66533
2024-04-09 22:08:25,668 - trainer - INFO -     recall         : 0.653315
2024-04-09 22:08:25,668 - trainer - INFO -     doc_entropy    : 2.400217
2024-04-09 22:08:25,668 - trainer - INFO -     val_loss       : 1.080494
2024-04-09 22:08:25,668 - trainer - INFO -     val_accuracy   : 0.687344
2024-04-09 22:08:25,668 - trainer - INFO -     val_macro_f    : 0.668427
2024-04-09 22:08:25,684 - trainer - INFO -     val_precision  : 0.698162
2024-04-09 22:08:25,684 - trainer - INFO -     val_recall     : 0.687344
2024-04-09 22:08:25,684 - trainer - INFO -     val_doc_entropy: 2.283981
2024-04-09 22:08:25,684 - trainer - INFO -     test_loss      : 1.083206
2024-04-09 22:08:25,684 - trainer - INFO -     test_accuracy  : 0.683411
2024-04-09 22:08:25,684 - trainer - INFO -     test_macro_f   : 0.665755
2024-04-09 22:08:25,684 - trainer - INFO -     test_precision : 0.698157
2024-04-09 22:08:25,684 - trainer - INFO -     test_recall    : 0.683411
2024-04-09 22:08:25,684 - trainer - INFO -     test_doc_entropy: 2.288364
2024-04-09 22:11:44,077 - trainer - INFO -     epoch          : 2
2024-04-09 22:11:44,077 - trainer - INFO -     loss           : 0.892002
2024-04-09 22:11:44,077 - trainer - INFO -     accuracy       : 0.737509
2024-04-09 22:11:44,077 - trainer - INFO -     macro_f        : 0.728004
2024-04-09 22:11:44,077 - trainer - INFO -     precision      : 0.762024
2024-04-09 22:11:44,077 - trainer - INFO -     recall         : 0.737509
2024-04-09 22:11:44,077 - trainer - INFO -     doc_entropy    : 2.111801
2024-04-09 22:11:44,077 - trainer - INFO -     val_loss       : 1.076223
2024-04-09 22:11:44,077 - trainer - INFO -     val_accuracy   : 0.691825
2024-04-09 22:11:44,077 - trainer - INFO -     val_macro_f    : 0.67877
2024-04-09 22:11:44,077 - trainer - INFO -     val_precision  : 0.711539
2024-04-09 22:11:44,077 - trainer - INFO -     val_recall     : 0.691825
2024-04-09 22:11:44,077 - trainer - INFO -     val_doc_entropy: 2.174529
2024-04-09 22:11:44,077 - trainer - INFO -     test_loss      : 1.072823
2024-04-09 22:11:44,092 - trainer - INFO -     test_accuracy  : 0.691029
2024-04-09 22:11:44,092 - trainer - INFO -     test_macro_f   : 0.677563
2024-04-09 22:11:44,092 - trainer - INFO -     test_precision : 0.710636
2024-04-09 22:11:44,092 - trainer - INFO -     test_recall    : 0.691029
2024-04-09 22:11:44,092 - trainer - INFO -     test_doc_entropy: 2.17827
2024-04-09 22:15:05,498 - trainer - INFO -     epoch          : 3
2024-04-09 22:15:05,498 - trainer - INFO -     loss           : 0.676682
2024-04-09 22:15:05,498 - trainer - INFO -     accuracy       : 0.799671
2024-04-09 22:15:05,498 - trainer - INFO -     macro_f        : 0.793655
2024-04-09 22:15:05,498 - trainer - INFO -     precision      : 0.824092
2024-04-09 22:15:05,498 - trainer - INFO -     recall         : 0.799671
2024-04-09 22:15:05,498 - trainer - INFO -     doc_entropy    : 1.961052
2024-04-09 22:15:05,498 - trainer - INFO -     val_loss       : 1.162752
2024-04-09 22:15:05,498 - trainer - INFO -     val_accuracy   : 0.678483
2024-04-09 22:15:05,498 - trainer - INFO -     val_macro_f    : 0.668602
2024-04-09 22:15:05,498 - trainer - INFO -     val_precision  : 0.707487
2024-04-09 22:15:05,498 - trainer - INFO -     val_recall     : 0.678483
2024-04-09 22:15:05,498 - trainer - INFO -     val_doc_entropy: 2.074564
2024-04-09 22:15:05,498 - trainer - INFO -     test_loss      : 1.164391
2024-04-09 22:15:05,498 - trainer - INFO -     test_accuracy  : 0.677686
2024-04-09 22:15:05,498 - trainer - INFO -     test_macro_f   : 0.668416
2024-04-09 22:15:05,498 - trainer - INFO -     test_precision : 0.706384
2024-04-09 22:15:05,498 - trainer - INFO -     test_recall    : 0.677686
2024-04-09 22:15:05,498 - trainer - INFO -     test_doc_entropy: 2.076611
2024-04-09 22:15:39,964 - gensim.corpora.dictionary - INFO - adding document #0 to Dictionary<0 unique tokens: []>
2024-04-09 22:15:40,339 - gensim.corpora.dictionary - INFO - adding document #10000 to Dictionary<23643 unique tokens: ['-', '.', '?', 'a', 'africa']...>
2024-04-09 22:15:40,714 - gensim.corpora.dictionary - INFO - adding document #20000 to Dictionary<32428 unique tokens: ['-', '.', '?', 'a', 'africa']...>
2024-04-09 22:15:40,714 - gensim.corpora.dictionary - INFO - built Dictionary<32503 unique tokens: ['-', '.', '?', 'a', 'africa']...> from 20086 documents (total 703102 corpus positions)
2024-04-09 22:15:40,730 - gensim.utils - INFO - Dictionary lifecycle event {'msg': "built Dictionary<32503 unique tokens: ['-', '.', '?', 'a', 'africa']...> from 20086 documents (total 703102 corpus positions)", 'datetime': '2024-04-09T22:15:40.714885', 'gensim': '4.3.2', 'python': '3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}
2024-04-09 22:15:40,746 - gensim.topic_coherence.probability_estimation - INFO - using ParallelWordOccurrenceAccumulator<processes=11, batch_size=64> to estimate probabilities from sliding windows
2024-04-09 22:16:36,839 - gensim.topic_coherence.text_analysis - INFO - 1 batches submitted to accumulate stats from 64 documents (1358 virtual)
2024-04-09 22:16:36,855 - gensim.topic_coherence.text_analysis - INFO - 2 batches submitted to accumulate stats from 128 documents (3142 virtual)
2024-04-09 22:16:36,855 - gensim.topic_coherence.text_analysis - INFO - 3 batches submitted to accumulate stats from 192 documents (4707 virtual)
2024-04-09 22:16:36,855 - gensim.topic_coherence.text_analysis - INFO - 4 batches submitted to accumulate stats from 256 documents (6346 virtual)
2024-04-09 22:16:36,855 - gensim.topic_coherence.text_analysis - INFO - 5 batches submitted to accumulate stats from 320 documents (7961 virtual)
2024-04-09 22:16:36,871 - gensim.topic_coherence.text_analysis - INFO - 6 batches submitted to accumulate stats from 384 documents (9298 virtual)
2024-04-09 22:16:36,871 - gensim.topic_coherence.text_analysis - INFO - 7 batches submitted to accumulate stats from 448 documents (11371 virtual)
2024-04-09 22:16:36,871 - gensim.topic_coherence.text_analysis - INFO - 8 batches submitted to accumulate stats from 512 documents (13011 virtual)
2024-04-09 22:16:36,871 - gensim.topic_coherence.text_analysis - INFO - 9 batches submitted to accumulate stats from 576 documents (14534 virtual)
2024-04-09 22:16:36,871 - gensim.topic_coherence.text_analysis - INFO - 10 batches submitted to accumulate stats from 640 documents (16161 virtual)
2024-04-09 22:16:36,871 - gensim.topic_coherence.text_analysis - INFO - 11 batches submitted to accumulate stats from 704 documents (17689 virtual)
2024-04-09 22:16:36,871 - gensim.topic_coherence.text_analysis - INFO - 12 batches submitted to accumulate stats from 768 documents (19256 virtual)
2024-04-09 22:16:36,886 - gensim.topic_coherence.text_analysis - INFO - 13 batches submitted to accumulate stats from 832 documents (21175 virtual)
2024-04-09 22:16:36,886 - gensim.topic_coherence.text_analysis - INFO - 14 batches submitted to accumulate stats from 896 documents (22850 virtual)
2024-04-09 22:16:36,886 - gensim.topic_coherence.text_analysis - INFO - 15 batches submitted to accumulate stats from 960 documents (24681 virtual)
2024-04-09 22:16:36,886 - gensim.topic_coherence.text_analysis - INFO - 16 batches submitted to accumulate stats from 1024 documents (26222 virtual)
2024-04-09 22:16:36,886 - gensim.topic_coherence.text_analysis - INFO - 17 batches submitted to accumulate stats from 1088 documents (27953 virtual)
2024-04-09 22:16:36,886 - gensim.topic_coherence.text_analysis - INFO - 18 batches submitted to accumulate stats from 1152 documents (29594 virtual)
2024-04-09 22:16:36,886 - gensim.topic_coherence.text_analysis - INFO - 19 batches submitted to accumulate stats from 1216 documents (31239 virtual)
2024-04-09 22:16:36,886 - gensim.topic_coherence.text_analysis - INFO - 20 batches submitted to accumulate stats from 1280 documents (32901 virtual)
2024-04-09 22:16:36,902 - gensim.topic_coherence.text_analysis - INFO - 21 batches submitted to accumulate stats from 1344 documents (34481 virtual)
2024-04-09 22:16:36,902 - gensim.topic_coherence.text_analysis - INFO - 22 batches submitted to accumulate stats from 1408 documents (36151 virtual)
2024-04-09 22:16:36,902 - gensim.topic_coherence.text_analysis - INFO - 23 batches submitted to accumulate stats from 1472 documents (37837 virtual)
2024-04-09 22:16:36,902 - gensim.topic_coherence.text_analysis - INFO - 24 batches submitted to accumulate stats from 1536 documents (39453 virtual)
2024-04-09 22:16:36,902 - gensim.topic_coherence.text_analysis - INFO - 25 batches submitted to accumulate stats from 1600 documents (41046 virtual)
2024-04-09 22:16:36,902 - gensim.topic_coherence.text_analysis - INFO - 26 batches submitted to accumulate stats from 1664 documents (42892 virtual)
2024-04-09 22:16:36,902 - gensim.topic_coherence.text_analysis - INFO - 27 batches submitted to accumulate stats from 1728 documents (44445 virtual)
2024-04-09 22:16:36,918 - gensim.topic_coherence.text_analysis - INFO - 28 batches submitted to accumulate stats from 1792 documents (46073 virtual)
2024-04-09 22:16:36,918 - gensim.topic_coherence.text_analysis - INFO - 29 batches submitted to accumulate stats from 1856 documents (47643 virtual)
2024-04-09 22:16:36,918 - gensim.topic_coherence.text_analysis - INFO - 30 batches submitted to accumulate stats from 1920 documents (49252 virtual)
2024-04-09 22:16:36,918 - gensim.topic_coherence.text_analysis - INFO - 31 batches submitted to accumulate stats from 1984 documents (50774 virtual)
2024-04-09 22:16:36,918 - gensim.topic_coherence.text_analysis - INFO - 32 batches submitted to accumulate stats from 2048 documents (52387 virtual)
2024-04-09 22:16:36,918 - gensim.topic_coherence.text_analysis - INFO - 33 batches submitted to accumulate stats from 2112 documents (53815 virtual)
2024-04-09 22:16:36,918 - gensim.topic_coherence.text_analysis - INFO - 34 batches submitted to accumulate stats from 2176 documents (55540 virtual)
2024-04-09 22:16:36,949 - gensim.topic_coherence.text_analysis - INFO - 35 batches submitted to accumulate stats from 2240 documents (57348 virtual)
2024-04-09 22:16:36,949 - gensim.topic_coherence.text_analysis - INFO - 36 batches submitted to accumulate stats from 2304 documents (59258 virtual)
2024-04-09 22:16:36,949 - gensim.topic_coherence.text_analysis - INFO - 37 batches submitted to accumulate stats from 2368 documents (60957 virtual)
2024-04-09 22:16:36,949 - gensim.topic_coherence.text_analysis - INFO - 38 batches submitted to accumulate stats from 2432 documents (62425 virtual)
2024-04-09 22:16:36,949 - gensim.topic_coherence.text_analysis - INFO - 39 batches submitted to accumulate stats from 2496 documents (64029 virtual)
2024-04-09 22:16:36,949 - gensim.topic_coherence.text_analysis - INFO - 40 batches submitted to accumulate stats from 2560 documents (65725 virtual)
2024-04-09 22:16:36,949 - gensim.topic_coherence.text_analysis - INFO - 41 batches submitted to accumulate stats from 2624 documents (67346 virtual)
2024-04-09 22:16:36,964 - gensim.topic_coherence.text_analysis - INFO - 42 batches submitted to accumulate stats from 2688 documents (68863 virtual)
2024-04-09 22:16:36,964 - gensim.topic_coherence.text_analysis - INFO - 43 batches submitted to accumulate stats from 2752 documents (70539 virtual)
2024-04-09 22:16:36,964 - gensim.topic_coherence.text_analysis - INFO - 44 batches submitted to accumulate stats from 2816 documents (71922 virtual)
2024-04-09 22:16:36,964 - gensim.topic_coherence.text_analysis - INFO - 45 batches submitted to accumulate stats from 2880 documents (73294 virtual)
2024-04-09 22:16:36,980 - gensim.topic_coherence.text_analysis - INFO - 46 batches submitted to accumulate stats from 2944 documents (75084 virtual)
2024-04-09 22:16:36,980 - gensim.topic_coherence.text_analysis - INFO - 47 batches submitted to accumulate stats from 3008 documents (76769 virtual)
2024-04-09 22:16:36,980 - gensim.topic_coherence.text_analysis - INFO - 48 batches submitted to accumulate stats from 3072 documents (78312 virtual)
2024-04-09 22:16:36,996 - gensim.topic_coherence.text_analysis - INFO - 49 batches submitted to accumulate stats from 3136 documents (80039 virtual)
2024-04-09 22:16:36,996 - gensim.topic_coherence.text_analysis - INFO - 50 batches submitted to accumulate stats from 3200 documents (81572 virtual)
2024-04-09 22:16:36,996 - gensim.topic_coherence.text_analysis - INFO - 51 batches submitted to accumulate stats from 3264 documents (83189 virtual)
2024-04-09 22:16:36,996 - gensim.topic_coherence.text_analysis - INFO - 52 batches submitted to accumulate stats from 3328 documents (84783 virtual)
2024-04-09 22:16:36,996 - gensim.topic_coherence.text_analysis - INFO - 53 batches submitted to accumulate stats from 3392 documents (86570 virtual)
2024-04-09 22:16:36,996 - gensim.topic_coherence.text_analysis - INFO - 54 batches submitted to accumulate stats from 3456 documents (88371 virtual)
2024-04-09 22:16:37,011 - gensim.topic_coherence.text_analysis - INFO - 55 batches submitted to accumulate stats from 3520 documents (90295 virtual)
2024-04-09 22:16:37,011 - gensim.topic_coherence.text_analysis - INFO - 56 batches submitted to accumulate stats from 3584 documents (92118 virtual)
2024-04-09 22:16:37,027 - gensim.topic_coherence.text_analysis - INFO - 57 batches submitted to accumulate stats from 3648 documents (93914 virtual)
2024-04-09 22:16:37,027 - gensim.topic_coherence.text_analysis - INFO - 58 batches submitted to accumulate stats from 3712 documents (95729 virtual)
2024-04-09 22:16:37,027 - gensim.topic_coherence.text_analysis - INFO - 59 batches submitted to accumulate stats from 3776 documents (97293 virtual)
2024-04-09 22:16:37,027 - gensim.topic_coherence.text_analysis - INFO - 60 batches submitted to accumulate stats from 3840 documents (98908 virtual)
2024-04-09 22:16:37,043 - gensim.topic_coherence.text_analysis - INFO - 61 batches submitted to accumulate stats from 3904 documents (100586 virtual)
2024-04-09 22:16:37,043 - gensim.topic_coherence.text_analysis - INFO - 62 batches submitted to accumulate stats from 3968 documents (102208 virtual)
2024-04-09 22:16:37,043 - gensim.topic_coherence.text_analysis - INFO - 63 batches submitted to accumulate stats from 4032 documents (103862 virtual)
2024-04-09 22:16:37,043 - gensim.topic_coherence.text_analysis - INFO - 64 batches submitted to accumulate stats from 4096 documents (105500 virtual)
2024-04-09 22:16:37,043 - gensim.topic_coherence.text_analysis - INFO - 65 batches submitted to accumulate stats from 4160 documents (106974 virtual)
2024-04-09 22:16:37,043 - gensim.topic_coherence.text_analysis - INFO - 66 batches submitted to accumulate stats from 4224 documents (108587 virtual)
2024-04-09 22:16:37,058 - gensim.topic_coherence.text_analysis - INFO - 67 batches submitted to accumulate stats from 4288 documents (110059 virtual)
2024-04-09 22:16:37,074 - gensim.topic_coherence.text_analysis - INFO - 68 batches submitted to accumulate stats from 4352 documents (111905 virtual)
2024-04-09 22:16:37,074 - gensim.topic_coherence.text_analysis - INFO - 69 batches submitted to accumulate stats from 4416 documents (113549 virtual)
2024-04-09 22:16:37,074 - gensim.topic_coherence.text_analysis - INFO - 70 batches submitted to accumulate stats from 4480 documents (115163 virtual)
2024-04-09 22:16:37,074 - gensim.topic_coherence.text_analysis - INFO - 71 batches submitted to accumulate stats from 4544 documents (117083 virtual)
2024-04-09 22:16:37,089 - gensim.topic_coherence.text_analysis - INFO - 72 batches submitted to accumulate stats from 4608 documents (118654 virtual)
2024-04-09 22:16:37,089 - gensim.topic_coherence.text_analysis - INFO - 73 batches submitted to accumulate stats from 4672 documents (120224 virtual)
2024-04-09 22:16:37,089 - gensim.topic_coherence.text_analysis - INFO - 74 batches submitted to accumulate stats from 4736 documents (121786 virtual)
2024-04-09 22:16:37,089 - gensim.topic_coherence.text_analysis - INFO - 75 batches submitted to accumulate stats from 4800 documents (123233 virtual)
2024-04-09 22:16:37,089 - gensim.topic_coherence.text_analysis - INFO - 76 batches submitted to accumulate stats from 4864 documents (124761 virtual)
2024-04-09 22:16:37,089 - gensim.topic_coherence.text_analysis - INFO - 77 batches submitted to accumulate stats from 4928 documents (126221 virtual)
2024-04-09 22:16:37,089 - gensim.topic_coherence.text_analysis - INFO - 78 batches submitted to accumulate stats from 4992 documents (127857 virtual)
2024-04-09 22:16:37,121 - gensim.topic_coherence.text_analysis - INFO - 79 batches submitted to accumulate stats from 5056 documents (129432 virtual)
2024-04-09 22:16:37,121 - gensim.topic_coherence.text_analysis - INFO - 80 batches submitted to accumulate stats from 5120 documents (130948 virtual)
2024-04-09 22:16:37,121 - gensim.topic_coherence.text_analysis - INFO - 81 batches submitted to accumulate stats from 5184 documents (132913 virtual)
2024-04-09 22:16:37,121 - gensim.topic_coherence.text_analysis - INFO - 82 batches submitted to accumulate stats from 5248 documents (134700 virtual)
2024-04-09 22:16:37,121 - gensim.topic_coherence.text_analysis - INFO - 83 batches submitted to accumulate stats from 5312 documents (136417 virtual)
2024-04-09 22:16:37,136 - gensim.topic_coherence.text_analysis - INFO - 84 batches submitted to accumulate stats from 5376 documents (138141 virtual)
2024-04-09 22:16:37,136 - gensim.topic_coherence.text_analysis - INFO - 85 batches submitted to accumulate stats from 5440 documents (139767 virtual)
2024-04-09 22:16:37,136 - gensim.topic_coherence.text_analysis - INFO - 86 batches submitted to accumulate stats from 5504 documents (141151 virtual)
2024-04-09 22:16:37,136 - gensim.topic_coherence.text_analysis - INFO - 87 batches submitted to accumulate stats from 5568 documents (142647 virtual)
2024-04-09 22:16:37,136 - gensim.topic_coherence.text_analysis - INFO - 88 batches submitted to accumulate stats from 5632 documents (144175 virtual)
2024-04-09 22:16:37,136 - gensim.topic_coherence.text_analysis - INFO - 89 batches submitted to accumulate stats from 5696 documents (145825 virtual)
2024-04-09 22:16:37,168 - gensim.topic_coherence.text_analysis - INFO - 90 batches submitted to accumulate stats from 5760 documents (147317 virtual)
2024-04-09 22:16:37,168 - gensim.topic_coherence.text_analysis - INFO - 91 batches submitted to accumulate stats from 5824 documents (149158 virtual)
2024-04-09 22:16:37,168 - gensim.topic_coherence.text_analysis - INFO - 92 batches submitted to accumulate stats from 5888 documents (150755 virtual)
2024-04-09 22:16:37,183 - gensim.topic_coherence.text_analysis - INFO - 93 batches submitted to accumulate stats from 5952 documents (152237 virtual)
2024-04-09 22:16:37,183 - gensim.topic_coherence.text_analysis - INFO - 94 batches submitted to accumulate stats from 6016 documents (154013 virtual)
2024-04-09 22:16:37,183 - gensim.topic_coherence.text_analysis - INFO - 95 batches submitted to accumulate stats from 6080 documents (155593 virtual)
2024-04-09 22:16:37,183 - gensim.topic_coherence.text_analysis - INFO - 96 batches submitted to accumulate stats from 6144 documents (157114 virtual)
2024-04-09 22:16:37,183 - gensim.topic_coherence.text_analysis - INFO - 97 batches submitted to accumulate stats from 6208 documents (158783 virtual)
2024-04-09 22:16:37,183 - gensim.topic_coherence.text_analysis - INFO - 98 batches submitted to accumulate stats from 6272 documents (160467 virtual)
2024-04-09 22:16:37,183 - gensim.topic_coherence.text_analysis - INFO - 99 batches submitted to accumulate stats from 6336 documents (162287 virtual)
2024-04-09 22:16:37,183 - gensim.topic_coherence.text_analysis - INFO - 100 batches submitted to accumulate stats from 6400 documents (163932 virtual)
2024-04-09 22:16:37,199 - gensim.topic_coherence.text_analysis - INFO - 101 batches submitted to accumulate stats from 6464 documents (165414 virtual)
2024-04-09 22:16:37,214 - gensim.topic_coherence.text_analysis - INFO - 102 batches submitted to accumulate stats from 6528 documents (166934 virtual)
2024-04-09 22:16:37,214 - gensim.topic_coherence.text_analysis - INFO - 103 batches submitted to accumulate stats from 6592 documents (168399 virtual)
2024-04-09 22:16:37,214 - gensim.topic_coherence.text_analysis - INFO - 104 batches submitted to accumulate stats from 6656 documents (170596 virtual)
2024-04-09 22:16:37,214 - gensim.topic_coherence.text_analysis - INFO - 105 batches submitted to accumulate stats from 6720 documents (172319 virtual)
2024-04-09 22:16:37,214 - gensim.topic_coherence.text_analysis - INFO - 106 batches submitted to accumulate stats from 6784 documents (173973 virtual)
2024-04-09 22:16:37,214 - gensim.topic_coherence.text_analysis - INFO - 107 batches submitted to accumulate stats from 6848 documents (175817 virtual)
2024-04-09 22:16:37,230 - gensim.topic_coherence.text_analysis - INFO - 108 batches submitted to accumulate stats from 6912 documents (177402 virtual)
2024-04-09 22:16:37,230 - gensim.topic_coherence.text_analysis - INFO - 109 batches submitted to accumulate stats from 6976 documents (179106 virtual)
2024-04-09 22:16:37,230 - gensim.topic_coherence.text_analysis - INFO - 110 batches submitted to accumulate stats from 7040 documents (181089 virtual)
2024-04-09 22:16:37,230 - gensim.topic_coherence.text_analysis - INFO - 111 batches submitted to accumulate stats from 7104 documents (182660 virtual)
2024-04-09 22:16:37,246 - gensim.topic_coherence.text_analysis - INFO - 112 batches submitted to accumulate stats from 7168 documents (184289 virtual)
2024-04-09 22:16:37,246 - gensim.topic_coherence.text_analysis - INFO - 113 batches submitted to accumulate stats from 7232 documents (185825 virtual)
2024-04-09 22:16:37,246 - gensim.topic_coherence.text_analysis - INFO - 114 batches submitted to accumulate stats from 7296 documents (187420 virtual)
2024-04-09 22:16:37,261 - gensim.topic_coherence.text_analysis - INFO - 115 batches submitted to accumulate stats from 7360 documents (189102 virtual)
2024-04-09 22:16:37,261 - gensim.topic_coherence.text_analysis - INFO - 116 batches submitted to accumulate stats from 7424 documents (190745 virtual)
2024-04-09 22:16:37,261 - gensim.topic_coherence.text_analysis - INFO - 117 batches submitted to accumulate stats from 7488 documents (192238 virtual)
2024-04-09 22:16:37,261 - gensim.topic_coherence.text_analysis - INFO - 118 batches submitted to accumulate stats from 7552 documents (194107 virtual)
2024-04-09 22:16:37,261 - gensim.topic_coherence.text_analysis - INFO - 119 batches submitted to accumulate stats from 7616 documents (195570 virtual)
2024-04-09 22:16:37,261 - gensim.topic_coherence.text_analysis - INFO - 120 batches submitted to accumulate stats from 7680 documents (197064 virtual)
2024-04-09 22:16:37,277 - gensim.topic_coherence.text_analysis - INFO - 121 batches submitted to accumulate stats from 7744 documents (198821 virtual)
2024-04-09 22:16:37,277 - gensim.topic_coherence.text_analysis - INFO - 122 batches submitted to accumulate stats from 7808 documents (200394 virtual)
2024-04-09 22:16:37,293 - gensim.topic_coherence.text_analysis - INFO - 123 batches submitted to accumulate stats from 7872 documents (202352 virtual)
2024-04-09 22:16:37,293 - gensim.topic_coherence.text_analysis - INFO - 124 batches submitted to accumulate stats from 7936 documents (204181 virtual)
2024-04-09 22:16:37,293 - gensim.topic_coherence.text_analysis - INFO - 125 batches submitted to accumulate stats from 8000 documents (206063 virtual)
2024-04-09 22:16:37,293 - gensim.topic_coherence.text_analysis - INFO - 126 batches submitted to accumulate stats from 8064 documents (207766 virtual)
2024-04-09 22:16:37,308 - gensim.topic_coherence.text_analysis - INFO - 127 batches submitted to accumulate stats from 8128 documents (209460 virtual)
2024-04-09 22:16:37,308 - gensim.topic_coherence.text_analysis - INFO - 128 batches submitted to accumulate stats from 8192 documents (211022 virtual)
2024-04-09 22:16:37,308 - gensim.topic_coherence.text_analysis - INFO - 129 batches submitted to accumulate stats from 8256 documents (212632 virtual)
2024-04-09 22:16:37,308 - gensim.topic_coherence.text_analysis - INFO - 130 batches submitted to accumulate stats from 8320 documents (214210 virtual)
2024-04-09 22:16:37,308 - gensim.topic_coherence.text_analysis - INFO - 131 batches submitted to accumulate stats from 8384 documents (215651 virtual)
2024-04-09 22:16:37,324 - gensim.topic_coherence.text_analysis - INFO - 132 batches submitted to accumulate stats from 8448 documents (217300 virtual)
2024-04-09 22:16:37,324 - gensim.topic_coherence.text_analysis - INFO - 133 batches submitted to accumulate stats from 8512 documents (219035 virtual)
2024-04-09 22:16:37,324 - gensim.topic_coherence.text_analysis - INFO - 134 batches submitted to accumulate stats from 8576 documents (220675 virtual)
2024-04-09 22:16:37,324 - gensim.topic_coherence.text_analysis - INFO - 135 batches submitted to accumulate stats from 8640 documents (222562 virtual)
2024-04-09 22:16:37,339 - gensim.topic_coherence.text_analysis - INFO - 136 batches submitted to accumulate stats from 8704 documents (224243 virtual)
2024-04-09 22:16:37,339 - gensim.topic_coherence.text_analysis - INFO - 137 batches submitted to accumulate stats from 8768 documents (225942 virtual)
2024-04-09 22:16:37,355 - gensim.topic_coherence.text_analysis - INFO - 138 batches submitted to accumulate stats from 8832 documents (227774 virtual)
2024-04-09 22:16:37,355 - gensim.topic_coherence.text_analysis - INFO - 139 batches submitted to accumulate stats from 8896 documents (229378 virtual)
2024-04-09 22:16:37,355 - gensim.topic_coherence.text_analysis - INFO - 140 batches submitted to accumulate stats from 8960 documents (231026 virtual)
2024-04-09 22:16:37,355 - gensim.topic_coherence.text_analysis - INFO - 141 batches submitted to accumulate stats from 9024 documents (232664 virtual)
2024-04-09 22:16:37,355 - gensim.topic_coherence.text_analysis - INFO - 142 batches submitted to accumulate stats from 9088 documents (234376 virtual)
2024-04-09 22:16:37,371 - gensim.topic_coherence.text_analysis - INFO - 143 batches submitted to accumulate stats from 9152 documents (236034 virtual)
2024-04-09 22:16:37,371 - gensim.topic_coherence.text_analysis - INFO - 144 batches submitted to accumulate stats from 9216 documents (237753 virtual)
2024-04-09 22:16:37,371 - gensim.topic_coherence.text_analysis - INFO - 145 batches submitted to accumulate stats from 9280 documents (239260 virtual)
2024-04-09 22:16:37,371 - gensim.topic_coherence.text_analysis - INFO - 146 batches submitted to accumulate stats from 9344 documents (240889 virtual)
2024-04-09 22:16:37,386 - gensim.topic_coherence.text_analysis - INFO - 147 batches submitted to accumulate stats from 9408 documents (242607 virtual)
2024-04-09 22:16:37,386 - gensim.topic_coherence.text_analysis - INFO - 148 batches submitted to accumulate stats from 9472 documents (244168 virtual)
2024-04-09 22:16:37,386 - gensim.topic_coherence.text_analysis - INFO - 149 batches submitted to accumulate stats from 9536 documents (245827 virtual)
2024-04-09 22:16:37,402 - gensim.topic_coherence.text_analysis - INFO - 150 batches submitted to accumulate stats from 9600 documents (247340 virtual)
2024-04-09 22:16:37,402 - gensim.topic_coherence.text_analysis - INFO - 151 batches submitted to accumulate stats from 9664 documents (248935 virtual)
2024-04-09 22:16:37,402 - gensim.topic_coherence.text_analysis - INFO - 152 batches submitted to accumulate stats from 9728 documents (250682 virtual)
2024-04-09 22:16:37,402 - gensim.topic_coherence.text_analysis - INFO - 153 batches submitted to accumulate stats from 9792 documents (252525 virtual)
2024-04-09 22:16:37,402 - gensim.topic_coherence.text_analysis - INFO - 154 batches submitted to accumulate stats from 9856 documents (253964 virtual)
2024-04-09 22:16:37,418 - gensim.topic_coherence.text_analysis - INFO - 155 batches submitted to accumulate stats from 9920 documents (255761 virtual)
2024-04-09 22:16:37,418 - gensim.topic_coherence.text_analysis - INFO - 156 batches submitted to accumulate stats from 9984 documents (257656 virtual)
2024-04-09 22:16:37,418 - gensim.topic_coherence.text_analysis - INFO - 157 batches submitted to accumulate stats from 10048 documents (259320 virtual)
2024-04-09 22:16:37,418 - gensim.topic_coherence.text_analysis - INFO - 158 batches submitted to accumulate stats from 10112 documents (261193 virtual)
2024-04-09 22:16:37,433 - gensim.topic_coherence.text_analysis - INFO - 159 batches submitted to accumulate stats from 10176 documents (262723 virtual)
2024-04-09 22:16:37,433 - gensim.topic_coherence.text_analysis - INFO - 160 batches submitted to accumulate stats from 10240 documents (264223 virtual)
2024-04-09 22:16:37,449 - gensim.topic_coherence.text_analysis - INFO - 161 batches submitted to accumulate stats from 10304 documents (265694 virtual)
2024-04-09 22:16:37,449 - gensim.topic_coherence.text_analysis - INFO - 162 batches submitted to accumulate stats from 10368 documents (267448 virtual)
2024-04-09 22:16:37,449 - gensim.topic_coherence.text_analysis - INFO - 163 batches submitted to accumulate stats from 10432 documents (269251 virtual)
2024-04-09 22:16:37,449 - gensim.topic_coherence.text_analysis - INFO - 164 batches submitted to accumulate stats from 10496 documents (270973 virtual)
2024-04-09 22:16:37,449 - gensim.topic_coherence.text_analysis - INFO - 165 batches submitted to accumulate stats from 10560 documents (272683 virtual)
2024-04-09 22:16:37,449 - gensim.topic_coherence.text_analysis - INFO - 166 batches submitted to accumulate stats from 10624 documents (274294 virtual)
2024-04-09 22:16:37,464 - gensim.topic_coherence.text_analysis - INFO - 167 batches submitted to accumulate stats from 10688 documents (276045 virtual)
2024-04-09 22:16:37,464 - gensim.topic_coherence.text_analysis - INFO - 168 batches submitted to accumulate stats from 10752 documents (277496 virtual)
2024-04-09 22:16:37,480 - gensim.topic_coherence.text_analysis - INFO - 169 batches submitted to accumulate stats from 10816 documents (279131 virtual)
2024-04-09 22:16:37,480 - gensim.topic_coherence.text_analysis - INFO - 170 batches submitted to accumulate stats from 10880 documents (280812 virtual)
2024-04-09 22:16:37,480 - gensim.topic_coherence.text_analysis - INFO - 171 batches submitted to accumulate stats from 10944 documents (282408 virtual)
2024-04-09 22:16:37,480 - gensim.topic_coherence.text_analysis - INFO - 172 batches submitted to accumulate stats from 11008 documents (284125 virtual)
2024-04-09 22:16:37,480 - gensim.topic_coherence.text_analysis - INFO - 173 batches submitted to accumulate stats from 11072 documents (285575 virtual)
2024-04-09 22:16:37,496 - gensim.topic_coherence.text_analysis - INFO - 174 batches submitted to accumulate stats from 11136 documents (287185 virtual)
2024-04-09 22:16:37,496 - gensim.topic_coherence.text_analysis - INFO - 175 batches submitted to accumulate stats from 11200 documents (289145 virtual)
2024-04-09 22:16:37,496 - gensim.topic_coherence.text_analysis - INFO - 176 batches submitted to accumulate stats from 11264 documents (290900 virtual)
2024-04-09 22:16:37,496 - gensim.topic_coherence.text_analysis - INFO - 177 batches submitted to accumulate stats from 11328 documents (292686 virtual)
2024-04-09 22:16:37,511 - gensim.topic_coherence.text_analysis - INFO - 178 batches submitted to accumulate stats from 11392 documents (294505 virtual)
2024-04-09 22:16:37,511 - gensim.topic_coherence.text_analysis - INFO - 179 batches submitted to accumulate stats from 11456 documents (296236 virtual)
2024-04-09 22:16:37,527 - gensim.topic_coherence.text_analysis - INFO - 180 batches submitted to accumulate stats from 11520 documents (297647 virtual)
2024-04-09 22:16:37,527 - gensim.topic_coherence.text_analysis - INFO - 181 batches submitted to accumulate stats from 11584 documents (299327 virtual)
2024-04-09 22:16:37,527 - gensim.topic_coherence.text_analysis - INFO - 182 batches submitted to accumulate stats from 11648 documents (300853 virtual)
2024-04-09 22:16:37,527 - gensim.topic_coherence.text_analysis - INFO - 183 batches submitted to accumulate stats from 11712 documents (302601 virtual)
2024-04-09 22:16:37,527 - gensim.topic_coherence.text_analysis - INFO - 184 batches submitted to accumulate stats from 11776 documents (304181 virtual)
2024-04-09 22:16:37,527 - gensim.topic_coherence.text_analysis - INFO - 185 batches submitted to accumulate stats from 11840 documents (305710 virtual)
2024-04-09 22:16:37,543 - gensim.topic_coherence.text_analysis - INFO - 186 batches submitted to accumulate stats from 11904 documents (307265 virtual)
2024-04-09 22:16:37,543 - gensim.topic_coherence.text_analysis - INFO - 187 batches submitted to accumulate stats from 11968 documents (309148 virtual)
2024-04-09 22:16:37,543 - gensim.topic_coherence.text_analysis - INFO - 188 batches submitted to accumulate stats from 12032 documents (310818 virtual)
2024-04-09 22:16:37,558 - gensim.topic_coherence.text_analysis - INFO - 189 batches submitted to accumulate stats from 12096 documents (312472 virtual)
2024-04-09 22:16:37,558 - gensim.topic_coherence.text_analysis - INFO - 190 batches submitted to accumulate stats from 12160 documents (314119 virtual)
2024-04-09 22:16:37,558 - gensim.topic_coherence.text_analysis - INFO - 191 batches submitted to accumulate stats from 12224 documents (316054 virtual)
2024-04-09 22:16:37,558 - gensim.topic_coherence.text_analysis - INFO - 192 batches submitted to accumulate stats from 12288 documents (317685 virtual)
2024-04-09 22:16:37,574 - gensim.topic_coherence.text_analysis - INFO - 193 batches submitted to accumulate stats from 12352 documents (319351 virtual)
2024-04-09 22:16:37,574 - gensim.topic_coherence.text_analysis - INFO - 194 batches submitted to accumulate stats from 12416 documents (321100 virtual)
2024-04-09 22:16:37,574 - gensim.topic_coherence.text_analysis - INFO - 195 batches submitted to accumulate stats from 12480 documents (323099 virtual)
2024-04-09 22:16:37,574 - gensim.topic_coherence.text_analysis - INFO - 196 batches submitted to accumulate stats from 12544 documents (324548 virtual)
2024-04-09 22:16:37,574 - gensim.topic_coherence.text_analysis - INFO - 197 batches submitted to accumulate stats from 12608 documents (326231 virtual)
2024-04-09 22:16:37,589 - gensim.topic_coherence.text_analysis - INFO - 198 batches submitted to accumulate stats from 12672 documents (327851 virtual)
2024-04-09 22:16:37,589 - gensim.topic_coherence.text_analysis - INFO - 199 batches submitted to accumulate stats from 12736 documents (329386 virtual)
2024-04-09 22:16:37,589 - gensim.topic_coherence.text_analysis - INFO - 200 batches submitted to accumulate stats from 12800 documents (331113 virtual)
2024-04-09 22:16:37,605 - gensim.topic_coherence.text_analysis - INFO - 201 batches submitted to accumulate stats from 12864 documents (332712 virtual)
2024-04-09 22:16:37,605 - gensim.topic_coherence.text_analysis - INFO - 202 batches submitted to accumulate stats from 12928 documents (334135 virtual)
2024-04-09 22:16:37,605 - gensim.topic_coherence.text_analysis - INFO - 203 batches submitted to accumulate stats from 12992 documents (335914 virtual)
2024-04-09 22:16:37,605 - gensim.topic_coherence.text_analysis - INFO - 204 batches submitted to accumulate stats from 13056 documents (337641 virtual)
2024-04-09 22:16:37,621 - gensim.topic_coherence.text_analysis - INFO - 205 batches submitted to accumulate stats from 13120 documents (339449 virtual)
2024-04-09 22:16:37,621 - gensim.topic_coherence.text_analysis - INFO - 206 batches submitted to accumulate stats from 13184 documents (341168 virtual)
2024-04-09 22:16:37,621 - gensim.topic_coherence.text_analysis - INFO - 207 batches submitted to accumulate stats from 13248 documents (342833 virtual)
2024-04-09 22:16:37,621 - gensim.topic_coherence.text_analysis - INFO - 208 batches submitted to accumulate stats from 13312 documents (344704 virtual)
2024-04-09 22:16:37,636 - gensim.topic_coherence.text_analysis - INFO - 209 batches submitted to accumulate stats from 13376 documents (346650 virtual)
2024-04-09 22:16:37,636 - gensim.topic_coherence.text_analysis - INFO - 210 batches submitted to accumulate stats from 13440 documents (348531 virtual)
2024-04-09 22:16:37,652 - gensim.topic_coherence.text_analysis - INFO - 211 batches submitted to accumulate stats from 13504 documents (350342 virtual)
2024-04-09 22:16:37,652 - gensim.topic_coherence.text_analysis - INFO - 212 batches submitted to accumulate stats from 13568 documents (352068 virtual)
2024-04-09 22:16:37,652 - gensim.topic_coherence.text_analysis - INFO - 213 batches submitted to accumulate stats from 13632 documents (353789 virtual)
2024-04-09 22:16:37,652 - gensim.topic_coherence.text_analysis - INFO - 214 batches submitted to accumulate stats from 13696 documents (355216 virtual)
2024-04-09 22:16:37,652 - gensim.topic_coherence.text_analysis - INFO - 215 batches submitted to accumulate stats from 13760 documents (356990 virtual)
2024-04-09 22:16:37,652 - gensim.topic_coherence.text_analysis - INFO - 216 batches submitted to accumulate stats from 13824 documents (358762 virtual)
2024-04-09 22:16:37,668 - gensim.topic_coherence.text_analysis - INFO - 217 batches submitted to accumulate stats from 13888 documents (360320 virtual)
2024-04-09 22:16:37,668 - gensim.topic_coherence.text_analysis - INFO - 218 batches submitted to accumulate stats from 13952 documents (361867 virtual)
2024-04-09 22:16:37,668 - gensim.topic_coherence.text_analysis - INFO - 219 batches submitted to accumulate stats from 14016 documents (363519 virtual)
2024-04-09 22:16:37,683 - gensim.topic_coherence.text_analysis - INFO - 220 batches submitted to accumulate stats from 14080 documents (365141 virtual)
2024-04-09 22:16:37,683 - gensim.topic_coherence.text_analysis - INFO - 221 batches submitted to accumulate stats from 14144 documents (366934 virtual)
2024-04-09 22:16:37,683 - gensim.topic_coherence.text_analysis - INFO - 222 batches submitted to accumulate stats from 14208 documents (368448 virtual)
2024-04-09 22:16:37,683 - gensim.topic_coherence.text_analysis - INFO - 223 batches submitted to accumulate stats from 14272 documents (370012 virtual)
2024-04-09 22:16:37,683 - gensim.topic_coherence.text_analysis - INFO - 224 batches submitted to accumulate stats from 14336 documents (371704 virtual)
2024-04-09 22:16:37,699 - gensim.topic_coherence.text_analysis - INFO - 225 batches submitted to accumulate stats from 14400 documents (373331 virtual)
2024-04-09 22:16:37,699 - gensim.topic_coherence.text_analysis - INFO - 226 batches submitted to accumulate stats from 14464 documents (375174 virtual)
2024-04-09 22:16:37,714 - gensim.topic_coherence.text_analysis - INFO - 227 batches submitted to accumulate stats from 14528 documents (377135 virtual)
2024-04-09 22:16:37,714 - gensim.topic_coherence.text_analysis - INFO - 228 batches submitted to accumulate stats from 14592 documents (378860 virtual)
2024-04-09 22:16:37,714 - gensim.topic_coherence.text_analysis - INFO - 229 batches submitted to accumulate stats from 14656 documents (380581 virtual)
2024-04-09 22:16:37,714 - gensim.topic_coherence.text_analysis - INFO - 230 batches submitted to accumulate stats from 14720 documents (382278 virtual)
2024-04-09 22:16:37,714 - gensim.topic_coherence.text_analysis - INFO - 231 batches submitted to accumulate stats from 14784 documents (383970 virtual)
2024-04-09 22:16:37,730 - gensim.topic_coherence.text_analysis - INFO - 232 batches submitted to accumulate stats from 14848 documents (385680 virtual)
2024-04-09 22:16:37,730 - gensim.topic_coherence.text_analysis - INFO - 233 batches submitted to accumulate stats from 14912 documents (387378 virtual)
2024-04-09 22:16:37,730 - gensim.topic_coherence.text_analysis - INFO - 234 batches submitted to accumulate stats from 14976 documents (388908 virtual)
2024-04-09 22:16:37,730 - gensim.topic_coherence.text_analysis - INFO - 235 batches submitted to accumulate stats from 15040 documents (390473 virtual)
2024-04-09 22:16:37,746 - gensim.topic_coherence.text_analysis - INFO - 236 batches submitted to accumulate stats from 15104 documents (391966 virtual)
2024-04-09 22:16:37,746 - gensim.topic_coherence.text_analysis - INFO - 237 batches submitted to accumulate stats from 15168 documents (393446 virtual)
2024-04-09 22:16:37,761 - gensim.topic_coherence.text_analysis - INFO - 238 batches submitted to accumulate stats from 15232 documents (394979 virtual)
2024-04-09 22:16:37,761 - gensim.topic_coherence.text_analysis - INFO - 239 batches submitted to accumulate stats from 15296 documents (396707 virtual)
2024-04-09 22:16:37,761 - gensim.topic_coherence.text_analysis - INFO - 240 batches submitted to accumulate stats from 15360 documents (398434 virtual)
2024-04-09 22:16:37,761 - gensim.topic_coherence.text_analysis - INFO - 241 batches submitted to accumulate stats from 15424 documents (400092 virtual)
2024-04-09 22:16:37,761 - gensim.topic_coherence.text_analysis - INFO - 242 batches submitted to accumulate stats from 15488 documents (402054 virtual)
2024-04-09 22:16:37,777 - gensim.topic_coherence.text_analysis - INFO - 243 batches submitted to accumulate stats from 15552 documents (403465 virtual)
2024-04-09 22:16:37,777 - gensim.topic_coherence.text_analysis - INFO - 244 batches submitted to accumulate stats from 15616 documents (405289 virtual)
2024-04-09 22:16:37,777 - gensim.topic_coherence.text_analysis - INFO - 245 batches submitted to accumulate stats from 15680 documents (406750 virtual)
2024-04-09 22:16:37,777 - gensim.topic_coherence.text_analysis - INFO - 246 batches submitted to accumulate stats from 15744 documents (408306 virtual)
2024-04-09 22:16:37,793 - gensim.topic_coherence.text_analysis - INFO - 247 batches submitted to accumulate stats from 15808 documents (410097 virtual)
2024-04-09 22:16:37,793 - gensim.topic_coherence.text_analysis - INFO - 248 batches submitted to accumulate stats from 15872 documents (411915 virtual)
2024-04-09 22:16:37,793 - gensim.topic_coherence.text_analysis - INFO - 249 batches submitted to accumulate stats from 15936 documents (413538 virtual)
2024-04-09 22:16:37,808 - gensim.topic_coherence.text_analysis - INFO - 250 batches submitted to accumulate stats from 16000 documents (415307 virtual)
2024-04-09 22:16:37,808 - gensim.topic_coherence.text_analysis - INFO - 251 batches submitted to accumulate stats from 16064 documents (416993 virtual)
2024-04-09 22:16:37,808 - gensim.topic_coherence.text_analysis - INFO - 252 batches submitted to accumulate stats from 16128 documents (418582 virtual)
2024-04-09 22:16:37,808 - gensim.topic_coherence.text_analysis - INFO - 253 batches submitted to accumulate stats from 16192 documents (420282 virtual)
2024-04-09 22:16:37,808 - gensim.topic_coherence.text_analysis - INFO - 254 batches submitted to accumulate stats from 16256 documents (421859 virtual)
2024-04-09 22:16:37,824 - gensim.topic_coherence.text_analysis - INFO - 255 batches submitted to accumulate stats from 16320 documents (423441 virtual)
2024-04-09 22:16:37,824 - gensim.topic_coherence.text_analysis - INFO - 256 batches submitted to accumulate stats from 16384 documents (425169 virtual)
2024-04-09 22:16:37,824 - gensim.topic_coherence.text_analysis - INFO - 257 batches submitted to accumulate stats from 16448 documents (426804 virtual)
2024-04-09 22:16:37,839 - gensim.topic_coherence.text_analysis - INFO - 258 batches submitted to accumulate stats from 16512 documents (428503 virtual)
2024-04-09 22:16:37,839 - gensim.topic_coherence.text_analysis - INFO - 259 batches submitted to accumulate stats from 16576 documents (430063 virtual)
2024-04-09 22:16:37,839 - gensim.topic_coherence.text_analysis - INFO - 260 batches submitted to accumulate stats from 16640 documents (431672 virtual)
2024-04-09 22:16:37,839 - gensim.topic_coherence.text_analysis - INFO - 261 batches submitted to accumulate stats from 16704 documents (433298 virtual)
2024-04-09 22:16:37,839 - gensim.topic_coherence.text_analysis - INFO - 262 batches submitted to accumulate stats from 16768 documents (434951 virtual)
2024-04-09 22:16:37,855 - gensim.topic_coherence.text_analysis - INFO - 263 batches submitted to accumulate stats from 16832 documents (436505 virtual)
2024-04-09 22:16:37,855 - gensim.topic_coherence.text_analysis - INFO - 264 batches submitted to accumulate stats from 16896 documents (438134 virtual)
2024-04-09 22:16:37,855 - gensim.topic_coherence.text_analysis - INFO - 265 batches submitted to accumulate stats from 16960 documents (439831 virtual)
2024-04-09 22:16:37,871 - gensim.topic_coherence.text_analysis - INFO - 266 batches submitted to accumulate stats from 17024 documents (441666 virtual)
2024-04-09 22:16:37,871 - gensim.topic_coherence.text_analysis - INFO - 267 batches submitted to accumulate stats from 17088 documents (443389 virtual)
2024-04-09 22:16:37,871 - gensim.topic_coherence.text_analysis - INFO - 268 batches submitted to accumulate stats from 17152 documents (445008 virtual)
2024-04-09 22:16:37,886 - gensim.topic_coherence.text_analysis - INFO - 269 batches submitted to accumulate stats from 17216 documents (446691 virtual)
2024-04-09 22:16:37,886 - gensim.topic_coherence.text_analysis - INFO - 270 batches submitted to accumulate stats from 17280 documents (448258 virtual)
2024-04-09 22:16:37,886 - gensim.topic_coherence.text_analysis - INFO - 271 batches submitted to accumulate stats from 17344 documents (450177 virtual)
2024-04-09 22:16:37,886 - gensim.topic_coherence.text_analysis - INFO - 272 batches submitted to accumulate stats from 17408 documents (451838 virtual)
2024-04-09 22:16:37,886 - gensim.topic_coherence.text_analysis - INFO - 273 batches submitted to accumulate stats from 17472 documents (453765 virtual)
2024-04-09 22:16:37,886 - gensim.topic_coherence.text_analysis - INFO - 274 batches submitted to accumulate stats from 17536 documents (455302 virtual)
2024-04-09 22:16:37,902 - gensim.topic_coherence.text_analysis - INFO - 275 batches submitted to accumulate stats from 17600 documents (457026 virtual)
2024-04-09 22:16:37,902 - gensim.topic_coherence.text_analysis - INFO - 276 batches submitted to accumulate stats from 17664 documents (458678 virtual)
2024-04-09 22:16:37,918 - gensim.topic_coherence.text_analysis - INFO - 277 batches submitted to accumulate stats from 17728 documents (460363 virtual)
2024-04-09 22:16:37,918 - gensim.topic_coherence.text_analysis - INFO - 278 batches submitted to accumulate stats from 17792 documents (462290 virtual)
2024-04-09 22:16:37,918 - gensim.topic_coherence.text_analysis - INFO - 279 batches submitted to accumulate stats from 17856 documents (464043 virtual)
2024-04-09 22:16:37,918 - gensim.topic_coherence.text_analysis - INFO - 280 batches submitted to accumulate stats from 17920 documents (465601 virtual)
2024-04-09 22:16:37,918 - gensim.topic_coherence.text_analysis - INFO - 281 batches submitted to accumulate stats from 17984 documents (467208 virtual)
2024-04-09 22:16:37,933 - gensim.topic_coherence.text_analysis - INFO - 282 batches submitted to accumulate stats from 18048 documents (468985 virtual)
2024-04-09 22:16:37,933 - gensim.topic_coherence.text_analysis - INFO - 283 batches submitted to accumulate stats from 18112 documents (470540 virtual)
2024-04-09 22:16:37,933 - gensim.topic_coherence.text_analysis - INFO - 284 batches submitted to accumulate stats from 18176 documents (472187 virtual)
2024-04-09 22:16:37,933 - gensim.topic_coherence.text_analysis - INFO - 285 batches submitted to accumulate stats from 18240 documents (473657 virtual)
2024-04-09 22:16:37,949 - gensim.topic_coherence.text_analysis - INFO - 286 batches submitted to accumulate stats from 18304 documents (475433 virtual)
2024-04-09 22:16:37,949 - gensim.topic_coherence.text_analysis - INFO - 287 batches submitted to accumulate stats from 18368 documents (476949 virtual)
2024-04-09 22:16:37,964 - gensim.topic_coherence.text_analysis - INFO - 288 batches submitted to accumulate stats from 18432 documents (478519 virtual)
2024-04-09 22:16:37,964 - gensim.topic_coherence.text_analysis - INFO - 289 batches submitted to accumulate stats from 18496 documents (480346 virtual)
2024-04-09 22:16:37,964 - gensim.topic_coherence.text_analysis - INFO - 290 batches submitted to accumulate stats from 18560 documents (482113 virtual)
2024-04-09 22:16:37,964 - gensim.topic_coherence.text_analysis - INFO - 291 batches submitted to accumulate stats from 18624 documents (483726 virtual)
2024-04-09 22:16:37,964 - gensim.topic_coherence.text_analysis - INFO - 292 batches submitted to accumulate stats from 18688 documents (485454 virtual)
2024-04-09 22:16:37,980 - gensim.topic_coherence.text_analysis - INFO - 293 batches submitted to accumulate stats from 18752 documents (487120 virtual)
2024-04-09 22:16:37,980 - gensim.topic_coherence.text_analysis - INFO - 294 batches submitted to accumulate stats from 18816 documents (488702 virtual)
2024-04-09 22:16:37,980 - gensim.topic_coherence.text_analysis - INFO - 295 batches submitted to accumulate stats from 18880 documents (490511 virtual)
2024-04-09 22:16:37,980 - gensim.topic_coherence.text_analysis - INFO - 296 batches submitted to accumulate stats from 18944 documents (492226 virtual)
2024-04-09 22:16:37,996 - gensim.topic_coherence.text_analysis - INFO - 297 batches submitted to accumulate stats from 19008 documents (494165 virtual)
2024-04-09 22:16:37,996 - gensim.topic_coherence.text_analysis - INFO - 298 batches submitted to accumulate stats from 19072 documents (495678 virtual)
2024-04-09 22:16:37,996 - gensim.topic_coherence.text_analysis - INFO - 299 batches submitted to accumulate stats from 19136 documents (497503 virtual)
2024-04-09 22:16:38,012 - gensim.topic_coherence.text_analysis - INFO - 300 batches submitted to accumulate stats from 19200 documents (499131 virtual)
2024-04-09 22:16:38,012 - gensim.topic_coherence.text_analysis - INFO - 301 batches submitted to accumulate stats from 19264 documents (500986 virtual)
2024-04-09 22:16:38,012 - gensim.topic_coherence.text_analysis - INFO - 302 batches submitted to accumulate stats from 19328 documents (502687 virtual)
2024-04-09 22:16:38,012 - gensim.topic_coherence.text_analysis - INFO - 303 batches submitted to accumulate stats from 19392 documents (504106 virtual)
2024-04-09 22:16:38,012 - gensim.topic_coherence.text_analysis - INFO - 304 batches submitted to accumulate stats from 19456 documents (505909 virtual)
2024-04-09 22:16:38,027 - gensim.topic_coherence.text_analysis - INFO - 305 batches submitted to accumulate stats from 19520 documents (507546 virtual)
2024-04-09 22:16:38,027 - gensim.topic_coherence.text_analysis - INFO - 306 batches submitted to accumulate stats from 19584 documents (509312 virtual)
2024-04-09 22:16:38,027 - gensim.topic_coherence.text_analysis - INFO - 307 batches submitted to accumulate stats from 19648 documents (510954 virtual)
2024-04-09 22:16:38,043 - gensim.topic_coherence.text_analysis - INFO - 308 batches submitted to accumulate stats from 19712 documents (512465 virtual)
2024-04-09 22:16:38,043 - gensim.topic_coherence.text_analysis - INFO - 309 batches submitted to accumulate stats from 19776 documents (514132 virtual)
2024-04-09 22:16:38,043 - gensim.topic_coherence.text_analysis - INFO - 310 batches submitted to accumulate stats from 19840 documents (515846 virtual)
2024-04-09 22:16:38,043 - gensim.topic_coherence.text_analysis - INFO - 311 batches submitted to accumulate stats from 19904 documents (517436 virtual)
2024-04-09 22:16:38,059 - gensim.topic_coherence.text_analysis - INFO - 312 batches submitted to accumulate stats from 19968 documents (519076 virtual)
2024-04-09 22:16:38,059 - gensim.topic_coherence.text_analysis - INFO - 313 batches submitted to accumulate stats from 20032 documents (520797 virtual)
2024-04-09 22:16:38,059 - gensim.topic_coherence.text_analysis - INFO - 314 batches submitted to accumulate stats from 20096 documents (522328 virtual)
2024-04-09 22:16:38,449 - gensim.topic_coherence.text_analysis - INFO - 11 accumulators retrieved from output queue
2024-04-09 22:16:38,637 - gensim.topic_coherence.text_analysis - INFO - accumulated word occurrence stats for 523696 virtual documents
2024-04-09 22:16:39,793 - gensim.corpora.dictionary - INFO - adding document #0 to Dictionary<0 unique tokens: []>
2024-04-09 22:16:40,152 - gensim.corpora.dictionary - INFO - adding document #10000 to Dictionary<23643 unique tokens: ['-', '.', '?', 'a', 'africa']...>
2024-04-09 22:16:40,496 - gensim.corpora.dictionary - INFO - adding document #20000 to Dictionary<32428 unique tokens: ['-', '.', '?', 'a', 'africa']...>
2024-04-09 22:16:40,496 - gensim.corpora.dictionary - INFO - built Dictionary<32503 unique tokens: ['-', '.', '?', 'a', 'africa']...> from 20086 documents (total 703102 corpus positions)
2024-04-09 22:16:40,512 - gensim.utils - INFO - Dictionary lifecycle event {'msg': "built Dictionary<32503 unique tokens: ['-', '.', '?', 'a', 'africa']...> from 20086 documents (total 703102 corpus positions)", 'datetime': '2024-04-09T22:16:40.512501', 'gensim': '4.3.2', 'python': '3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}
2024-04-09 22:16:40,528 - gensim.topic_coherence.probability_estimation - INFO - using ParallelWordOccurrenceAccumulator<processes=11, batch_size=64> to estimate probabilities from sliding windows
2024-04-09 22:17:34,358 - gensim.topic_coherence.text_analysis - INFO - 11 accumulators retrieved from output queue
2024-04-09 22:17:34,514 - gensim.topic_coherence.text_analysis - INFO - accumulated word occurrence stats for 21620 virtual documents
2024-04-09 22:17:39,344 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,760,523
Freeze params: 0
2024-04-09 22:21:00,878 - trainer - INFO -     epoch          : 1
2024-04-09 22:21:00,878 - trainer - INFO -     loss           : 1.220877
2024-04-09 22:21:00,878 - trainer - INFO -     accuracy       : 0.652879
2024-04-09 22:21:00,878 - trainer - INFO -     macro_f        : 0.633773
2024-04-09 22:21:00,878 - trainer - INFO -     precision      : 0.665303
2024-04-09 22:21:00,878 - trainer - INFO -     recall         : 0.652879
2024-04-09 22:21:00,878 - trainer - INFO -     doc_entropy    : 2.362214
2024-04-09 22:21:00,878 - trainer - INFO -     val_loss       : 1.071866
2024-04-09 22:21:00,878 - trainer - INFO -     val_accuracy   : 0.685851
2024-04-09 22:21:00,878 - trainer - INFO -     val_macro_f    : 0.671339
2024-04-09 22:21:00,878 - trainer - INFO -     val_precision  : 0.704105
2024-04-09 22:21:00,878 - trainer - INFO -     val_recall     : 0.685851
2024-04-09 22:21:00,878 - trainer - INFO -     val_doc_entropy: 2.337225
2024-04-09 22:21:00,878 - trainer - INFO -     test_loss      : 1.074951
2024-04-09 22:21:00,878 - trainer - INFO -     test_accuracy  : 0.68615
2024-04-09 22:21:00,878 - trainer - INFO -     test_macro_f   : 0.672258
2024-04-09 22:21:00,878 - trainer - INFO -     test_precision : 0.705254
2024-04-09 22:21:00,878 - trainer - INFO -     test_recall    : 0.68615
2024-04-09 22:21:00,878 - trainer - INFO -     test_doc_entropy: 2.341018
2024-04-09 22:24:21,703 - trainer - INFO -     epoch          : 2
2024-04-09 22:24:21,703 - trainer - INFO -     loss           : 0.890347
2024-04-09 22:24:21,703 - trainer - INFO -     accuracy       : 0.736588
2024-04-09 22:24:21,703 - trainer - INFO -     macro_f        : 0.727224
2024-04-09 22:24:21,703 - trainer - INFO -     precision      : 0.761769
2024-04-09 22:24:21,703 - trainer - INFO -     recall         : 0.736588
2024-04-09 22:24:21,703 - trainer - INFO -     doc_entropy    : 2.063776
2024-04-09 22:24:21,703 - trainer - INFO -     val_loss       : 1.085508
2024-04-09 22:24:21,703 - trainer - INFO -     val_accuracy   : 0.682167
2024-04-09 22:24:21,703 - trainer - INFO -     val_macro_f    : 0.667277
2024-04-09 22:24:21,703 - trainer - INFO -     val_precision  : 0.70009
2024-04-09 22:24:21,703 - trainer - INFO -     val_recall     : 0.682167
2024-04-09 22:24:21,703 - trainer - INFO -     val_doc_entropy: 2.090531
2024-04-09 22:24:21,703 - trainer - INFO -     test_loss      : 1.086973
2024-04-09 22:24:21,703 - trainer - INFO -     test_accuracy  : 0.686199
2024-04-09 22:24:21,703 - trainer - INFO -     test_macro_f   : 0.671912
2024-04-09 22:24:21,703 - trainer - INFO -     test_precision : 0.704988
2024-04-09 22:24:21,718 - trainer - INFO -     test_recall    : 0.686199
2024-04-09 22:24:21,718 - trainer - INFO -     test_doc_entropy: 2.094153
2024-04-09 22:27:42,512 - trainer - INFO -     epoch          : 3
2024-04-09 22:27:42,512 - trainer - INFO -     loss           : 0.674526
2024-04-09 22:27:42,512 - trainer - INFO -     accuracy       : 0.799603
2024-04-09 22:27:42,512 - trainer - INFO -     macro_f        : 0.793913
2024-04-09 22:27:42,512 - trainer - INFO -     precision      : 0.824669
2024-04-09 22:27:42,512 - trainer - INFO -     recall         : 0.799603
2024-04-09 22:27:42,512 - trainer - INFO -     doc_entropy    : 1.908268
2024-04-09 22:27:42,512 - trainer - INFO -     val_loss       : 1.182609
2024-04-09 22:27:42,512 - trainer - INFO -     val_accuracy   : 0.674301
2024-04-09 22:27:42,512 - trainer - INFO -     val_macro_f    : 0.6674
2024-04-09 22:27:42,512 - trainer - INFO -     val_precision  : 0.710458
2024-04-09 22:27:42,512 - trainer - INFO -     val_recall     : 0.674301
2024-04-09 22:27:42,512 - trainer - INFO -     val_doc_entropy: 2.032633
2024-04-09 22:27:42,512 - trainer - INFO -     test_loss      : 1.180142
2024-04-09 22:27:42,512 - trainer - INFO -     test_accuracy  : 0.678582
2024-04-09 22:27:42,512 - trainer - INFO -     test_macro_f   : 0.671371
2024-04-09 22:27:42,512 - trainer - INFO -     test_precision : 0.713034
2024-04-09 22:27:42,512 - trainer - INFO -     test_recall    : 0.678582
2024-04-09 22:27:42,512 - trainer - INFO -     test_doc_entropy: 2.037428
2024-04-09 22:28:18,269 - gensim.corpora.dictionary - INFO - adding document #0 to Dictionary<0 unique tokens: []>
2024-04-09 22:28:18,644 - gensim.corpora.dictionary - INFO - adding document #10000 to Dictionary<23643 unique tokens: ['-', '.', '?', 'a', 'africa']...>
2024-04-09 22:28:19,035 - gensim.corpora.dictionary - INFO - adding document #20000 to Dictionary<32428 unique tokens: ['-', '.', '?', 'a', 'africa']...>
2024-04-09 22:28:19,035 - gensim.corpora.dictionary - INFO - built Dictionary<32503 unique tokens: ['-', '.', '?', 'a', 'africa']...> from 20086 documents (total 703102 corpus positions)
2024-04-09 22:28:19,035 - gensim.utils - INFO - Dictionary lifecycle event {'msg': "built Dictionary<32503 unique tokens: ['-', '.', '?', 'a', 'africa']...> from 20086 documents (total 703102 corpus positions)", 'datetime': '2024-04-09T22:28:19.035453', 'gensim': '4.3.2', 'python': '3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}
2024-04-09 22:28:19,051 - gensim.topic_coherence.probability_estimation - INFO - using ParallelWordOccurrenceAccumulator<processes=11, batch_size=64> to estimate probabilities from sliding windows
2024-04-09 22:29:16,842 - gensim.topic_coherence.text_analysis - INFO - 1 batches submitted to accumulate stats from 64 documents (1358 virtual)
2024-04-09 22:29:16,858 - gensim.topic_coherence.text_analysis - INFO - 2 batches submitted to accumulate stats from 128 documents (3142 virtual)
2024-04-09 22:29:16,858 - gensim.topic_coherence.text_analysis - INFO - 3 batches submitted to accumulate stats from 192 documents (4707 virtual)
2024-04-09 22:29:16,858 - gensim.topic_coherence.text_analysis - INFO - 4 batches submitted to accumulate stats from 256 documents (6346 virtual)
2024-04-09 22:29:16,858 - gensim.topic_coherence.text_analysis - INFO - 5 batches submitted to accumulate stats from 320 documents (7961 virtual)
2024-04-09 22:29:16,858 - gensim.topic_coherence.text_analysis - INFO - 6 batches submitted to accumulate stats from 384 documents (9298 virtual)
2024-04-09 22:29:16,858 - gensim.topic_coherence.text_analysis - INFO - 7 batches submitted to accumulate stats from 448 documents (11371 virtual)
2024-04-09 22:29:16,858 - gensim.topic_coherence.text_analysis - INFO - 8 batches submitted to accumulate stats from 512 documents (13011 virtual)
2024-04-09 22:29:16,858 - gensim.topic_coherence.text_analysis - INFO - 9 batches submitted to accumulate stats from 576 documents (14534 virtual)
2024-04-09 22:29:16,858 - gensim.topic_coherence.text_analysis - INFO - 10 batches submitted to accumulate stats from 640 documents (16161 virtual)
2024-04-09 22:29:16,858 - gensim.topic_coherence.text_analysis - INFO - 11 batches submitted to accumulate stats from 704 documents (17689 virtual)
2024-04-09 22:29:16,858 - gensim.topic_coherence.text_analysis - INFO - 12 batches submitted to accumulate stats from 768 documents (19256 virtual)
2024-04-09 22:29:16,873 - gensim.topic_coherence.text_analysis - INFO - 13 batches submitted to accumulate stats from 832 documents (21175 virtual)
2024-04-09 22:29:16,873 - gensim.topic_coherence.text_analysis - INFO - 14 batches submitted to accumulate stats from 896 documents (22850 virtual)
2024-04-09 22:29:16,873 - gensim.topic_coherence.text_analysis - INFO - 15 batches submitted to accumulate stats from 960 documents (24681 virtual)
2024-04-09 22:29:16,873 - gensim.topic_coherence.text_analysis - INFO - 16 batches submitted to accumulate stats from 1024 documents (26222 virtual)
2024-04-09 22:29:16,873 - gensim.topic_coherence.text_analysis - INFO - 17 batches submitted to accumulate stats from 1088 documents (27953 virtual)
2024-04-09 22:29:16,873 - gensim.topic_coherence.text_analysis - INFO - 18 batches submitted to accumulate stats from 1152 documents (29594 virtual)
2024-04-09 22:29:16,889 - gensim.topic_coherence.text_analysis - INFO - 19 batches submitted to accumulate stats from 1216 documents (31239 virtual)
2024-04-09 22:29:16,889 - gensim.topic_coherence.text_analysis - INFO - 20 batches submitted to accumulate stats from 1280 documents (32901 virtual)
2024-04-09 22:29:16,889 - gensim.topic_coherence.text_analysis - INFO - 21 batches submitted to accumulate stats from 1344 documents (34481 virtual)
2024-04-09 22:29:16,904 - gensim.topic_coherence.text_analysis - INFO - 22 batches submitted to accumulate stats from 1408 documents (36151 virtual)
2024-04-09 22:29:16,920 - gensim.topic_coherence.text_analysis - INFO - 23 batches submitted to accumulate stats from 1472 documents (37837 virtual)
2024-04-09 22:29:16,920 - gensim.topic_coherence.text_analysis - INFO - 24 batches submitted to accumulate stats from 1536 documents (39453 virtual)
2024-04-09 22:29:16,920 - gensim.topic_coherence.text_analysis - INFO - 25 batches submitted to accumulate stats from 1600 documents (41046 virtual)
2024-04-09 22:29:16,936 - gensim.topic_coherence.text_analysis - INFO - 26 batches submitted to accumulate stats from 1664 documents (42892 virtual)
2024-04-09 22:29:16,936 - gensim.topic_coherence.text_analysis - INFO - 27 batches submitted to accumulate stats from 1728 documents (44445 virtual)
2024-04-09 22:29:16,936 - gensim.topic_coherence.text_analysis - INFO - 28 batches submitted to accumulate stats from 1792 documents (46073 virtual)
2024-04-09 22:29:16,936 - gensim.topic_coherence.text_analysis - INFO - 29 batches submitted to accumulate stats from 1856 documents (47643 virtual)
2024-04-09 22:29:16,951 - gensim.topic_coherence.text_analysis - INFO - 30 batches submitted to accumulate stats from 1920 documents (49252 virtual)
2024-04-09 22:29:16,967 - gensim.topic_coherence.text_analysis - INFO - 31 batches submitted to accumulate stats from 1984 documents (50774 virtual)
2024-04-09 22:29:16,967 - gensim.topic_coherence.text_analysis - INFO - 32 batches submitted to accumulate stats from 2048 documents (52387 virtual)
2024-04-09 22:29:16,983 - gensim.topic_coherence.text_analysis - INFO - 33 batches submitted to accumulate stats from 2112 documents (53815 virtual)
2024-04-09 22:29:16,983 - gensim.topic_coherence.text_analysis - INFO - 34 batches submitted to accumulate stats from 2176 documents (55540 virtual)
2024-04-09 22:29:16,998 - gensim.topic_coherence.text_analysis - INFO - 35 batches submitted to accumulate stats from 2240 documents (57348 virtual)
2024-04-09 22:29:16,998 - gensim.topic_coherence.text_analysis - INFO - 36 batches submitted to accumulate stats from 2304 documents (59258 virtual)
2024-04-09 22:29:16,998 - gensim.topic_coherence.text_analysis - INFO - 37 batches submitted to accumulate stats from 2368 documents (60957 virtual)
2024-04-09 22:29:16,998 - gensim.topic_coherence.text_analysis - INFO - 38 batches submitted to accumulate stats from 2432 documents (62425 virtual)
2024-04-09 22:29:17,014 - gensim.topic_coherence.text_analysis - INFO - 39 batches submitted to accumulate stats from 2496 documents (64029 virtual)
2024-04-09 22:29:17,014 - gensim.topic_coherence.text_analysis - INFO - 40 batches submitted to accumulate stats from 2560 documents (65725 virtual)
2024-04-09 22:29:17,014 - gensim.topic_coherence.text_analysis - INFO - 41 batches submitted to accumulate stats from 2624 documents (67346 virtual)
2024-04-09 22:29:17,029 - gensim.topic_coherence.text_analysis - INFO - 42 batches submitted to accumulate stats from 2688 documents (68863 virtual)
2024-04-09 22:29:17,045 - gensim.topic_coherence.text_analysis - INFO - 43 batches submitted to accumulate stats from 2752 documents (70539 virtual)
2024-04-09 22:29:17,061 - gensim.topic_coherence.text_analysis - INFO - 44 batches submitted to accumulate stats from 2816 documents (71922 virtual)
2024-04-09 22:29:17,061 - gensim.topic_coherence.text_analysis - INFO - 45 batches submitted to accumulate stats from 2880 documents (73294 virtual)
2024-04-09 22:29:17,076 - gensim.topic_coherence.text_analysis - INFO - 46 batches submitted to accumulate stats from 2944 documents (75084 virtual)
2024-04-09 22:29:17,076 - gensim.topic_coherence.text_analysis - INFO - 47 batches submitted to accumulate stats from 3008 documents (76769 virtual)
2024-04-09 22:29:17,076 - gensim.topic_coherence.text_analysis - INFO - 48 batches submitted to accumulate stats from 3072 documents (78312 virtual)
2024-04-09 22:29:17,076 - gensim.topic_coherence.text_analysis - INFO - 49 batches submitted to accumulate stats from 3136 documents (80039 virtual)
2024-04-09 22:29:17,076 - gensim.topic_coherence.text_analysis - INFO - 50 batches submitted to accumulate stats from 3200 documents (81572 virtual)
2024-04-09 22:29:17,092 - gensim.topic_coherence.text_analysis - INFO - 51 batches submitted to accumulate stats from 3264 documents (83189 virtual)
2024-04-09 22:29:17,092 - gensim.topic_coherence.text_analysis - INFO - 52 batches submitted to accumulate stats from 3328 documents (84783 virtual)
2024-04-09 22:29:17,092 - gensim.topic_coherence.text_analysis - INFO - 53 batches submitted to accumulate stats from 3392 documents (86570 virtual)
2024-04-09 22:29:17,108 - gensim.topic_coherence.text_analysis - INFO - 54 batches submitted to accumulate stats from 3456 documents (88371 virtual)
2024-04-09 22:29:17,123 - gensim.topic_coherence.text_analysis - INFO - 55 batches submitted to accumulate stats from 3520 documents (90295 virtual)
2024-04-09 22:29:17,139 - gensim.topic_coherence.text_analysis - INFO - 56 batches submitted to accumulate stats from 3584 documents (92118 virtual)
2024-04-09 22:29:17,139 - gensim.topic_coherence.text_analysis - INFO - 57 batches submitted to accumulate stats from 3648 documents (93914 virtual)
2024-04-09 22:29:17,154 - gensim.topic_coherence.text_analysis - INFO - 58 batches submitted to accumulate stats from 3712 documents (95729 virtual)
2024-04-09 22:29:17,154 - gensim.topic_coherence.text_analysis - INFO - 59 batches submitted to accumulate stats from 3776 documents (97293 virtual)
2024-04-09 22:29:17,170 - gensim.topic_coherence.text_analysis - INFO - 60 batches submitted to accumulate stats from 3840 documents (98908 virtual)
2024-04-09 22:29:17,170 - gensim.topic_coherence.text_analysis - INFO - 61 batches submitted to accumulate stats from 3904 documents (100586 virtual)
2024-04-09 22:29:17,170 - gensim.topic_coherence.text_analysis - INFO - 62 batches submitted to accumulate stats from 3968 documents (102208 virtual)
2024-04-09 22:29:17,170 - gensim.topic_coherence.text_analysis - INFO - 63 batches submitted to accumulate stats from 4032 documents (103862 virtual)
2024-04-09 22:29:17,186 - gensim.topic_coherence.text_analysis - INFO - 64 batches submitted to accumulate stats from 4096 documents (105500 virtual)
2024-04-09 22:29:17,201 - gensim.topic_coherence.text_analysis - INFO - 65 batches submitted to accumulate stats from 4160 documents (106974 virtual)
2024-04-09 22:29:17,201 - gensim.topic_coherence.text_analysis - INFO - 66 batches submitted to accumulate stats from 4224 documents (108587 virtual)
2024-04-09 22:29:17,217 - gensim.topic_coherence.text_analysis - INFO - 67 batches submitted to accumulate stats from 4288 documents (110059 virtual)
2024-04-09 22:29:17,217 - gensim.topic_coherence.text_analysis - INFO - 68 batches submitted to accumulate stats from 4352 documents (111905 virtual)
2024-04-09 22:29:17,233 - gensim.topic_coherence.text_analysis - INFO - 69 batches submitted to accumulate stats from 4416 documents (113549 virtual)
2024-04-09 22:29:17,233 - gensim.topic_coherence.text_analysis - INFO - 70 batches submitted to accumulate stats from 4480 documents (115163 virtual)
2024-04-09 22:29:17,248 - gensim.topic_coherence.text_analysis - INFO - 71 batches submitted to accumulate stats from 4544 documents (117083 virtual)
2024-04-09 22:29:17,264 - gensim.topic_coherence.text_analysis - INFO - 72 batches submitted to accumulate stats from 4608 documents (118654 virtual)
2024-04-09 22:29:17,264 - gensim.topic_coherence.text_analysis - INFO - 73 batches submitted to accumulate stats from 4672 documents (120224 virtual)
2024-04-09 22:29:17,264 - gensim.topic_coherence.text_analysis - INFO - 74 batches submitted to accumulate stats from 4736 documents (121786 virtual)
2024-04-09 22:29:17,279 - gensim.topic_coherence.text_analysis - INFO - 75 batches submitted to accumulate stats from 4800 documents (123233 virtual)
2024-04-09 22:29:17,295 - gensim.topic_coherence.text_analysis - INFO - 76 batches submitted to accumulate stats from 4864 documents (124761 virtual)
2024-04-09 22:29:17,311 - gensim.topic_coherence.text_analysis - INFO - 77 batches submitted to accumulate stats from 4928 documents (126221 virtual)
2024-04-09 22:29:17,311 - gensim.topic_coherence.text_analysis - INFO - 78 batches submitted to accumulate stats from 4992 documents (127857 virtual)
2024-04-09 22:29:17,311 - gensim.topic_coherence.text_analysis - INFO - 79 batches submitted to accumulate stats from 5056 documents (129432 virtual)
2024-04-09 22:29:17,311 - gensim.topic_coherence.text_analysis - INFO - 80 batches submitted to accumulate stats from 5120 documents (130948 virtual)
2024-04-09 22:29:17,326 - gensim.topic_coherence.text_analysis - INFO - 81 batches submitted to accumulate stats from 5184 documents (132913 virtual)
2024-04-09 22:29:17,358 - gensim.topic_coherence.text_analysis - INFO - 82 batches submitted to accumulate stats from 5248 documents (134700 virtual)
2024-04-09 22:29:17,373 - gensim.topic_coherence.text_analysis - INFO - 83 batches submitted to accumulate stats from 5312 documents (136417 virtual)
2024-04-09 22:29:17,389 - gensim.topic_coherence.text_analysis - INFO - 84 batches submitted to accumulate stats from 5376 documents (138141 virtual)
2024-04-09 22:29:17,404 - gensim.topic_coherence.text_analysis - INFO - 85 batches submitted to accumulate stats from 5440 documents (139767 virtual)
2024-04-09 22:29:17,404 - gensim.topic_coherence.text_analysis - INFO - 86 batches submitted to accumulate stats from 5504 documents (141151 virtual)
2024-04-09 22:29:17,404 - gensim.topic_coherence.text_analysis - INFO - 87 batches submitted to accumulate stats from 5568 documents (142647 virtual)
2024-04-09 22:29:17,404 - gensim.topic_coherence.text_analysis - INFO - 88 batches submitted to accumulate stats from 5632 documents (144175 virtual)
2024-04-09 22:29:17,404 - gensim.topic_coherence.text_analysis - INFO - 89 batches submitted to accumulate stats from 5696 documents (145825 virtual)
2024-04-09 22:29:17,404 - gensim.topic_coherence.text_analysis - INFO - 90 batches submitted to accumulate stats from 5760 documents (147317 virtual)
2024-04-09 22:29:17,420 - gensim.topic_coherence.text_analysis - INFO - 91 batches submitted to accumulate stats from 5824 documents (149158 virtual)
2024-04-09 22:29:17,436 - gensim.topic_coherence.text_analysis - INFO - 92 batches submitted to accumulate stats from 5888 documents (150755 virtual)
2024-04-09 22:29:17,451 - gensim.topic_coherence.text_analysis - INFO - 93 batches submitted to accumulate stats from 5952 documents (152237 virtual)
2024-04-09 22:29:17,467 - gensim.topic_coherence.text_analysis - INFO - 94 batches submitted to accumulate stats from 6016 documents (154013 virtual)
2024-04-09 22:29:17,467 - gensim.topic_coherence.text_analysis - INFO - 95 batches submitted to accumulate stats from 6080 documents (155593 virtual)
2024-04-09 22:29:17,467 - gensim.topic_coherence.text_analysis - INFO - 96 batches submitted to accumulate stats from 6144 documents (157114 virtual)
2024-04-09 22:29:17,467 - gensim.topic_coherence.text_analysis - INFO - 97 batches submitted to accumulate stats from 6208 documents (158783 virtual)
2024-04-09 22:29:17,467 - gensim.topic_coherence.text_analysis - INFO - 98 batches submitted to accumulate stats from 6272 documents (160467 virtual)
2024-04-09 22:29:17,467 - gensim.topic_coherence.text_analysis - INFO - 99 batches submitted to accumulate stats from 6336 documents (162287 virtual)
2024-04-09 22:29:17,483 - gensim.topic_coherence.text_analysis - INFO - 100 batches submitted to accumulate stats from 6400 documents (163932 virtual)
2024-04-09 22:29:17,483 - gensim.topic_coherence.text_analysis - INFO - 101 batches submitted to accumulate stats from 6464 documents (165414 virtual)
2024-04-09 22:29:17,483 - gensim.topic_coherence.text_analysis - INFO - 102 batches submitted to accumulate stats from 6528 documents (166934 virtual)
2024-04-09 22:29:17,483 - gensim.topic_coherence.text_analysis - INFO - 103 batches submitted to accumulate stats from 6592 documents (168399 virtual)
2024-04-09 22:29:17,483 - gensim.topic_coherence.text_analysis - INFO - 104 batches submitted to accumulate stats from 6656 documents (170596 virtual)
2024-04-09 22:29:17,498 - gensim.topic_coherence.text_analysis - INFO - 105 batches submitted to accumulate stats from 6720 documents (172319 virtual)
2024-04-09 22:29:17,514 - gensim.topic_coherence.text_analysis - INFO - 106 batches submitted to accumulate stats from 6784 documents (173973 virtual)
2024-04-09 22:29:17,514 - gensim.topic_coherence.text_analysis - INFO - 107 batches submitted to accumulate stats from 6848 documents (175817 virtual)
2024-04-09 22:29:17,514 - gensim.topic_coherence.text_analysis - INFO - 108 batches submitted to accumulate stats from 6912 documents (177402 virtual)
2024-04-09 22:29:17,514 - gensim.topic_coherence.text_analysis - INFO - 109 batches submitted to accumulate stats from 6976 documents (179106 virtual)
2024-04-09 22:29:17,514 - gensim.topic_coherence.text_analysis - INFO - 110 batches submitted to accumulate stats from 7040 documents (181089 virtual)
2024-04-09 22:29:17,545 - gensim.topic_coherence.text_analysis - INFO - 111 batches submitted to accumulate stats from 7104 documents (182660 virtual)
2024-04-09 22:29:17,561 - gensim.topic_coherence.text_analysis - INFO - 112 batches submitted to accumulate stats from 7168 documents (184289 virtual)
2024-04-09 22:29:17,608 - gensim.topic_coherence.text_analysis - INFO - 113 batches submitted to accumulate stats from 7232 documents (185825 virtual)
2024-04-09 22:29:17,608 - gensim.topic_coherence.text_analysis - INFO - 114 batches submitted to accumulate stats from 7296 documents (187420 virtual)
2024-04-09 22:29:17,608 - gensim.topic_coherence.text_analysis - INFO - 115 batches submitted to accumulate stats from 7360 documents (189102 virtual)
2024-04-09 22:29:17,623 - gensim.topic_coherence.text_analysis - INFO - 116 batches submitted to accumulate stats from 7424 documents (190745 virtual)
2024-04-09 22:29:17,623 - gensim.topic_coherence.text_analysis - INFO - 117 batches submitted to accumulate stats from 7488 documents (192238 virtual)
2024-04-09 22:29:17,639 - gensim.topic_coherence.text_analysis - INFO - 118 batches submitted to accumulate stats from 7552 documents (194107 virtual)
2024-04-09 22:29:17,639 - gensim.topic_coherence.text_analysis - INFO - 119 batches submitted to accumulate stats from 7616 documents (195570 virtual)
2024-04-09 22:29:17,639 - gensim.topic_coherence.text_analysis - INFO - 120 batches submitted to accumulate stats from 7680 documents (197064 virtual)
2024-04-09 22:29:17,639 - gensim.topic_coherence.text_analysis - INFO - 121 batches submitted to accumulate stats from 7744 documents (198821 virtual)
2024-04-09 22:29:17,639 - gensim.topic_coherence.text_analysis - INFO - 122 batches submitted to accumulate stats from 7808 documents (200394 virtual)
2024-04-09 22:29:17,639 - gensim.topic_coherence.text_analysis - INFO - 123 batches submitted to accumulate stats from 7872 documents (202352 virtual)
2024-04-09 22:29:17,639 - gensim.topic_coherence.text_analysis - INFO - 124 batches submitted to accumulate stats from 7936 documents (204181 virtual)
2024-04-09 22:29:17,639 - gensim.topic_coherence.text_analysis - INFO - 125 batches submitted to accumulate stats from 8000 documents (206063 virtual)
2024-04-09 22:29:17,654 - gensim.topic_coherence.text_analysis - INFO - 126 batches submitted to accumulate stats from 8064 documents (207766 virtual)
2024-04-09 22:29:17,654 - gensim.topic_coherence.text_analysis - INFO - 127 batches submitted to accumulate stats from 8128 documents (209460 virtual)
2024-04-09 22:29:17,654 - gensim.topic_coherence.text_analysis - INFO - 128 batches submitted to accumulate stats from 8192 documents (211022 virtual)
2024-04-09 22:29:17,670 - gensim.topic_coherence.text_analysis - INFO - 129 batches submitted to accumulate stats from 8256 documents (212632 virtual)
2024-04-09 22:29:17,686 - gensim.topic_coherence.text_analysis - INFO - 130 batches submitted to accumulate stats from 8320 documents (214210 virtual)
2024-04-09 22:29:17,686 - gensim.topic_coherence.text_analysis - INFO - 131 batches submitted to accumulate stats from 8384 documents (215651 virtual)
2024-04-09 22:29:17,686 - gensim.topic_coherence.text_analysis - INFO - 132 batches submitted to accumulate stats from 8448 documents (217300 virtual)
2024-04-09 22:29:17,686 - gensim.topic_coherence.text_analysis - INFO - 133 batches submitted to accumulate stats from 8512 documents (219035 virtual)
2024-04-09 22:29:17,686 - gensim.topic_coherence.text_analysis - INFO - 134 batches submitted to accumulate stats from 8576 documents (220675 virtual)
2024-04-09 22:29:17,701 - gensim.topic_coherence.text_analysis - INFO - 135 batches submitted to accumulate stats from 8640 documents (222562 virtual)
2024-04-09 22:29:17,701 - gensim.topic_coherence.text_analysis - INFO - 136 batches submitted to accumulate stats from 8704 documents (224243 virtual)
2024-04-09 22:29:17,701 - gensim.topic_coherence.text_analysis - INFO - 137 batches submitted to accumulate stats from 8768 documents (225942 virtual)
2024-04-09 22:29:17,701 - gensim.topic_coherence.text_analysis - INFO - 138 batches submitted to accumulate stats from 8832 documents (227774 virtual)
2024-04-09 22:29:17,717 - gensim.topic_coherence.text_analysis - INFO - 139 batches submitted to accumulate stats from 8896 documents (229378 virtual)
2024-04-09 22:29:17,717 - gensim.topic_coherence.text_analysis - INFO - 140 batches submitted to accumulate stats from 8960 documents (231026 virtual)
2024-04-09 22:29:17,733 - gensim.topic_coherence.text_analysis - INFO - 141 batches submitted to accumulate stats from 9024 documents (232664 virtual)
2024-04-09 22:29:17,733 - gensim.topic_coherence.text_analysis - INFO - 142 batches submitted to accumulate stats from 9088 documents (234376 virtual)
2024-04-09 22:29:17,733 - gensim.topic_coherence.text_analysis - INFO - 143 batches submitted to accumulate stats from 9152 documents (236034 virtual)
2024-04-09 22:29:17,764 - gensim.topic_coherence.text_analysis - INFO - 144 batches submitted to accumulate stats from 9216 documents (237753 virtual)
2024-04-09 22:29:17,764 - gensim.topic_coherence.text_analysis - INFO - 145 batches submitted to accumulate stats from 9280 documents (239260 virtual)
2024-04-09 22:29:17,764 - gensim.topic_coherence.text_analysis - INFO - 146 batches submitted to accumulate stats from 9344 documents (240889 virtual)
2024-04-09 22:29:17,779 - gensim.topic_coherence.text_analysis - INFO - 147 batches submitted to accumulate stats from 9408 documents (242607 virtual)
2024-04-09 22:29:17,779 - gensim.topic_coherence.text_analysis - INFO - 148 batches submitted to accumulate stats from 9472 documents (244168 virtual)
2024-04-09 22:29:17,779 - gensim.topic_coherence.text_analysis - INFO - 149 batches submitted to accumulate stats from 9536 documents (245827 virtual)
2024-04-09 22:29:17,795 - gensim.topic_coherence.text_analysis - INFO - 150 batches submitted to accumulate stats from 9600 documents (247340 virtual)
2024-04-09 22:29:17,795 - gensim.topic_coherence.text_analysis - INFO - 151 batches submitted to accumulate stats from 9664 documents (248935 virtual)
2024-04-09 22:29:17,811 - gensim.topic_coherence.text_analysis - INFO - 152 batches submitted to accumulate stats from 9728 documents (250682 virtual)
2024-04-09 22:29:17,826 - gensim.topic_coherence.text_analysis - INFO - 153 batches submitted to accumulate stats from 9792 documents (252525 virtual)
2024-04-09 22:29:17,826 - gensim.topic_coherence.text_analysis - INFO - 154 batches submitted to accumulate stats from 9856 documents (253964 virtual)
2024-04-09 22:29:17,826 - gensim.topic_coherence.text_analysis - INFO - 155 batches submitted to accumulate stats from 9920 documents (255761 virtual)
2024-04-09 22:29:17,842 - gensim.topic_coherence.text_analysis - INFO - 156 batches submitted to accumulate stats from 9984 documents (257656 virtual)
2024-04-09 22:29:17,842 - gensim.topic_coherence.text_analysis - INFO - 157 batches submitted to accumulate stats from 10048 documents (259320 virtual)
2024-04-09 22:29:17,858 - gensim.topic_coherence.text_analysis - INFO - 158 batches submitted to accumulate stats from 10112 documents (261193 virtual)
2024-04-09 22:29:17,858 - gensim.topic_coherence.text_analysis - INFO - 159 batches submitted to accumulate stats from 10176 documents (262723 virtual)
2024-04-09 22:29:17,858 - gensim.topic_coherence.text_analysis - INFO - 160 batches submitted to accumulate stats from 10240 documents (264223 virtual)
2024-04-09 22:29:17,858 - gensim.topic_coherence.text_analysis - INFO - 161 batches submitted to accumulate stats from 10304 documents (265694 virtual)
2024-04-09 22:29:17,873 - gensim.topic_coherence.text_analysis - INFO - 162 batches submitted to accumulate stats from 10368 documents (267448 virtual)
2024-04-09 22:29:18,077 - gensim.topic_coherence.text_analysis - INFO - 163 batches submitted to accumulate stats from 10432 documents (269251 virtual)
2024-04-09 22:29:18,077 - gensim.topic_coherence.text_analysis - INFO - 164 batches submitted to accumulate stats from 10496 documents (270973 virtual)
2024-04-09 22:29:18,077 - gensim.topic_coherence.text_analysis - INFO - 165 batches submitted to accumulate stats from 10560 documents (272683 virtual)
2024-04-09 22:29:18,077 - gensim.topic_coherence.text_analysis - INFO - 166 batches submitted to accumulate stats from 10624 documents (274294 virtual)
2024-04-09 22:29:18,077 - gensim.topic_coherence.text_analysis - INFO - 167 batches submitted to accumulate stats from 10688 documents (276045 virtual)
2024-04-09 22:29:18,077 - gensim.topic_coherence.text_analysis - INFO - 168 batches submitted to accumulate stats from 10752 documents (277496 virtual)
2024-04-09 22:29:18,077 - gensim.topic_coherence.text_analysis - INFO - 169 batches submitted to accumulate stats from 10816 documents (279131 virtual)
2024-04-09 22:29:18,092 - gensim.topic_coherence.text_analysis - INFO - 170 batches submitted to accumulate stats from 10880 documents (280812 virtual)
2024-04-09 22:29:18,092 - gensim.topic_coherence.text_analysis - INFO - 171 batches submitted to accumulate stats from 10944 documents (282408 virtual)
2024-04-09 22:29:18,124 - gensim.topic_coherence.text_analysis - INFO - 172 batches submitted to accumulate stats from 11008 documents (284125 virtual)
2024-04-09 22:29:18,139 - gensim.topic_coherence.text_analysis - INFO - 173 batches submitted to accumulate stats from 11072 documents (285575 virtual)
2024-04-09 22:29:18,139 - gensim.topic_coherence.text_analysis - INFO - 174 batches submitted to accumulate stats from 11136 documents (287185 virtual)
2024-04-09 22:29:18,139 - gensim.topic_coherence.text_analysis - INFO - 175 batches submitted to accumulate stats from 11200 documents (289145 virtual)
2024-04-09 22:29:18,139 - gensim.topic_coherence.text_analysis - INFO - 176 batches submitted to accumulate stats from 11264 documents (290900 virtual)
2024-04-09 22:29:18,139 - gensim.topic_coherence.text_analysis - INFO - 177 batches submitted to accumulate stats from 11328 documents (292686 virtual)
2024-04-09 22:29:18,139 - gensim.topic_coherence.text_analysis - INFO - 178 batches submitted to accumulate stats from 11392 documents (294505 virtual)
2024-04-09 22:29:18,139 - gensim.topic_coherence.text_analysis - INFO - 179 batches submitted to accumulate stats from 11456 documents (296236 virtual)
2024-04-09 22:29:18,155 - gensim.topic_coherence.text_analysis - INFO - 180 batches submitted to accumulate stats from 11520 documents (297647 virtual)
2024-04-09 22:29:18,170 - gensim.topic_coherence.text_analysis - INFO - 181 batches submitted to accumulate stats from 11584 documents (299327 virtual)
2024-04-09 22:29:18,186 - gensim.topic_coherence.text_analysis - INFO - 182 batches submitted to accumulate stats from 11648 documents (300853 virtual)
2024-04-09 22:29:18,186 - gensim.topic_coherence.text_analysis - INFO - 183 batches submitted to accumulate stats from 11712 documents (302601 virtual)
2024-04-09 22:29:18,202 - gensim.topic_coherence.text_analysis - INFO - 184 batches submitted to accumulate stats from 11776 documents (304181 virtual)
2024-04-09 22:29:18,202 - gensim.topic_coherence.text_analysis - INFO - 185 batches submitted to accumulate stats from 11840 documents (305710 virtual)
2024-04-09 22:29:18,202 - gensim.topic_coherence.text_analysis - INFO - 186 batches submitted to accumulate stats from 11904 documents (307265 virtual)
2024-04-09 22:29:18,217 - gensim.topic_coherence.text_analysis - INFO - 187 batches submitted to accumulate stats from 11968 documents (309148 virtual)
2024-04-09 22:29:18,217 - gensim.topic_coherence.text_analysis - INFO - 188 batches submitted to accumulate stats from 12032 documents (310818 virtual)
2024-04-09 22:29:18,217 - gensim.topic_coherence.text_analysis - INFO - 189 batches submitted to accumulate stats from 12096 documents (312472 virtual)
2024-04-09 22:29:18,233 - gensim.topic_coherence.text_analysis - INFO - 190 batches submitted to accumulate stats from 12160 documents (314119 virtual)
2024-04-09 22:29:18,233 - gensim.topic_coherence.text_analysis - INFO - 191 batches submitted to accumulate stats from 12224 documents (316054 virtual)
2024-04-09 22:29:18,249 - gensim.topic_coherence.text_analysis - INFO - 192 batches submitted to accumulate stats from 12288 documents (317685 virtual)
2024-04-09 22:29:18,249 - gensim.topic_coherence.text_analysis - INFO - 193 batches submitted to accumulate stats from 12352 documents (319351 virtual)
2024-04-09 22:29:18,249 - gensim.topic_coherence.text_analysis - INFO - 194 batches submitted to accumulate stats from 12416 documents (321100 virtual)
2024-04-09 22:29:18,249 - gensim.topic_coherence.text_analysis - INFO - 195 batches submitted to accumulate stats from 12480 documents (323099 virtual)
2024-04-09 22:29:18,249 - gensim.topic_coherence.text_analysis - INFO - 196 batches submitted to accumulate stats from 12544 documents (324548 virtual)
2024-04-09 22:29:18,249 - gensim.topic_coherence.text_analysis - INFO - 197 batches submitted to accumulate stats from 12608 documents (326231 virtual)
2024-04-09 22:29:18,264 - gensim.topic_coherence.text_analysis - INFO - 198 batches submitted to accumulate stats from 12672 documents (327851 virtual)
2024-04-09 22:29:18,264 - gensim.topic_coherence.text_analysis - INFO - 199 batches submitted to accumulate stats from 12736 documents (329386 virtual)
2024-04-09 22:29:18,264 - gensim.topic_coherence.text_analysis - INFO - 200 batches submitted to accumulate stats from 12800 documents (331113 virtual)
2024-04-09 22:29:18,264 - gensim.topic_coherence.text_analysis - INFO - 201 batches submitted to accumulate stats from 12864 documents (332712 virtual)
2024-04-09 22:29:18,264 - gensim.topic_coherence.text_analysis - INFO - 202 batches submitted to accumulate stats from 12928 documents (334135 virtual)
2024-04-09 22:29:18,280 - gensim.topic_coherence.text_analysis - INFO - 203 batches submitted to accumulate stats from 12992 documents (335914 virtual)
2024-04-09 22:29:18,280 - gensim.topic_coherence.text_analysis - INFO - 204 batches submitted to accumulate stats from 13056 documents (337641 virtual)
2024-04-09 22:29:18,280 - gensim.topic_coherence.text_analysis - INFO - 205 batches submitted to accumulate stats from 13120 documents (339449 virtual)
2024-04-09 22:29:18,280 - gensim.topic_coherence.text_analysis - INFO - 206 batches submitted to accumulate stats from 13184 documents (341168 virtual)
2024-04-09 22:29:18,280 - gensim.topic_coherence.text_analysis - INFO - 207 batches submitted to accumulate stats from 13248 documents (342833 virtual)
2024-04-09 22:29:18,295 - gensim.topic_coherence.text_analysis - INFO - 208 batches submitted to accumulate stats from 13312 documents (344704 virtual)
2024-04-09 22:29:18,295 - gensim.topic_coherence.text_analysis - INFO - 209 batches submitted to accumulate stats from 13376 documents (346650 virtual)
2024-04-09 22:29:18,295 - gensim.topic_coherence.text_analysis - INFO - 210 batches submitted to accumulate stats from 13440 documents (348531 virtual)
2024-04-09 22:29:18,295 - gensim.topic_coherence.text_analysis - INFO - 211 batches submitted to accumulate stats from 13504 documents (350342 virtual)
2024-04-09 22:29:18,295 - gensim.topic_coherence.text_analysis - INFO - 212 batches submitted to accumulate stats from 13568 documents (352068 virtual)
2024-04-09 22:29:18,311 - gensim.topic_coherence.text_analysis - INFO - 213 batches submitted to accumulate stats from 13632 documents (353789 virtual)
2024-04-09 22:29:18,311 - gensim.topic_coherence.text_analysis - INFO - 214 batches submitted to accumulate stats from 13696 documents (355216 virtual)
2024-04-09 22:29:18,342 - gensim.topic_coherence.text_analysis - INFO - 215 batches submitted to accumulate stats from 13760 documents (356990 virtual)
2024-04-09 22:29:18,389 - gensim.topic_coherence.text_analysis - INFO - 216 batches submitted to accumulate stats from 13824 documents (358762 virtual)
2024-04-09 22:29:18,405 - gensim.topic_coherence.text_analysis - INFO - 217 batches submitted to accumulate stats from 13888 documents (360320 virtual)
2024-04-09 22:29:18,405 - gensim.topic_coherence.text_analysis - INFO - 218 batches submitted to accumulate stats from 13952 documents (361867 virtual)
2024-04-09 22:29:18,420 - gensim.topic_coherence.text_analysis - INFO - 219 batches submitted to accumulate stats from 14016 documents (363519 virtual)
2024-04-09 22:29:18,452 - gensim.topic_coherence.text_analysis - INFO - 220 batches submitted to accumulate stats from 14080 documents (365141 virtual)
2024-04-09 22:29:18,514 - gensim.topic_coherence.text_analysis - INFO - 221 batches submitted to accumulate stats from 14144 documents (366934 virtual)
2024-04-09 22:29:18,530 - gensim.topic_coherence.text_analysis - INFO - 222 batches submitted to accumulate stats from 14208 documents (368448 virtual)
2024-04-09 22:29:18,624 - gensim.topic_coherence.text_analysis - INFO - 223 batches submitted to accumulate stats from 14272 documents (370012 virtual)
2024-04-09 22:29:18,670 - gensim.topic_coherence.text_analysis - INFO - 224 batches submitted to accumulate stats from 14336 documents (371704 virtual)
2024-04-09 22:29:18,686 - gensim.topic_coherence.text_analysis - INFO - 225 batches submitted to accumulate stats from 14400 documents (373331 virtual)
2024-04-09 22:29:18,686 - gensim.topic_coherence.text_analysis - INFO - 226 batches submitted to accumulate stats from 14464 documents (375174 virtual)
2024-04-09 22:29:18,717 - gensim.topic_coherence.text_analysis - INFO - 227 batches submitted to accumulate stats from 14528 documents (377135 virtual)
2024-04-09 22:29:18,717 - gensim.topic_coherence.text_analysis - INFO - 228 batches submitted to accumulate stats from 14592 documents (378860 virtual)
2024-04-09 22:29:18,733 - gensim.topic_coherence.text_analysis - INFO - 229 batches submitted to accumulate stats from 14656 documents (380581 virtual)
2024-04-09 22:29:18,764 - gensim.topic_coherence.text_analysis - INFO - 230 batches submitted to accumulate stats from 14720 documents (382278 virtual)
2024-04-09 22:29:18,764 - gensim.topic_coherence.text_analysis - INFO - 231 batches submitted to accumulate stats from 14784 documents (383970 virtual)
2024-04-09 22:29:18,780 - gensim.topic_coherence.text_analysis - INFO - 232 batches submitted to accumulate stats from 14848 documents (385680 virtual)
2024-04-09 22:29:18,780 - gensim.topic_coherence.text_analysis - INFO - 233 batches submitted to accumulate stats from 14912 documents (387378 virtual)
2024-04-09 22:29:18,795 - gensim.topic_coherence.text_analysis - INFO - 234 batches submitted to accumulate stats from 14976 documents (388908 virtual)
2024-04-09 22:29:18,811 - gensim.topic_coherence.text_analysis - INFO - 235 batches submitted to accumulate stats from 15040 documents (390473 virtual)
2024-04-09 22:29:18,827 - gensim.topic_coherence.text_analysis - INFO - 236 batches submitted to accumulate stats from 15104 documents (391966 virtual)
2024-04-09 22:29:18,827 - gensim.topic_coherence.text_analysis - INFO - 237 batches submitted to accumulate stats from 15168 documents (393446 virtual)
2024-04-09 22:29:18,827 - gensim.topic_coherence.text_analysis - INFO - 238 batches submitted to accumulate stats from 15232 documents (394979 virtual)
2024-04-09 22:29:18,827 - gensim.topic_coherence.text_analysis - INFO - 239 batches submitted to accumulate stats from 15296 documents (396707 virtual)
2024-04-09 22:29:18,858 - gensim.topic_coherence.text_analysis - INFO - 240 batches submitted to accumulate stats from 15360 documents (398434 virtual)
2024-04-09 22:29:18,858 - gensim.topic_coherence.text_analysis - INFO - 241 batches submitted to accumulate stats from 15424 documents (400092 virtual)
2024-04-09 22:29:18,874 - gensim.topic_coherence.text_analysis - INFO - 242 batches submitted to accumulate stats from 15488 documents (402054 virtual)
2024-04-09 22:29:18,905 - gensim.topic_coherence.text_analysis - INFO - 243 batches submitted to accumulate stats from 15552 documents (403465 virtual)
2024-04-09 22:29:18,920 - gensim.topic_coherence.text_analysis - INFO - 244 batches submitted to accumulate stats from 15616 documents (405289 virtual)
2024-04-09 22:29:18,920 - gensim.topic_coherence.text_analysis - INFO - 245 batches submitted to accumulate stats from 15680 documents (406750 virtual)
2024-04-09 22:29:18,920 - gensim.topic_coherence.text_analysis - INFO - 246 batches submitted to accumulate stats from 15744 documents (408306 virtual)
2024-04-09 22:29:18,936 - gensim.topic_coherence.text_analysis - INFO - 247 batches submitted to accumulate stats from 15808 documents (410097 virtual)
2024-04-09 22:29:18,936 - gensim.topic_coherence.text_analysis - INFO - 248 batches submitted to accumulate stats from 15872 documents (411915 virtual)
2024-04-09 22:29:18,936 - gensim.topic_coherence.text_analysis - INFO - 249 batches submitted to accumulate stats from 15936 documents (413538 virtual)
2024-04-09 22:29:18,936 - gensim.topic_coherence.text_analysis - INFO - 250 batches submitted to accumulate stats from 16000 documents (415307 virtual)
2024-04-09 22:29:18,952 - gensim.topic_coherence.text_analysis - INFO - 251 batches submitted to accumulate stats from 16064 documents (416993 virtual)
2024-04-09 22:29:18,983 - gensim.topic_coherence.text_analysis - INFO - 252 batches submitted to accumulate stats from 16128 documents (418582 virtual)
2024-04-09 22:29:18,999 - gensim.topic_coherence.text_analysis - INFO - 253 batches submitted to accumulate stats from 16192 documents (420282 virtual)
2024-04-09 22:29:19,483 - gensim.topic_coherence.text_analysis - INFO - 254 batches submitted to accumulate stats from 16256 documents (421859 virtual)
2024-04-09 22:29:19,483 - gensim.topic_coherence.text_analysis - INFO - 255 batches submitted to accumulate stats from 16320 documents (423441 virtual)
2024-04-09 22:29:19,483 - gensim.topic_coherence.text_analysis - INFO - 256 batches submitted to accumulate stats from 16384 documents (425169 virtual)
2024-04-09 22:29:19,483 - gensim.topic_coherence.text_analysis - INFO - 257 batches submitted to accumulate stats from 16448 documents (426804 virtual)
2024-04-09 22:29:19,483 - gensim.topic_coherence.text_analysis - INFO - 258 batches submitted to accumulate stats from 16512 documents (428503 virtual)
2024-04-09 22:29:19,483 - gensim.topic_coherence.text_analysis - INFO - 259 batches submitted to accumulate stats from 16576 documents (430063 virtual)
2024-04-09 22:29:19,483 - gensim.topic_coherence.text_analysis - INFO - 260 batches submitted to accumulate stats from 16640 documents (431672 virtual)
2024-04-09 22:29:19,499 - gensim.topic_coherence.text_analysis - INFO - 261 batches submitted to accumulate stats from 16704 documents (433298 virtual)
2024-04-09 22:29:19,514 - gensim.topic_coherence.text_analysis - INFO - 262 batches submitted to accumulate stats from 16768 documents (434951 virtual)
2024-04-09 22:29:19,514 - gensim.topic_coherence.text_analysis - INFO - 263 batches submitted to accumulate stats from 16832 documents (436505 virtual)
2024-04-09 22:29:19,545 - gensim.topic_coherence.text_analysis - INFO - 264 batches submitted to accumulate stats from 16896 documents (438134 virtual)
2024-04-09 22:29:19,545 - gensim.topic_coherence.text_analysis - INFO - 265 batches submitted to accumulate stats from 16960 documents (439831 virtual)
2024-04-09 22:29:19,545 - gensim.topic_coherence.text_analysis - INFO - 266 batches submitted to accumulate stats from 17024 documents (441666 virtual)
2024-04-09 22:29:19,545 - gensim.topic_coherence.text_analysis - INFO - 267 batches submitted to accumulate stats from 17088 documents (443389 virtual)
2024-04-09 22:29:19,561 - gensim.topic_coherence.text_analysis - INFO - 268 batches submitted to accumulate stats from 17152 documents (445008 virtual)
2024-04-09 22:29:19,561 - gensim.topic_coherence.text_analysis - INFO - 269 batches submitted to accumulate stats from 17216 documents (446691 virtual)
2024-04-09 22:29:19,561 - gensim.topic_coherence.text_analysis - INFO - 270 batches submitted to accumulate stats from 17280 documents (448258 virtual)
2024-04-09 22:29:19,577 - gensim.topic_coherence.text_analysis - INFO - 271 batches submitted to accumulate stats from 17344 documents (450177 virtual)
2024-04-09 22:29:19,592 - gensim.topic_coherence.text_analysis - INFO - 272 batches submitted to accumulate stats from 17408 documents (451838 virtual)
2024-04-09 22:29:19,592 - gensim.topic_coherence.text_analysis - INFO - 273 batches submitted to accumulate stats from 17472 documents (453765 virtual)
2024-04-09 22:29:19,592 - gensim.topic_coherence.text_analysis - INFO - 274 batches submitted to accumulate stats from 17536 documents (455302 virtual)
2024-04-09 22:29:19,608 - gensim.topic_coherence.text_analysis - INFO - 275 batches submitted to accumulate stats from 17600 documents (457026 virtual)
2024-04-09 22:29:19,624 - gensim.topic_coherence.text_analysis - INFO - 276 batches submitted to accumulate stats from 17664 documents (458678 virtual)
2024-04-09 22:29:19,624 - gensim.topic_coherence.text_analysis - INFO - 277 batches submitted to accumulate stats from 17728 documents (460363 virtual)
2024-04-09 22:29:19,639 - gensim.topic_coherence.text_analysis - INFO - 278 batches submitted to accumulate stats from 17792 documents (462290 virtual)
2024-04-09 22:29:19,639 - gensim.topic_coherence.text_analysis - INFO - 279 batches submitted to accumulate stats from 17856 documents (464043 virtual)
2024-04-09 22:29:19,655 - gensim.topic_coherence.text_analysis - INFO - 280 batches submitted to accumulate stats from 17920 documents (465601 virtual)
2024-04-09 22:29:19,670 - gensim.topic_coherence.text_analysis - INFO - 281 batches submitted to accumulate stats from 17984 documents (467208 virtual)
2024-04-09 22:29:19,686 - gensim.topic_coherence.text_analysis - INFO - 282 batches submitted to accumulate stats from 18048 documents (468985 virtual)
2024-04-09 22:29:19,686 - gensim.topic_coherence.text_analysis - INFO - 283 batches submitted to accumulate stats from 18112 documents (470540 virtual)
2024-04-09 22:29:19,702 - gensim.topic_coherence.text_analysis - INFO - 284 batches submitted to accumulate stats from 18176 documents (472187 virtual)
2024-04-09 22:29:19,702 - gensim.topic_coherence.text_analysis - INFO - 285 batches submitted to accumulate stats from 18240 documents (473657 virtual)
2024-04-09 22:29:19,717 - gensim.topic_coherence.text_analysis - INFO - 286 batches submitted to accumulate stats from 18304 documents (475433 virtual)
2024-04-09 22:29:19,733 - gensim.topic_coherence.text_analysis - INFO - 287 batches submitted to accumulate stats from 18368 documents (476949 virtual)
2024-04-09 22:29:19,733 - gensim.topic_coherence.text_analysis - INFO - 288 batches submitted to accumulate stats from 18432 documents (478519 virtual)
2024-04-09 22:29:19,749 - gensim.topic_coherence.text_analysis - INFO - 289 batches submitted to accumulate stats from 18496 documents (480346 virtual)
2024-04-09 22:29:19,749 - gensim.topic_coherence.text_analysis - INFO - 290 batches submitted to accumulate stats from 18560 documents (482113 virtual)
2024-04-09 22:29:19,749 - gensim.topic_coherence.text_analysis - INFO - 291 batches submitted to accumulate stats from 18624 documents (483726 virtual)
2024-04-09 22:29:19,764 - gensim.topic_coherence.text_analysis - INFO - 292 batches submitted to accumulate stats from 18688 documents (485454 virtual)
2024-04-09 22:29:19,764 - gensim.topic_coherence.text_analysis - INFO - 293 batches submitted to accumulate stats from 18752 documents (487120 virtual)
2024-04-09 22:29:19,780 - gensim.topic_coherence.text_analysis - INFO - 294 batches submitted to accumulate stats from 18816 documents (488702 virtual)
2024-04-09 22:29:19,795 - gensim.topic_coherence.text_analysis - INFO - 295 batches submitted to accumulate stats from 18880 documents (490511 virtual)
2024-04-09 22:29:19,795 - gensim.topic_coherence.text_analysis - INFO - 296 batches submitted to accumulate stats from 18944 documents (492226 virtual)
2024-04-09 22:29:19,811 - gensim.topic_coherence.text_analysis - INFO - 297 batches submitted to accumulate stats from 19008 documents (494165 virtual)
2024-04-09 22:29:19,811 - gensim.topic_coherence.text_analysis - INFO - 298 batches submitted to accumulate stats from 19072 documents (495678 virtual)
2024-04-09 22:29:19,811 - gensim.topic_coherence.text_analysis - INFO - 299 batches submitted to accumulate stats from 19136 documents (497503 virtual)
2024-04-09 22:29:19,827 - gensim.topic_coherence.text_analysis - INFO - 300 batches submitted to accumulate stats from 19200 documents (499131 virtual)
2024-04-09 22:29:19,827 - gensim.topic_coherence.text_analysis - INFO - 301 batches submitted to accumulate stats from 19264 documents (500986 virtual)
2024-04-09 22:29:19,827 - gensim.topic_coherence.text_analysis - INFO - 302 batches submitted to accumulate stats from 19328 documents (502687 virtual)
2024-04-09 22:29:19,827 - gensim.topic_coherence.text_analysis - INFO - 303 batches submitted to accumulate stats from 19392 documents (504106 virtual)
2024-04-09 22:29:19,842 - gensim.topic_coherence.text_analysis - INFO - 304 batches submitted to accumulate stats from 19456 documents (505909 virtual)
2024-04-09 22:29:19,858 - gensim.topic_coherence.text_analysis - INFO - 305 batches submitted to accumulate stats from 19520 documents (507546 virtual)
2024-04-09 22:29:19,858 - gensim.topic_coherence.text_analysis - INFO - 306 batches submitted to accumulate stats from 19584 documents (509312 virtual)
2024-04-09 22:29:19,874 - gensim.topic_coherence.text_analysis - INFO - 307 batches submitted to accumulate stats from 19648 documents (510954 virtual)
2024-04-09 22:29:19,874 - gensim.topic_coherence.text_analysis - INFO - 308 batches submitted to accumulate stats from 19712 documents (512465 virtual)
2024-04-09 22:29:19,874 - gensim.topic_coherence.text_analysis - INFO - 309 batches submitted to accumulate stats from 19776 documents (514132 virtual)
2024-04-09 22:29:19,889 - gensim.topic_coherence.text_analysis - INFO - 310 batches submitted to accumulate stats from 19840 documents (515846 virtual)
2024-04-09 22:29:19,889 - gensim.topic_coherence.text_analysis - INFO - 311 batches submitted to accumulate stats from 19904 documents (517436 virtual)
2024-04-09 22:29:19,905 - gensim.topic_coherence.text_analysis - INFO - 312 batches submitted to accumulate stats from 19968 documents (519076 virtual)
2024-04-09 22:29:19,905 - gensim.topic_coherence.text_analysis - INFO - 313 batches submitted to accumulate stats from 20032 documents (520797 virtual)
2024-04-09 22:29:19,905 - gensim.topic_coherence.text_analysis - INFO - 314 batches submitted to accumulate stats from 20096 documents (522328 virtual)
2024-04-09 22:29:20,592 - gensim.topic_coherence.text_analysis - INFO - 11 accumulators retrieved from output queue
2024-04-09 22:29:21,124 - gensim.topic_coherence.text_analysis - INFO - accumulated word occurrence stats for 523696 virtual documents
2024-04-09 22:29:22,233 - gensim.corpora.dictionary - INFO - adding document #0 to Dictionary<0 unique tokens: []>
2024-04-09 22:29:22,624 - gensim.corpora.dictionary - INFO - adding document #10000 to Dictionary<23643 unique tokens: ['-', '.', '?', 'a', 'africa']...>
2024-04-09 22:29:22,983 - gensim.corpora.dictionary - INFO - adding document #20000 to Dictionary<32428 unique tokens: ['-', '.', '?', 'a', 'africa']...>
2024-04-09 22:29:22,983 - gensim.corpora.dictionary - INFO - built Dictionary<32503 unique tokens: ['-', '.', '?', 'a', 'africa']...> from 20086 documents (total 703102 corpus positions)
2024-04-09 22:29:22,999 - gensim.utils - INFO - Dictionary lifecycle event {'msg': "built Dictionary<32503 unique tokens: ['-', '.', '?', 'a', 'africa']...> from 20086 documents (total 703102 corpus positions)", 'datetime': '2024-04-09T22:29:22.999488', 'gensim': '4.3.2', 'python': '3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}
2024-04-09 22:29:22,999 - gensim.topic_coherence.probability_estimation - INFO - using ParallelWordOccurrenceAccumulator<processes=11, batch_size=64> to estimate probabilities from sliding windows
2024-04-09 22:30:19,971 - gensim.topic_coherence.text_analysis - INFO - 11 accumulators retrieved from output queue
2024-04-09 22:30:20,096 - gensim.topic_coherence.text_analysis - INFO - accumulated word occurrence stats for 21620 virtual documents
2024-04-09 22:30:24,659 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,760,523
Freeze params: 0
2024-04-09 22:33:46,318 - trainer - INFO -     epoch          : 1
2024-04-09 22:33:46,318 - trainer - INFO -     loss           : 1.223616
2024-04-09 22:33:46,318 - trainer - INFO -     accuracy       : 0.652568
2024-04-09 22:33:46,319 - trainer - INFO -     macro_f        : 0.632749
2024-04-09 22:33:46,319 - trainer - INFO -     precision      : 0.662901
2024-04-09 22:33:46,319 - trainer - INFO -     recall         : 0.652568
2024-04-09 22:33:46,319 - trainer - INFO -     doc_entropy    : 2.46547
2024-04-09 22:33:46,319 - trainer - INFO -     val_loss       : 1.064926
2024-04-09 22:33:46,319 - trainer - INFO -     val_accuracy   : 0.686896
2024-04-09 22:33:46,319 - trainer - INFO -     val_macro_f    : 0.671385
2024-04-09 22:33:46,319 - trainer - INFO -     val_precision  : 0.702634
2024-04-09 22:33:46,319 - trainer - INFO -     val_recall     : 0.686896
2024-04-09 22:33:46,319 - trainer - INFO -     val_doc_entropy: 2.319956
2024-04-09 22:33:46,319 - trainer - INFO -     test_loss      : 1.068078
2024-04-09 22:33:46,319 - trainer - INFO -     test_accuracy  : 0.690531
2024-04-09 22:33:46,319 - trainer - INFO -     test_macro_f   : 0.676668
2024-04-09 22:33:46,319 - trainer - INFO -     test_precision : 0.710912
2024-04-09 22:33:46,319 - trainer - INFO -     test_recall    : 0.690531
2024-04-09 22:33:46,319 - trainer - INFO -     test_doc_entropy: 2.322687
2024-04-09 22:37:07,316 - trainer - INFO -     epoch          : 2
2024-04-09 22:37:07,316 - trainer - INFO -     loss           : 0.890723
2024-04-09 22:37:07,316 - trainer - INFO -     accuracy       : 0.738592
2024-04-09 22:37:07,316 - trainer - INFO -     macro_f        : 0.728735
2024-04-09 22:37:07,316 - trainer - INFO -     precision      : 0.762387
2024-04-09 22:37:07,316 - trainer - INFO -     recall         : 0.738592
2024-04-09 22:37:07,316 - trainer - INFO -     doc_entropy    : 2.18234
2024-04-09 22:37:07,316 - trainer - INFO -     val_loss       : 1.085813
2024-04-09 22:37:07,316 - trainer - INFO -     val_accuracy   : 0.685602
2024-04-09 22:37:07,316 - trainer - INFO -     val_macro_f    : 0.674023
2024-04-09 22:37:07,316 - trainer - INFO -     val_precision  : 0.712603
2024-04-09 22:37:07,316 - trainer - INFO -     val_recall     : 0.685602
2024-04-09 22:37:07,316 - trainer - INFO -     val_doc_entropy: 2.247012
2024-04-09 22:37:07,316 - trainer - INFO -     test_loss      : 1.092566
2024-04-09 22:37:07,316 - trainer - INFO -     test_accuracy  : 0.685602
2024-04-09 22:37:07,316 - trainer - INFO -     test_macro_f   : 0.674551
2024-04-09 22:37:07,316 - trainer - INFO -     test_precision : 0.71395
2024-04-09 22:37:07,316 - trainer - INFO -     test_recall    : 0.685602
2024-04-09 22:37:07,316 - trainer - INFO -     test_doc_entropy: 2.251724
2024-04-09 22:40:27,251 - trainer - INFO -     epoch          : 3
2024-04-09 22:40:27,251 - trainer - INFO -     loss           : 0.67839
2024-04-09 22:40:27,251 - trainer - INFO -     accuracy       : 0.798402
2024-04-09 22:40:27,251 - trainer - INFO -     macro_f        : 0.792308
2024-04-09 22:40:27,251 - trainer - INFO -     precision      : 0.822924
2024-04-09 22:40:27,251 - trainer - INFO -     recall         : 0.798402
2024-04-09 22:40:27,251 - trainer - INFO -     doc_entropy    : 2.017239
2024-04-09 22:40:27,251 - trainer - INFO -     val_loss       : 1.155579
2024-04-09 22:40:27,251 - trainer - INFO -     val_accuracy   : 0.679727
2024-04-09 22:40:27,251 - trainer - INFO -     val_macro_f    : 0.672537
2024-04-09 22:40:27,251 - trainer - INFO -     val_precision  : 0.713946
2024-04-09 22:40:27,251 - trainer - INFO -     val_recall     : 0.679727
2024-04-09 22:40:27,251 - trainer - INFO -     val_doc_entropy: 2.083366
2024-04-09 22:40:27,251 - trainer - INFO -     test_loss      : 1.150312
2024-04-09 22:40:27,251 - trainer - INFO -     test_accuracy  : 0.681619
2024-04-09 22:40:27,251 - trainer - INFO -     test_macro_f   : 0.674447
2024-04-09 22:40:27,251 - trainer - INFO -     test_precision : 0.715353
2024-04-09 22:40:27,251 - trainer - INFO -     test_recall    : 0.681619
2024-04-09 22:40:27,251 - trainer - INFO -     test_doc_entropy: 2.086809
2024-04-09 22:41:02,497 - gensim.corpora.dictionary - INFO - adding document #0 to Dictionary<0 unique tokens: []>
2024-04-09 22:41:02,856 - gensim.corpora.dictionary - INFO - adding document #10000 to Dictionary<23643 unique tokens: ['-', '.', '?', 'a', 'africa']...>
2024-04-09 22:41:03,263 - gensim.corpora.dictionary - INFO - adding document #20000 to Dictionary<32428 unique tokens: ['-', '.', '?', 'a', 'africa']...>
2024-04-09 22:41:03,263 - gensim.corpora.dictionary - INFO - built Dictionary<32503 unique tokens: ['-', '.', '?', 'a', 'africa']...> from 20086 documents (total 703102 corpus positions)
2024-04-09 22:41:03,263 - gensim.utils - INFO - Dictionary lifecycle event {'msg': "built Dictionary<32503 unique tokens: ['-', '.', '?', 'a', 'africa']...> from 20086 documents (total 703102 corpus positions)", 'datetime': '2024-04-09T22:41:03.263088', 'gensim': '4.3.2', 'python': '3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}
2024-04-09 22:41:03,278 - gensim.topic_coherence.probability_estimation - INFO - using ParallelWordOccurrenceAccumulator<processes=11, batch_size=64> to estimate probabilities from sliding windows
2024-04-09 22:42:01,988 - gensim.topic_coherence.text_analysis - INFO - 1 batches submitted to accumulate stats from 64 documents (1358 virtual)
2024-04-09 22:42:01,988 - gensim.topic_coherence.text_analysis - INFO - 2 batches submitted to accumulate stats from 128 documents (3142 virtual)
2024-04-09 22:42:02,004 - gensim.topic_coherence.text_analysis - INFO - 3 batches submitted to accumulate stats from 192 documents (4707 virtual)
2024-04-09 22:42:02,004 - gensim.topic_coherence.text_analysis - INFO - 4 batches submitted to accumulate stats from 256 documents (6346 virtual)
2024-04-09 22:42:02,004 - gensim.topic_coherence.text_analysis - INFO - 5 batches submitted to accumulate stats from 320 documents (7961 virtual)
2024-04-09 22:42:02,004 - gensim.topic_coherence.text_analysis - INFO - 6 batches submitted to accumulate stats from 384 documents (9298 virtual)
2024-04-09 22:42:02,004 - gensim.topic_coherence.text_analysis - INFO - 7 batches submitted to accumulate stats from 448 documents (11371 virtual)
2024-04-09 22:42:02,004 - gensim.topic_coherence.text_analysis - INFO - 8 batches submitted to accumulate stats from 512 documents (13011 virtual)
2024-04-09 22:42:02,004 - gensim.topic_coherence.text_analysis - INFO - 9 batches submitted to accumulate stats from 576 documents (14534 virtual)
2024-04-09 22:42:02,004 - gensim.topic_coherence.text_analysis - INFO - 10 batches submitted to accumulate stats from 640 documents (16161 virtual)
2024-04-09 22:42:02,004 - gensim.topic_coherence.text_analysis - INFO - 11 batches submitted to accumulate stats from 704 documents (17689 virtual)
2024-04-09 22:42:02,004 - gensim.topic_coherence.text_analysis - INFO - 12 batches submitted to accumulate stats from 768 documents (19256 virtual)
2024-04-09 22:42:02,004 - gensim.topic_coherence.text_analysis - INFO - 13 batches submitted to accumulate stats from 832 documents (21175 virtual)
2024-04-09 22:42:02,004 - gensim.topic_coherence.text_analysis - INFO - 14 batches submitted to accumulate stats from 896 documents (22850 virtual)
2024-04-09 22:42:02,020 - gensim.topic_coherence.text_analysis - INFO - 15 batches submitted to accumulate stats from 960 documents (24681 virtual)
2024-04-09 22:42:02,020 - gensim.topic_coherence.text_analysis - INFO - 16 batches submitted to accumulate stats from 1024 documents (26222 virtual)
2024-04-09 22:42:02,020 - gensim.topic_coherence.text_analysis - INFO - 17 batches submitted to accumulate stats from 1088 documents (27953 virtual)
2024-04-09 22:42:02,020 - gensim.topic_coherence.text_analysis - INFO - 18 batches submitted to accumulate stats from 1152 documents (29594 virtual)
2024-04-09 22:42:02,020 - gensim.topic_coherence.text_analysis - INFO - 19 batches submitted to accumulate stats from 1216 documents (31239 virtual)
2024-04-09 22:42:02,020 - gensim.topic_coherence.text_analysis - INFO - 20 batches submitted to accumulate stats from 1280 documents (32901 virtual)
2024-04-09 22:42:02,020 - gensim.topic_coherence.text_analysis - INFO - 21 batches submitted to accumulate stats from 1344 documents (34481 virtual)
2024-04-09 22:42:02,020 - gensim.topic_coherence.text_analysis - INFO - 22 batches submitted to accumulate stats from 1408 documents (36151 virtual)
2024-04-09 22:42:02,051 - gensim.topic_coherence.text_analysis - INFO - 23 batches submitted to accumulate stats from 1472 documents (37837 virtual)
2024-04-09 22:42:02,051 - gensim.topic_coherence.text_analysis - INFO - 24 batches submitted to accumulate stats from 1536 documents (39453 virtual)
2024-04-09 22:42:02,051 - gensim.topic_coherence.text_analysis - INFO - 25 batches submitted to accumulate stats from 1600 documents (41046 virtual)
2024-04-09 22:42:02,051 - gensim.topic_coherence.text_analysis - INFO - 26 batches submitted to accumulate stats from 1664 documents (42892 virtual)
2024-04-09 22:42:02,051 - gensim.topic_coherence.text_analysis - INFO - 27 batches submitted to accumulate stats from 1728 documents (44445 virtual)
2024-04-09 22:42:02,051 - gensim.topic_coherence.text_analysis - INFO - 28 batches submitted to accumulate stats from 1792 documents (46073 virtual)
2024-04-09 22:42:02,067 - gensim.topic_coherence.text_analysis - INFO - 29 batches submitted to accumulate stats from 1856 documents (47643 virtual)
2024-04-09 22:42:02,067 - gensim.topic_coherence.text_analysis - INFO - 30 batches submitted to accumulate stats from 1920 documents (49252 virtual)
2024-04-09 22:42:02,067 - gensim.topic_coherence.text_analysis - INFO - 31 batches submitted to accumulate stats from 1984 documents (50774 virtual)
2024-04-09 22:42:02,082 - gensim.topic_coherence.text_analysis - INFO - 32 batches submitted to accumulate stats from 2048 documents (52387 virtual)
2024-04-09 22:42:02,082 - gensim.topic_coherence.text_analysis - INFO - 33 batches submitted to accumulate stats from 2112 documents (53815 virtual)
2024-04-09 22:42:02,098 - gensim.topic_coherence.text_analysis - INFO - 34 batches submitted to accumulate stats from 2176 documents (55540 virtual)
2024-04-09 22:42:02,098 - gensim.topic_coherence.text_analysis - INFO - 35 batches submitted to accumulate stats from 2240 documents (57348 virtual)
2024-04-09 22:42:02,098 - gensim.topic_coherence.text_analysis - INFO - 36 batches submitted to accumulate stats from 2304 documents (59258 virtual)
2024-04-09 22:42:02,098 - gensim.topic_coherence.text_analysis - INFO - 37 batches submitted to accumulate stats from 2368 documents (60957 virtual)
2024-04-09 22:42:02,098 - gensim.topic_coherence.text_analysis - INFO - 38 batches submitted to accumulate stats from 2432 documents (62425 virtual)
2024-04-09 22:42:02,098 - gensim.topic_coherence.text_analysis - INFO - 39 batches submitted to accumulate stats from 2496 documents (64029 virtual)
2024-04-09 22:42:02,098 - gensim.topic_coherence.text_analysis - INFO - 40 batches submitted to accumulate stats from 2560 documents (65725 virtual)
2024-04-09 22:42:02,113 - gensim.topic_coherence.text_analysis - INFO - 41 batches submitted to accumulate stats from 2624 documents (67346 virtual)
2024-04-09 22:42:02,113 - gensim.topic_coherence.text_analysis - INFO - 42 batches submitted to accumulate stats from 2688 documents (68863 virtual)
2024-04-09 22:42:02,129 - gensim.topic_coherence.text_analysis - INFO - 43 batches submitted to accumulate stats from 2752 documents (70539 virtual)
2024-04-09 22:42:02,129 - gensim.topic_coherence.text_analysis - INFO - 44 batches submitted to accumulate stats from 2816 documents (71922 virtual)
2024-04-09 22:42:02,129 - gensim.topic_coherence.text_analysis - INFO - 45 batches submitted to accumulate stats from 2880 documents (73294 virtual)
2024-04-09 22:42:02,129 - gensim.topic_coherence.text_analysis - INFO - 46 batches submitted to accumulate stats from 2944 documents (75084 virtual)
2024-04-09 22:42:02,129 - gensim.topic_coherence.text_analysis - INFO - 47 batches submitted to accumulate stats from 3008 documents (76769 virtual)
2024-04-09 22:42:02,145 - gensim.topic_coherence.text_analysis - INFO - 48 batches submitted to accumulate stats from 3072 documents (78312 virtual)
2024-04-09 22:42:02,145 - gensim.topic_coherence.text_analysis - INFO - 49 batches submitted to accumulate stats from 3136 documents (80039 virtual)
2024-04-09 22:42:02,145 - gensim.topic_coherence.text_analysis - INFO - 50 batches submitted to accumulate stats from 3200 documents (81572 virtual)
2024-04-09 22:42:02,145 - gensim.topic_coherence.text_analysis - INFO - 51 batches submitted to accumulate stats from 3264 documents (83189 virtual)
2024-04-09 22:42:02,145 - gensim.topic_coherence.text_analysis - INFO - 52 batches submitted to accumulate stats from 3328 documents (84783 virtual)
2024-04-09 22:42:02,160 - gensim.topic_coherence.text_analysis - INFO - 53 batches submitted to accumulate stats from 3392 documents (86570 virtual)
2024-04-09 22:42:02,160 - gensim.topic_coherence.text_analysis - INFO - 54 batches submitted to accumulate stats from 3456 documents (88371 virtual)
2024-04-09 22:42:02,176 - gensim.topic_coherence.text_analysis - INFO - 55 batches submitted to accumulate stats from 3520 documents (90295 virtual)
2024-04-09 22:42:02,176 - gensim.topic_coherence.text_analysis - INFO - 56 batches submitted to accumulate stats from 3584 documents (92118 virtual)
2024-04-09 22:42:02,176 - gensim.topic_coherence.text_analysis - INFO - 57 batches submitted to accumulate stats from 3648 documents (93914 virtual)
2024-04-09 22:42:02,176 - gensim.topic_coherence.text_analysis - INFO - 58 batches submitted to accumulate stats from 3712 documents (95729 virtual)
2024-04-09 22:42:02,192 - gensim.topic_coherence.text_analysis - INFO - 59 batches submitted to accumulate stats from 3776 documents (97293 virtual)
2024-04-09 22:42:02,192 - gensim.topic_coherence.text_analysis - INFO - 60 batches submitted to accumulate stats from 3840 documents (98908 virtual)
2024-04-09 22:42:02,192 - gensim.topic_coherence.text_analysis - INFO - 61 batches submitted to accumulate stats from 3904 documents (100586 virtual)
2024-04-09 22:42:02,192 - gensim.topic_coherence.text_analysis - INFO - 62 batches submitted to accumulate stats from 3968 documents (102208 virtual)
2024-04-09 22:42:02,192 - gensim.topic_coherence.text_analysis - INFO - 63 batches submitted to accumulate stats from 4032 documents (103862 virtual)
2024-04-09 22:42:02,192 - gensim.topic_coherence.text_analysis - INFO - 64 batches submitted to accumulate stats from 4096 documents (105500 virtual)
2024-04-09 22:42:02,207 - gensim.topic_coherence.text_analysis - INFO - 65 batches submitted to accumulate stats from 4160 documents (106974 virtual)
2024-04-09 22:42:02,207 - gensim.topic_coherence.text_analysis - INFO - 66 batches submitted to accumulate stats from 4224 documents (108587 virtual)
2024-04-09 22:42:02,223 - gensim.topic_coherence.text_analysis - INFO - 67 batches submitted to accumulate stats from 4288 documents (110059 virtual)
2024-04-09 22:42:02,223 - gensim.topic_coherence.text_analysis - INFO - 68 batches submitted to accumulate stats from 4352 documents (111905 virtual)
2024-04-09 22:42:02,223 - gensim.topic_coherence.text_analysis - INFO - 69 batches submitted to accumulate stats from 4416 documents (113549 virtual)
2024-04-09 22:42:02,223 - gensim.topic_coherence.text_analysis - INFO - 70 batches submitted to accumulate stats from 4480 documents (115163 virtual)
2024-04-09 22:42:02,223 - gensim.topic_coherence.text_analysis - INFO - 71 batches submitted to accumulate stats from 4544 documents (117083 virtual)
2024-04-09 22:42:02,223 - gensim.topic_coherence.text_analysis - INFO - 72 batches submitted to accumulate stats from 4608 documents (118654 virtual)
2024-04-09 22:42:02,238 - gensim.topic_coherence.text_analysis - INFO - 73 batches submitted to accumulate stats from 4672 documents (120224 virtual)
2024-04-09 22:42:02,238 - gensim.topic_coherence.text_analysis - INFO - 74 batches submitted to accumulate stats from 4736 documents (121786 virtual)
2024-04-09 22:42:02,238 - gensim.topic_coherence.text_analysis - INFO - 75 batches submitted to accumulate stats from 4800 documents (123233 virtual)
2024-04-09 22:42:02,254 - gensim.topic_coherence.text_analysis - INFO - 76 batches submitted to accumulate stats from 4864 documents (124761 virtual)
2024-04-09 22:42:02,254 - gensim.topic_coherence.text_analysis - INFO - 77 batches submitted to accumulate stats from 4928 documents (126221 virtual)
2024-04-09 22:42:02,254 - gensim.topic_coherence.text_analysis - INFO - 78 batches submitted to accumulate stats from 4992 documents (127857 virtual)
2024-04-09 22:42:02,254 - gensim.topic_coherence.text_analysis - INFO - 79 batches submitted to accumulate stats from 5056 documents (129432 virtual)
2024-04-09 22:42:02,254 - gensim.topic_coherence.text_analysis - INFO - 80 batches submitted to accumulate stats from 5120 documents (130948 virtual)
2024-04-09 22:42:02,270 - gensim.topic_coherence.text_analysis - INFO - 81 batches submitted to accumulate stats from 5184 documents (132913 virtual)
2024-04-09 22:42:02,270 - gensim.topic_coherence.text_analysis - INFO - 82 batches submitted to accumulate stats from 5248 documents (134700 virtual)
2024-04-09 22:42:02,270 - gensim.topic_coherence.text_analysis - INFO - 83 batches submitted to accumulate stats from 5312 documents (136417 virtual)
2024-04-09 22:42:02,270 - gensim.topic_coherence.text_analysis - INFO - 84 batches submitted to accumulate stats from 5376 documents (138141 virtual)
2024-04-09 22:42:02,270 - gensim.topic_coherence.text_analysis - INFO - 85 batches submitted to accumulate stats from 5440 documents (139767 virtual)
2024-04-09 22:42:02,285 - gensim.topic_coherence.text_analysis - INFO - 86 batches submitted to accumulate stats from 5504 documents (141151 virtual)
2024-04-09 22:42:02,285 - gensim.topic_coherence.text_analysis - INFO - 87 batches submitted to accumulate stats from 5568 documents (142647 virtual)
2024-04-09 22:42:02,285 - gensim.topic_coherence.text_analysis - INFO - 88 batches submitted to accumulate stats from 5632 documents (144175 virtual)
2024-04-09 22:42:02,301 - gensim.topic_coherence.text_analysis - INFO - 89 batches submitted to accumulate stats from 5696 documents (145825 virtual)
2024-04-09 22:42:02,301 - gensim.topic_coherence.text_analysis - INFO - 90 batches submitted to accumulate stats from 5760 documents (147317 virtual)
2024-04-09 22:42:02,301 - gensim.topic_coherence.text_analysis - INFO - 91 batches submitted to accumulate stats from 5824 documents (149158 virtual)
2024-04-09 22:42:02,301 - gensim.topic_coherence.text_analysis - INFO - 92 batches submitted to accumulate stats from 5888 documents (150755 virtual)
2024-04-09 22:42:02,301 - gensim.topic_coherence.text_analysis - INFO - 93 batches submitted to accumulate stats from 5952 documents (152237 virtual)
2024-04-09 22:42:02,317 - gensim.topic_coherence.text_analysis - INFO - 94 batches submitted to accumulate stats from 6016 documents (154013 virtual)
2024-04-09 22:42:02,317 - gensim.topic_coherence.text_analysis - INFO - 95 batches submitted to accumulate stats from 6080 documents (155593 virtual)
2024-04-09 22:42:02,317 - gensim.topic_coherence.text_analysis - INFO - 96 batches submitted to accumulate stats from 6144 documents (157114 virtual)
2024-04-09 22:42:02,317 - gensim.topic_coherence.text_analysis - INFO - 97 batches submitted to accumulate stats from 6208 documents (158783 virtual)
2024-04-09 22:42:02,332 - gensim.topic_coherence.text_analysis - INFO - 98 batches submitted to accumulate stats from 6272 documents (160467 virtual)
2024-04-09 22:42:02,332 - gensim.topic_coherence.text_analysis - INFO - 99 batches submitted to accumulate stats from 6336 documents (162287 virtual)
2024-04-09 22:42:02,332 - gensim.topic_coherence.text_analysis - INFO - 100 batches submitted to accumulate stats from 6400 documents (163932 virtual)
2024-04-09 22:42:02,332 - gensim.topic_coherence.text_analysis - INFO - 101 batches submitted to accumulate stats from 6464 documents (165414 virtual)
2024-04-09 22:42:02,332 - gensim.topic_coherence.text_analysis - INFO - 102 batches submitted to accumulate stats from 6528 documents (166934 virtual)
2024-04-09 22:42:02,348 - gensim.topic_coherence.text_analysis - INFO - 103 batches submitted to accumulate stats from 6592 documents (168399 virtual)
2024-04-09 22:42:02,348 - gensim.topic_coherence.text_analysis - INFO - 104 batches submitted to accumulate stats from 6656 documents (170596 virtual)
2024-04-09 22:42:02,348 - gensim.topic_coherence.text_analysis - INFO - 105 batches submitted to accumulate stats from 6720 documents (172319 virtual)
2024-04-09 22:42:02,348 - gensim.topic_coherence.text_analysis - INFO - 106 batches submitted to accumulate stats from 6784 documents (173973 virtual)
2024-04-09 22:42:02,363 - gensim.topic_coherence.text_analysis - INFO - 107 batches submitted to accumulate stats from 6848 documents (175817 virtual)
2024-04-09 22:42:02,363 - gensim.topic_coherence.text_analysis - INFO - 108 batches submitted to accumulate stats from 6912 documents (177402 virtual)
2024-04-09 22:42:02,363 - gensim.topic_coherence.text_analysis - INFO - 109 batches submitted to accumulate stats from 6976 documents (179106 virtual)
2024-04-09 22:42:02,363 - gensim.topic_coherence.text_analysis - INFO - 110 batches submitted to accumulate stats from 7040 documents (181089 virtual)
2024-04-09 22:42:02,363 - gensim.topic_coherence.text_analysis - INFO - 111 batches submitted to accumulate stats from 7104 documents (182660 virtual)
2024-04-09 22:42:02,379 - gensim.topic_coherence.text_analysis - INFO - 112 batches submitted to accumulate stats from 7168 documents (184289 virtual)
2024-04-09 22:42:02,379 - gensim.topic_coherence.text_analysis - INFO - 113 batches submitted to accumulate stats from 7232 documents (185825 virtual)
2024-04-09 22:42:02,379 - gensim.topic_coherence.text_analysis - INFO - 114 batches submitted to accumulate stats from 7296 documents (187420 virtual)
2024-04-09 22:42:02,395 - gensim.topic_coherence.text_analysis - INFO - 115 batches submitted to accumulate stats from 7360 documents (189102 virtual)
2024-04-09 22:42:02,395 - gensim.topic_coherence.text_analysis - INFO - 116 batches submitted to accumulate stats from 7424 documents (190745 virtual)
2024-04-09 22:42:02,395 - gensim.topic_coherence.text_analysis - INFO - 117 batches submitted to accumulate stats from 7488 documents (192238 virtual)
2024-04-09 22:42:02,395 - gensim.topic_coherence.text_analysis - INFO - 118 batches submitted to accumulate stats from 7552 documents (194107 virtual)
2024-04-09 22:42:02,395 - gensim.topic_coherence.text_analysis - INFO - 119 batches submitted to accumulate stats from 7616 documents (195570 virtual)
2024-04-09 22:42:02,410 - gensim.topic_coherence.text_analysis - INFO - 120 batches submitted to accumulate stats from 7680 documents (197064 virtual)
2024-04-09 22:42:02,410 - gensim.topic_coherence.text_analysis - INFO - 121 batches submitted to accumulate stats from 7744 documents (198821 virtual)
2024-04-09 22:42:02,410 - gensim.topic_coherence.text_analysis - INFO - 122 batches submitted to accumulate stats from 7808 documents (200394 virtual)
2024-04-09 22:42:02,410 - gensim.topic_coherence.text_analysis - INFO - 123 batches submitted to accumulate stats from 7872 documents (202352 virtual)
2024-04-09 22:42:02,426 - gensim.topic_coherence.text_analysis - INFO - 124 batches submitted to accumulate stats from 7936 documents (204181 virtual)
2024-04-09 22:42:02,426 - gensim.topic_coherence.text_analysis - INFO - 125 batches submitted to accumulate stats from 8000 documents (206063 virtual)
2024-04-09 22:42:02,442 - gensim.topic_coherence.text_analysis - INFO - 126 batches submitted to accumulate stats from 8064 documents (207766 virtual)
2024-04-09 22:42:02,442 - gensim.topic_coherence.text_analysis - INFO - 127 batches submitted to accumulate stats from 8128 documents (209460 virtual)
2024-04-09 22:42:02,442 - gensim.topic_coherence.text_analysis - INFO - 128 batches submitted to accumulate stats from 8192 documents (211022 virtual)
2024-04-09 22:42:02,442 - gensim.topic_coherence.text_analysis - INFO - 129 batches submitted to accumulate stats from 8256 documents (212632 virtual)
2024-04-09 22:42:02,442 - gensim.topic_coherence.text_analysis - INFO - 130 batches submitted to accumulate stats from 8320 documents (214210 virtual)
2024-04-09 22:42:02,442 - gensim.topic_coherence.text_analysis - INFO - 131 batches submitted to accumulate stats from 8384 documents (215651 virtual)
2024-04-09 22:42:02,457 - gensim.topic_coherence.text_analysis - INFO - 132 batches submitted to accumulate stats from 8448 documents (217300 virtual)
2024-04-09 22:42:02,457 - gensim.topic_coherence.text_analysis - INFO - 133 batches submitted to accumulate stats from 8512 documents (219035 virtual)
2024-04-09 22:42:02,457 - gensim.topic_coherence.text_analysis - INFO - 134 batches submitted to accumulate stats from 8576 documents (220675 virtual)
2024-04-09 22:42:02,457 - gensim.topic_coherence.text_analysis - INFO - 135 batches submitted to accumulate stats from 8640 documents (222562 virtual)
2024-04-09 22:42:02,473 - gensim.topic_coherence.text_analysis - INFO - 136 batches submitted to accumulate stats from 8704 documents (224243 virtual)
2024-04-09 22:42:02,473 - gensim.topic_coherence.text_analysis - INFO - 137 batches submitted to accumulate stats from 8768 documents (225942 virtual)
2024-04-09 22:42:02,473 - gensim.topic_coherence.text_analysis - INFO - 138 batches submitted to accumulate stats from 8832 documents (227774 virtual)
2024-04-09 22:42:02,473 - gensim.topic_coherence.text_analysis - INFO - 139 batches submitted to accumulate stats from 8896 documents (229378 virtual)
2024-04-09 22:42:02,488 - gensim.topic_coherence.text_analysis - INFO - 140 batches submitted to accumulate stats from 8960 documents (231026 virtual)
2024-04-09 22:42:02,488 - gensim.topic_coherence.text_analysis - INFO - 141 batches submitted to accumulate stats from 9024 documents (232664 virtual)
2024-04-09 22:42:02,488 - gensim.topic_coherence.text_analysis - INFO - 142 batches submitted to accumulate stats from 9088 documents (234376 virtual)
2024-04-09 22:42:02,488 - gensim.topic_coherence.text_analysis - INFO - 143 batches submitted to accumulate stats from 9152 documents (236034 virtual)
2024-04-09 22:42:02,504 - gensim.topic_coherence.text_analysis - INFO - 144 batches submitted to accumulate stats from 9216 documents (237753 virtual)
2024-04-09 22:42:02,504 - gensim.topic_coherence.text_analysis - INFO - 145 batches submitted to accumulate stats from 9280 documents (239260 virtual)
2024-04-09 22:42:02,504 - gensim.topic_coherence.text_analysis - INFO - 146 batches submitted to accumulate stats from 9344 documents (240889 virtual)
2024-04-09 22:42:02,504 - gensim.topic_coherence.text_analysis - INFO - 147 batches submitted to accumulate stats from 9408 documents (242607 virtual)
2024-04-09 22:42:02,520 - gensim.topic_coherence.text_analysis - INFO - 148 batches submitted to accumulate stats from 9472 documents (244168 virtual)
2024-04-09 22:42:02,520 - gensim.topic_coherence.text_analysis - INFO - 149 batches submitted to accumulate stats from 9536 documents (245827 virtual)
2024-04-09 22:42:02,520 - gensim.topic_coherence.text_analysis - INFO - 150 batches submitted to accumulate stats from 9600 documents (247340 virtual)
2024-04-09 22:42:02,520 - gensim.topic_coherence.text_analysis - INFO - 151 batches submitted to accumulate stats from 9664 documents (248935 virtual)
2024-04-09 22:42:02,520 - gensim.topic_coherence.text_analysis - INFO - 152 batches submitted to accumulate stats from 9728 documents (250682 virtual)
2024-04-09 22:42:02,520 - gensim.topic_coherence.text_analysis - INFO - 153 batches submitted to accumulate stats from 9792 documents (252525 virtual)
2024-04-09 22:42:02,535 - gensim.topic_coherence.text_analysis - INFO - 154 batches submitted to accumulate stats from 9856 documents (253964 virtual)
2024-04-09 22:42:02,535 - gensim.topic_coherence.text_analysis - INFO - 155 batches submitted to accumulate stats from 9920 documents (255761 virtual)
2024-04-09 22:42:02,551 - gensim.topic_coherence.text_analysis - INFO - 156 batches submitted to accumulate stats from 9984 documents (257656 virtual)
2024-04-09 22:42:02,551 - gensim.topic_coherence.text_analysis - INFO - 157 batches submitted to accumulate stats from 10048 documents (259320 virtual)
2024-04-09 22:42:02,567 - gensim.topic_coherence.text_analysis - INFO - 158 batches submitted to accumulate stats from 10112 documents (261193 virtual)
2024-04-09 22:42:02,567 - gensim.topic_coherence.text_analysis - INFO - 159 batches submitted to accumulate stats from 10176 documents (262723 virtual)
2024-04-09 22:42:02,567 - gensim.topic_coherence.text_analysis - INFO - 160 batches submitted to accumulate stats from 10240 documents (264223 virtual)
2024-04-09 22:42:02,567 - gensim.topic_coherence.text_analysis - INFO - 161 batches submitted to accumulate stats from 10304 documents (265694 virtual)
2024-04-09 22:42:02,567 - gensim.topic_coherence.text_analysis - INFO - 162 batches submitted to accumulate stats from 10368 documents (267448 virtual)
2024-04-09 22:42:02,567 - gensim.topic_coherence.text_analysis - INFO - 163 batches submitted to accumulate stats from 10432 documents (269251 virtual)
2024-04-09 22:42:02,567 - gensim.topic_coherence.text_analysis - INFO - 164 batches submitted to accumulate stats from 10496 documents (270973 virtual)
2024-04-09 22:42:02,582 - gensim.topic_coherence.text_analysis - INFO - 165 batches submitted to accumulate stats from 10560 documents (272683 virtual)
2024-04-09 22:42:02,582 - gensim.topic_coherence.text_analysis - INFO - 166 batches submitted to accumulate stats from 10624 documents (274294 virtual)
2024-04-09 22:42:02,582 - gensim.topic_coherence.text_analysis - INFO - 167 batches submitted to accumulate stats from 10688 documents (276045 virtual)
2024-04-09 22:42:02,598 - gensim.topic_coherence.text_analysis - INFO - 168 batches submitted to accumulate stats from 10752 documents (277496 virtual)
2024-04-09 22:42:02,598 - gensim.topic_coherence.text_analysis - INFO - 169 batches submitted to accumulate stats from 10816 documents (279131 virtual)
2024-04-09 22:42:02,598 - gensim.topic_coherence.text_analysis - INFO - 170 batches submitted to accumulate stats from 10880 documents (280812 virtual)
2024-04-09 22:42:02,598 - gensim.topic_coherence.text_analysis - INFO - 171 batches submitted to accumulate stats from 10944 documents (282408 virtual)
2024-04-09 22:42:02,598 - gensim.topic_coherence.text_analysis - INFO - 172 batches submitted to accumulate stats from 11008 documents (284125 virtual)
2024-04-09 22:42:02,613 - gensim.topic_coherence.text_analysis - INFO - 173 batches submitted to accumulate stats from 11072 documents (285575 virtual)
2024-04-09 22:42:02,613 - gensim.topic_coherence.text_analysis - INFO - 174 batches submitted to accumulate stats from 11136 documents (287185 virtual)
2024-04-09 22:42:02,613 - gensim.topic_coherence.text_analysis - INFO - 175 batches submitted to accumulate stats from 11200 documents (289145 virtual)
2024-04-09 22:42:02,629 - gensim.topic_coherence.text_analysis - INFO - 176 batches submitted to accumulate stats from 11264 documents (290900 virtual)
2024-04-09 22:42:02,629 - gensim.topic_coherence.text_analysis - INFO - 177 batches submitted to accumulate stats from 11328 documents (292686 virtual)
2024-04-09 22:42:02,629 - gensim.topic_coherence.text_analysis - INFO - 178 batches submitted to accumulate stats from 11392 documents (294505 virtual)
2024-04-09 22:42:02,645 - gensim.topic_coherence.text_analysis - INFO - 179 batches submitted to accumulate stats from 11456 documents (296236 virtual)
2024-04-09 22:42:02,645 - gensim.topic_coherence.text_analysis - INFO - 180 batches submitted to accumulate stats from 11520 documents (297647 virtual)
2024-04-09 22:42:02,645 - gensim.topic_coherence.text_analysis - INFO - 181 batches submitted to accumulate stats from 11584 documents (299327 virtual)
2024-04-09 22:42:02,645 - gensim.topic_coherence.text_analysis - INFO - 182 batches submitted to accumulate stats from 11648 documents (300853 virtual)
2024-04-09 22:42:02,645 - gensim.topic_coherence.text_analysis - INFO - 183 batches submitted to accumulate stats from 11712 documents (302601 virtual)
2024-04-09 22:42:02,645 - gensim.topic_coherence.text_analysis - INFO - 184 batches submitted to accumulate stats from 11776 documents (304181 virtual)
2024-04-09 22:42:02,660 - gensim.topic_coherence.text_analysis - INFO - 185 batches submitted to accumulate stats from 11840 documents (305710 virtual)
2024-04-09 22:42:02,660 - gensim.topic_coherence.text_analysis - INFO - 186 batches submitted to accumulate stats from 11904 documents (307265 virtual)
2024-04-09 22:42:02,660 - gensim.topic_coherence.text_analysis - INFO - 187 batches submitted to accumulate stats from 11968 documents (309148 virtual)
2024-04-09 22:42:02,676 - gensim.topic_coherence.text_analysis - INFO - 188 batches submitted to accumulate stats from 12032 documents (310818 virtual)
2024-04-09 22:42:02,676 - gensim.topic_coherence.text_analysis - INFO - 189 batches submitted to accumulate stats from 12096 documents (312472 virtual)
2024-04-09 22:42:02,676 - gensim.topic_coherence.text_analysis - INFO - 190 batches submitted to accumulate stats from 12160 documents (314119 virtual)
2024-04-09 22:42:02,676 - gensim.topic_coherence.text_analysis - INFO - 191 batches submitted to accumulate stats from 12224 documents (316054 virtual)
2024-04-09 22:42:02,692 - gensim.topic_coherence.text_analysis - INFO - 192 batches submitted to accumulate stats from 12288 documents (317685 virtual)
2024-04-09 22:42:02,692 - gensim.topic_coherence.text_analysis - INFO - 193 batches submitted to accumulate stats from 12352 documents (319351 virtual)
2024-04-09 22:42:02,692 - gensim.topic_coherence.text_analysis - INFO - 194 batches submitted to accumulate stats from 12416 documents (321100 virtual)
2024-04-09 22:42:02,692 - gensim.topic_coherence.text_analysis - INFO - 195 batches submitted to accumulate stats from 12480 documents (323099 virtual)
2024-04-09 22:42:02,692 - gensim.topic_coherence.text_analysis - INFO - 196 batches submitted to accumulate stats from 12544 documents (324548 virtual)
2024-04-09 22:42:02,707 - gensim.topic_coherence.text_analysis - INFO - 197 batches submitted to accumulate stats from 12608 documents (326231 virtual)
2024-04-09 22:42:02,723 - gensim.topic_coherence.text_analysis - INFO - 198 batches submitted to accumulate stats from 12672 documents (327851 virtual)
2024-04-09 22:42:02,723 - gensim.topic_coherence.text_analysis - INFO - 199 batches submitted to accumulate stats from 12736 documents (329386 virtual)
2024-04-09 22:42:02,723 - gensim.topic_coherence.text_analysis - INFO - 200 batches submitted to accumulate stats from 12800 documents (331113 virtual)
2024-04-09 22:42:02,723 - gensim.topic_coherence.text_analysis - INFO - 201 batches submitted to accumulate stats from 12864 documents (332712 virtual)
2024-04-09 22:42:02,723 - gensim.topic_coherence.text_analysis - INFO - 202 batches submitted to accumulate stats from 12928 documents (334135 virtual)
2024-04-09 22:42:02,723 - gensim.topic_coherence.text_analysis - INFO - 203 batches submitted to accumulate stats from 12992 documents (335914 virtual)
2024-04-09 22:42:02,738 - gensim.topic_coherence.text_analysis - INFO - 204 batches submitted to accumulate stats from 13056 documents (337641 virtual)
2024-04-09 22:42:02,738 - gensim.topic_coherence.text_analysis - INFO - 205 batches submitted to accumulate stats from 13120 documents (339449 virtual)
2024-04-09 22:42:02,738 - gensim.topic_coherence.text_analysis - INFO - 206 batches submitted to accumulate stats from 13184 documents (341168 virtual)
2024-04-09 22:42:02,754 - gensim.topic_coherence.text_analysis - INFO - 207 batches submitted to accumulate stats from 13248 documents (342833 virtual)
2024-04-09 22:42:02,754 - gensim.topic_coherence.text_analysis - INFO - 208 batches submitted to accumulate stats from 13312 documents (344704 virtual)
2024-04-09 22:42:02,770 - gensim.topic_coherence.text_analysis - INFO - 209 batches submitted to accumulate stats from 13376 documents (346650 virtual)
2024-04-09 22:42:02,770 - gensim.topic_coherence.text_analysis - INFO - 210 batches submitted to accumulate stats from 13440 documents (348531 virtual)
2024-04-09 22:42:02,770 - gensim.topic_coherence.text_analysis - INFO - 211 batches submitted to accumulate stats from 13504 documents (350342 virtual)
2024-04-09 22:42:02,770 - gensim.topic_coherence.text_analysis - INFO - 212 batches submitted to accumulate stats from 13568 documents (352068 virtual)
2024-04-09 22:42:02,770 - gensim.topic_coherence.text_analysis - INFO - 213 batches submitted to accumulate stats from 13632 documents (353789 virtual)
2024-04-09 22:42:02,770 - gensim.topic_coherence.text_analysis - INFO - 214 batches submitted to accumulate stats from 13696 documents (355216 virtual)
2024-04-09 22:42:02,770 - gensim.topic_coherence.text_analysis - INFO - 215 batches submitted to accumulate stats from 13760 documents (356990 virtual)
2024-04-09 22:42:02,785 - gensim.topic_coherence.text_analysis - INFO - 216 batches submitted to accumulate stats from 13824 documents (358762 virtual)
2024-04-09 22:42:02,801 - gensim.topic_coherence.text_analysis - INFO - 217 batches submitted to accumulate stats from 13888 documents (360320 virtual)
2024-04-09 22:42:02,801 - gensim.topic_coherence.text_analysis - INFO - 218 batches submitted to accumulate stats from 13952 documents (361867 virtual)
2024-04-09 22:42:02,801 - gensim.topic_coherence.text_analysis - INFO - 219 batches submitted to accumulate stats from 14016 documents (363519 virtual)
2024-04-09 22:42:02,801 - gensim.topic_coherence.text_analysis - INFO - 220 batches submitted to accumulate stats from 14080 documents (365141 virtual)
2024-04-09 22:42:02,817 - gensim.topic_coherence.text_analysis - INFO - 221 batches submitted to accumulate stats from 14144 documents (366934 virtual)
2024-04-09 22:42:02,832 - gensim.topic_coherence.text_analysis - INFO - 222 batches submitted to accumulate stats from 14208 documents (368448 virtual)
2024-04-09 22:42:02,848 - gensim.topic_coherence.text_analysis - INFO - 223 batches submitted to accumulate stats from 14272 documents (370012 virtual)
2024-04-09 22:42:02,848 - gensim.topic_coherence.text_analysis - INFO - 224 batches submitted to accumulate stats from 14336 documents (371704 virtual)
2024-04-09 22:42:02,863 - gensim.topic_coherence.text_analysis - INFO - 225 batches submitted to accumulate stats from 14400 documents (373331 virtual)
2024-04-09 22:42:02,863 - gensim.topic_coherence.text_analysis - INFO - 226 batches submitted to accumulate stats from 14464 documents (375174 virtual)
2024-04-09 22:42:02,879 - gensim.topic_coherence.text_analysis - INFO - 227 batches submitted to accumulate stats from 14528 documents (377135 virtual)
2024-04-09 22:42:02,879 - gensim.topic_coherence.text_analysis - INFO - 228 batches submitted to accumulate stats from 14592 documents (378860 virtual)
2024-04-09 22:42:02,879 - gensim.topic_coherence.text_analysis - INFO - 229 batches submitted to accumulate stats from 14656 documents (380581 virtual)
2024-04-09 22:42:02,879 - gensim.topic_coherence.text_analysis - INFO - 230 batches submitted to accumulate stats from 14720 documents (382278 virtual)
2024-04-09 22:42:02,879 - gensim.topic_coherence.text_analysis - INFO - 231 batches submitted to accumulate stats from 14784 documents (383970 virtual)
2024-04-09 22:42:02,879 - gensim.topic_coherence.text_analysis - INFO - 232 batches submitted to accumulate stats from 14848 documents (385680 virtual)
2024-04-09 22:42:02,879 - gensim.topic_coherence.text_analysis - INFO - 233 batches submitted to accumulate stats from 14912 documents (387378 virtual)
2024-04-09 22:42:02,879 - gensim.topic_coherence.text_analysis - INFO - 234 batches submitted to accumulate stats from 14976 documents (388908 virtual)
2024-04-09 22:42:02,879 - gensim.topic_coherence.text_analysis - INFO - 235 batches submitted to accumulate stats from 15040 documents (390473 virtual)
2024-04-09 22:42:02,895 - gensim.topic_coherence.text_analysis - INFO - 236 batches submitted to accumulate stats from 15104 documents (391966 virtual)
2024-04-09 22:42:02,895 - gensim.topic_coherence.text_analysis - INFO - 237 batches submitted to accumulate stats from 15168 documents (393446 virtual)
2024-04-09 22:42:02,895 - gensim.topic_coherence.text_analysis - INFO - 238 batches submitted to accumulate stats from 15232 documents (394979 virtual)
2024-04-09 22:42:02,895 - gensim.topic_coherence.text_analysis - INFO - 239 batches submitted to accumulate stats from 15296 documents (396707 virtual)
2024-04-09 22:42:02,895 - gensim.topic_coherence.text_analysis - INFO - 240 batches submitted to accumulate stats from 15360 documents (398434 virtual)
2024-04-09 22:42:02,895 - gensim.topic_coherence.text_analysis - INFO - 241 batches submitted to accumulate stats from 15424 documents (400092 virtual)
2024-04-09 22:42:02,895 - gensim.topic_coherence.text_analysis - INFO - 242 batches submitted to accumulate stats from 15488 documents (402054 virtual)
2024-04-09 22:42:02,895 - gensim.topic_coherence.text_analysis - INFO - 243 batches submitted to accumulate stats from 15552 documents (403465 virtual)
2024-04-09 22:42:02,910 - gensim.topic_coherence.text_analysis - INFO - 244 batches submitted to accumulate stats from 15616 documents (405289 virtual)
2024-04-09 22:42:02,910 - gensim.topic_coherence.text_analysis - INFO - 245 batches submitted to accumulate stats from 15680 documents (406750 virtual)
2024-04-09 22:42:02,926 - gensim.topic_coherence.text_analysis - INFO - 246 batches submitted to accumulate stats from 15744 documents (408306 virtual)
2024-04-09 22:42:02,926 - gensim.topic_coherence.text_analysis - INFO - 247 batches submitted to accumulate stats from 15808 documents (410097 virtual)
2024-04-09 22:42:02,926 - gensim.topic_coherence.text_analysis - INFO - 248 batches submitted to accumulate stats from 15872 documents (411915 virtual)
2024-04-09 22:42:02,926 - gensim.topic_coherence.text_analysis - INFO - 249 batches submitted to accumulate stats from 15936 documents (413538 virtual)
2024-04-09 22:42:02,926 - gensim.topic_coherence.text_analysis - INFO - 250 batches submitted to accumulate stats from 16000 documents (415307 virtual)
2024-04-09 22:42:02,942 - gensim.topic_coherence.text_analysis - INFO - 251 batches submitted to accumulate stats from 16064 documents (416993 virtual)
2024-04-09 22:42:02,942 - gensim.topic_coherence.text_analysis - INFO - 252 batches submitted to accumulate stats from 16128 documents (418582 virtual)
2024-04-09 22:42:02,957 - gensim.topic_coherence.text_analysis - INFO - 253 batches submitted to accumulate stats from 16192 documents (420282 virtual)
2024-04-09 22:42:02,957 - gensim.topic_coherence.text_analysis - INFO - 254 batches submitted to accumulate stats from 16256 documents (421859 virtual)
2024-04-09 22:42:02,957 - gensim.topic_coherence.text_analysis - INFO - 255 batches submitted to accumulate stats from 16320 documents (423441 virtual)
2024-04-09 22:42:02,957 - gensim.topic_coherence.text_analysis - INFO - 256 batches submitted to accumulate stats from 16384 documents (425169 virtual)
2024-04-09 22:42:02,957 - gensim.topic_coherence.text_analysis - INFO - 257 batches submitted to accumulate stats from 16448 documents (426804 virtual)
2024-04-09 22:42:02,957 - gensim.topic_coherence.text_analysis - INFO - 258 batches submitted to accumulate stats from 16512 documents (428503 virtual)
2024-04-09 22:42:02,973 - gensim.topic_coherence.text_analysis - INFO - 259 batches submitted to accumulate stats from 16576 documents (430063 virtual)
2024-04-09 22:42:02,988 - gensim.topic_coherence.text_analysis - INFO - 260 batches submitted to accumulate stats from 16640 documents (431672 virtual)
2024-04-09 22:42:03,004 - gensim.topic_coherence.text_analysis - INFO - 261 batches submitted to accumulate stats from 16704 documents (433298 virtual)
2024-04-09 22:42:03,004 - gensim.topic_coherence.text_analysis - INFO - 262 batches submitted to accumulate stats from 16768 documents (434951 virtual)
2024-04-09 22:42:03,004 - gensim.topic_coherence.text_analysis - INFO - 263 batches submitted to accumulate stats from 16832 documents (436505 virtual)
2024-04-09 22:42:03,020 - gensim.topic_coherence.text_analysis - INFO - 264 batches submitted to accumulate stats from 16896 documents (438134 virtual)
2024-04-09 22:42:03,020 - gensim.topic_coherence.text_analysis - INFO - 265 batches submitted to accumulate stats from 16960 documents (439831 virtual)
2024-04-09 22:42:03,020 - gensim.topic_coherence.text_analysis - INFO - 266 batches submitted to accumulate stats from 17024 documents (441666 virtual)
2024-04-09 22:42:03,035 - gensim.topic_coherence.text_analysis - INFO - 267 batches submitted to accumulate stats from 17088 documents (443389 virtual)
2024-04-09 22:42:03,035 - gensim.topic_coherence.text_analysis - INFO - 268 batches submitted to accumulate stats from 17152 documents (445008 virtual)
2024-04-09 22:42:03,035 - gensim.topic_coherence.text_analysis - INFO - 269 batches submitted to accumulate stats from 17216 documents (446691 virtual)
2024-04-09 22:42:03,035 - gensim.topic_coherence.text_analysis - INFO - 270 batches submitted to accumulate stats from 17280 documents (448258 virtual)
2024-04-09 22:42:03,035 - gensim.topic_coherence.text_analysis - INFO - 271 batches submitted to accumulate stats from 17344 documents (450177 virtual)
2024-04-09 22:42:03,035 - gensim.topic_coherence.text_analysis - INFO - 272 batches submitted to accumulate stats from 17408 documents (451838 virtual)
2024-04-09 22:42:03,035 - gensim.topic_coherence.text_analysis - INFO - 273 batches submitted to accumulate stats from 17472 documents (453765 virtual)
2024-04-09 22:42:03,051 - gensim.topic_coherence.text_analysis - INFO - 274 batches submitted to accumulate stats from 17536 documents (455302 virtual)
2024-04-09 22:42:03,051 - gensim.topic_coherence.text_analysis - INFO - 275 batches submitted to accumulate stats from 17600 documents (457026 virtual)
2024-04-09 22:42:03,051 - gensim.topic_coherence.text_analysis - INFO - 276 batches submitted to accumulate stats from 17664 documents (458678 virtual)
2024-04-09 22:42:03,051 - gensim.topic_coherence.text_analysis - INFO - 277 batches submitted to accumulate stats from 17728 documents (460363 virtual)
2024-04-09 22:42:03,067 - gensim.topic_coherence.text_analysis - INFO - 278 batches submitted to accumulate stats from 17792 documents (462290 virtual)
2024-04-09 22:42:03,067 - gensim.topic_coherence.text_analysis - INFO - 279 batches submitted to accumulate stats from 17856 documents (464043 virtual)
2024-04-09 22:42:03,067 - gensim.topic_coherence.text_analysis - INFO - 280 batches submitted to accumulate stats from 17920 documents (465601 virtual)
2024-04-09 22:42:03,067 - gensim.topic_coherence.text_analysis - INFO - 281 batches submitted to accumulate stats from 17984 documents (467208 virtual)
2024-04-09 22:42:03,082 - gensim.topic_coherence.text_analysis - INFO - 282 batches submitted to accumulate stats from 18048 documents (468985 virtual)
2024-04-09 22:42:03,098 - gensim.topic_coherence.text_analysis - INFO - 283 batches submitted to accumulate stats from 18112 documents (470540 virtual)
2024-04-09 22:42:03,098 - gensim.topic_coherence.text_analysis - INFO - 284 batches submitted to accumulate stats from 18176 documents (472187 virtual)
2024-04-09 22:42:03,098 - gensim.topic_coherence.text_analysis - INFO - 285 batches submitted to accumulate stats from 18240 documents (473657 virtual)
2024-04-09 22:42:03,113 - gensim.topic_coherence.text_analysis - INFO - 286 batches submitted to accumulate stats from 18304 documents (475433 virtual)
2024-04-09 22:42:03,145 - gensim.topic_coherence.text_analysis - INFO - 287 batches submitted to accumulate stats from 18368 documents (476949 virtual)
2024-04-09 22:42:03,145 - gensim.topic_coherence.text_analysis - INFO - 288 batches submitted to accumulate stats from 18432 documents (478519 virtual)
2024-04-09 22:42:03,145 - gensim.topic_coherence.text_analysis - INFO - 289 batches submitted to accumulate stats from 18496 documents (480346 virtual)
2024-04-09 22:42:03,145 - gensim.topic_coherence.text_analysis - INFO - 290 batches submitted to accumulate stats from 18560 documents (482113 virtual)
2024-04-09 22:42:03,145 - gensim.topic_coherence.text_analysis - INFO - 291 batches submitted to accumulate stats from 18624 documents (483726 virtual)
2024-04-09 22:42:03,160 - gensim.topic_coherence.text_analysis - INFO - 292 batches submitted to accumulate stats from 18688 documents (485454 virtual)
2024-04-09 22:42:03,160 - gensim.topic_coherence.text_analysis - INFO - 293 batches submitted to accumulate stats from 18752 documents (487120 virtual)
2024-04-09 22:42:03,160 - gensim.topic_coherence.text_analysis - INFO - 294 batches submitted to accumulate stats from 18816 documents (488702 virtual)
2024-04-09 22:42:03,160 - gensim.topic_coherence.text_analysis - INFO - 295 batches submitted to accumulate stats from 18880 documents (490511 virtual)
2024-04-09 22:42:03,160 - gensim.topic_coherence.text_analysis - INFO - 296 batches submitted to accumulate stats from 18944 documents (492226 virtual)
2024-04-09 22:42:03,160 - gensim.topic_coherence.text_analysis - INFO - 297 batches submitted to accumulate stats from 19008 documents (494165 virtual)
2024-04-09 22:42:03,160 - gensim.topic_coherence.text_analysis - INFO - 298 batches submitted to accumulate stats from 19072 documents (495678 virtual)
2024-04-09 22:42:03,160 - gensim.topic_coherence.text_analysis - INFO - 299 batches submitted to accumulate stats from 19136 documents (497503 virtual)
2024-04-09 22:42:03,160 - gensim.topic_coherence.text_analysis - INFO - 300 batches submitted to accumulate stats from 19200 documents (499131 virtual)
2024-04-09 22:42:03,176 - gensim.topic_coherence.text_analysis - INFO - 301 batches submitted to accumulate stats from 19264 documents (500986 virtual)
2024-04-09 22:42:03,176 - gensim.topic_coherence.text_analysis - INFO - 302 batches submitted to accumulate stats from 19328 documents (502687 virtual)
2024-04-09 22:42:03,176 - gensim.topic_coherence.text_analysis - INFO - 303 batches submitted to accumulate stats from 19392 documents (504106 virtual)
2024-04-09 22:42:03,176 - gensim.topic_coherence.text_analysis - INFO - 304 batches submitted to accumulate stats from 19456 documents (505909 virtual)
2024-04-09 22:42:03,176 - gensim.topic_coherence.text_analysis - INFO - 305 batches submitted to accumulate stats from 19520 documents (507546 virtual)
2024-04-09 22:42:03,176 - gensim.topic_coherence.text_analysis - INFO - 306 batches submitted to accumulate stats from 19584 documents (509312 virtual)
2024-04-09 22:42:03,176 - gensim.topic_coherence.text_analysis - INFO - 307 batches submitted to accumulate stats from 19648 documents (510954 virtual)
2024-04-09 22:42:03,176 - gensim.topic_coherence.text_analysis - INFO - 308 batches submitted to accumulate stats from 19712 documents (512465 virtual)
2024-04-09 22:42:03,192 - gensim.topic_coherence.text_analysis - INFO - 309 batches submitted to accumulate stats from 19776 documents (514132 virtual)
2024-04-09 22:42:03,192 - gensim.topic_coherence.text_analysis - INFO - 310 batches submitted to accumulate stats from 19840 documents (515846 virtual)
2024-04-09 22:42:03,207 - gensim.topic_coherence.text_analysis - INFO - 311 batches submitted to accumulate stats from 19904 documents (517436 virtual)
2024-04-09 22:42:03,223 - gensim.topic_coherence.text_analysis - INFO - 312 batches submitted to accumulate stats from 19968 documents (519076 virtual)
2024-04-09 22:42:03,223 - gensim.topic_coherence.text_analysis - INFO - 313 batches submitted to accumulate stats from 20032 documents (520797 virtual)
2024-04-09 22:42:03,223 - gensim.topic_coherence.text_analysis - INFO - 314 batches submitted to accumulate stats from 20096 documents (522328 virtual)
2024-04-09 22:42:03,629 - gensim.topic_coherence.text_analysis - INFO - 11 accumulators retrieved from output queue
2024-04-09 22:42:03,926 - gensim.topic_coherence.text_analysis - INFO - accumulated word occurrence stats for 523696 virtual documents
2024-04-09 22:42:04,942 - gensim.corpora.dictionary - INFO - adding document #0 to Dictionary<0 unique tokens: []>
2024-04-09 22:42:05,270 - gensim.corpora.dictionary - INFO - adding document #10000 to Dictionary<23643 unique tokens: ['-', '.', '?', 'a', 'africa']...>
2024-04-09 22:42:05,614 - gensim.corpora.dictionary - INFO - adding document #20000 to Dictionary<32428 unique tokens: ['-', '.', '?', 'a', 'africa']...>
2024-04-09 22:42:05,630 - gensim.corpora.dictionary - INFO - built Dictionary<32503 unique tokens: ['-', '.', '?', 'a', 'africa']...> from 20086 documents (total 703102 corpus positions)
2024-04-09 22:42:05,634 - gensim.utils - INFO - Dictionary lifecycle event {'msg': "built Dictionary<32503 unique tokens: ['-', '.', '?', 'a', 'africa']...> from 20086 documents (total 703102 corpus positions)", 'datetime': '2024-04-09T22:42:05.634095', 'gensim': '4.3.2', 'python': '3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}
2024-04-09 22:42:05,646 - gensim.topic_coherence.probability_estimation - INFO - using ParallelWordOccurrenceAccumulator<processes=11, batch_size=64> to estimate probabilities from sliding windows
2024-04-09 22:42:59,415 - gensim.topic_coherence.text_analysis - INFO - 11 accumulators retrieved from output queue
2024-04-09 22:42:59,524 - gensim.topic_coherence.text_analysis - INFO - accumulated word occurrence stats for 21620 virtual documents
2024-04-09 22:43:03,854 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,760,523
Freeze params: 0
2024-04-27 17:26:20,488 - train - INFO - BiAttentionClassifyModel(
  (embedding): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 48,035,427
Freeze params: 0
2024-04-27 17:38:20,856 - trainer - INFO -     epoch          : 1
2024-04-27 17:38:20,856 - trainer - INFO -     loss           : 1.190042
2024-04-27 17:38:20,856 - trainer - INFO -     accuracy       : 0.658095
2024-04-27 17:38:20,856 - trainer - INFO -     macro_f        : 0.636703
2024-04-27 17:38:20,856 - trainer - INFO -     precision      : 0.665436
2024-04-27 17:38:20,856 - trainer - INFO -     recall         : 0.658095
2024-04-27 17:38:20,856 - trainer - INFO -     doc_entropy    : 2.483525
2024-04-27 17:38:20,856 - trainer - INFO -     val_loss       : 1.005024
2024-04-27 17:38:20,856 - trainer - INFO -     val_accuracy   : 0.707309
2024-04-27 17:38:20,856 - trainer - INFO -     val_macro_f    : 0.699785
2024-04-27 17:38:20,856 - trainer - INFO -     val_precision  : 0.737597
2024-04-27 17:38:20,856 - trainer - INFO -     val_recall     : 0.707309
2024-04-27 17:38:20,856 - trainer - INFO -     val_doc_entropy: 2.412375
2024-04-27 17:38:20,871 - trainer - INFO -     test_loss      : 1.016056
2024-04-27 17:38:20,871 - trainer - INFO -     test_accuracy  : 0.701981
2024-04-27 17:38:20,871 - trainer - INFO -     test_macro_f   : 0.695273
2024-04-27 17:38:20,871 - trainer - INFO -     test_precision : 0.734214
2024-04-27 17:38:20,871 - trainer - INFO -     test_recall    : 0.701981
2024-04-27 17:38:20,871 - trainer - INFO -     test_doc_entropy: 2.409908
2024-04-27 17:50:29,784 - trainer - INFO -     epoch          : 2
2024-04-27 17:50:29,784 - trainer - INFO -     loss           : 0.868707
2024-04-27 17:50:29,784 - trainer - INFO -     accuracy       : 0.740677
2024-04-27 17:50:29,784 - trainer - INFO -     macro_f        : 0.731058
2024-04-27 17:50:29,784 - trainer - INFO -     precision      : 0.764341
2024-04-27 17:50:29,784 - trainer - INFO -     recall         : 0.740677
2024-04-27 17:50:29,784 - trainer - INFO -     doc_entropy    : 2.417215
2024-04-27 17:50:29,784 - trainer - INFO -     val_loss       : 0.949744
2024-04-27 17:50:29,784 - trainer - INFO -     val_accuracy   : 0.721
2024-04-27 17:50:29,784 - trainer - INFO -     val_macro_f    : 0.714639
2024-04-27 17:50:29,784 - trainer - INFO -     val_precision  : 0.753124
2024-04-27 17:50:29,784 - trainer - INFO -     val_recall     : 0.721
2024-04-27 17:50:29,784 - trainer - INFO -     val_doc_entropy: 2.336138
2024-04-27 17:50:29,784 - trainer - INFO -     test_loss      : 0.956437
2024-04-27 17:50:29,784 - trainer - INFO -     test_accuracy  : 0.718859
2024-04-27 17:50:29,784 - trainer - INFO -     test_macro_f   : 0.715869
2024-04-27 17:50:29,784 - trainer - INFO -     test_precision : 0.758495
2024-04-27 17:50:29,784 - trainer - INFO -     test_recall    : 0.718859
2024-04-27 17:50:29,784 - trainer - INFO -     test_doc_entropy: 2.33318
2024-04-27 18:02:43,316 - trainer - INFO -     epoch          : 3
2024-04-27 18:02:43,316 - trainer - INFO -     loss           : 0.696531
2024-04-27 18:02:43,316 - trainer - INFO -     accuracy       : 0.789352
2024-04-27 18:02:43,332 - trainer - INFO -     macro_f        : 0.782042
2024-04-27 18:02:43,332 - trainer - INFO -     precision      : 0.812236
2024-04-27 18:02:43,332 - trainer - INFO -     recall         : 0.789352
2024-04-27 18:02:43,332 - trainer - INFO -     doc_entropy    : 2.287574
2024-04-27 18:02:43,332 - trainer - INFO -     val_loss       : 0.963944
2024-04-27 18:02:43,332 - trainer - INFO -     val_accuracy   : 0.723638
2024-04-27 18:02:43,332 - trainer - INFO -     val_macro_f    : 0.719675
2024-04-27 18:02:43,332 - trainer - INFO -     val_precision  : 0.759276
2024-04-27 18:02:43,332 - trainer - INFO -     val_recall     : 0.723638
2024-04-27 18:02:43,332 - trainer - INFO -     val_doc_entropy: 2.258008
2024-04-27 18:02:43,332 - trainer - INFO -     test_loss      : 0.976036
2024-04-27 18:02:43,332 - trainer - INFO -     test_accuracy  : 0.72334
2024-04-27 18:02:43,332 - trainer - INFO -     test_macro_f   : 0.719352
2024-04-27 18:02:43,332 - trainer - INFO -     test_precision : 0.761126
2024-04-27 18:02:43,332 - trainer - INFO -     test_recall    : 0.72334
2024-04-27 18:02:43,332 - trainer - INFO -     test_doc_entropy: 2.254934
2024-04-27 18:04:01,400 - train - INFO - BiAttentionClassifyModel(
  (embedding): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 48,035,427
Freeze params: 0
2024-04-27 18:16:19,726 - trainer - INFO -     epoch          : 1
2024-04-27 18:16:20,586 - trainer - INFO -     loss           : 1.193257
2024-04-27 18:16:20,586 - trainer - INFO -     accuracy       : 0.656788
2024-04-27 18:16:20,586 - trainer - INFO -     macro_f        : 0.635895
2024-04-27 18:16:20,586 - trainer - INFO -     precision      : 0.665075
2024-04-27 18:16:20,586 - trainer - INFO -     recall         : 0.656788
2024-04-27 18:16:20,586 - trainer - INFO -     doc_entropy    : 2.5296
2024-04-27 18:16:20,586 - trainer - INFO -     val_loss       : 1.001507
2024-04-27 18:16:20,586 - trainer - INFO -     val_accuracy   : 0.70467
2024-04-27 18:16:20,586 - trainer - INFO -     val_macro_f    : 0.698591
2024-04-27 18:16:20,586 - trainer - INFO -     val_precision  : 0.737816
2024-04-27 18:16:20,586 - trainer - INFO -     val_recall     : 0.70467
2024-04-27 18:16:20,586 - trainer - INFO -     val_doc_entropy: 2.503093
2024-04-27 18:16:20,586 - trainer - INFO -     test_loss      : 1.004808
2024-04-27 18:16:20,586 - trainer - INFO -     test_accuracy  : 0.703276
2024-04-27 18:16:20,586 - trainer - INFO -     test_macro_f   : 0.6993
2024-04-27 18:16:20,586 - trainer - INFO -     test_precision : 0.742118
2024-04-27 18:16:20,586 - trainer - INFO -     test_recall    : 0.703276
2024-04-27 18:16:20,586 - trainer - INFO -     test_doc_entropy: 2.500982
2024-04-27 18:24:08,826 - train - INFO - BiAttentionClassifyModel(
  (embedding): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 48,035,427
Freeze params: 0
2024-04-27 18:36:15,380 - trainer - INFO -     epoch          : 1
2024-04-27 18:36:15,380 - trainer - INFO -     loss           : 1.193257
2024-04-27 18:36:15,380 - trainer - INFO -     accuracy       : 0.656788
2024-04-27 18:36:15,380 - trainer - INFO -     macro_f        : 0.635895
2024-04-27 18:36:15,380 - trainer - INFO -     precision      : 0.665075
2024-04-27 18:36:15,380 - trainer - INFO -     recall         : 0.656788
2024-04-27 18:36:15,380 - trainer - INFO -     doc_entropy    : 2.5296
2024-04-27 18:36:15,380 - trainer - INFO -     val_loss       : 1.001507
2024-04-27 18:36:15,380 - trainer - INFO -     val_accuracy   : 0.70467
2024-04-27 18:36:15,396 - trainer - INFO -     val_macro_f    : 0.698591
2024-04-27 18:36:15,396 - trainer - INFO -     val_precision  : 0.737816
2024-04-27 18:36:15,396 - trainer - INFO -     val_recall     : 0.70467
2024-04-27 18:36:15,396 - trainer - INFO -     val_doc_entropy: 2.503093
2024-04-27 18:36:15,396 - trainer - INFO -     test_loss      : 1.004808
2024-04-27 18:36:15,396 - trainer - INFO -     test_accuracy  : 0.703276
2024-04-27 18:36:15,396 - trainer - INFO -     test_macro_f   : 0.6993
2024-04-27 18:36:15,396 - trainer - INFO -     test_precision : 0.742118
2024-04-27 18:36:15,396 - trainer - INFO -     test_recall    : 0.703276
2024-04-27 18:36:15,396 - trainer - INFO -     test_doc_entropy: 2.500982
2024-04-27 18:48:16,977 - trainer - INFO -     epoch          : 2
2024-04-27 18:48:16,977 - trainer - INFO -     loss           : 0.872097
2024-04-27 18:48:16,977 - trainer - INFO -     accuracy       : 0.738791
2024-04-27 18:48:16,977 - trainer - INFO -     macro_f        : 0.728927
2024-04-27 18:48:16,977 - trainer - INFO -     precision      : 0.762361
2024-04-27 18:48:16,977 - trainer - INFO -     recall         : 0.738791
2024-04-27 18:48:16,977 - trainer - INFO -     doc_entropy    : 2.459673
2024-04-27 18:48:16,977 - trainer - INFO -     val_loss       : 0.940284
2024-04-27 18:48:16,977 - trainer - INFO -     val_accuracy   : 0.721796
2024-04-27 18:48:16,977 - trainer - INFO -     val_macro_f    : 0.715756
2024-04-27 18:48:16,977 - trainer - INFO -     val_precision  : 0.754466
2024-04-27 18:48:16,977 - trainer - INFO -     val_recall     : 0.721796
2024-04-27 18:48:16,977 - trainer - INFO -     val_doc_entropy: 2.476518
2024-04-27 18:48:16,977 - trainer - INFO -     test_loss      : 0.946789
2024-04-27 18:48:16,977 - trainer - INFO -     test_accuracy  : 0.720153
2024-04-27 18:48:16,977 - trainer - INFO -     test_macro_f   : 0.716056
2024-04-27 18:48:16,977 - trainer - INFO -     test_precision : 0.757283
2024-04-27 18:48:16,977 - trainer - INFO -     test_recall    : 0.720153
2024-04-27 18:48:16,977 - trainer - INFO -     test_doc_entropy: 2.474971
2024-04-27 19:00:21,598 - trainer - INFO -     epoch          : 3
2024-04-27 19:00:21,598 - trainer - INFO -     loss           : 0.698232
2024-04-27 19:00:21,598 - trainer - INFO -     accuracy       : 0.786788
2024-04-27 19:00:21,598 - trainer - INFO -     macro_f        : 0.779034
2024-04-27 19:00:21,598 - trainer - INFO -     precision      : 0.809318
2024-04-27 19:00:21,598 - trainer - INFO -     recall         : 0.786788
2024-04-27 19:00:21,598 - trainer - INFO -     doc_entropy    : 2.3243
2024-04-27 19:00:21,598 - trainer - INFO -     val_loss       : 0.975455
2024-04-27 19:00:21,598 - trainer - INFO -     val_accuracy   : 0.72324
2024-04-27 19:00:21,598 - trainer - INFO -     val_macro_f    : 0.719359
2024-04-27 19:00:21,598 - trainer - INFO -     val_precision  : 0.759413
2024-04-27 19:00:21,598 - trainer - INFO -     val_recall     : 0.72324
2024-04-27 19:00:21,598 - trainer - INFO -     val_doc_entropy: 2.280466
2024-04-27 19:00:21,598 - trainer - INFO -     test_loss      : 0.981822
2024-04-27 19:00:21,598 - trainer - INFO -     test_accuracy  : 0.71871
2024-04-27 19:00:21,598 - trainer - INFO -     test_macro_f   : 0.71477
2024-04-27 19:00:21,598 - trainer - INFO -     test_precision : 0.754976
2024-04-27 19:00:21,598 - trainer - INFO -     test_recall    : 0.71871
2024-04-27 19:00:21,598 - trainer - INFO -     test_doc_entropy: 2.277306
2024-04-27 19:01:40,607 - train - INFO - BiAttentionClassifyModel(
  (embedding): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 48,035,427
Freeze params: 0
2024-04-27 19:13:45,380 - trainer - INFO -     epoch          : 1
2024-04-27 19:13:45,380 - trainer - INFO -     loss           : 1.190234
2024-04-27 19:13:45,380 - trainer - INFO -     accuracy       : 0.656657
2024-04-27 19:13:45,380 - trainer - INFO -     macro_f        : 0.635791
2024-04-27 19:13:45,380 - trainer - INFO -     precision      : 0.664675
2024-04-27 19:13:45,380 - trainer - INFO -     recall         : 0.656657
2024-04-27 19:13:45,380 - trainer - INFO -     doc_entropy    : 2.505354
2024-04-27 19:13:45,380 - trainer - INFO -     val_loss       : 1.007589
2024-04-27 19:13:45,380 - trainer - INFO -     val_accuracy   : 0.703824
2024-04-27 19:13:45,380 - trainer - INFO -     val_macro_f    : 0.693203
2024-04-27 19:13:45,380 - trainer - INFO -     val_precision  : 0.729707
2024-04-27 19:13:45,380 - trainer - INFO -     val_recall     : 0.703824
2024-04-27 19:13:45,380 - trainer - INFO -     val_doc_entropy: 2.425836
2024-04-27 19:13:45,380 - trainer - INFO -     test_loss      : 1.013208
2024-04-27 19:13:45,380 - trainer - INFO -     test_accuracy  : 0.704521
2024-04-27 19:13:45,380 - trainer - INFO -     test_macro_f   : 0.695242
2024-04-27 19:13:45,380 - trainer - INFO -     test_precision : 0.735719
2024-04-27 19:13:45,380 - trainer - INFO -     test_recall    : 0.704521
2024-04-27 19:13:45,380 - trainer - INFO -     test_doc_entropy: 2.420735
2024-04-27 19:25:48,383 - trainer - INFO -     epoch          : 2
2024-04-27 19:25:48,383 - trainer - INFO -     loss           : 0.86836
2024-04-27 19:25:48,383 - trainer - INFO -     accuracy       : 0.74021
2024-04-27 19:25:48,383 - trainer - INFO -     macro_f        : 0.730217
2024-04-27 19:25:48,383 - trainer - INFO -     precision      : 0.763798
2024-04-27 19:25:48,383 - trainer - INFO -     recall         : 0.74021
2024-04-27 19:25:48,383 - trainer - INFO -     doc_entropy    : 2.430945
2024-04-27 19:25:48,383 - trainer - INFO -     val_loss       : 0.945191
2024-04-27 19:25:48,383 - trainer - INFO -     val_accuracy   : 0.723439
2024-04-27 19:25:48,383 - trainer - INFO -     val_macro_f    : 0.716046
2024-04-27 19:25:48,383 - trainer - INFO -     val_precision  : 0.754402
2024-04-27 19:25:48,383 - trainer - INFO -     val_recall     : 0.723439
2024-04-27 19:25:48,383 - trainer - INFO -     val_doc_entropy: 2.474231
2024-04-27 19:25:48,383 - trainer - INFO -     test_loss      : 0.959232
2024-04-27 19:25:48,383 - trainer - INFO -     test_accuracy  : 0.713681
2024-04-27 19:25:48,383 - trainer - INFO -     test_macro_f   : 0.708097
2024-04-27 19:25:48,383 - trainer - INFO -     test_precision : 0.749428
2024-04-27 19:25:48,383 - trainer - INFO -     test_recall    : 0.713681
2024-04-27 19:25:48,383 - trainer - INFO -     test_doc_entropy: 2.470088
2024-04-27 19:37:45,961 - trainer - INFO -     epoch          : 3
2024-04-27 19:37:45,961 - trainer - INFO -     loss           : 0.698682
2024-04-27 19:37:45,961 - trainer - INFO -     accuracy       : 0.787828
2024-04-27 19:37:45,961 - trainer - INFO -     macro_f        : 0.779918
2024-04-27 19:37:45,961 - trainer - INFO -     precision      : 0.809798
2024-04-27 19:37:45,961 - trainer - INFO -     recall         : 0.787828
2024-04-27 19:37:45,961 - trainer - INFO -     doc_entropy    : 2.301632
2024-04-27 19:37:45,961 - trainer - INFO -     val_loss       : 0.965545
2024-04-27 19:37:45,961 - trainer - INFO -     val_accuracy   : 0.724286
2024-04-27 19:37:45,961 - trainer - INFO -     val_macro_f    : 0.721999
2024-04-27 19:37:45,961 - trainer - INFO -     val_precision  : 0.763705
2024-04-27 19:37:45,961 - trainer - INFO -     val_recall     : 0.724286
2024-04-27 19:37:45,961 - trainer - INFO -     val_doc_entropy: 2.263597
2024-04-27 19:37:45,961 - trainer - INFO -     test_loss      : 0.970808
2024-04-27 19:37:45,961 - trainer - INFO -     test_accuracy  : 0.722941
2024-04-27 19:37:45,961 - trainer - INFO -     test_macro_f   : 0.720471
2024-04-27 19:37:45,961 - trainer - INFO -     test_precision : 0.763247
2024-04-27 19:37:45,961 - trainer - INFO -     test_recall    : 0.722941
2024-04-27 19:37:45,961 - trainer - INFO -     test_doc_entropy: 2.259922
2024-04-27 19:39:02,968 - train - INFO - BiAttentionClassifyModel(
  (embedding): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 48,035,427
Freeze params: 0
2024-04-27 19:50:59,108 - trainer - INFO -     epoch          : 1
2024-04-27 19:50:59,108 - trainer - INFO -     loss           : 1.189197
2024-04-27 19:50:59,108 - trainer - INFO -     accuracy       : 0.657727
2024-04-27 19:50:59,108 - trainer - INFO -     macro_f        : 0.636636
2024-04-27 19:50:59,108 - trainer - INFO -     precision      : 0.665524
2024-04-27 19:50:59,108 - trainer - INFO -     recall         : 0.657727
2024-04-27 19:50:59,108 - trainer - INFO -     doc_entropy    : 2.463126
2024-04-27 19:50:59,108 - trainer - INFO -     val_loss       : 1.01228
2024-04-27 19:50:59,108 - trainer - INFO -     val_accuracy   : 0.703176
2024-04-27 19:50:59,108 - trainer - INFO -     val_macro_f    : 0.698398
2024-04-27 19:50:59,108 - trainer - INFO -     val_precision  : 0.739475
2024-04-27 19:50:59,108 - trainer - INFO -     val_recall     : 0.703176
2024-04-27 19:50:59,108 - trainer - INFO -     val_doc_entropy: 2.364305
2024-04-27 19:50:59,108 - trainer - INFO -     test_loss      : 1.015351
2024-04-27 19:50:59,108 - trainer - INFO -     test_accuracy  : 0.70238
2024-04-27 19:50:59,108 - trainer - INFO -     test_macro_f   : 0.69849
2024-04-27 19:50:59,108 - trainer - INFO -     test_precision : 0.743711
2024-04-27 19:50:59,108 - trainer - INFO -     test_recall    : 0.70238
2024-04-27 19:50:59,108 - trainer - INFO -     test_doc_entropy: 2.359773
2024-04-27 20:03:06,103 - trainer - INFO -     epoch          : 2
2024-04-27 20:03:06,103 - trainer - INFO -     loss           : 0.86803
2024-04-27 20:03:06,103 - trainer - INFO -     accuracy       : 0.740664
2024-04-27 20:03:06,103 - trainer - INFO -     macro_f        : 0.730739
2024-04-27 20:03:06,103 - trainer - INFO -     precision      : 0.76404
2024-04-27 20:03:06,103 - trainer - INFO -     recall         : 0.740664
2024-04-27 20:03:06,103 - trainer - INFO -     doc_entropy    : 2.393875
2024-04-27 20:03:06,103 - trainer - INFO -     val_loss       : 0.962662
2024-04-27 20:03:06,103 - trainer - INFO -     val_accuracy   : 0.717017
2024-04-27 20:03:06,103 - trainer - INFO -     val_macro_f    : 0.710938
2024-04-27 20:03:06,103 - trainer - INFO -     val_precision  : 0.750436
2024-04-27 20:03:06,103 - trainer - INFO -     val_recall     : 0.717017
2024-04-27 20:03:06,103 - trainer - INFO -     val_doc_entropy: 2.278667
2024-04-27 20:03:06,103 - trainer - INFO -     test_loss      : 0.967164
2024-04-27 20:03:06,103 - trainer - INFO -     test_accuracy  : 0.718212
2024-04-27 20:03:06,103 - trainer - INFO -     test_macro_f   : 0.712847
2024-04-27 20:03:06,103 - trainer - INFO -     test_precision : 0.75454
2024-04-27 20:03:06,103 - trainer - INFO -     test_recall    : 0.718212
2024-04-27 20:03:06,103 - trainer - INFO -     test_doc_entropy: 2.274776
2024-04-27 20:15:15,239 - trainer - INFO -     epoch          : 3
2024-04-27 20:15:15,239 - trainer - INFO -     loss           : 0.697727
2024-04-27 20:15:15,239 - trainer - INFO -     accuracy       : 0.787268
2024-04-27 20:15:15,239 - trainer - INFO -     macro_f        : 0.779972
2024-04-27 20:15:15,239 - trainer - INFO -     precision      : 0.810827
2024-04-27 20:15:15,239 - trainer - INFO -     recall         : 0.787268
2024-04-27 20:15:15,239 - trainer - INFO -     doc_entropy    : 2.265913
2024-04-27 20:15:15,239 - trainer - INFO -     val_loss       : 0.957835
2024-04-27 20:15:15,239 - trainer - INFO -     val_accuracy   : 0.724584
2024-04-27 20:15:15,239 - trainer - INFO -     val_macro_f    : 0.716692
2024-04-27 20:15:15,239 - trainer - INFO -     val_precision  : 0.752484
2024-04-27 20:15:15,239 - trainer - INFO -     val_recall     : 0.724584
2024-04-27 20:15:15,239 - trainer - INFO -     val_doc_entropy: 2.293407
2024-04-27 20:15:15,239 - trainer - INFO -     test_loss      : 0.967094
2024-04-27 20:15:15,255 - trainer - INFO -     test_accuracy  : 0.72314
2024-04-27 20:15:15,255 - trainer - INFO -     test_macro_f   : 0.716437
2024-04-27 20:15:15,255 - trainer - INFO -     test_precision : 0.755705
2024-04-27 20:15:15,255 - trainer - INFO -     test_recall    : 0.72314
2024-04-27 20:15:15,255 - trainer - INFO -     test_doc_entropy: 2.290271
2024-04-27 20:16:32,184 - train - INFO - BiAttentionClassifyModel(
  (embedding): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 48,035,427
Freeze params: 0
2024-04-27 20:28:29,862 - trainer - INFO -     epoch          : 1
2024-04-27 20:28:29,862 - trainer - INFO -     loss           : 1.190645
2024-04-27 20:28:29,862 - trainer - INFO -     accuracy       : 0.657852
2024-04-27 20:28:29,862 - trainer - INFO -     macro_f        : 0.63682
2024-04-27 20:28:29,862 - trainer - INFO -     precision      : 0.666017
2024-04-27 20:28:29,862 - trainer - INFO -     recall         : 0.657852
2024-04-27 20:28:29,862 - trainer - INFO -     doc_entropy    : 2.451011
2024-04-27 20:28:29,862 - trainer - INFO -     val_loss       : 0.99913
2024-04-27 20:28:29,862 - trainer - INFO -     val_accuracy   : 0.707209
2024-04-27 20:28:29,862 - trainer - INFO -     val_macro_f    : 0.695167
2024-04-27 20:28:29,862 - trainer - INFO -     val_precision  : 0.729938
2024-04-27 20:28:29,862 - trainer - INFO -     val_recall     : 0.707209
2024-04-27 20:28:29,862 - trainer - INFO -     val_doc_entropy: 2.479617
2024-04-27 20:28:29,862 - trainer - INFO -     test_loss      : 0.997853
2024-04-27 20:28:29,862 - trainer - INFO -     test_accuracy  : 0.704072
2024-04-27 20:28:29,862 - trainer - INFO -     test_macro_f   : 0.693202
2024-04-27 20:28:29,862 - trainer - INFO -     test_precision : 0.730477
2024-04-27 20:28:29,862 - trainer - INFO -     test_recall    : 0.704072
2024-04-27 20:28:29,862 - trainer - INFO -     test_doc_entropy: 2.474616
2024-04-27 20:40:25,623 - trainer - INFO -     epoch          : 2
2024-04-27 20:40:25,623 - trainer - INFO -     loss           : 0.868314
2024-04-27 20:40:25,623 - trainer - INFO -     accuracy       : 0.740235
2024-04-27 20:40:25,623 - trainer - INFO -     macro_f        : 0.730713
2024-04-27 20:40:25,623 - trainer - INFO -     precision      : 0.764686
2024-04-27 20:40:25,623 - trainer - INFO -     recall         : 0.740235
2024-04-27 20:40:25,638 - trainer - INFO -     doc_entropy    : 2.409198
2024-04-27 20:40:25,638 - trainer - INFO -     val_loss       : 0.972694
2024-04-27 20:40:25,638 - trainer - INFO -     val_accuracy   : 0.71856
2024-04-27 20:40:25,638 - trainer - INFO -     val_macro_f    : 0.70833
2024-04-27 20:40:25,638 - trainer - INFO -     val_precision  : 0.742998
2024-04-27 20:40:25,638 - trainer - INFO -     val_recall     : 0.71856
2024-04-27 20:40:25,638 - trainer - INFO -     val_doc_entropy: 2.364863
2024-04-27 20:40:25,638 - trainer - INFO -     test_loss      : 0.977074
2024-04-27 20:40:25,638 - trainer - INFO -     test_accuracy  : 0.71637
2024-04-27 20:40:25,638 - trainer - INFO -     test_macro_f   : 0.707309
2024-04-27 20:40:25,638 - trainer - INFO -     test_precision : 0.743857
2024-04-27 20:40:25,638 - trainer - INFO -     test_recall    : 0.71637
2024-04-27 20:40:25,638 - trainer - INFO -     test_doc_entropy: 2.359382
2024-04-27 20:52:20,141 - trainer - INFO -     epoch          : 3
2024-04-27 20:52:20,141 - trainer - INFO -     loss           : 0.696554
2024-04-27 20:52:20,141 - trainer - INFO -     accuracy       : 0.787174
2024-04-27 20:52:20,141 - trainer - INFO -     macro_f        : 0.780124
2024-04-27 20:52:20,141 - trainer - INFO -     precision      : 0.810863
2024-04-27 20:52:20,141 - trainer - INFO -     recall         : 0.787174
2024-04-27 20:52:20,141 - trainer - INFO -     doc_entropy    : 2.271161
2024-04-27 20:52:20,141 - trainer - INFO -     val_loss       : 0.973944
2024-04-27 20:52:20,141 - trainer - INFO -     val_accuracy   : 0.725281
2024-04-27 20:52:20,141 - trainer - INFO -     val_macro_f    : 0.717051
2024-04-27 20:52:20,141 - trainer - INFO -     val_precision  : 0.751609
2024-04-27 20:52:20,141 - trainer - INFO -     val_recall     : 0.725281
2024-04-27 20:52:20,141 - trainer - INFO -     val_doc_entropy: 2.229994
2024-04-27 20:52:20,141 - trainer - INFO -     test_loss      : 0.977417
2024-04-27 20:52:20,145 - trainer - INFO -     test_accuracy  : 0.725729
2024-04-27 20:52:20,145 - trainer - INFO -     test_macro_f   : 0.719725
2024-04-27 20:52:20,145 - trainer - INFO -     test_precision : 0.758319
2024-04-27 20:52:20,145 - trainer - INFO -     test_recall    : 0.725729
2024-04-27 20:52:20,145 - trainer - INFO -     test_doc_entropy: 2.222329
2024-04-29 13:54:52,647 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,049,395
Freeze params: 0
2024-04-29 13:55:16,310 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,049,395
Freeze params: 0
2024-04-29 13:56:02,902 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=400, bias=True)
    (1): Tanh()
    (2): Linear(in_features=400, out_features=20, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=20, bias=False)
  (W_q): Linear(in_features=300, out_features=20, bias=False)
  (W_v): Linear(in_features=300, out_features=20, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((20, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,882,975
Freeze params: 0
2024-04-29 13:59:48,686 - trainer - INFO -     epoch          : 1
2024-04-29 13:59:48,686 - trainer - INFO -     loss           : 1.156639
2024-04-29 13:59:48,686 - trainer - INFO -     accuracy       : 0.668214
2024-04-29 13:59:48,686 - trainer - INFO -     macro_f        : 0.64952
2024-04-29 13:59:48,686 - trainer - INFO -     precision      : 0.680416
2024-04-29 13:59:48,686 - trainer - INFO -     recall         : 0.668214
2024-04-29 13:59:48,686 - trainer - INFO -     doc_entropy    : 2.399706
2024-04-29 13:59:48,686 - trainer - INFO -     val_loss       : 1.018368
2024-04-29 13:59:48,686 - trainer - INFO -     val_accuracy   : 0.700388
2024-04-29 13:59:48,686 - trainer - INFO -     val_macro_f    : 0.686702
2024-04-29 13:59:48,686 - trainer - INFO -     val_precision  : 0.719016
2024-04-29 13:59:48,686 - trainer - INFO -     val_recall     : 0.700388
2024-04-29 13:59:48,686 - trainer - INFO -     val_doc_entropy: 2.81034
2024-04-29 13:59:48,686 - trainer - INFO -     test_loss      : 1.010235
2024-04-29 13:59:48,686 - trainer - INFO -     test_accuracy  : 0.703674
2024-04-29 13:59:48,686 - trainer - INFO -     test_macro_f   : 0.691504
2024-04-29 13:59:48,686 - trainer - INFO -     test_precision : 0.723031
2024-04-29 13:59:48,686 - trainer - INFO -     test_recall    : 0.703674
2024-04-29 13:59:48,686 - trainer - INFO -     test_doc_entropy: 2.813979
2024-04-29 14:03:52,886 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=400, bias=True)
    (1): Tanh()
    (2): Linear(in_features=400, out_features=20, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=20, bias=False)
  (W_q): Linear(in_features=300, out_features=20, bias=False)
  (W_v): Linear(in_features=300, out_features=20, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((20, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,882,975
Freeze params: 0
2024-04-29 14:07:50,117 - trainer - INFO -     epoch          : 1
2024-04-29 14:07:50,117 - trainer - INFO -     loss           : 1.156639
2024-04-29 14:07:50,117 - trainer - INFO -     accuracy       : 0.668214
2024-04-29 14:07:50,117 - trainer - INFO -     macro_f        : 0.64952
2024-04-29 14:07:50,117 - trainer - INFO -     precision      : 0.680416
2024-04-29 14:07:50,117 - trainer - INFO -     recall         : 0.668214
2024-04-29 14:07:50,117 - trainer - INFO -     doc_entropy    : 2.399706
2024-04-29 14:07:50,117 - trainer - INFO -     val_loss       : 1.018368
2024-04-29 14:07:50,117 - trainer - INFO -     val_accuracy   : 0.700388
2024-04-29 14:07:50,117 - trainer - INFO -     val_macro_f    : 0.686702
2024-04-29 14:07:50,117 - trainer - INFO -     val_precision  : 0.719016
2024-04-29 14:07:50,117 - trainer - INFO -     val_recall     : 0.700388
2024-04-29 14:07:50,117 - trainer - INFO -     val_doc_entropy: 2.81034
2024-04-29 14:07:50,117 - trainer - INFO -     test_loss      : 1.010235
2024-04-29 14:07:50,117 - trainer - INFO -     test_accuracy  : 0.703674
2024-04-29 14:07:50,117 - trainer - INFO -     test_macro_f   : 0.691504
2024-04-29 14:07:50,117 - trainer - INFO -     test_precision : 0.723031
2024-04-29 14:07:50,117 - trainer - INFO -     test_recall    : 0.703674
2024-04-29 14:07:50,117 - trainer - INFO -     test_doc_entropy: 2.813979
2024-04-29 14:11:56,227 - trainer - INFO -     epoch          : 2
2024-04-29 14:11:56,227 - trainer - INFO -     loss           : 0.815695
2024-04-29 14:11:56,227 - trainer - INFO -     accuracy       : 0.755458
2024-04-29 14:11:56,227 - trainer - INFO -     macro_f        : 0.746568
2024-04-29 14:11:56,227 - trainer - INFO -     precision      : 0.7797
2024-04-29 14:11:56,227 - trainer - INFO -     recall         : 0.755458
2024-04-29 14:11:56,227 - trainer - INFO -     doc_entropy    : 2.497726
2024-04-29 14:11:56,227 - trainer - INFO -     val_loss       : 0.993271
2024-04-29 14:11:56,227 - trainer - INFO -     val_accuracy   : 0.712486
2024-04-29 14:11:56,227 - trainer - INFO -     val_macro_f    : 0.705127
2024-04-29 14:11:56,227 - trainer - INFO -     val_precision  : 0.741746
2024-04-29 14:11:56,227 - trainer - INFO -     val_recall     : 0.712486
2024-04-29 14:11:56,227 - trainer - INFO -     val_doc_entropy: 2.741778
2024-04-29 14:11:56,227 - trainer - INFO -     test_loss      : 0.981688
2024-04-29 14:11:56,227 - trainer - INFO -     test_accuracy  : 0.71393
2024-04-29 14:11:56,227 - trainer - INFO -     test_macro_f   : 0.705546
2024-04-29 14:11:56,227 - trainer - INFO -     test_precision : 0.743228
2024-04-29 14:11:56,227 - trainer - INFO -     test_recall    : 0.71393
2024-04-29 14:11:56,227 - trainer - INFO -     test_doc_entropy: 2.742927
2024-04-29 14:16:04,983 - trainer - INFO -     epoch          : 3
2024-04-29 14:16:04,983 - trainer - INFO -     loss           : 0.592748
2024-04-29 14:16:04,983 - trainer - INFO -     accuracy       : 0.819793
2024-04-29 14:16:04,983 - trainer - INFO -     macro_f        : 0.814021
2024-04-29 14:16:04,983 - trainer - INFO -     precision      : 0.842262
2024-04-29 14:16:04,983 - trainer - INFO -     recall         : 0.819793
2024-04-29 14:16:04,983 - trainer - INFO -     doc_entropy    : 2.372802
2024-04-29 14:16:04,983 - trainer - INFO -     val_loss       : 1.081745
2024-04-29 14:16:04,983 - trainer - INFO -     val_accuracy   : 0.697551
2024-04-29 14:16:04,983 - trainer - INFO -     val_macro_f    : 0.693582
2024-04-29 14:16:04,983 - trainer - INFO -     val_precision  : 0.735141
2024-04-29 14:16:04,983 - trainer - INFO -     val_recall     : 0.697551
2024-04-29 14:16:04,983 - trainer - INFO -     val_doc_entropy: 2.755032
2024-04-29 14:16:04,983 - trainer - INFO -     test_loss      : 1.068567
2024-04-29 14:16:04,983 - trainer - INFO -     test_accuracy  : 0.699393
2024-04-29 14:16:04,983 - trainer - INFO -     test_macro_f   : 0.696195
2024-04-29 14:16:04,983 - trainer - INFO -     test_precision : 0.740565
2024-04-29 14:16:04,983 - trainer - INFO -     test_recall    : 0.699393
2024-04-29 14:16:04,983 - trainer - INFO -     test_doc_entropy: 2.756075
2024-04-29 14:16:49,010 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=400, bias=True)
    (1): Tanh()
    (2): Linear(in_features=400, out_features=20, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=20, bias=False)
  (W_q): Linear(in_features=300, out_features=20, bias=False)
  (W_v): Linear(in_features=300, out_features=20, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((20, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,882,975
Freeze params: 0
2024-04-29 14:21:00,307 - trainer - INFO -     epoch          : 1
2024-04-29 14:21:00,307 - trainer - INFO -     loss           : 1.154661
2024-04-29 14:21:00,307 - trainer - INFO -     accuracy       : 0.66972
2024-04-29 14:21:00,307 - trainer - INFO -     macro_f        : 0.650348
2024-04-29 14:21:00,307 - trainer - INFO -     precision      : 0.680104
2024-04-29 14:21:00,307 - trainer - INFO -     recall         : 0.66972
2024-04-29 14:21:00,307 - trainer - INFO -     doc_entropy    : 2.331861
2024-04-29 14:21:00,307 - trainer - INFO -     val_loss       : 0.996703
2024-04-29 14:21:00,307 - trainer - INFO -     val_accuracy   : 0.710047
2024-04-29 14:21:00,307 - trainer - INFO -     val_macro_f    : 0.701542
2024-04-29 14:21:00,307 - trainer - INFO -     val_precision  : 0.736787
2024-04-29 14:21:00,307 - trainer - INFO -     val_recall     : 0.710047
2024-04-29 14:21:00,307 - trainer - INFO -     val_doc_entropy: 2.777187
2024-04-29 14:21:00,307 - trainer - INFO -     test_loss      : 0.998256
2024-04-29 14:21:00,307 - trainer - INFO -     test_accuracy  : 0.708603
2024-04-29 14:21:00,307 - trainer - INFO -     test_macro_f   : 0.700922
2024-04-29 14:21:00,307 - trainer - INFO -     test_precision : 0.737402
2024-04-29 14:21:00,307 - trainer - INFO -     test_recall    : 0.708603
2024-04-29 14:21:00,307 - trainer - INFO -     test_doc_entropy: 2.780958
2024-04-29 14:25:15,943 - trainer - INFO -     epoch          : 2
2024-04-29 14:25:15,943 - trainer - INFO -     loss           : 0.818224
2024-04-29 14:25:15,943 - trainer - INFO -     accuracy       : 0.755415
2024-04-29 14:25:15,943 - trainer - INFO -     macro_f        : 0.74622
2024-04-29 14:25:15,943 - trainer - INFO -     precision      : 0.778898
2024-04-29 14:25:15,943 - trainer - INFO -     recall         : 0.755415
2024-04-29 14:25:15,943 - trainer - INFO -     doc_entropy    : 2.493352
2024-04-29 14:25:15,943 - trainer - INFO -     val_loss       : 0.990766
2024-04-29 14:25:15,943 - trainer - INFO -     val_accuracy   : 0.710296
2024-04-29 14:25:15,943 - trainer - INFO -     val_macro_f    : 0.700559
2024-04-29 14:25:15,943 - trainer - INFO -     val_precision  : 0.737077
2024-04-29 14:25:15,943 - trainer - INFO -     val_recall     : 0.710296
2024-04-29 14:25:15,943 - trainer - INFO -     val_doc_entropy: 2.786187
2024-04-29 14:25:15,943 - trainer - INFO -     test_loss      : 0.983286
2024-04-29 14:25:15,943 - trainer - INFO -     test_accuracy  : 0.709698
2024-04-29 14:25:15,943 - trainer - INFO -     test_macro_f   : 0.700253
2024-04-29 14:25:15,943 - trainer - INFO -     test_precision : 0.736996
2024-04-29 14:25:15,943 - trainer - INFO -     test_recall    : 0.709698
2024-04-29 14:25:15,943 - trainer - INFO -     test_doc_entropy: 2.789123
2024-04-29 14:29:16,502 - trainer - INFO -     epoch          : 3
2024-04-29 14:29:16,502 - trainer - INFO -     loss           : 0.591509
2024-04-29 14:29:16,502 - trainer - INFO -     accuracy       : 0.819637
2024-04-29 14:29:16,502 - trainer - INFO -     macro_f        : 0.814521
2024-04-29 14:29:16,502 - trainer - INFO -     precision      : 0.84354
2024-04-29 14:29:16,502 - trainer - INFO -     recall         : 0.819637
2024-04-29 14:29:16,502 - trainer - INFO -     doc_entropy    : 2.352694
2024-04-29 14:29:16,502 - trainer - INFO -     val_loss       : 1.08564
2024-04-29 14:29:16,502 - trainer - INFO -     val_accuracy   : 0.699592
2024-04-29 14:29:16,502 - trainer - INFO -     val_macro_f    : 0.691517
2024-04-29 14:29:16,502 - trainer - INFO -     val_precision  : 0.729006
2024-04-29 14:29:16,517 - trainer - INFO -     val_recall     : 0.699592
2024-04-29 14:29:16,517 - trainer - INFO -     val_doc_entropy: 2.82052
2024-04-29 14:29:16,517 - trainer - INFO -     test_loss      : 1.065272
2024-04-29 14:29:16,517 - trainer - INFO -     test_accuracy  : 0.701882
2024-04-29 14:29:16,517 - trainer - INFO -     test_macro_f   : 0.691556
2024-04-29 14:29:16,517 - trainer - INFO -     test_precision : 0.726159
2024-04-29 14:29:16,517 - trainer - INFO -     test_recall    : 0.701882
2024-04-29 14:29:16,517 - trainer - INFO -     test_doc_entropy: 2.822608
2024-04-29 14:29:54,833 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=400, bias=True)
    (1): Tanh()
    (2): Linear(in_features=400, out_features=20, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=20, bias=False)
  (W_q): Linear(in_features=300, out_features=20, bias=False)
  (W_v): Linear(in_features=300, out_features=20, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((20, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,882,975
Freeze params: 0
2024-04-29 14:34:02,486 - trainer - INFO -     epoch          : 1
2024-04-29 14:34:02,486 - trainer - INFO -     loss           : 1.156761
2024-04-29 14:34:02,486 - trainer - INFO -     accuracy       : 0.668308
2024-04-29 14:34:02,486 - trainer - INFO -     macro_f        : 0.649491
2024-04-29 14:34:02,486 - trainer - INFO -     precision      : 0.680549
2024-04-29 14:34:02,486 - trainer - INFO -     recall         : 0.668308
2024-04-29 14:34:02,486 - trainer - INFO -     doc_entropy    : 2.264519
2024-04-29 14:34:02,486 - trainer - INFO -     val_loss       : 1.010083
2024-04-29 14:34:02,486 - trainer - INFO -     val_accuracy   : 0.70706
2024-04-29 14:34:02,486 - trainer - INFO -     val_macro_f    : 0.696504
2024-04-29 14:34:02,486 - trainer - INFO -     val_precision  : 0.733088
2024-04-29 14:34:02,486 - trainer - INFO -     val_recall     : 0.70706
2024-04-29 14:34:02,486 - trainer - INFO -     val_doc_entropy: 2.83219
2024-04-29 14:34:02,486 - trainer - INFO -     test_loss      : 1.00746
2024-04-29 14:34:02,486 - trainer - INFO -     test_accuracy  : 0.704321
2024-04-29 14:34:02,486 - trainer - INFO -     test_macro_f   : 0.693709
2024-04-29 14:34:02,486 - trainer - INFO -     test_precision : 0.729704
2024-04-29 14:34:02,486 - trainer - INFO -     test_recall    : 0.704321
2024-04-29 14:34:02,486 - trainer - INFO -     test_doc_entropy: 2.836001
2024-04-29 14:38:05,969 - trainer - INFO -     epoch          : 2
2024-04-29 14:38:05,969 - trainer - INFO -     loss           : 0.819558
2024-04-29 14:38:05,969 - trainer - INFO -     accuracy       : 0.754935
2024-04-29 14:38:05,969 - trainer - INFO -     macro_f        : 0.745913
2024-04-29 14:38:05,969 - trainer - INFO -     precision      : 0.778585
2024-04-29 14:38:05,969 - trainer - INFO -     recall         : 0.754935
2024-04-29 14:38:05,969 - trainer - INFO -     doc_entropy    : 2.417638
2024-04-29 14:38:05,969 - trainer - INFO -     val_loss       : 0.990437
2024-04-29 14:38:05,969 - trainer - INFO -     val_accuracy   : 0.711291
2024-04-29 14:38:05,969 - trainer - INFO -     val_macro_f    : 0.704118
2024-04-29 14:38:05,969 - trainer - INFO -     val_precision  : 0.74366
2024-04-29 14:38:05,969 - trainer - INFO -     val_recall     : 0.711291
2024-04-29 14:38:05,969 - trainer - INFO -     val_doc_entropy: 2.739244
2024-04-29 14:38:05,969 - trainer - INFO -     test_loss      : 0.980079
2024-04-29 14:38:05,969 - trainer - INFO -     test_accuracy  : 0.714079
2024-04-29 14:38:05,969 - trainer - INFO -     test_macro_f   : 0.706904
2024-04-29 14:38:05,969 - trainer - INFO -     test_precision : 0.746728
2024-04-29 14:38:05,969 - trainer - INFO -     test_recall    : 0.714079
2024-04-29 14:38:05,969 - trainer - INFO -     test_doc_entropy: 2.742703
2024-04-29 14:42:12,064 - trainer - INFO -     epoch          : 3
2024-04-29 14:42:12,064 - trainer - INFO -     loss           : 0.592963
2024-04-29 14:42:12,064 - trainer - INFO -     accuracy       : 0.818374
2024-04-29 14:42:12,064 - trainer - INFO -     macro_f        : 0.813226
2024-04-29 14:42:12,064 - trainer - INFO -     precision      : 0.842253
2024-04-29 14:42:12,064 - trainer - INFO -     recall         : 0.818374
2024-04-29 14:42:12,064 - trainer - INFO -     doc_entropy    : 2.274372
2024-04-29 14:42:12,064 - trainer - INFO -     val_loss       : 1.091147
2024-04-29 14:42:12,064 - trainer - INFO -     val_accuracy   : 0.700637
2024-04-29 14:42:12,064 - trainer - INFO -     val_macro_f    : 0.690423
2024-04-29 14:42:12,064 - trainer - INFO -     val_precision  : 0.727253
2024-04-29 14:42:12,064 - trainer - INFO -     val_recall     : 0.700637
2024-04-29 14:42:12,064 - trainer - INFO -     val_doc_entropy: 2.773933
2024-04-29 14:42:12,064 - trainer - INFO -     test_loss      : 1.072735
2024-04-29 14:42:12,064 - trainer - INFO -     test_accuracy  : 0.701832
2024-04-29 14:42:12,064 - trainer - INFO -     test_macro_f   : 0.69186
2024-04-29 14:42:12,064 - trainer - INFO -     test_precision : 0.72865
2024-04-29 14:42:12,064 - trainer - INFO -     test_recall    : 0.701832
2024-04-29 14:42:12,064 - trainer - INFO -     test_doc_entropy: 2.775262
2024-04-29 14:42:53,189 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=400, bias=True)
    (1): Tanh()
    (2): Linear(in_features=400, out_features=20, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=20, bias=False)
  (W_q): Linear(in_features=300, out_features=20, bias=False)
  (W_v): Linear(in_features=300, out_features=20, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((20, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,882,975
Freeze params: 0
2024-04-29 14:46:58,280 - trainer - INFO -     epoch          : 1
2024-04-29 14:46:58,280 - trainer - INFO -     loss           : 1.15958
2024-04-29 14:46:58,280 - trainer - INFO -     accuracy       : 0.667978
2024-04-29 14:46:58,280 - trainer - INFO -     macro_f        : 0.648841
2024-04-29 14:46:58,280 - trainer - INFO -     precision      : 0.679235
2024-04-29 14:46:58,280 - trainer - INFO -     recall         : 0.667978
2024-04-29 14:46:58,280 - trainer - INFO -     doc_entropy    : 2.393021
2024-04-29 14:46:58,280 - trainer - INFO -     val_loss       : 1.006324
2024-04-29 14:46:58,280 - trainer - INFO -     val_accuracy   : 0.705964
2024-04-29 14:46:58,280 - trainer - INFO -     val_macro_f    : 0.691411
2024-04-29 14:46:58,280 - trainer - INFO -     val_precision  : 0.723783
2024-04-29 14:46:58,280 - trainer - INFO -     val_recall     : 0.705964
2024-04-29 14:46:58,280 - trainer - INFO -     val_doc_entropy: 2.824053
2024-04-29 14:46:58,280 - trainer - INFO -     test_loss      : 1.009127
2024-04-29 14:46:58,280 - trainer - INFO -     test_accuracy  : 0.704521
2024-04-29 14:46:58,280 - trainer - INFO -     test_macro_f   : 0.69218
2024-04-29 14:46:58,280 - trainer - INFO -     test_precision : 0.728005
2024-04-29 14:46:58,280 - trainer - INFO -     test_recall    : 0.704521
2024-04-29 14:46:58,280 - trainer - INFO -     test_doc_entropy: 2.828003
2024-04-29 14:51:00,564 - trainer - INFO -     epoch          : 2
2024-04-29 14:51:00,564 - trainer - INFO -     loss           : 0.817736
2024-04-29 14:51:00,564 - trainer - INFO -     accuracy       : 0.754512
2024-04-29 14:51:00,564 - trainer - INFO -     macro_f        : 0.745292
2024-04-29 14:51:00,564 - trainer - INFO -     precision      : 0.77784
2024-04-29 14:51:00,564 - trainer - INFO -     recall         : 0.754512
2024-04-29 14:51:00,564 - trainer - INFO -     doc_entropy    : 2.576103
2024-04-29 14:51:00,564 - trainer - INFO -     val_loss       : 0.988415
2024-04-29 14:51:00,564 - trainer - INFO -     val_accuracy   : 0.712188
2024-04-29 14:51:00,564 - trainer - INFO -     val_macro_f    : 0.701442
2024-04-29 14:51:00,564 - trainer - INFO -     val_precision  : 0.73722
2024-04-29 14:51:00,564 - trainer - INFO -     val_recall     : 0.712188
2024-04-29 14:51:00,564 - trainer - INFO -     val_doc_entropy: 2.813904
2024-04-29 14:51:00,564 - trainer - INFO -     test_loss      : 0.979712
2024-04-29 14:51:00,564 - trainer - INFO -     test_accuracy  : 0.711391
2024-04-29 14:51:00,564 - trainer - INFO -     test_macro_f   : 0.700539
2024-04-29 14:51:00,564 - trainer - INFO -     test_precision : 0.735738
2024-04-29 14:51:00,564 - trainer - INFO -     test_recall    : 0.711391
2024-04-29 14:51:00,564 - trainer - INFO -     test_doc_entropy: 2.815821
2024-04-29 14:55:04,469 - trainer - INFO -     epoch          : 3
2024-04-29 14:55:04,469 - trainer - INFO -     loss           : 0.594736
2024-04-29 14:55:04,469 - trainer - INFO -     accuracy       : 0.819046
2024-04-29 14:55:04,469 - trainer - INFO -     macro_f        : 0.813621
2024-04-29 14:55:04,469 - trainer - INFO -     precision      : 0.84232
2024-04-29 14:55:04,469 - trainer - INFO -     recall         : 0.819046
2024-04-29 14:55:04,469 - trainer - INFO -     doc_entropy    : 2.424545
2024-04-29 14:55:04,469 - trainer - INFO -     val_loss       : 1.080132
2024-04-29 14:55:04,469 - trainer - INFO -     val_accuracy   : 0.698347
2024-04-29 14:55:04,469 - trainer - INFO -     val_macro_f    : 0.690407
2024-04-29 14:55:04,469 - trainer - INFO -     val_precision  : 0.728327
2024-04-29 14:55:04,469 - trainer - INFO -     val_recall     : 0.698347
2024-04-29 14:55:04,469 - trainer - INFO -     val_doc_entropy: 2.766436
2024-04-29 14:55:04,469 - trainer - INFO -     test_loss      : 1.063907
2024-04-29 14:55:04,469 - trainer - INFO -     test_accuracy  : 0.702131
2024-04-29 14:55:04,469 - trainer - INFO -     test_macro_f   : 0.695408
2024-04-29 14:55:04,469 - trainer - INFO -     test_precision : 0.735638
2024-04-29 14:55:04,469 - trainer - INFO -     test_recall    : 0.702131
2024-04-29 14:55:04,469 - trainer - INFO -     test_doc_entropy: 2.768504
2024-04-29 14:55:43,397 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=400, bias=True)
    (1): Tanh()
    (2): Linear(in_features=400, out_features=20, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=20, bias=False)
  (W_q): Linear(in_features=300, out_features=20, bias=False)
  (W_v): Linear(in_features=300, out_features=20, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((20, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,882,975
Freeze params: 0
2024-04-29 14:59:57,588 - trainer - INFO -     epoch          : 1
2024-04-29 14:59:57,588 - trainer - INFO -     loss           : 1.156354
2024-04-29 14:59:57,588 - trainer - INFO -     accuracy       : 0.66753
2024-04-29 14:59:57,588 - trainer - INFO -     macro_f        : 0.64921
2024-04-29 14:59:57,588 - trainer - INFO -     precision      : 0.680553
2024-04-29 14:59:57,588 - trainer - INFO -     recall         : 0.66753
2024-04-29 14:59:57,588 - trainer - INFO -     doc_entropy    : 2.356857
2024-04-29 14:59:57,588 - trainer - INFO -     val_loss       : 1.012881
2024-04-29 14:59:57,588 - trainer - INFO -     val_accuracy   : 0.703027
2024-04-29 14:59:57,588 - trainer - INFO -     val_macro_f    : 0.68993
2024-04-29 14:59:57,588 - trainer - INFO -     val_precision  : 0.723917
2024-04-29 14:59:57,588 - trainer - INFO -     val_recall     : 0.703027
2024-04-29 14:59:57,588 - trainer - INFO -     val_doc_entropy: 2.846376
2024-04-29 14:59:57,588 - trainer - INFO -     test_loss      : 1.008455
2024-04-29 14:59:57,588 - trainer - INFO -     test_accuracy  : 0.704919
2024-04-29 14:59:57,588 - trainer - INFO -     test_macro_f   : 0.691224
2024-04-29 14:59:57,588 - trainer - INFO -     test_precision : 0.722892
2024-04-29 14:59:57,588 - trainer - INFO -     test_recall    : 0.704919
2024-04-29 14:59:57,588 - trainer - INFO -     test_doc_entropy: 2.851045
2024-04-29 15:04:12,502 - trainer - INFO -     epoch          : 2
2024-04-29 15:04:12,502 - trainer - INFO -     loss           : 0.8163
2024-04-29 15:04:12,502 - trainer - INFO -     accuracy       : 0.754139
2024-04-29 15:04:12,502 - trainer - INFO -     macro_f        : 0.745137
2024-04-29 15:04:12,502 - trainer - INFO -     precision      : 0.778516
2024-04-29 15:04:12,502 - trainer - INFO -     recall         : 0.754139
2024-04-29 15:04:12,502 - trainer - INFO -     doc_entropy    : 2.484442
2024-04-29 15:04:12,502 - trainer - INFO -     val_loss       : 0.992602
2024-04-29 15:04:12,502 - trainer - INFO -     val_accuracy   : 0.710246
2024-04-29 15:04:12,502 - trainer - INFO -     val_macro_f    : 0.703056
2024-04-29 15:04:12,502 - trainer - INFO -     val_precision  : 0.740281
2024-04-29 15:04:12,502 - trainer - INFO -     val_recall     : 0.710246
2024-04-29 15:04:12,502 - trainer - INFO -     val_doc_entropy: 2.790012
2024-04-29 15:04:12,502 - trainer - INFO -     test_loss      : 0.989161
2024-04-29 15:04:12,502 - trainer - INFO -     test_accuracy  : 0.710644
2024-04-29 15:04:12,502 - trainer - INFO -     test_macro_f   : 0.703792
2024-04-29 15:04:12,502 - trainer - INFO -     test_precision : 0.744489
2024-04-29 15:04:12,502 - trainer - INFO -     test_recall    : 0.710644
2024-04-29 15:04:12,502 - trainer - INFO -     test_doc_entropy: 2.792714
2024-04-29 15:08:21,128 - trainer - INFO -     epoch          : 3
2024-04-29 15:08:21,128 - trainer - INFO -     loss           : 0.590724
2024-04-29 15:08:21,128 - trainer - INFO -     accuracy       : 0.819855
2024-04-29 15:08:21,129 - trainer - INFO -     macro_f        : 0.814497
2024-04-29 15:08:21,129 - trainer - INFO -     precision      : 0.843409
2024-04-29 15:08:21,129 - trainer - INFO -     recall         : 0.819855
2024-04-29 15:08:21,129 - trainer - INFO -     doc_entropy    : 2.325732
2024-04-29 15:08:21,129 - trainer - INFO -     val_loss       : 1.088705
2024-04-29 15:08:21,130 - trainer - INFO -     val_accuracy   : 0.6976
2024-04-29 15:08:21,130 - trainer - INFO -     val_macro_f    : 0.691023
2024-04-29 15:08:21,130 - trainer - INFO -     val_precision  : 0.732307
2024-04-29 15:08:21,130 - trainer - INFO -     val_recall     : 0.6976
2024-04-29 15:08:21,130 - trainer - INFO -     val_doc_entropy: 2.732404
2024-04-29 15:08:21,131 - trainer - INFO -     test_loss      : 1.076951
2024-04-29 15:08:21,131 - trainer - INFO -     test_accuracy  : 0.701235
2024-04-29 15:08:21,131 - trainer - INFO -     test_macro_f   : 0.694672
2024-04-29 15:08:21,131 - trainer - INFO -     test_precision : 0.733213
2024-04-29 15:08:21,131 - trainer - INFO -     test_recall    : 0.701235
2024-04-29 15:08:21,132 - trainer - INFO -     test_doc_entropy: 2.735013
2024-04-29 15:16:09,221 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,049,395
Freeze params: 0
2024-04-29 15:19:55,723 - trainer - INFO -     epoch          : 1
2024-04-29 15:19:55,723 - trainer - INFO -     loss           : 1.15599
2024-04-29 15:19:55,723 - trainer - INFO -     accuracy       : 0.668252
2024-04-29 15:19:55,723 - trainer - INFO -     macro_f        : 0.64976
2024-04-29 15:19:55,723 - trainer - INFO -     precision      : 0.680722
2024-04-29 15:19:55,723 - trainer - INFO -     recall         : 0.668252
2024-04-29 15:19:55,723 - trainer - INFO -     doc_entropy    : 2.310653
2024-04-29 15:19:55,723 - trainer - INFO -     val_loss       : 1.008048
2024-04-29 15:19:55,723 - trainer - INFO -     val_accuracy   : 0.705616
2024-04-29 15:19:55,723 - trainer - INFO -     val_macro_f    : 0.693647
2024-04-29 15:19:55,723 - trainer - INFO -     val_precision  : 0.726153
2024-04-29 15:19:55,723 - trainer - INFO -     val_recall     : 0.705616
2024-04-29 15:19:55,723 - trainer - INFO -     val_doc_entropy: 2.876728
2024-04-29 15:19:55,723 - trainer - INFO -     test_loss      : 1.010519
2024-04-29 15:19:55,723 - trainer - INFO -     test_accuracy  : 0.703923
2024-04-29 15:19:55,723 - trainer - INFO -     test_macro_f   : 0.692144
2024-04-29 15:19:55,723 - trainer - INFO -     test_precision : 0.725422
2024-04-29 15:19:55,723 - trainer - INFO -     test_recall    : 0.703923
2024-04-29 15:19:55,723 - trainer - INFO -     test_doc_entropy: 2.878882
2024-04-29 15:23:56,347 - trainer - INFO -     epoch          : 2
2024-04-29 15:23:56,347 - trainer - INFO -     loss           : 0.820677
2024-04-29 15:23:56,347 - trainer - INFO -     accuracy       : 0.754543
2024-04-29 15:23:56,347 - trainer - INFO -     macro_f        : 0.745223
2024-04-29 15:23:56,347 - trainer - INFO -     precision      : 0.777613
2024-04-29 15:23:56,347 - trainer - INFO -     recall         : 0.754543
2024-04-29 15:23:56,347 - trainer - INFO -     doc_entropy    : 2.484276
2024-04-29 15:23:56,347 - trainer - INFO -     val_loss       : 1.003883
2024-04-29 15:23:56,347 - trainer - INFO -     val_accuracy   : 0.706811
2024-04-29 15:23:56,347 - trainer - INFO -     val_macro_f    : 0.699046
2024-04-29 15:23:56,347 - trainer - INFO -     val_precision  : 0.737666
2024-04-29 15:23:56,347 - trainer - INFO -     val_recall     : 0.706811
2024-04-29 15:23:56,347 - trainer - INFO -     val_doc_entropy: 2.808447
2024-04-29 15:23:56,347 - trainer - INFO -     test_loss      : 0.994319
2024-04-29 15:23:56,347 - trainer - INFO -     test_accuracy  : 0.705616
2024-04-29 15:23:56,347 - trainer - INFO -     test_macro_f   : 0.696916
2024-04-29 15:23:56,347 - trainer - INFO -     test_precision : 0.735738
2024-04-29 15:23:56,347 - trainer - INFO -     test_recall    : 0.705616
2024-04-29 15:23:56,347 - trainer - INFO -     test_doc_entropy: 2.809426
2024-04-29 15:24:42,974 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,049,395
Freeze params: 0
2024-04-29 15:28:34,274 - trainer - INFO -     epoch          : 1
2024-04-29 15:28:34,274 - trainer - INFO -     loss           : 1.153318
2024-04-29 15:28:34,274 - trainer - INFO -     accuracy       : 0.668557
2024-04-29 15:28:34,274 - trainer - INFO -     macro_f        : 0.650026
2024-04-29 15:28:34,274 - trainer - INFO -     precision      : 0.681365
2024-04-29 15:28:34,274 - trainer - INFO -     recall         : 0.668557
2024-04-29 15:28:34,274 - trainer - INFO -     doc_entropy    : 2.289192
2024-04-29 15:28:34,274 - trainer - INFO -     val_loss       : 1.000206
2024-04-29 15:28:34,274 - trainer - INFO -     val_accuracy   : 0.70706
2024-04-29 15:28:34,274 - trainer - INFO -     val_macro_f    : 0.693436
2024-04-29 15:28:34,274 - trainer - INFO -     val_precision  : 0.7267
2024-04-29 15:28:34,274 - trainer - INFO -     val_recall     : 0.70706
2024-04-29 15:28:34,290 - trainer - INFO -     val_doc_entropy: 2.607124
2024-04-29 15:28:34,290 - trainer - INFO -     test_loss      : 1.001061
2024-04-29 15:28:34,290 - trainer - INFO -     test_accuracy  : 0.704471
2024-04-29 15:28:34,290 - trainer - INFO -     test_macro_f   : 0.691237
2024-04-29 15:28:34,290 - trainer - INFO -     test_precision : 0.725167
2024-04-29 15:28:34,290 - trainer - INFO -     test_recall    : 0.704471
2024-04-29 15:28:34,290 - trainer - INFO -     test_doc_entropy: 2.612012
2024-04-29 15:32:33,635 - trainer - INFO -     epoch          : 2
2024-04-29 15:32:33,635 - trainer - INFO -     loss           : 0.815643
2024-04-29 15:32:33,635 - trainer - INFO -     accuracy       : 0.755913
2024-04-29 15:32:33,635 - trainer - INFO -     macro_f        : 0.746793
2024-04-29 15:32:33,635 - trainer - INFO -     precision      : 0.779499
2024-04-29 15:32:33,635 - trainer - INFO -     recall         : 0.755913
2024-04-29 15:32:33,635 - trainer - INFO -     doc_entropy    : 2.420715
2024-04-29 15:32:33,635 - trainer - INFO -     val_loss       : 0.994594
2024-04-29 15:32:33,635 - trainer - INFO -     val_accuracy   : 0.711839
2024-04-29 15:32:33,635 - trainer - INFO -     val_macro_f    : 0.699693
2024-04-29 15:32:33,635 - trainer - INFO -     val_precision  : 0.732445
2024-04-29 15:32:33,635 - trainer - INFO -     val_recall     : 0.711839
2024-04-29 15:32:33,635 - trainer - INFO -     val_doc_entropy: 2.767245
2024-04-29 15:32:33,635 - trainer - INFO -     test_loss      : 0.987878
2024-04-29 15:32:33,635 - trainer - INFO -     test_accuracy  : 0.711142
2024-04-29 15:32:33,635 - trainer - INFO -     test_macro_f   : 0.699507
2024-04-29 15:32:33,635 - trainer - INFO -     test_precision : 0.731739
2024-04-29 15:32:33,635 - trainer - INFO -     test_recall    : 0.711142
2024-04-29 15:32:33,635 - trainer - INFO -     test_doc_entropy: 2.770675
2024-04-29 15:33:11,052 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,049,395
Freeze params: 0
2024-04-29 15:37:08,311 - trainer - INFO -     epoch          : 1
2024-04-29 15:37:08,311 - trainer - INFO -     loss           : 1.152608
2024-04-29 15:37:08,311 - trainer - INFO -     accuracy       : 0.668874
2024-04-29 15:37:08,311 - trainer - INFO -     macro_f        : 0.650155
2024-04-29 15:37:08,311 - trainer - INFO -     precision      : 0.681327
2024-04-29 15:37:08,311 - trainer - INFO -     recall         : 0.668874
2024-04-29 15:37:08,311 - trainer - INFO -     doc_entropy    : 2.282242
2024-04-29 15:37:08,311 - trainer - INFO -     val_loss       : 1.025471
2024-04-29 15:37:08,311 - trainer - INFO -     val_accuracy   : 0.701085
2024-04-29 15:37:08,311 - trainer - INFO -     val_macro_f    : 0.688091
2024-04-29 15:37:08,311 - trainer - INFO -     val_precision  : 0.723102
2024-04-29 15:37:08,311 - trainer - INFO -     val_recall     : 0.701085
2024-04-29 15:37:08,311 - trainer - INFO -     val_doc_entropy: 2.774335
2024-04-29 15:37:08,311 - trainer - INFO -     test_loss      : 1.021722
2024-04-29 15:37:08,311 - trainer - INFO -     test_accuracy  : 0.699841
2024-04-29 15:37:08,311 - trainer - INFO -     test_macro_f   : 0.688587
2024-04-29 15:37:08,311 - trainer - INFO -     test_precision : 0.724525
2024-04-29 15:37:08,311 - trainer - INFO -     test_recall    : 0.699841
2024-04-29 15:37:08,311 - trainer - INFO -     test_doc_entropy: 2.778735
2024-04-29 15:41:06,258 - trainer - INFO -     epoch          : 2
2024-04-29 15:41:06,274 - trainer - INFO -     loss           : 0.818125
2024-04-29 15:41:06,274 - trainer - INFO -     accuracy       : 0.755496
2024-04-29 15:41:06,274 - trainer - INFO -     macro_f        : 0.746079
2024-04-29 15:41:06,274 - trainer - INFO -     precision      : 0.778349
2024-04-29 15:41:06,274 - trainer - INFO -     recall         : 0.755496
2024-04-29 15:41:06,274 - trainer - INFO -     doc_entropy    : 2.452751
2024-04-29 15:41:06,274 - trainer - INFO -     val_loss       : 1.003082
2024-04-29 15:41:06,274 - trainer - INFO -     val_accuracy   : 0.708553
2024-04-29 15:41:06,274 - trainer - INFO -     val_macro_f    : 0.702315
2024-04-29 15:41:06,274 - trainer - INFO -     val_precision  : 0.742109
2024-04-29 15:41:06,274 - trainer - INFO -     val_recall     : 0.708553
2024-04-29 15:41:06,274 - trainer - INFO -     val_doc_entropy: 2.800979
2024-04-29 15:41:06,274 - trainer - INFO -     test_loss      : 0.998602
2024-04-29 15:41:06,274 - trainer - INFO -     test_accuracy  : 0.707707
2024-04-29 15:41:06,274 - trainer - INFO -     test_macro_f   : 0.700096
2024-04-29 15:41:06,274 - trainer - INFO -     test_precision : 0.739848
2024-04-29 15:41:06,274 - trainer - INFO -     test_recall    : 0.707707
2024-04-29 15:41:06,274 - trainer - INFO -     test_doc_entropy: 2.80365
2024-04-29 15:41:43,770 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,049,395
Freeze params: 0
2024-04-29 15:45:39,244 - trainer - INFO -     epoch          : 1
2024-04-29 15:45:39,244 - trainer - INFO -     loss           : 1.151029
2024-04-29 15:45:39,244 - trainer - INFO -     accuracy       : 0.669857
2024-04-29 15:45:39,244 - trainer - INFO -     macro_f        : 0.65198
2024-04-29 15:45:39,244 - trainer - INFO -     precision      : 0.683915
2024-04-29 15:45:39,244 - trainer - INFO -     recall         : 0.669857
2024-04-29 15:45:39,244 - trainer - INFO -     doc_entropy    : 2.29297
2024-04-29 15:45:39,244 - trainer - INFO -     val_loss       : 1.012326
2024-04-29 15:45:39,244 - trainer - INFO -     val_accuracy   : 0.702977
2024-04-29 15:45:39,244 - trainer - INFO -     val_macro_f    : 0.68594
2024-04-29 15:45:39,244 - trainer - INFO -     val_precision  : 0.7153
2024-04-29 15:45:39,244 - trainer - INFO -     val_recall     : 0.702977
2024-04-29 15:45:39,244 - trainer - INFO -     val_doc_entropy: 2.707352
2024-04-29 15:45:39,244 - trainer - INFO -     test_loss      : 1.007055
2024-04-29 15:45:39,244 - trainer - INFO -     test_accuracy  : 0.705865
2024-04-29 15:45:39,244 - trainer - INFO -     test_macro_f   : 0.689639
2024-04-29 15:45:39,244 - trainer - INFO -     test_precision : 0.720895
2024-04-29 15:45:39,244 - trainer - INFO -     test_recall    : 0.705865
2024-04-29 15:45:39,244 - trainer - INFO -     test_doc_entropy: 2.711926
2024-04-29 15:49:35,153 - trainer - INFO -     epoch          : 2
2024-04-29 15:49:35,153 - trainer - INFO -     loss           : 0.815974
2024-04-29 15:49:35,153 - trainer - INFO -     accuracy       : 0.755944
2024-04-29 15:49:35,153 - trainer - INFO -     macro_f        : 0.746369
2024-04-29 15:49:35,153 - trainer - INFO -     precision      : 0.778559
2024-04-29 15:49:35,153 - trainer - INFO -     recall         : 0.755944
2024-04-29 15:49:35,153 - trainer - INFO -     doc_entropy    : 2.470415
2024-04-29 15:49:35,153 - trainer - INFO -     val_loss       : 1.002413
2024-04-29 15:49:35,153 - trainer - INFO -     val_accuracy   : 0.708553
2024-04-29 15:49:35,153 - trainer - INFO -     val_macro_f    : 0.699002
2024-04-29 15:49:35,153 - trainer - INFO -     val_precision  : 0.735502
2024-04-29 15:49:35,153 - trainer - INFO -     val_recall     : 0.708553
2024-04-29 15:49:35,153 - trainer - INFO -     val_doc_entropy: 2.743427
2024-04-29 15:49:35,153 - trainer - INFO -     test_loss      : 0.997471
2024-04-29 15:49:35,153 - trainer - INFO -     test_accuracy  : 0.709001
2024-04-29 15:49:35,153 - trainer - INFO -     test_macro_f   : 0.701624
2024-04-29 15:49:35,153 - trainer - INFO -     test_precision : 0.74074
2024-04-29 15:49:35,153 - trainer - INFO -     test_recall    : 0.709001
2024-04-29 15:49:35,153 - trainer - INFO -     test_doc_entropy: 2.745633
2024-04-29 15:50:12,789 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,049,395
Freeze params: 0
2024-04-29 15:54:05,807 - trainer - INFO -     epoch          : 1
2024-04-29 15:54:05,807 - trainer - INFO -     loss           : 1.150518
2024-04-29 15:54:05,807 - trainer - INFO -     accuracy       : 0.669714
2024-04-29 15:54:05,807 - trainer - INFO -     macro_f        : 0.651029
2024-04-29 15:54:05,807 - trainer - INFO -     precision      : 0.682352
2024-04-29 15:54:05,807 - trainer - INFO -     recall         : 0.669714
2024-04-29 15:54:05,807 - trainer - INFO -     doc_entropy    : 2.336203
2024-04-29 15:54:05,807 - trainer - INFO -     val_loss       : 1.013115
2024-04-29 15:54:05,807 - trainer - INFO -     val_accuracy   : 0.701583
2024-04-29 15:54:05,807 - trainer - INFO -     val_macro_f    : 0.693343
2024-04-29 15:54:05,807 - trainer - INFO -     val_precision  : 0.733032
2024-04-29 15:54:05,807 - trainer - INFO -     val_recall     : 0.701583
2024-04-29 15:54:05,807 - trainer - INFO -     val_doc_entropy: 2.824331
2024-04-29 15:54:05,807 - trainer - INFO -     test_loss      : 1.007178
2024-04-29 15:54:05,807 - trainer - INFO -     test_accuracy  : 0.706612
2024-04-29 15:54:05,807 - trainer - INFO -     test_macro_f   : 0.69874
2024-04-29 15:54:05,807 - trainer - INFO -     test_precision : 0.738251
2024-04-29 15:54:05,807 - trainer - INFO -     test_recall    : 0.706612
2024-04-29 15:54:05,807 - trainer - INFO -     test_doc_entropy: 2.828102
2024-04-29 15:58:04,269 - trainer - INFO -     epoch          : 2
2024-04-29 15:58:04,269 - trainer - INFO -     loss           : 0.813931
2024-04-29 15:58:04,269 - trainer - INFO -     accuracy       : 0.756454
2024-04-29 15:58:04,269 - trainer - INFO -     macro_f        : 0.747601
2024-04-29 15:58:04,269 - trainer - INFO -     precision      : 0.780355
2024-04-29 15:58:04,269 - trainer - INFO -     recall         : 0.756454
2024-04-29 15:58:04,269 - trainer - INFO -     doc_entropy    : 2.446461
2024-04-29 15:58:04,269 - trainer - INFO -     val_loss       : 0.999398
2024-04-29 15:58:04,269 - trainer - INFO -     val_accuracy   : 0.704919
2024-04-29 15:58:04,269 - trainer - INFO -     val_macro_f    : 0.699485
2024-04-29 15:58:04,269 - trainer - INFO -     val_precision  : 0.742789
2024-04-29 15:58:04,269 - trainer - INFO -     val_recall     : 0.704919
2024-04-29 15:58:04,269 - trainer - INFO -     val_doc_entropy: 2.790222
2024-04-29 15:58:04,269 - trainer - INFO -     test_loss      : 0.984703
2024-04-29 15:58:04,269 - trainer - INFO -     test_accuracy  : 0.710495
2024-04-29 15:58:04,269 - trainer - INFO -     test_macro_f   : 0.704288
2024-04-29 15:58:04,269 - trainer - INFO -     test_precision : 0.744216
2024-04-29 15:58:04,269 - trainer - INFO -     test_recall    : 0.710495
2024-04-29 15:58:04,269 - trainer - INFO -     test_doc_entropy: 2.792723
2024-04-29 16:16:25,477 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1200, out_features=60, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=60, bias=False)
  (W_q): Linear(in_features=300, out_features=60, bias=False)
  (W_v): Linear(in_features=300, out_features=60, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((60, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,231,815
Freeze params: 0
2024-04-29 16:20:03,676 - trainer - INFO -     epoch          : 1
2024-04-29 16:20:03,676 - trainer - INFO -     loss           : 1.150206
2024-04-29 16:20:03,676 - trainer - INFO -     accuracy       : 0.669615
2024-04-29 16:20:03,676 - trainer - INFO -     macro_f        : 0.651119
2024-04-29 16:20:03,676 - trainer - INFO -     precision      : 0.681877
2024-04-29 16:20:03,676 - trainer - INFO -     recall         : 0.669615
2024-04-29 16:20:03,676 - trainer - INFO -     doc_entropy    : 2.287801
2024-04-29 16:20:03,676 - trainer - INFO -     val_loss       : 1.004809
2024-04-29 16:20:03,676 - trainer - INFO -     val_accuracy   : 0.705566
2024-04-29 16:20:03,676 - trainer - INFO -     val_macro_f    : 0.690166
2024-04-29 16:20:03,676 - trainer - INFO -     val_precision  : 0.720033
2024-04-29 16:20:03,676 - trainer - INFO -     val_recall     : 0.705566
2024-04-29 16:20:03,676 - trainer - INFO -     val_doc_entropy: 2.67042
2024-04-29 16:20:03,676 - trainer - INFO -     test_loss      : 1.004102
2024-04-29 16:20:03,676 - trainer - INFO -     test_accuracy  : 0.704919
2024-04-29 16:20:03,676 - trainer - INFO -     test_macro_f   : 0.69051
2024-04-29 16:20:03,676 - trainer - INFO -     test_precision : 0.720792
2024-04-29 16:20:03,676 - trainer - INFO -     test_recall    : 0.704919
2024-04-29 16:20:03,676 - trainer - INFO -     test_doc_entropy: 2.675149
2024-04-29 16:23:54,079 - trainer - INFO -     epoch          : 2
2024-04-29 16:23:54,079 - trainer - INFO -     loss           : 0.814143
2024-04-29 16:23:54,079 - trainer - INFO -     accuracy       : 0.756
2024-04-29 16:23:54,079 - trainer - INFO -     macro_f        : 0.746964
2024-04-29 16:23:54,079 - trainer - INFO -     precision      : 0.779709
2024-04-29 16:23:54,079 - trainer - INFO -     recall         : 0.756
2024-04-29 16:23:54,079 - trainer - INFO -     doc_entropy    : 2.403617
2024-04-29 16:23:54,079 - trainer - INFO -     val_loss       : 0.995835
2024-04-29 16:23:54,079 - trainer - INFO -     val_accuracy   : 0.71154
2024-04-29 16:23:54,079 - trainer - INFO -     val_macro_f    : 0.702216
2024-04-29 16:23:54,079 - trainer - INFO -     val_precision  : 0.73846
2024-04-29 16:23:54,079 - trainer - INFO -     val_recall     : 0.71154
2024-04-29 16:23:54,079 - trainer - INFO -     val_doc_entropy: 2.821331
2024-04-29 16:23:54,079 - trainer - INFO -     test_loss      : 0.985243
2024-04-29 16:23:54,079 - trainer - INFO -     test_accuracy  : 0.709001
2024-04-29 16:23:54,079 - trainer - INFO -     test_macro_f   : 0.699051
2024-04-29 16:23:54,079 - trainer - INFO -     test_precision : 0.734999
2024-04-29 16:23:54,079 - trainer - INFO -     test_recall    : 0.709001
2024-04-29 16:23:54,079 - trainer - INFO -     test_doc_entropy: 2.825378
2024-04-29 16:24:32,564 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1200, out_features=60, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=60, bias=False)
  (W_q): Linear(in_features=300, out_features=60, bias=False)
  (W_v): Linear(in_features=300, out_features=60, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((60, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,231,815
Freeze params: 0
2024-04-29 16:28:26,498 - trainer - INFO -     epoch          : 1
2024-04-29 16:28:26,498 - trainer - INFO -     loss           : 1.149264
2024-04-29 16:28:26,498 - trainer - INFO -     accuracy       : 0.670243
2024-04-29 16:28:26,498 - trainer - INFO -     macro_f        : 0.652048
2024-04-29 16:28:26,498 - trainer - INFO -     precision      : 0.68319
2024-04-29 16:28:26,498 - trainer - INFO -     recall         : 0.670243
2024-04-29 16:28:26,498 - trainer - INFO -     doc_entropy    : 2.284528
2024-04-29 16:28:26,498 - trainer - INFO -     val_loss       : 1.006006
2024-04-29 16:28:26,498 - trainer - INFO -     val_accuracy   : 0.703724
2024-04-29 16:28:26,498 - trainer - INFO -     val_macro_f    : 0.689416
2024-04-29 16:28:26,498 - trainer - INFO -     val_precision  : 0.722352
2024-04-29 16:28:26,498 - trainer - INFO -     val_recall     : 0.703724
2024-04-29 16:28:26,498 - trainer - INFO -     val_doc_entropy: 2.76311
2024-04-29 16:28:26,498 - trainer - INFO -     test_loss      : 1.004617
2024-04-29 16:28:26,498 - trainer - INFO -     test_accuracy  : 0.704122
2024-04-29 16:28:26,498 - trainer - INFO -     test_macro_f   : 0.690603
2024-04-29 16:28:26,498 - trainer - INFO -     test_precision : 0.722975
2024-04-29 16:28:26,498 - trainer - INFO -     test_recall    : 0.704122
2024-04-29 16:28:26,498 - trainer - INFO -     test_doc_entropy: 2.765296
2024-04-29 16:32:24,680 - trainer - INFO -     epoch          : 2
2024-04-29 16:32:24,680 - trainer - INFO -     loss           : 0.81532
2024-04-29 16:32:24,680 - trainer - INFO -     accuracy       : 0.756404
2024-04-29 16:32:24,680 - trainer - INFO -     macro_f        : 0.747185
2024-04-29 16:32:24,680 - trainer - INFO -     precision      : 0.779151
2024-04-29 16:32:24,680 - trainer - INFO -     recall         : 0.756404
2024-04-29 16:32:24,680 - trainer - INFO -     doc_entropy    : 2.464401
2024-04-29 16:32:24,680 - trainer - INFO -     val_loss       : 1.005102
2024-04-29 16:32:24,680 - trainer - INFO -     val_accuracy   : 0.70467
2024-04-29 16:32:24,680 - trainer - INFO -     val_macro_f    : 0.69367
2024-04-29 16:32:24,680 - trainer - INFO -     val_precision  : 0.7293
2024-04-29 16:32:24,680 - trainer - INFO -     val_recall     : 0.70467
2024-04-29 16:32:24,680 - trainer - INFO -     val_doc_entropy: 2.802101
2024-04-29 16:32:24,680 - trainer - INFO -     test_loss      : 1.000106
2024-04-29 16:32:24,680 - trainer - INFO -     test_accuracy  : 0.706462
2024-04-29 16:32:24,680 - trainer - INFO -     test_macro_f   : 0.697179
2024-04-29 16:32:24,680 - trainer - INFO -     test_precision : 0.734755
2024-04-29 16:32:24,680 - trainer - INFO -     test_recall    : 0.706462
2024-04-29 16:32:24,680 - trainer - INFO -     test_doc_entropy: 2.804566
2024-04-29 16:33:03,131 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1200, out_features=60, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=60, bias=False)
  (W_q): Linear(in_features=300, out_features=60, bias=False)
  (W_v): Linear(in_features=300, out_features=60, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((60, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,231,815
Freeze params: 0
2024-04-29 16:36:58,311 - trainer - INFO -     epoch          : 1
2024-04-29 16:36:58,311 - trainer - INFO -     loss           : 1.150335
2024-04-29 16:36:58,311 - trainer - INFO -     accuracy       : 0.66982
2024-04-29 16:36:58,311 - trainer - INFO -     macro_f        : 0.651017
2024-04-29 16:36:58,311 - trainer - INFO -     precision      : 0.681786
2024-04-29 16:36:58,311 - trainer - INFO -     recall         : 0.66982
2024-04-29 16:36:58,311 - trainer - INFO -     doc_entropy    : 2.255256
2024-04-29 16:36:58,311 - trainer - INFO -     val_loss       : 0.999121
2024-04-29 16:36:58,311 - trainer - INFO -     val_accuracy   : 0.706612
2024-04-29 16:36:58,311 - trainer - INFO -     val_macro_f    : 0.695197
2024-04-29 16:36:58,311 - trainer - INFO -     val_precision  : 0.729131
2024-04-29 16:36:58,311 - trainer - INFO -     val_recall     : 0.706612
2024-04-29 16:36:58,311 - trainer - INFO -     val_doc_entropy: 2.625291
2024-04-29 16:36:58,311 - trainer - INFO -     test_loss      : 0.988876
2024-04-29 16:36:58,311 - trainer - INFO -     test_accuracy  : 0.708553
2024-04-29 16:36:58,311 - trainer - INFO -     test_macro_f   : 0.698629
2024-04-29 16:36:58,311 - trainer - INFO -     test_precision : 0.736093
2024-04-29 16:36:58,311 - trainer - INFO -     test_recall    : 0.708553
2024-04-29 16:36:58,311 - trainer - INFO -     test_doc_entropy: 2.628867
2024-04-29 16:40:57,081 - trainer - INFO -     epoch          : 2
2024-04-29 16:40:57,081 - trainer - INFO -     loss           : 0.810929
2024-04-29 16:40:57,081 - trainer - INFO -     accuracy       : 0.757506
2024-04-29 16:40:57,081 - trainer - INFO -     macro_f        : 0.748747
2024-04-29 16:40:57,081 - trainer - INFO -     precision      : 0.781329
2024-04-29 16:40:57,081 - trainer - INFO -     recall         : 0.757506
2024-04-29 16:40:57,081 - trainer - INFO -     doc_entropy    : 2.396364
2024-04-29 16:40:57,081 - trainer - INFO -     val_loss       : 0.995246
2024-04-29 16:40:57,081 - trainer - INFO -     val_accuracy   : 0.710097
2024-04-29 16:40:57,081 - trainer - INFO -     val_macro_f    : 0.703528
2024-04-29 16:40:57,081 - trainer - INFO -     val_precision  : 0.74246
2024-04-29 16:40:57,081 - trainer - INFO -     val_recall     : 0.710097
2024-04-29 16:40:57,081 - trainer - INFO -     val_doc_entropy: 2.739018
2024-04-29 16:40:57,081 - trainer - INFO -     test_loss      : 0.990022
2024-04-29 16:40:57,081 - trainer - INFO -     test_accuracy  : 0.710644
2024-04-29 16:40:57,097 - trainer - INFO -     test_macro_f   : 0.70317
2024-04-29 16:40:57,097 - trainer - INFO -     test_precision : 0.740623
2024-04-29 16:40:57,097 - trainer - INFO -     test_recall    : 0.710644
2024-04-29 16:40:57,097 - trainer - INFO -     test_doc_entropy: 2.741605
2024-04-29 16:41:35,503 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1200, out_features=60, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=60, bias=False)
  (W_q): Linear(in_features=300, out_features=60, bias=False)
  (W_v): Linear(in_features=300, out_features=60, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((60, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,231,815
Freeze params: 0
2024-04-29 16:45:32,940 - trainer - INFO -     epoch          : 1
2024-04-29 16:45:32,940 - trainer - INFO -     loss           : 1.149257
2024-04-29 16:45:32,940 - trainer - INFO -     accuracy       : 0.668712
2024-04-29 16:45:32,940 - trainer - INFO -     macro_f        : 0.649902
2024-04-29 16:45:32,940 - trainer - INFO -     precision      : 0.680849
2024-04-29 16:45:32,940 - trainer - INFO -     recall         : 0.668712
2024-04-29 16:45:32,940 - trainer - INFO -     doc_entropy    : 2.346423
2024-04-29 16:45:32,940 - trainer - INFO -     val_loss       : 1.007473
2024-04-29 16:45:32,940 - trainer - INFO -     val_accuracy   : 0.706064
2024-04-29 16:45:32,940 - trainer - INFO -     val_macro_f    : 0.693134
2024-04-29 16:45:32,940 - trainer - INFO -     val_precision  : 0.724743
2024-04-29 16:45:32,940 - trainer - INFO -     val_recall     : 0.706064
2024-04-29 16:45:32,940 - trainer - INFO -     val_doc_entropy: 2.760077
2024-04-29 16:45:32,940 - trainer - INFO -     test_loss      : 1.004413
2024-04-29 16:45:32,940 - trainer - INFO -     test_accuracy  : 0.705018
2024-04-29 16:45:32,956 - trainer - INFO -     test_macro_f   : 0.694513
2024-04-29 16:45:32,956 - trainer - INFO -     test_precision : 0.72936
2024-04-29 16:45:32,956 - trainer - INFO -     test_recall    : 0.705018
2024-04-29 16:45:32,956 - trainer - INFO -     test_doc_entropy: 2.764542
2024-04-29 16:49:30,542 - trainer - INFO -     epoch          : 2
2024-04-29 16:49:30,542 - trainer - INFO -     loss           : 0.812011
2024-04-29 16:49:30,542 - trainer - INFO -     accuracy       : 0.757437
2024-04-29 16:49:30,542 - trainer - INFO -     macro_f        : 0.74819
2024-04-29 16:49:30,542 - trainer - INFO -     precision      : 0.78051
2024-04-29 16:49:30,542 - trainer - INFO -     recall         : 0.757437
2024-04-29 16:49:30,542 - trainer - INFO -     doc_entropy    : 2.484841
2024-04-29 16:49:30,542 - trainer - INFO -     val_loss       : 0.999091
2024-04-29 16:49:30,542 - trainer - INFO -     val_accuracy   : 0.710047
2024-04-29 16:49:30,542 - trainer - INFO -     val_macro_f    : 0.698086
2024-04-29 16:49:30,542 - trainer - INFO -     val_precision  : 0.733301
2024-04-29 16:49:30,542 - trainer - INFO -     val_recall     : 0.710047
2024-04-29 16:49:30,542 - trainer - INFO -     val_doc_entropy: 2.766103
2024-04-29 16:49:30,542 - trainer - INFO -     test_loss      : 1.000616
2024-04-29 16:49:30,542 - trainer - INFO -     test_accuracy  : 0.710843
2024-04-29 16:49:30,542 - trainer - INFO -     test_macro_f   : 0.69807
2024-04-29 16:49:30,542 - trainer - INFO -     test_precision : 0.732019
2024-04-29 16:49:30,542 - trainer - INFO -     test_recall    : 0.710843
2024-04-29 16:49:30,542 - trainer - INFO -     test_doc_entropy: 2.768901
2024-04-29 16:50:09,292 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1200, out_features=60, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=60, bias=False)
  (W_q): Linear(in_features=300, out_features=60, bias=False)
  (W_v): Linear(in_features=300, out_features=60, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((60, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,231,815
Freeze params: 0
2024-04-29 16:54:04,646 - trainer - INFO -     epoch          : 1
2024-04-29 16:54:04,646 - trainer - INFO -     loss           : 1.144337
2024-04-29 16:54:04,646 - trainer - INFO -     accuracy       : 0.670766
2024-04-29 16:54:04,646 - trainer - INFO -     macro_f        : 0.652061
2024-04-29 16:54:04,646 - trainer - INFO -     precision      : 0.682593
2024-04-29 16:54:04,646 - trainer - INFO -     recall         : 0.670766
2024-04-29 16:54:04,646 - trainer - INFO -     doc_entropy    : 2.309785
2024-04-29 16:54:04,646 - trainer - INFO -     val_loss       : 1.001138
2024-04-29 16:54:04,646 - trainer - INFO -     val_accuracy   : 0.708205
2024-04-29 16:54:04,646 - trainer - INFO -     val_macro_f    : 0.696364
2024-04-29 16:54:04,646 - trainer - INFO -     val_precision  : 0.729961
2024-04-29 16:54:04,646 - trainer - INFO -     val_recall     : 0.708205
2024-04-29 16:54:04,646 - trainer - INFO -     val_doc_entropy: 2.757997
2024-04-29 16:54:04,646 - trainer - INFO -     test_loss      : 0.99549
2024-04-29 16:54:04,646 - trainer - INFO -     test_accuracy  : 0.70701
2024-04-29 16:54:04,646 - trainer - INFO -     test_macro_f   : 0.694789
2024-04-29 16:54:04,646 - trainer - INFO -     test_precision : 0.728073
2024-04-29 16:54:04,646 - trainer - INFO -     test_recall    : 0.70701
2024-04-29 16:54:04,646 - trainer - INFO -     test_doc_entropy: 2.763402
2024-04-29 16:58:05,128 - trainer - INFO -     epoch          : 2
2024-04-29 16:58:05,128 - trainer - INFO -     loss           : 0.812281
2024-04-29 16:58:05,128 - trainer - INFO -     accuracy       : 0.75646
2024-04-29 16:58:05,128 - trainer - INFO -     macro_f        : 0.747109
2024-04-29 16:58:05,128 - trainer - INFO -     precision      : 0.779127
2024-04-29 16:58:05,128 - trainer - INFO -     recall         : 0.75646
2024-04-29 16:58:05,128 - trainer - INFO -     doc_entropy    : 2.459322
2024-04-29 16:58:05,128 - trainer - INFO -     val_loss       : 0.993642
2024-04-29 16:58:05,128 - trainer - INFO -     val_accuracy   : 0.712138
2024-04-29 16:58:05,128 - trainer - INFO -     val_macro_f    : 0.70261
2024-04-29 16:58:05,128 - trainer - INFO -     val_precision  : 0.738679
2024-04-29 16:58:05,128 - trainer - INFO -     val_recall     : 0.712138
2024-04-29 16:58:05,128 - trainer - INFO -     val_doc_entropy: 2.780648
2024-04-29 16:58:05,128 - trainer - INFO -     test_loss      : 0.978813
2024-04-29 16:58:05,128 - trainer - INFO -     test_accuracy  : 0.710296
2024-04-29 16:58:05,128 - trainer - INFO -     test_macro_f   : 0.700579
2024-04-29 16:58:05,128 - trainer - INFO -     test_precision : 0.736115
2024-04-29 16:58:05,128 - trainer - INFO -     test_recall    : 0.710296
2024-04-29 16:58:05,128 - trainer - INFO -     test_doc_entropy: 2.784881
2024-04-29 16:59:25,489 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1600, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1600, out_features=80, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=80, bias=False)
  (W_q): Linear(in_features=300, out_features=80, bias=False)
  (W_v): Linear(in_features=300, out_features=80, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((80, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,430,235
Freeze params: 0
2024-04-29 17:03:21,887 - trainer - INFO -     epoch          : 1
2024-04-29 17:03:21,887 - trainer - INFO -     loss           : 1.144408
2024-04-29 17:03:21,887 - trainer - INFO -     accuracy       : 0.671264
2024-04-29 17:03:21,887 - trainer - INFO -     macro_f        : 0.653353
2024-04-29 17:03:21,887 - trainer - INFO -     precision      : 0.685064
2024-04-29 17:03:21,887 - trainer - INFO -     recall         : 0.671264
2024-04-29 17:03:21,887 - trainer - INFO -     doc_entropy    : 2.185923
2024-04-29 17:03:21,887 - trainer - INFO -     val_loss       : 1.014088
2024-04-29 17:03:21,887 - trainer - INFO -     val_accuracy   : 0.706363
2024-04-29 17:03:21,887 - trainer - INFO -     val_macro_f    : 0.692975
2024-04-29 17:03:21,887 - trainer - INFO -     val_precision  : 0.727374
2024-04-29 17:03:21,887 - trainer - INFO -     val_recall     : 0.706363
2024-04-29 17:03:21,887 - trainer - INFO -     val_doc_entropy: 2.621618
2024-04-29 17:03:21,887 - trainer - INFO -     test_loss      : 1.0082
2024-04-29 17:03:21,887 - trainer - INFO -     test_accuracy  : 0.707259
2024-04-29 17:03:21,887 - trainer - INFO -     test_macro_f   : 0.694463
2024-04-29 17:03:21,887 - trainer - INFO -     test_precision : 0.728551
2024-04-29 17:03:21,887 - trainer - INFO -     test_recall    : 0.707259
2024-04-29 17:03:21,887 - trainer - INFO -     test_doc_entropy: 2.625549
2024-04-29 17:07:20,406 - trainer - INFO -     epoch          : 2
2024-04-29 17:07:20,406 - trainer - INFO -     loss           : 0.810455
2024-04-29 17:07:20,406 - trainer - INFO -     accuracy       : 0.756921
2024-04-29 17:07:20,406 - trainer - INFO -     macro_f        : 0.74761
2024-04-29 17:07:20,406 - trainer - INFO -     precision      : 0.779741
2024-04-29 17:07:20,406 - trainer - INFO -     recall         : 0.756921
2024-04-29 17:07:20,406 - trainer - INFO -     doc_entropy    : 2.349767
2024-04-29 17:07:20,406 - trainer - INFO -     val_loss       : 0.995525
2024-04-29 17:07:20,406 - trainer - INFO -     val_accuracy   : 0.711839
2024-04-29 17:07:20,406 - trainer - INFO -     val_macro_f    : 0.705395
2024-04-29 17:07:20,406 - trainer - INFO -     val_precision  : 0.745177
2024-04-29 17:07:20,406 - trainer - INFO -     val_recall     : 0.711839
2024-04-29 17:07:20,406 - trainer - INFO -     val_doc_entropy: 2.667955
2024-04-29 17:07:20,406 - trainer - INFO -     test_loss      : 0.983873
2024-04-29 17:07:20,406 - trainer - INFO -     test_accuracy  : 0.710395
2024-04-29 17:07:20,406 - trainer - INFO -     test_macro_f   : 0.702354
2024-04-29 17:07:20,406 - trainer - INFO -     test_precision : 0.739728
2024-04-29 17:07:20,406 - trainer - INFO -     test_recall    : 0.710395
2024-04-29 17:07:20,406 - trainer - INFO -     test_doc_entropy: 2.670861
2024-04-29 17:08:00,274 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1600, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1600, out_features=80, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=80, bias=False)
  (W_q): Linear(in_features=300, out_features=80, bias=False)
  (W_v): Linear(in_features=300, out_features=80, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((80, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,430,235
Freeze params: 0
2024-04-29 17:11:56,020 - trainer - INFO -     epoch          : 1
2024-04-29 17:11:56,020 - trainer - INFO -     loss           : 1.146853
2024-04-29 17:11:56,020 - trainer - INFO -     accuracy       : 0.669036
2024-04-29 17:11:56,020 - trainer - INFO -     macro_f        : 0.651042
2024-04-29 17:11:56,020 - trainer - INFO -     precision      : 0.682784
2024-04-29 17:11:56,020 - trainer - INFO -     recall         : 0.669036
2024-04-29 17:11:56,020 - trainer - INFO -     doc_entropy    : 2.288783
2024-04-29 17:11:56,020 - trainer - INFO -     val_loss       : 0.998502
2024-04-29 17:11:56,020 - trainer - INFO -     val_accuracy   : 0.708902
2024-04-29 17:11:56,020 - trainer - INFO -     val_macro_f    : 0.697043
2024-04-29 17:11:56,020 - trainer - INFO -     val_precision  : 0.730955
2024-04-29 17:11:56,020 - trainer - INFO -     val_recall     : 0.708902
2024-04-29 17:11:56,020 - trainer - INFO -     val_doc_entropy: 2.638227
2024-04-29 17:11:56,020 - trainer - INFO -     test_loss      : 1.000003
2024-04-29 17:11:56,020 - trainer - INFO -     test_accuracy  : 0.705068
2024-04-29 17:11:56,020 - trainer - INFO -     test_macro_f   : 0.693137
2024-04-29 17:11:56,020 - trainer - INFO -     test_precision : 0.728175
2024-04-29 17:11:56,020 - trainer - INFO -     test_recall    : 0.705068
2024-04-29 17:11:56,020 - trainer - INFO -     test_doc_entropy: 2.64314
2024-04-29 17:15:55,649 - trainer - INFO -     epoch          : 2
2024-04-29 17:15:55,649 - trainer - INFO -     loss           : 0.815045
2024-04-29 17:15:55,649 - trainer - INFO -     accuracy       : 0.756174
2024-04-29 17:15:55,649 - trainer - INFO -     macro_f        : 0.747048
2024-04-29 17:15:55,649 - trainer - INFO -     precision      : 0.779412
2024-04-29 17:15:55,649 - trainer - INFO -     recall         : 0.756174
2024-04-29 17:15:55,649 - trainer - INFO -     doc_entropy    : 2.445922
2024-04-29 17:15:55,649 - trainer - INFO -     val_loss       : 1.012519
2024-04-29 17:15:55,649 - trainer - INFO -     val_accuracy   : 0.707159
2024-04-29 17:15:55,649 - trainer - INFO -     val_macro_f    : 0.699768
2024-04-29 17:15:55,649 - trainer - INFO -     val_precision  : 0.737843
2024-04-29 17:15:55,649 - trainer - INFO -     val_recall     : 0.707159
2024-04-29 17:15:55,649 - trainer - INFO -     val_doc_entropy: 2.765346
2024-04-29 17:15:55,649 - trainer - INFO -     test_loss      : 0.99378
2024-04-29 17:15:55,649 - trainer - INFO -     test_accuracy  : 0.712337
2024-04-29 17:15:55,649 - trainer - INFO -     test_macro_f   : 0.706344
2024-04-29 17:15:55,649 - trainer - INFO -     test_precision : 0.746392
2024-04-29 17:15:55,649 - trainer - INFO -     test_recall    : 0.712337
2024-04-29 17:15:55,649 - trainer - INFO -     test_doc_entropy: 2.769216
2024-04-29 17:16:35,754 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1600, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1600, out_features=80, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=80, bias=False)
  (W_q): Linear(in_features=300, out_features=80, bias=False)
  (W_v): Linear(in_features=300, out_features=80, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((80, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,430,235
Freeze params: 0
2024-04-29 17:20:34,596 - trainer - INFO -     epoch          : 1
2024-04-29 17:20:34,596 - trainer - INFO -     loss           : 1.150031
2024-04-29 17:20:34,596 - trainer - INFO -     accuracy       : 0.669428
2024-04-29 17:20:34,596 - trainer - INFO -     macro_f        : 0.650638
2024-04-29 17:20:34,596 - trainer - INFO -     precision      : 0.681344
2024-04-29 17:20:34,596 - trainer - INFO -     recall         : 0.669428
2024-04-29 17:20:34,596 - trainer - INFO -     doc_entropy    : 2.307312
2024-04-29 17:20:34,596 - trainer - INFO -     val_loss       : 1.00015
2024-04-29 17:20:34,596 - trainer - INFO -     val_accuracy   : 0.705068
2024-04-29 17:20:34,596 - trainer - INFO -     val_macro_f    : 0.693857
2024-04-29 17:20:34,596 - trainer - INFO -     val_precision  : 0.728177
2024-04-29 17:20:34,596 - trainer - INFO -     val_recall     : 0.705068
2024-04-29 17:20:34,596 - trainer - INFO -     val_doc_entropy: 2.656269
2024-04-29 17:20:34,596 - trainer - INFO -     test_loss      : 1.005023
2024-04-29 17:20:34,596 - trainer - INFO -     test_accuracy  : 0.706462
2024-04-29 17:20:34,596 - trainer - INFO -     test_macro_f   : 0.696388
2024-04-29 17:20:34,596 - trainer - INFO -     test_precision : 0.732695
2024-04-29 17:20:34,596 - trainer - INFO -     test_recall    : 0.706462
2024-04-29 17:20:34,596 - trainer - INFO -     test_doc_entropy: 2.661531
2024-04-29 17:24:33,765 - trainer - INFO -     epoch          : 2
2024-04-29 17:24:33,765 - trainer - INFO -     loss           : 0.811351
2024-04-29 17:24:33,765 - trainer - INFO -     accuracy       : 0.756715
2024-04-29 17:24:33,765 - trainer - INFO -     macro_f        : 0.747633
2024-04-29 17:24:33,765 - trainer - INFO -     precision      : 0.780565
2024-04-29 17:24:33,765 - trainer - INFO -     recall         : 0.756715
2024-04-29 17:24:33,765 - trainer - INFO -     doc_entropy    : 2.475235
2024-04-29 17:24:33,765 - trainer - INFO -     val_loss       : 0.988582
2024-04-29 17:24:33,781 - trainer - INFO -     val_accuracy   : 0.71154
2024-04-29 17:24:33,781 - trainer - INFO -     val_macro_f    : 0.700143
2024-04-29 17:24:33,781 - trainer - INFO -     val_precision  : 0.733928
2024-04-29 17:24:33,781 - trainer - INFO -     val_recall     : 0.71154
2024-04-29 17:24:33,781 - trainer - INFO -     val_doc_entropy: 2.794025
2024-04-29 17:24:33,781 - trainer - INFO -     test_loss      : 0.986001
2024-04-29 17:24:33,781 - trainer - INFO -     test_accuracy  : 0.710993
2024-04-29 17:24:33,781 - trainer - INFO -     test_macro_f   : 0.69817
2024-04-29 17:24:33,781 - trainer - INFO -     test_precision : 0.731075
2024-04-29 17:24:33,781 - trainer - INFO -     test_recall    : 0.710993
2024-04-29 17:24:33,781 - trainer - INFO -     test_doc_entropy: 2.797767
2024-04-29 17:25:12,720 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1600, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1600, out_features=80, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=80, bias=False)
  (W_q): Linear(in_features=300, out_features=80, bias=False)
  (W_v): Linear(in_features=300, out_features=80, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((80, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,430,235
Freeze params: 0
2024-04-29 17:29:12,521 - trainer - INFO -     epoch          : 1
2024-04-29 17:29:12,521 - trainer - INFO -     loss           : 1.150283
2024-04-29 17:29:12,521 - trainer - INFO -     accuracy       : 0.669509
2024-04-29 17:29:12,521 - trainer - INFO -     macro_f        : 0.651451
2024-04-29 17:29:12,521 - trainer - INFO -     precision      : 0.683197
2024-04-29 17:29:12,521 - trainer - INFO -     recall         : 0.669509
2024-04-29 17:29:12,521 - trainer - INFO -     doc_entropy    : 2.339994
2024-04-29 17:29:12,521 - trainer - INFO -     val_loss       : 1.019
2024-04-29 17:29:12,521 - trainer - INFO -     val_accuracy   : 0.70243
2024-04-29 17:29:12,521 - trainer - INFO -     val_macro_f    : 0.685257
2024-04-29 17:29:12,521 - trainer - INFO -     val_precision  : 0.714773
2024-04-29 17:29:12,521 - trainer - INFO -     val_recall     : 0.70243
2024-04-29 17:29:12,521 - trainer - INFO -     val_doc_entropy: 2.722466
2024-04-29 17:29:12,521 - trainer - INFO -     test_loss      : 1.015397
2024-04-29 17:29:12,521 - trainer - INFO -     test_accuracy  : 0.702579
2024-04-29 17:29:12,521 - trainer - INFO -     test_macro_f   : 0.687962
2024-04-29 17:29:12,521 - trainer - INFO -     test_precision : 0.720902
2024-04-29 17:29:12,521 - trainer - INFO -     test_recall    : 0.702579
2024-04-29 17:29:12,521 - trainer - INFO -     test_doc_entropy: 2.727068
2024-04-29 17:33:11,688 - trainer - INFO -     epoch          : 2
2024-04-29 17:33:11,688 - trainer - INFO -     loss           : 0.818183
2024-04-29 17:33:11,688 - trainer - INFO -     accuracy       : 0.754817
2024-04-29 17:33:11,688 - trainer - INFO -     macro_f        : 0.745895
2024-04-29 17:33:11,688 - trainer - INFO -     precision      : 0.779367
2024-04-29 17:33:11,688 - trainer - INFO -     recall         : 0.754817
2024-04-29 17:33:11,688 - trainer - INFO -     doc_entropy    : 2.464822
2024-04-29 17:33:11,688 - trainer - INFO -     val_loss       : 0.993356
2024-04-29 17:33:11,688 - trainer - INFO -     val_accuracy   : 0.711092
2024-04-29 17:33:11,688 - trainer - INFO -     val_macro_f    : 0.698491
2024-04-29 17:33:11,688 - trainer - INFO -     val_precision  : 0.731484
2024-04-29 17:33:11,688 - trainer - INFO -     val_recall     : 0.711092
2024-04-29 17:33:11,688 - trainer - INFO -     val_doc_entropy: 2.77662
2024-04-29 17:33:11,688 - trainer - INFO -     test_loss      : 0.987262
2024-04-29 17:33:11,688 - trainer - INFO -     test_accuracy  : 0.710445
2024-04-29 17:33:11,688 - trainer - INFO -     test_macro_f   : 0.697166
2024-04-29 17:33:11,688 - trainer - INFO -     test_precision : 0.729467
2024-04-29 17:33:11,688 - trainer - INFO -     test_recall    : 0.710445
2024-04-29 17:33:11,688 - trainer - INFO -     test_doc_entropy: 2.781027
2024-04-29 17:33:51,630 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1600, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1600, out_features=80, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=80, bias=False)
  (W_q): Linear(in_features=300, out_features=80, bias=False)
  (W_v): Linear(in_features=300, out_features=80, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((80, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,430,235
Freeze params: 0
2024-04-29 17:37:48,394 - trainer - INFO -     epoch          : 1
2024-04-29 17:37:48,394 - trainer - INFO -     loss           : 1.146484
2024-04-29 17:37:48,394 - trainer - INFO -     accuracy       : 0.670984
2024-04-29 17:37:48,394 - trainer - INFO -     macro_f        : 0.652409
2024-04-29 17:37:48,394 - trainer - INFO -     precision      : 0.683701
2024-04-29 17:37:48,410 - trainer - INFO -     recall         : 0.670984
2024-04-29 17:37:48,410 - trainer - INFO -     doc_entropy    : 2.354603
2024-04-29 17:37:48,410 - trainer - INFO -     val_loss       : 1.007242
2024-04-29 17:37:48,410 - trainer - INFO -     val_accuracy   : 0.705218
2024-04-29 17:37:48,410 - trainer - INFO -     val_macro_f    : 0.687624
2024-04-29 17:37:48,410 - trainer - INFO -     val_precision  : 0.715595
2024-04-29 17:37:48,410 - trainer - INFO -     val_recall     : 0.705218
2024-04-29 17:37:48,410 - trainer - INFO -     val_doc_entropy: 2.800483
2024-04-29 17:37:48,410 - trainer - INFO -     test_loss      : 1.005305
2024-04-29 17:37:48,410 - trainer - INFO -     test_accuracy  : 0.704421
2024-04-29 17:37:48,410 - trainer - INFO -     test_macro_f   : 0.688094
2024-04-29 17:37:48,410 - trainer - INFO -     test_precision : 0.718448
2024-04-29 17:37:48,410 - trainer - INFO -     test_recall    : 0.704421
2024-04-29 17:37:48,410 - trainer - INFO -     test_doc_entropy: 2.804871
2024-04-29 17:41:49,567 - trainer - INFO -     epoch          : 2
2024-04-29 17:41:49,567 - trainer - INFO -     loss           : 0.81273
2024-04-29 17:41:49,567 - trainer - INFO -     accuracy       : 0.757002
2024-04-29 17:41:49,567 - trainer - INFO -     macro_f        : 0.747869
2024-04-29 17:41:49,567 - trainer - INFO -     precision      : 0.780304
2024-04-29 17:41:49,567 - trainer - INFO -     recall         : 0.757002
2024-04-29 17:41:49,567 - trainer - INFO -     doc_entropy    : 2.543801
2024-04-29 17:41:49,567 - trainer - INFO -     val_loss       : 0.996409
2024-04-29 17:41:49,567 - trainer - INFO -     val_accuracy   : 0.70925
2024-04-29 17:41:49,567 - trainer - INFO -     val_macro_f    : 0.700089
2024-04-29 17:41:49,567 - trainer - INFO -     val_precision  : 0.735436
2024-04-29 17:41:49,567 - trainer - INFO -     val_recall     : 0.70925
2024-04-29 17:41:49,567 - trainer - INFO -     val_doc_entropy: 2.829087
2024-04-29 17:41:49,567 - trainer - INFO -     test_loss      : 0.991113
2024-04-29 17:41:49,567 - trainer - INFO -     test_accuracy  : 0.714428
2024-04-29 17:41:49,567 - trainer - INFO -     test_macro_f   : 0.706366
2024-04-29 17:41:49,567 - trainer - INFO -     test_precision : 0.745048
2024-04-29 17:41:49,567 - trainer - INFO -     test_recall    : 0.714428
2024-04-29 17:41:49,567 - trainer - INFO -     test_doc_entropy: 2.833145
2024-04-29 17:55:01,960 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=2000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=2000, out_features=100, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=100, bias=False)
  (W_q): Linear(in_features=300, out_features=100, bias=False)
  (W_v): Linear(in_features=300, out_features=100, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((100, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,644,655
Freeze params: 0
2024-04-29 17:58:43,420 - trainer - INFO -     epoch          : 1
2024-04-29 17:58:43,420 - trainer - INFO -     loss           : 1.142736
2024-04-29 17:58:43,420 - trainer - INFO -     accuracy       : 0.672204
2024-04-29 17:58:43,420 - trainer - INFO -     macro_f        : 0.654896
2024-04-29 17:58:43,420 - trainer - INFO -     precision      : 0.686877
2024-04-29 17:58:43,420 - trainer - INFO -     recall         : 0.672204
2024-04-29 17:58:43,420 - trainer - INFO -     doc_entropy    : 2.302983
2024-04-29 17:58:43,420 - trainer - INFO -     val_loss       : 0.99853
2024-04-29 17:58:43,420 - trainer - INFO -     val_accuracy   : 0.710246
2024-04-29 17:58:43,420 - trainer - INFO -     val_macro_f    : 0.699655
2024-04-29 17:58:43,420 - trainer - INFO -     val_precision  : 0.735861
2024-04-29 17:58:43,420 - trainer - INFO -     val_recall     : 0.710246
2024-04-29 17:58:43,420 - trainer - INFO -     val_doc_entropy: 2.700399
2024-04-29 17:58:43,420 - trainer - INFO -     test_loss      : 0.999378
2024-04-29 17:58:43,420 - trainer - INFO -     test_accuracy  : 0.70457
2024-04-29 17:58:43,420 - trainer - INFO -     test_macro_f   : 0.694269
2024-04-29 17:58:43,420 - trainer - INFO -     test_precision : 0.730476
2024-04-29 17:58:43,420 - trainer - INFO -     test_recall    : 0.70457
2024-04-29 17:58:43,420 - trainer - INFO -     test_doc_entropy: 2.704947
2024-04-29 18:02:35,884 - trainer - INFO -     epoch          : 2
2024-04-29 18:02:35,884 - trainer - INFO -     loss           : 0.808188
2024-04-29 18:02:35,884 - trainer - INFO -     accuracy       : 0.757238
2024-04-29 18:02:35,884 - trainer - INFO -     macro_f        : 0.74847
2024-04-29 18:02:35,884 - trainer - INFO -     precision      : 0.781467
2024-04-29 18:02:35,884 - trainer - INFO -     recall         : 0.757238
2024-04-29 18:02:35,884 - trainer - INFO -     doc_entropy    : 2.501747
2024-04-29 18:02:35,884 - trainer - INFO -     val_loss       : 1.004282
2024-04-29 18:02:35,884 - trainer - INFO -     val_accuracy   : 0.710545
2024-04-29 18:02:35,884 - trainer - INFO -     val_macro_f    : 0.702562
2024-04-29 18:02:35,900 - trainer - INFO -     val_precision  : 0.739272
2024-04-29 18:02:35,900 - trainer - INFO -     val_recall     : 0.710545
2024-04-29 18:02:35,900 - trainer - INFO -     val_doc_entropy: 2.829023
2024-04-29 18:02:35,900 - trainer - INFO -     test_loss      : 1.000173
2024-04-29 18:02:35,900 - trainer - INFO -     test_accuracy  : 0.708105
2024-04-29 18:02:35,900 - trainer - INFO -     test_macro_f   : 0.700672
2024-04-29 18:02:35,900 - trainer - INFO -     test_precision : 0.738589
2024-04-29 18:02:35,900 - trainer - INFO -     test_recall    : 0.708105
2024-04-29 18:02:35,900 - trainer - INFO -     test_doc_entropy: 2.833892
2024-04-29 18:03:14,411 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=2000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=2000, out_features=100, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=100, bias=False)
  (W_q): Linear(in_features=300, out_features=100, bias=False)
  (W_v): Linear(in_features=300, out_features=100, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((100, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,644,655
Freeze params: 0
2024-04-29 18:07:08,672 - trainer - INFO -     epoch          : 1
2024-04-29 18:07:08,672 - trainer - INFO -     loss           : 1.14598
2024-04-29 18:07:08,672 - trainer - INFO -     accuracy       : 0.671239
2024-04-29 18:07:08,672 - trainer - INFO -     macro_f        : 0.652921
2024-04-29 18:07:08,672 - trainer - INFO -     precision      : 0.684011
2024-04-29 18:07:08,672 - trainer - INFO -     recall         : 0.671239
2024-04-29 18:07:08,672 - trainer - INFO -     doc_entropy    : 2.362192
2024-04-29 18:07:08,672 - trainer - INFO -     val_loss       : 1.00031
2024-04-29 18:07:08,672 - trainer - INFO -     val_accuracy   : 0.708553
2024-04-29 18:07:08,672 - trainer - INFO -     val_macro_f    : 0.699435
2024-04-29 18:07:08,672 - trainer - INFO -     val_precision  : 0.737662
2024-04-29 18:07:08,672 - trainer - INFO -     val_recall     : 0.708553
2024-04-29 18:07:08,672 - trainer - INFO -     val_doc_entropy: 2.761837
2024-04-29 18:07:08,672 - trainer - INFO -     test_loss      : 0.999298
2024-04-29 18:07:08,672 - trainer - INFO -     test_accuracy  : 0.707757
2024-04-29 18:07:08,672 - trainer - INFO -     test_macro_f   : 0.698115
2024-04-29 18:07:08,672 - trainer - INFO -     test_precision : 0.734918
2024-04-29 18:07:08,672 - trainer - INFO -     test_recall    : 0.707757
2024-04-29 18:07:08,672 - trainer - INFO -     test_doc_entropy: 2.766072
2024-04-29 18:11:05,593 - trainer - INFO -     epoch          : 2
2024-04-29 18:11:05,593 - trainer - INFO -     loss           : 0.812604
2024-04-29 18:11:05,593 - trainer - INFO -     accuracy       : 0.756479
2024-04-29 18:11:05,593 - trainer - INFO -     macro_f        : 0.747335
2024-04-29 18:11:05,593 - trainer - INFO -     precision      : 0.779408
2024-04-29 18:11:05,609 - trainer - INFO -     recall         : 0.756479
2024-04-29 18:11:05,609 - trainer - INFO -     doc_entropy    : 2.528384
2024-04-29 18:11:05,609 - trainer - INFO -     val_loss       : 0.986792
2024-04-29 18:11:05,609 - trainer - INFO -     val_accuracy   : 0.709947
2024-04-29 18:11:05,609 - trainer - INFO -     val_macro_f    : 0.702402
2024-04-29 18:11:05,609 - trainer - INFO -     val_precision  : 0.740094
2024-04-29 18:11:05,609 - trainer - INFO -     val_recall     : 0.709947
2024-04-29 18:11:05,609 - trainer - INFO -     val_doc_entropy: 2.870581
2024-04-29 18:11:05,609 - trainer - INFO -     test_loss      : 0.983122
2024-04-29 18:11:05,609 - trainer - INFO -     test_accuracy  : 0.710943
2024-04-29 18:11:05,609 - trainer - INFO -     test_macro_f   : 0.701254
2024-04-29 18:11:05,609 - trainer - INFO -     test_precision : 0.737237
2024-04-29 18:11:05,609 - trainer - INFO -     test_recall    : 0.710943
2024-04-29 18:11:05,609 - trainer - INFO -     test_doc_entropy: 2.874426
2024-04-29 18:11:43,574 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=2000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=2000, out_features=100, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=100, bias=False)
  (W_q): Linear(in_features=300, out_features=100, bias=False)
  (W_v): Linear(in_features=300, out_features=100, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((100, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,644,655
Freeze params: 0
2024-04-29 18:15:33,865 - trainer - INFO -     epoch          : 1
2024-04-29 18:15:33,865 - trainer - INFO -     loss           : 1.146966
2024-04-29 18:15:33,865 - trainer - INFO -     accuracy       : 0.671034
2024-04-29 18:15:33,865 - trainer - INFO -     macro_f        : 0.652653
2024-04-29 18:15:33,865 - trainer - INFO -     precision      : 0.683644
2024-04-29 18:15:33,865 - trainer - INFO -     recall         : 0.671034
2024-04-29 18:15:33,865 - trainer - INFO -     doc_entropy    : 2.240987
2024-04-29 18:15:33,865 - trainer - INFO -     val_loss       : 0.994678
2024-04-29 18:15:33,865 - trainer - INFO -     val_accuracy   : 0.711789
2024-04-29 18:15:33,865 - trainer - INFO -     val_macro_f    : 0.701506
2024-04-29 18:15:33,865 - trainer - INFO -     val_precision  : 0.737082
2024-04-29 18:15:33,865 - trainer - INFO -     val_recall     : 0.711789
2024-04-29 18:15:33,865 - trainer - INFO -     val_doc_entropy: 2.656487
2024-04-29 18:15:33,865 - trainer - INFO -     test_loss      : 0.994183
2024-04-29 18:15:33,865 - trainer - INFO -     test_accuracy  : 0.706711
2024-04-29 18:15:33,865 - trainer - INFO -     test_macro_f   : 0.696611
2024-04-29 18:15:33,865 - trainer - INFO -     test_precision : 0.733171
2024-04-29 18:15:33,865 - trainer - INFO -     test_recall    : 0.706711
2024-04-29 18:15:33,865 - trainer - INFO -     test_doc_entropy: 2.660736
2024-04-29 18:19:28,542 - trainer - INFO -     epoch          : 2
2024-04-29 18:19:28,542 - trainer - INFO -     loss           : 0.810923
2024-04-29 18:19:28,542 - trainer - INFO -     accuracy       : 0.756734
2024-04-29 18:19:28,542 - trainer - INFO -     macro_f        : 0.747615
2024-04-29 18:19:28,542 - trainer - INFO -     precision      : 0.780108
2024-04-29 18:19:28,542 - trainer - INFO -     recall         : 0.756734
2024-04-29 18:19:28,542 - trainer - INFO -     doc_entropy    : 2.385645
2024-04-29 18:19:28,542 - trainer - INFO -     val_loss       : 1.009884
2024-04-29 18:19:28,542 - trainer - INFO -     val_accuracy   : 0.704023
2024-04-29 18:19:28,542 - trainer - INFO -     val_macro_f    : 0.698869
2024-04-29 18:19:28,542 - trainer - INFO -     val_precision  : 0.740546
2024-04-29 18:19:28,542 - trainer - INFO -     val_recall     : 0.704023
2024-04-29 18:19:28,542 - trainer - INFO -     val_doc_entropy: 2.704048
2024-04-29 18:19:28,542 - trainer - INFO -     test_loss      : 0.99652
2024-04-29 18:19:28,542 - trainer - INFO -     test_accuracy  : 0.704471
2024-04-29 18:19:28,542 - trainer - INFO -     test_macro_f   : 0.700018
2024-04-29 18:19:28,542 - trainer - INFO -     test_precision : 0.74238
2024-04-29 18:19:28,542 - trainer - INFO -     test_recall    : 0.704471
2024-04-29 18:19:28,542 - trainer - INFO -     test_doc_entropy: 2.707109
2024-04-29 18:20:10,925 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=2000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=2000, out_features=100, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=100, bias=False)
  (W_q): Linear(in_features=300, out_features=100, bias=False)
  (W_v): Linear(in_features=300, out_features=100, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((100, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,644,655
Freeze params: 0
2024-04-29 18:24:00,531 - trainer - INFO -     epoch          : 1
2024-04-29 18:24:00,531 - trainer - INFO -     loss           : 1.149013
2024-04-29 18:24:00,531 - trainer - INFO -     accuracy       : 0.669783
2024-04-29 18:24:00,531 - trainer - INFO -     macro_f        : 0.651515
2024-04-29 18:24:00,531 - trainer - INFO -     precision      : 0.682431
2024-04-29 18:24:00,531 - trainer - INFO -     recall         : 0.669783
2024-04-29 18:24:00,531 - trainer - INFO -     doc_entropy    : 2.448214
2024-04-29 18:24:00,531 - trainer - INFO -     val_loss       : 1.004382
2024-04-29 18:24:00,531 - trainer - INFO -     val_accuracy   : 0.703624
2024-04-29 18:24:00,531 - trainer - INFO -     val_macro_f    : 0.692389
2024-04-29 18:24:00,531 - trainer - INFO -     val_precision  : 0.728222
2024-04-29 18:24:00,531 - trainer - INFO -     val_recall     : 0.703624
2024-04-29 18:24:00,531 - trainer - INFO -     val_doc_entropy: 2.814232
2024-04-29 18:24:00,531 - trainer - INFO -     test_loss      : 1.004806
2024-04-29 18:24:00,531 - trainer - INFO -     test_accuracy  : 0.704321
2024-04-29 18:24:00,531 - trainer - INFO -     test_macro_f   : 0.692061
2024-04-29 18:24:00,531 - trainer - INFO -     test_precision : 0.726857
2024-04-29 18:24:00,531 - trainer - INFO -     test_recall    : 0.704321
2024-04-29 18:24:00,531 - trainer - INFO -     test_doc_entropy: 2.81963
2024-04-29 18:27:59,555 - trainer - INFO -     epoch          : 2
2024-04-29 18:27:59,555 - trainer - INFO -     loss           : 0.813068
2024-04-29 18:27:59,555 - trainer - INFO -     accuracy       : 0.756815
2024-04-29 18:27:59,555 - trainer - INFO -     macro_f        : 0.748211
2024-04-29 18:27:59,555 - trainer - INFO -     precision      : 0.781038
2024-04-29 18:27:59,555 - trainer - INFO -     recall         : 0.756815
2024-04-29 18:27:59,555 - trainer - INFO -     doc_entropy    : 2.588291
2024-04-29 18:27:59,555 - trainer - INFO -     val_loss       : 0.991717
2024-04-29 18:27:59,555 - trainer - INFO -     val_accuracy   : 0.709449
2024-04-29 18:27:59,555 - trainer - INFO -     val_macro_f    : 0.695878
2024-04-29 18:27:59,555 - trainer - INFO -     val_precision  : 0.728417
2024-04-29 18:27:59,555 - trainer - INFO -     val_recall     : 0.709449
2024-04-29 18:27:59,555 - trainer - INFO -     val_doc_entropy: 2.905471
2024-04-29 18:27:59,555 - trainer - INFO -     test_loss      : 0.982865
2024-04-29 18:27:59,555 - trainer - INFO -     test_accuracy  : 0.713283
2024-04-29 18:27:59,555 - trainer - INFO -     test_macro_f   : 0.698922
2024-04-29 18:27:59,555 - trainer - INFO -     test_precision : 0.731276
2024-04-29 18:27:59,555 - trainer - INFO -     test_recall    : 0.713283
2024-04-29 18:27:59,555 - trainer - INFO -     test_doc_entropy: 2.909831
2024-04-29 18:28:37,820 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=2000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=2000, out_features=100, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=100, bias=False)
  (W_q): Linear(in_features=300, out_features=100, bias=False)
  (W_v): Linear(in_features=300, out_features=100, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((100, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,644,655
Freeze params: 0
2024-04-29 18:32:33,026 - trainer - INFO -     epoch          : 1
2024-04-29 18:32:33,026 - trainer - INFO -     loss           : 1.145617
2024-04-29 18:32:33,026 - trainer - INFO -     accuracy       : 0.671936
2024-04-29 18:32:33,026 - trainer - INFO -     macro_f        : 0.653413
2024-04-29 18:32:33,026 - trainer - INFO -     precision      : 0.684262
2024-04-29 18:32:33,026 - trainer - INFO -     recall         : 0.671936
2024-04-29 18:32:33,026 - trainer - INFO -     doc_entropy    : 2.368925
2024-04-29 18:32:33,026 - trainer - INFO -     val_loss       : 0.998167
2024-04-29 18:32:33,026 - trainer - INFO -     val_accuracy   : 0.704471
2024-04-29 18:32:33,026 - trainer - INFO -     val_macro_f    : 0.693281
2024-04-29 18:32:33,026 - trainer - INFO -     val_precision  : 0.727486
2024-04-29 18:32:33,026 - trainer - INFO -     val_recall     : 0.704471
2024-04-29 18:32:33,026 - trainer - INFO -     val_doc_entropy: 2.685415
2024-04-29 18:32:33,026 - trainer - INFO -     test_loss      : 0.991679
2024-04-29 18:32:33,026 - trainer - INFO -     test_accuracy  : 0.709649
2024-04-29 18:32:33,026 - trainer - INFO -     test_macro_f   : 0.698447
2024-04-29 18:32:33,026 - trainer - INFO -     test_precision : 0.735388
2024-04-29 18:32:33,026 - trainer - INFO -     test_recall    : 0.709649
2024-04-29 18:32:33,026 - trainer - INFO -     test_doc_entropy: 2.68894
2024-04-29 18:36:26,605 - trainer - INFO -     epoch          : 2
2024-04-29 18:36:26,605 - trainer - INFO -     loss           : 0.811859
2024-04-29 18:36:26,605 - trainer - INFO -     accuracy       : 0.757568
2024-04-29 18:36:26,605 - trainer - INFO -     macro_f        : 0.748551
2024-04-29 18:36:26,605 - trainer - INFO -     precision      : 0.781237
2024-04-29 18:36:26,605 - trainer - INFO -     recall         : 0.757568
2024-04-29 18:36:26,605 - trainer - INFO -     doc_entropy    : 2.479854
2024-04-29 18:36:26,605 - trainer - INFO -     val_loss       : 0.994698
2024-04-29 18:36:26,605 - trainer - INFO -     val_accuracy   : 0.708553
2024-04-29 18:36:26,605 - trainer - INFO -     val_macro_f    : 0.698914
2024-04-29 18:36:26,605 - trainer - INFO -     val_precision  : 0.736595
2024-04-29 18:36:26,605 - trainer - INFO -     val_recall     : 0.708553
2024-04-29 18:36:26,605 - trainer - INFO -     val_doc_entropy: 2.838069
2024-04-29 18:36:26,605 - trainer - INFO -     test_loss      : 0.98765
2024-04-29 18:36:26,605 - trainer - INFO -     test_accuracy  : 0.710893
2024-04-29 18:36:26,605 - trainer - INFO -     test_macro_f   : 0.701088
2024-04-29 18:36:26,605 - trainer - INFO -     test_precision : 0.736576
2024-04-29 18:36:26,605 - trainer - INFO -     test_recall    : 0.710893
2024-04-29 18:36:26,605 - trainer - INFO -     test_doc_entropy: 2.84139
2024-04-29 19:14:42,812 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1800, out_features=90, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=90, bias=False)
  (W_q): Linear(in_features=300, out_features=90, bias=False)
  (W_v): Linear(in_features=300, out_features=90, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((90, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,535,445
Freeze params: 0
2024-04-29 19:18:22,347 - trainer - INFO -     epoch          : 1
2024-04-29 19:18:22,347 - trainer - INFO -     loss           : 1.1449
2024-04-29 19:18:22,347 - trainer - INFO -     accuracy       : 0.669888
2024-04-29 19:18:22,347 - trainer - INFO -     macro_f        : 0.651934
2024-04-29 19:18:22,347 - trainer - INFO -     precision      : 0.683522
2024-04-29 19:18:22,347 - trainer - INFO -     recall         : 0.669888
2024-04-29 19:18:22,347 - trainer - INFO -     doc_entropy    : 2.36364
2024-04-29 19:18:22,347 - trainer - INFO -     val_loss       : 1.012921
2024-04-29 19:18:22,347 - trainer - INFO -     val_accuracy   : 0.70243
2024-04-29 19:18:22,347 - trainer - INFO -     val_macro_f    : 0.687432
2024-04-29 19:18:22,347 - trainer - INFO -     val_precision  : 0.72013
2024-04-29 19:18:22,347 - trainer - INFO -     val_recall     : 0.70243
2024-04-29 19:18:22,347 - trainer - INFO -     val_doc_entropy: 2.7761
2024-04-29 19:18:22,347 - trainer - INFO -     test_loss      : 1.008073
2024-04-29 19:18:22,347 - trainer - INFO -     test_accuracy  : 0.703674
2024-04-29 19:18:22,347 - trainer - INFO -     test_macro_f   : 0.688561
2024-04-29 19:18:22,347 - trainer - INFO -     test_precision : 0.720122
2024-04-29 19:18:22,347 - trainer - INFO -     test_recall    : 0.703674
2024-04-29 19:18:22,347 - trainer - INFO -     test_doc_entropy: 2.780069
2024-04-29 19:22:10,821 - trainer - INFO -     epoch          : 2
2024-04-29 19:22:10,821 - trainer - INFO -     loss           : 0.811637
2024-04-29 19:22:10,821 - trainer - INFO -     accuracy       : 0.757232
2024-04-29 19:22:10,821 - trainer - INFO -     macro_f        : 0.747904
2024-04-29 19:22:10,821 - trainer - INFO -     precision      : 0.78033
2024-04-29 19:22:10,821 - trainer - INFO -     recall         : 0.757232
2024-04-29 19:22:10,821 - trainer - INFO -     doc_entropy    : 2.524341
2024-04-29 19:22:10,821 - trainer - INFO -     val_loss       : 0.990913
2024-04-29 19:22:10,821 - trainer - INFO -     val_accuracy   : 0.713382
2024-04-29 19:22:10,821 - trainer - INFO -     val_macro_f    : 0.703798
2024-04-29 19:22:10,821 - trainer - INFO -     val_precision  : 0.740663
2024-04-29 19:22:10,821 - trainer - INFO -     val_recall     : 0.713382
2024-04-29 19:22:10,821 - trainer - INFO -     val_doc_entropy: 2.852198
2024-04-29 19:22:10,821 - trainer - INFO -     test_loss      : 0.97975
2024-04-29 19:22:10,821 - trainer - INFO -     test_accuracy  : 0.711441
2024-04-29 19:22:10,821 - trainer - INFO -     test_macro_f   : 0.700911
2024-04-29 19:22:10,821 - trainer - INFO -     test_precision : 0.735139
2024-04-29 19:22:10,821 - trainer - INFO -     test_recall    : 0.711441
2024-04-29 19:22:10,821 - trainer - INFO -     test_doc_entropy: 2.855233
2024-04-29 19:22:48,968 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1800, out_features=90, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=90, bias=False)
  (W_q): Linear(in_features=300, out_features=90, bias=False)
  (W_v): Linear(in_features=300, out_features=90, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((90, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,535,445
Freeze params: 0
2024-04-29 19:26:42,745 - trainer - INFO -     epoch          : 1
2024-04-29 19:26:42,745 - trainer - INFO -     loss           : 1.146763
2024-04-29 19:26:42,745 - trainer - INFO -     accuracy       : 0.67048
2024-04-29 19:26:42,745 - trainer - INFO -     macro_f        : 0.652132
2024-04-29 19:26:42,745 - trainer - INFO -     precision      : 0.683305
2024-04-29 19:26:42,745 - trainer - INFO -     recall         : 0.67048
2024-04-29 19:26:42,745 - trainer - INFO -     doc_entropy    : 2.425862
2024-04-29 19:26:42,745 - trainer - INFO -     val_loss       : 1.009454
2024-04-29 19:26:42,745 - trainer - INFO -     val_accuracy   : 0.701384
2024-04-29 19:26:42,745 - trainer - INFO -     val_macro_f    : 0.691794
2024-04-29 19:26:42,745 - trainer - INFO -     val_precision  : 0.728569
2024-04-29 19:26:42,745 - trainer - INFO -     val_recall     : 0.701384
2024-04-29 19:26:42,745 - trainer - INFO -     val_doc_entropy: 2.76405
2024-04-29 19:26:42,745 - trainer - INFO -     test_loss      : 1.004565
2024-04-29 19:26:42,745 - trainer - INFO -     test_accuracy  : 0.701782
2024-04-29 19:26:42,745 - trainer - INFO -     test_macro_f   : 0.694024
2024-04-29 19:26:42,745 - trainer - INFO -     test_precision : 0.732544
2024-04-29 19:26:42,745 - trainer - INFO -     test_recall    : 0.701782
2024-04-29 19:26:42,745 - trainer - INFO -     test_doc_entropy: 2.768324
2024-04-29 19:30:41,911 - trainer - INFO -     epoch          : 2
2024-04-29 19:30:41,911 - trainer - INFO -     loss           : 0.813803
2024-04-29 19:30:41,911 - trainer - INFO -     accuracy       : 0.755247
2024-04-29 19:30:41,911 - trainer - INFO -     macro_f        : 0.746405
2024-04-29 19:30:41,911 - trainer - INFO -     precision      : 0.779001
2024-04-29 19:30:41,911 - trainer - INFO -     recall         : 0.755247
2024-04-29 19:30:41,911 - trainer - INFO -     doc_entropy    : 2.601812
2024-04-29 19:30:41,911 - trainer - INFO -     val_loss       : 0.994709
2024-04-29 19:30:41,911 - trainer - INFO -     val_accuracy   : 0.708703
2024-04-29 19:30:41,911 - trainer - INFO -     val_macro_f    : 0.699625
2024-04-29 19:30:41,911 - trainer - INFO -     val_precision  : 0.736113
2024-04-29 19:30:41,911 - trainer - INFO -     val_recall     : 0.708703
2024-04-29 19:30:41,911 - trainer - INFO -     val_doc_entropy: 2.83695
2024-04-29 19:30:41,911 - trainer - INFO -     test_loss      : 0.986379
2024-04-29 19:30:41,911 - trainer - INFO -     test_accuracy  : 0.712835
2024-04-29 19:30:41,911 - trainer - INFO -     test_macro_f   : 0.705338
2024-04-29 19:30:41,911 - trainer - INFO -     test_precision : 0.743079
2024-04-29 19:30:41,911 - trainer - INFO -     test_recall    : 0.712835
2024-04-29 19:30:41,911 - trainer - INFO -     test_doc_entropy: 2.840352
2024-04-29 19:31:20,937 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1800, out_features=90, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=90, bias=False)
  (W_q): Linear(in_features=300, out_features=90, bias=False)
  (W_v): Linear(in_features=300, out_features=90, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((90, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,535,445
Freeze params: 0
2024-04-29 19:35:17,575 - trainer - INFO -     epoch          : 1
2024-04-29 19:35:17,575 - trainer - INFO -     loss           : 1.14518
2024-04-29 19:35:17,575 - trainer - INFO -     accuracy       : 0.671363
2024-04-29 19:35:17,575 - trainer - INFO -     macro_f        : 0.653092
2024-04-29 19:35:17,575 - trainer - INFO -     precision      : 0.684579
2024-04-29 19:35:17,575 - trainer - INFO -     recall         : 0.671363
2024-04-29 19:35:17,575 - trainer - INFO -     doc_entropy    : 2.361088
2024-04-29 19:35:17,575 - trainer - INFO -     val_loss       : 1.010624
2024-04-29 19:35:17,575 - trainer - INFO -     val_accuracy   : 0.705267
2024-04-29 19:35:17,575 - trainer - INFO -     val_macro_f    : 0.693825
2024-04-29 19:35:17,575 - trainer - INFO -     val_precision  : 0.728884
2024-04-29 19:35:17,575 - trainer - INFO -     val_recall     : 0.705267
2024-04-29 19:35:17,575 - trainer - INFO -     val_doc_entropy: 2.743333
2024-04-29 19:35:17,575 - trainer - INFO -     test_loss      : 0.995736
2024-04-29 19:35:17,575 - trainer - INFO -     test_accuracy  : 0.708852
2024-04-29 19:35:17,575 - trainer - INFO -     test_macro_f   : 0.69829
2024-04-29 19:35:17,575 - trainer - INFO -     test_precision : 0.73546
2024-04-29 19:35:17,575 - trainer - INFO -     test_recall    : 0.708852
2024-04-29 19:35:17,575 - trainer - INFO -     test_doc_entropy: 2.74697
2024-04-29 19:39:16,428 - trainer - INFO -     epoch          : 2
2024-04-29 19:39:16,428 - trainer - INFO -     loss           : 0.810821
2024-04-29 19:39:16,428 - trainer - INFO -     accuracy       : 0.756834
2024-04-29 19:39:16,428 - trainer - INFO -     macro_f        : 0.747987
2024-04-29 19:39:16,428 - trainer - INFO -     precision      : 0.780546
2024-04-29 19:39:16,428 - trainer - INFO -     recall         : 0.756834
2024-04-29 19:39:16,428 - trainer - INFO -     doc_entropy    : 2.522451
2024-04-29 19:39:16,428 - trainer - INFO -     val_loss       : 0.994262
2024-04-29 19:39:16,428 - trainer - INFO -     val_accuracy   : 0.712735
2024-04-29 19:39:16,428 - trainer - INFO -     val_macro_f    : 0.705066
2024-04-29 19:39:16,428 - trainer - INFO -     val_precision  : 0.741515
2024-04-29 19:39:16,428 - trainer - INFO -     val_recall     : 0.712735
2024-04-29 19:39:16,428 - trainer - INFO -     val_doc_entropy: 2.831152
2024-04-29 19:39:16,428 - trainer - INFO -     test_loss      : 0.98832
2024-04-29 19:39:16,428 - trainer - INFO -     test_accuracy  : 0.712188
2024-04-29 19:39:16,428 - trainer - INFO -     test_macro_f   : 0.705888
2024-04-29 19:39:16,428 - trainer - INFO -     test_precision : 0.744249
2024-04-29 19:39:16,428 - trainer - INFO -     test_recall    : 0.712188
2024-04-29 19:39:16,428 - trainer - INFO -     test_doc_entropy: 2.833469
2024-04-29 19:39:56,006 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1800, out_features=90, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=90, bias=False)
  (W_q): Linear(in_features=300, out_features=90, bias=False)
  (W_v): Linear(in_features=300, out_features=90, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((90, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,535,445
Freeze params: 0
2024-04-29 19:43:51,778 - trainer - INFO -     epoch          : 1
2024-04-29 19:43:51,778 - trainer - INFO -     loss           : 1.148169
2024-04-29 19:43:51,778 - trainer - INFO -     accuracy       : 0.670187
2024-04-29 19:43:51,778 - trainer - INFO -     macro_f        : 0.651893
2024-04-29 19:43:51,778 - trainer - INFO -     precision      : 0.682933
2024-04-29 19:43:51,778 - trainer - INFO -     recall         : 0.670187
2024-04-29 19:43:51,778 - trainer - INFO -     doc_entropy    : 2.281461
2024-04-29 19:43:51,778 - trainer - INFO -     val_loss       : 0.995265
2024-04-29 19:43:51,778 - trainer - INFO -     val_accuracy   : 0.706313
2024-04-29 19:43:51,778 - trainer - INFO -     val_macro_f    : 0.691919
2024-04-29 19:43:51,778 - trainer - INFO -     val_precision  : 0.723318
2024-04-29 19:43:51,778 - trainer - INFO -     val_recall     : 0.706313
2024-04-29 19:43:51,778 - trainer - INFO -     val_doc_entropy: 2.680173
2024-04-29 19:43:51,778 - trainer - INFO -     test_loss      : 0.991519
2024-04-29 19:43:51,778 - trainer - INFO -     test_accuracy  : 0.708503
2024-04-29 19:43:51,778 - trainer - INFO -     test_macro_f   : 0.696094
2024-04-29 19:43:51,778 - trainer - INFO -     test_precision : 0.72915
2024-04-29 19:43:51,778 - trainer - INFO -     test_recall    : 0.708503
2024-04-29 19:43:51,778 - trainer - INFO -     test_doc_entropy: 2.684053
2024-04-29 19:59:03,227 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1800, out_features=90, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=90, bias=False)
  (W_q): Linear(in_features=300, out_features=90, bias=False)
  (W_v): Linear(in_features=300, out_features=90, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((90, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,535,445
Freeze params: 0
2024-04-29 20:02:46,420 - trainer - INFO -     epoch          : 1
2024-04-29 20:02:46,420 - trainer - INFO -     loss           : 1.148169
2024-04-29 20:02:46,420 - trainer - INFO -     accuracy       : 0.670187
2024-04-29 20:02:46,420 - trainer - INFO -     macro_f        : 0.651893
2024-04-29 20:02:46,420 - trainer - INFO -     precision      : 0.682933
2024-04-29 20:02:46,420 - trainer - INFO -     recall         : 0.670187
2024-04-29 20:02:46,420 - trainer - INFO -     doc_entropy    : 2.281461
2024-04-29 20:02:46,420 - trainer - INFO -     val_loss       : 0.995265
2024-04-29 20:02:46,436 - trainer - INFO -     val_accuracy   : 0.706313
2024-04-29 20:02:46,436 - trainer - INFO -     val_macro_f    : 0.691919
2024-04-29 20:02:46,436 - trainer - INFO -     val_precision  : 0.723318
2024-04-29 20:02:46,436 - trainer - INFO -     val_recall     : 0.706313
2024-04-29 20:02:46,436 - trainer - INFO -     val_doc_entropy: 2.680173
2024-04-29 20:02:46,436 - trainer - INFO -     test_loss      : 0.991519
2024-04-29 20:02:46,436 - trainer - INFO -     test_accuracy  : 0.708503
2024-04-29 20:02:46,436 - trainer - INFO -     test_macro_f   : 0.696094
2024-04-29 20:02:46,436 - trainer - INFO -     test_precision : 0.72915
2024-04-29 20:02:46,436 - trainer - INFO -     test_recall    : 0.708503
2024-04-29 20:02:46,436 - trainer - INFO -     test_doc_entropy: 2.684053
2024-04-29 20:06:38,650 - trainer - INFO -     epoch          : 2
2024-04-29 20:06:38,650 - trainer - INFO -     loss           : 0.813033
2024-04-29 20:06:38,650 - trainer - INFO -     accuracy       : 0.757045
2024-04-29 20:06:38,650 - trainer - INFO -     macro_f        : 0.748079
2024-04-29 20:06:38,650 - trainer - INFO -     precision      : 0.780622
2024-04-29 20:06:38,650 - trainer - INFO -     recall         : 0.757045
2024-04-29 20:06:38,650 - trainer - INFO -     doc_entropy    : 2.423394
2024-04-29 20:06:38,650 - trainer - INFO -     val_loss       : 1.001051
2024-04-29 20:06:38,650 - trainer - INFO -     val_accuracy   : 0.711988
2024-04-29 20:06:38,650 - trainer - INFO -     val_macro_f    : 0.701585
2024-04-29 20:06:38,650 - trainer - INFO -     val_precision  : 0.735991
2024-04-29 20:06:38,650 - trainer - INFO -     val_recall     : 0.711988
2024-04-29 20:06:38,650 - trainer - INFO -     val_doc_entropy: 2.820782
2024-04-29 20:06:38,650 - trainer - INFO -     test_loss      : 0.991484
2024-04-29 20:06:38,650 - trainer - INFO -     test_accuracy  : 0.712735
2024-04-29 20:06:38,650 - trainer - INFO -     test_macro_f   : 0.703694
2024-04-29 20:06:38,650 - trainer - INFO -     test_precision : 0.741204
2024-04-29 20:06:38,650 - trainer - INFO -     test_recall    : 0.712735
2024-04-29 20:06:38,650 - trainer - INFO -     test_doc_entropy: 2.824046
2024-04-29 20:07:17,132 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1800, out_features=90, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=90, bias=False)
  (W_q): Linear(in_features=300, out_features=90, bias=False)
  (W_v): Linear(in_features=300, out_features=90, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((90, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,535,445
Freeze params: 0
2024-04-29 20:11:12,217 - trainer - INFO -     epoch          : 1
2024-04-29 20:11:12,217 - trainer - INFO -     loss           : 1.145325
2024-04-29 20:11:12,217 - trainer - INFO -     accuracy       : 0.670897
2024-04-29 20:11:12,217 - trainer - INFO -     macro_f        : 0.653015
2024-04-29 20:11:12,217 - trainer - INFO -     precision      : 0.684774
2024-04-29 20:11:12,217 - trainer - INFO -     recall         : 0.670897
2024-04-29 20:11:12,217 - trainer - INFO -     doc_entropy    : 2.320021
2024-04-29 20:11:12,217 - trainer - INFO -     val_loss       : 1.011815
2024-04-29 20:11:12,217 - trainer - INFO -     val_accuracy   : 0.705218
2024-04-29 20:11:12,217 - trainer - INFO -     val_macro_f    : 0.693709
2024-04-29 20:11:12,217 - trainer - INFO -     val_precision  : 0.730262
2024-04-29 20:11:12,217 - trainer - INFO -     val_recall     : 0.705218
2024-04-29 20:11:12,217 - trainer - INFO -     val_doc_entropy: 2.758111
2024-04-29 20:11:12,217 - trainer - INFO -     test_loss      : 1.010719
2024-04-29 20:11:12,217 - trainer - INFO -     test_accuracy  : 0.705417
2024-04-29 20:11:12,217 - trainer - INFO -     test_macro_f   : 0.69369
2024-04-29 20:11:12,217 - trainer - INFO -     test_precision : 0.730073
2024-04-29 20:11:12,217 - trainer - INFO -     test_recall    : 0.705417
2024-04-29 20:11:12,217 - trainer - INFO -     test_doc_entropy: 2.762604
2024-04-29 20:15:09,322 - trainer - INFO -     epoch          : 2
2024-04-29 20:15:09,322 - trainer - INFO -     loss           : 0.81011
2024-04-29 20:15:09,322 - trainer - INFO -     accuracy       : 0.756771
2024-04-29 20:15:09,322 - trainer - INFO -     macro_f        : 0.747742
2024-04-29 20:15:09,322 - trainer - INFO -     precision      : 0.780626
2024-04-29 20:15:09,322 - trainer - INFO -     recall         : 0.756771
2024-04-29 20:15:09,322 - trainer - INFO -     doc_entropy    : 2.456739
2024-04-29 20:15:09,322 - trainer - INFO -     val_loss       : 0.98808
2024-04-29 20:15:09,322 - trainer - INFO -     val_accuracy   : 0.711839
2024-04-29 20:15:09,322 - trainer - INFO -     val_macro_f    : 0.702276
2024-04-29 20:15:09,322 - trainer - INFO -     val_precision  : 0.737999
2024-04-29 20:15:09,322 - trainer - INFO -     val_recall     : 0.711839
2024-04-29 20:15:09,322 - trainer - INFO -     val_doc_entropy: 2.843267
2024-04-29 20:15:09,322 - trainer - INFO -     test_loss      : 0.989562
2024-04-29 20:15:09,322 - trainer - INFO -     test_accuracy  : 0.71174
2024-04-29 20:15:09,322 - trainer - INFO -     test_macro_f   : 0.701865
2024-04-29 20:15:09,322 - trainer - INFO -     test_precision : 0.738114
2024-04-29 20:15:09,322 - trainer - INFO -     test_recall    : 0.71174
2024-04-29 20:15:09,322 - trainer - INFO -     test_doc_entropy: 2.847348
2024-04-29 20:21:01,322 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1400, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1400, out_features=70, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=70, bias=False)
  (W_q): Linear(in_features=300, out_features=70, bias=False)
  (W_v): Linear(in_features=300, out_features=70, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((70, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,329,025
Freeze params: 0
2024-04-29 20:24:42,349 - trainer - INFO -     epoch          : 1
2024-04-29 20:24:42,349 - trainer - INFO -     loss           : 1.145633
2024-04-29 20:24:42,349 - trainer - INFO -     accuracy       : 0.670361
2024-04-29 20:24:42,349 - trainer - INFO -     macro_f        : 0.652829
2024-04-29 20:24:42,349 - trainer - INFO -     precision      : 0.685399
2024-04-29 20:24:42,349 - trainer - INFO -     recall         : 0.670361
2024-04-29 20:24:42,349 - trainer - INFO -     doc_entropy    : 2.263731
2024-04-29 20:24:42,349 - trainer - INFO -     val_loss       : 1.009016
2024-04-29 20:24:42,349 - trainer - INFO -     val_accuracy   : 0.707408
2024-04-29 20:24:42,349 - trainer - INFO -     val_macro_f    : 0.69943
2024-04-29 20:24:42,349 - trainer - INFO -     val_precision  : 0.738017
2024-04-29 20:24:42,349 - trainer - INFO -     val_recall     : 0.707408
2024-04-29 20:24:42,349 - trainer - INFO -     val_doc_entropy: 2.606266
2024-04-29 20:24:42,349 - trainer - INFO -     test_loss      : 1.004248
2024-04-29 20:24:42,349 - trainer - INFO -     test_accuracy  : 0.705915
2024-04-29 20:24:42,349 - trainer - INFO -     test_macro_f   : 0.700534
2024-04-29 20:24:42,349 - trainer - INFO -     test_precision : 0.74305
2024-04-29 20:24:42,349 - trainer - INFO -     test_recall    : 0.705915
2024-04-29 20:24:42,349 - trainer - INFO -     test_doc_entropy: 2.610187
2024-04-29 20:28:32,857 - trainer - INFO -     epoch          : 2
2024-04-29 20:28:32,857 - trainer - INFO -     loss           : 0.811945
2024-04-29 20:28:32,857 - trainer - INFO -     accuracy       : 0.757002
2024-04-29 20:28:32,857 - trainer - INFO -     macro_f        : 0.747916
2024-04-29 20:28:32,857 - trainer - INFO -     precision      : 0.780462
2024-04-29 20:28:32,857 - trainer - INFO -     recall         : 0.757002
2024-04-29 20:28:32,857 - trainer - INFO -     doc_entropy    : 2.442979
2024-04-29 20:28:32,857 - trainer - INFO -     val_loss       : 0.997614
2024-04-29 20:28:32,857 - trainer - INFO -     val_accuracy   : 0.712536
2024-04-29 20:28:32,857 - trainer - INFO -     val_macro_f    : 0.70486
2024-04-29 20:28:32,857 - trainer - INFO -     val_precision  : 0.742126
2024-04-29 20:28:32,857 - trainer - INFO -     val_recall     : 0.712536
2024-04-29 20:28:32,857 - trainer - INFO -     val_doc_entropy: 2.784596
2024-04-29 20:28:32,857 - trainer - INFO -     test_loss      : 0.984936
2024-04-29 20:28:32,857 - trainer - INFO -     test_accuracy  : 0.714926
2024-04-29 20:28:32,857 - trainer - INFO -     test_macro_f   : 0.706338
2024-04-29 20:28:32,857 - trainer - INFO -     test_precision : 0.742588
2024-04-29 20:28:32,857 - trainer - INFO -     test_recall    : 0.714926
2024-04-29 20:28:32,857 - trainer - INFO -     test_doc_entropy: 2.788578
2024-04-29 20:29:11,649 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1400, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1400, out_features=70, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=70, bias=False)
  (W_q): Linear(in_features=300, out_features=70, bias=False)
  (W_v): Linear(in_features=300, out_features=70, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((70, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,329,025
Freeze params: 0
2024-04-29 20:33:03,646 - trainer - INFO -     epoch          : 1
2024-04-29 20:33:03,646 - trainer - INFO -     loss           : 1.145615
2024-04-29 20:33:03,646 - trainer - INFO -     accuracy       : 0.669272
2024-04-29 20:33:03,646 - trainer - INFO -     macro_f        : 0.650247
2024-04-29 20:33:03,646 - trainer - INFO -     precision      : 0.680441
2024-04-29 20:33:03,646 - trainer - INFO -     recall         : 0.669272
2024-04-29 20:33:03,646 - trainer - INFO -     doc_entropy    : 2.321851
2024-04-29 20:33:03,646 - trainer - INFO -     val_loss       : 1.005418
2024-04-29 20:33:03,646 - trainer - INFO -     val_accuracy   : 0.706661
2024-04-29 20:33:03,646 - trainer - INFO -     val_macro_f    : 0.701897
2024-04-29 20:33:03,646 - trainer - INFO -     val_precision  : 0.743193
2024-04-29 20:33:03,646 - trainer - INFO -     val_recall     : 0.706661
2024-04-29 20:33:03,646 - trainer - INFO -     val_doc_entropy: 2.686669
2024-04-29 20:33:03,646 - trainer - INFO -     test_loss      : 1.0002
2024-04-29 20:33:03,646 - trainer - INFO -     test_accuracy  : 0.706512
2024-04-29 20:33:03,646 - trainer - INFO -     test_macro_f   : 0.700446
2024-04-29 20:33:03,646 - trainer - INFO -     test_precision : 0.741455
2024-04-29 20:33:03,646 - trainer - INFO -     test_recall    : 0.706512
2024-04-29 20:33:03,646 - trainer - INFO -     test_doc_entropy: 2.691812
2024-04-29 20:36:58,584 - trainer - INFO -     epoch          : 2
2024-04-29 20:36:58,584 - trainer - INFO -     loss           : 0.816143
2024-04-29 20:36:58,584 - trainer - INFO -     accuracy       : 0.756311
2024-04-29 20:36:58,584 - trainer - INFO -     macro_f        : 0.747323
2024-04-29 20:36:58,584 - trainer - INFO -     precision      : 0.779945
2024-04-29 20:36:58,584 - trainer - INFO -     recall         : 0.756311
2024-04-29 20:36:58,584 - trainer - INFO -     doc_entropy    : 2.439683
2024-04-29 20:36:58,600 - trainer - INFO -     val_loss       : 0.994069
2024-04-29 20:36:58,600 - trainer - INFO -     val_accuracy   : 0.709748
2024-04-29 20:36:58,600 - trainer - INFO -     val_macro_f    : 0.700056
2024-04-29 20:36:58,600 - trainer - INFO -     val_precision  : 0.735287
2024-04-29 20:36:58,600 - trainer - INFO -     val_recall     : 0.709748
2024-04-29 20:36:58,600 - trainer - INFO -     val_doc_entropy: 2.785027
2024-04-29 20:36:58,600 - trainer - INFO -     test_loss      : 0.987918
2024-04-29 20:36:58,600 - trainer - INFO -     test_accuracy  : 0.713283
2024-04-29 20:36:58,600 - trainer - INFO -     test_macro_f   : 0.703003
2024-04-29 20:36:58,600 - trainer - INFO -     test_precision : 0.736979
2024-04-29 20:36:58,600 - trainer - INFO -     test_recall    : 0.713283
2024-04-29 20:36:58,600 - trainer - INFO -     test_doc_entropy: 2.787752
2024-04-29 20:37:35,548 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1400, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1400, out_features=70, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=70, bias=False)
  (W_q): Linear(in_features=300, out_features=70, bias=False)
  (W_v): Linear(in_features=300, out_features=70, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((70, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,329,025
Freeze params: 0
2024-04-29 20:41:30,244 - trainer - INFO -     epoch          : 1
2024-04-29 20:41:30,244 - trainer - INFO -     loss           : 1.146529
2024-04-29 20:41:30,244 - trainer - INFO -     accuracy       : 0.6702
2024-04-29 20:41:30,244 - trainer - INFO -     macro_f        : 0.651923
2024-04-29 20:41:30,244 - trainer - INFO -     precision      : 0.683733
2024-04-29 20:41:30,244 - trainer - INFO -     recall         : 0.6702
2024-04-29 20:41:30,244 - trainer - INFO -     doc_entropy    : 2.329556
2024-04-29 20:41:30,244 - trainer - INFO -     val_loss       : 1.016261
2024-04-29 20:41:30,244 - trainer - INFO -     val_accuracy   : 0.703774
2024-04-29 20:41:30,244 - trainer - INFO -     val_macro_f    : 0.690885
2024-04-29 20:41:30,244 - trainer - INFO -     val_precision  : 0.724254
2024-04-29 20:41:30,244 - trainer - INFO -     val_recall     : 0.703774
2024-04-29 20:41:30,244 - trainer - INFO -     val_doc_entropy: 2.786886
2024-04-29 20:41:30,244 - trainer - INFO -     test_loss      : 1.00591
2024-04-29 20:41:30,244 - trainer - INFO -     test_accuracy  : 0.705118
2024-04-29 20:41:30,244 - trainer - INFO -     test_macro_f   : 0.694188
2024-04-29 20:41:30,244 - trainer - INFO -     test_precision : 0.730087
2024-04-29 20:41:30,244 - trainer - INFO -     test_recall    : 0.705118
2024-04-29 20:41:30,244 - trainer - INFO -     test_doc_entropy: 2.791653
2024-04-29 20:45:27,556 - trainer - INFO -     epoch          : 2
2024-04-29 20:45:27,556 - trainer - INFO -     loss           : 0.81393
2024-04-29 20:45:27,556 - trainer - INFO -     accuracy       : 0.755993
2024-04-29 20:45:27,556 - trainer - INFO -     macro_f        : 0.747374
2024-04-29 20:45:27,556 - trainer - INFO -     precision      : 0.781028
2024-04-29 20:45:27,556 - trainer - INFO -     recall         : 0.755993
2024-04-29 20:45:27,556 - trainer - INFO -     doc_entropy    : 2.462608
2024-04-29 20:45:27,556 - trainer - INFO -     val_loss       : 0.983438
2024-04-29 20:45:27,556 - trainer - INFO -     val_accuracy   : 0.713831
2024-04-29 20:45:27,556 - trainer - INFO -     val_macro_f    : 0.708798
2024-04-29 20:45:27,556 - trainer - INFO -     val_precision  : 0.746799
2024-04-29 20:45:27,556 - trainer - INFO -     val_recall     : 0.713831
2024-04-29 20:45:27,556 - trainer - INFO -     val_doc_entropy: 2.760504
2024-04-29 20:45:27,556 - trainer - INFO -     test_loss      : 0.979634
2024-04-29 20:45:27,556 - trainer - INFO -     test_accuracy  : 0.713482
2024-04-29 20:45:27,556 - trainer - INFO -     test_macro_f   : 0.708736
2024-04-29 20:45:27,556 - trainer - INFO -     test_precision : 0.750125
2024-04-29 20:45:27,556 - trainer - INFO -     test_recall    : 0.713482
2024-04-29 20:45:27,556 - trainer - INFO -     test_doc_entropy: 2.764405
2024-04-29 20:46:07,284 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1400, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1400, out_features=70, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=70, bias=False)
  (W_q): Linear(in_features=300, out_features=70, bias=False)
  (W_v): Linear(in_features=300, out_features=70, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((70, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,329,025
Freeze params: 0
2024-04-29 20:50:02,835 - trainer - INFO -     epoch          : 1
2024-04-29 20:50:02,835 - trainer - INFO -     loss           : 1.149125
2024-04-29 20:50:02,835 - trainer - INFO -     accuracy       : 0.669739
2024-04-29 20:50:02,835 - trainer - INFO -     macro_f        : 0.65133
2024-04-29 20:50:02,835 - trainer - INFO -     precision      : 0.682664
2024-04-29 20:50:02,835 - trainer - INFO -     recall         : 0.669739
2024-04-29 20:50:02,835 - trainer - INFO -     doc_entropy    : 2.358239
2024-04-29 20:50:02,835 - trainer - INFO -     val_loss       : 1.016053
2024-04-29 20:50:02,835 - trainer - INFO -     val_accuracy   : 0.703824
2024-04-29 20:50:02,835 - trainer - INFO -     val_macro_f    : 0.692716
2024-04-29 20:50:02,835 - trainer - INFO -     val_precision  : 0.729153
2024-04-29 20:50:02,835 - trainer - INFO -     val_recall     : 0.703824
2024-04-29 20:50:02,835 - trainer - INFO -     val_doc_entropy: 2.705242
2024-04-29 20:50:02,835 - trainer - INFO -     test_loss      : 1.01217
2024-04-29 20:50:02,835 - trainer - INFO -     test_accuracy  : 0.703027
2024-04-29 20:50:02,835 - trainer - INFO -     test_macro_f   : 0.692999
2024-04-29 20:50:02,835 - trainer - INFO -     test_precision : 0.73047
2024-04-29 20:50:02,835 - trainer - INFO -     test_recall    : 0.703027
2024-04-29 20:50:02,835 - trainer - INFO -     test_doc_entropy: 2.709082
2024-04-29 20:53:55,747 - trainer - INFO -     epoch          : 2
2024-04-29 20:53:55,747 - trainer - INFO -     loss           : 0.811312
2024-04-29 20:53:55,747 - trainer - INFO -     accuracy       : 0.757188
2024-04-29 20:53:55,747 - trainer - INFO -     macro_f        : 0.748001
2024-04-29 20:53:55,747 - trainer - INFO -     precision      : 0.780525
2024-04-29 20:53:55,747 - trainer - INFO -     recall         : 0.757188
2024-04-29 20:53:55,747 - trainer - INFO -     doc_entropy    : 2.478114
2024-04-29 20:53:55,747 - trainer - INFO -     val_loss       : 1.007553
2024-04-29 20:53:55,747 - trainer - INFO -     val_accuracy   : 0.708703
2024-04-29 20:53:55,747 - trainer - INFO -     val_macro_f    : 0.703689
2024-04-29 20:53:55,747 - trainer - INFO -     val_precision  : 0.7452
2024-04-29 20:53:55,747 - trainer - INFO -     val_recall     : 0.708703
2024-04-29 20:53:55,747 - trainer - INFO -     val_doc_entropy: 2.803382
2024-04-29 20:53:55,747 - trainer - INFO -     test_loss      : 1.003674
2024-04-29 20:53:55,747 - trainer - INFO -     test_accuracy  : 0.705516
2024-04-29 20:53:55,747 - trainer - INFO -     test_macro_f   : 0.702569
2024-04-29 20:53:55,747 - trainer - INFO -     test_precision : 0.745807
2024-04-29 20:53:55,747 - trainer - INFO -     test_recall    : 0.705516
2024-04-29 20:53:55,747 - trainer - INFO -     test_doc_entropy: 2.806424
2024-04-29 20:54:34,106 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1400, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1400, out_features=70, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=70, bias=False)
  (W_q): Linear(in_features=300, out_features=70, bias=False)
  (W_v): Linear(in_features=300, out_features=70, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((70, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,329,025
Freeze params: 0
2024-04-29 20:58:26,685 - trainer - INFO -     epoch          : 1
2024-04-29 20:58:26,685 - trainer - INFO -     loss           : 1.147521
2024-04-29 20:58:26,685 - trainer - INFO -     accuracy       : 0.671227
2024-04-29 20:58:26,685 - trainer - INFO -     macro_f        : 0.652601
2024-04-29 20:58:26,685 - trainer - INFO -     precision      : 0.682957
2024-04-29 20:58:26,685 - trainer - INFO -     recall         : 0.671227
2024-04-29 20:58:26,685 - trainer - INFO -     doc_entropy    : 2.317613
2024-04-29 20:58:26,685 - trainer - INFO -     val_loss       : 1.002888
2024-04-29 20:58:26,685 - trainer - INFO -     val_accuracy   : 0.709101
2024-04-29 20:58:26,685 - trainer - INFO -     val_macro_f    : 0.696338
2024-04-29 20:58:26,685 - trainer - INFO -     val_precision  : 0.728829
2024-04-29 20:58:26,685 - trainer - INFO -     val_recall     : 0.709101
2024-04-29 20:58:26,685 - trainer - INFO -     val_doc_entropy: 2.709219
2024-04-29 20:58:26,685 - trainer - INFO -     test_loss      : 0.999852
2024-04-29 20:58:26,685 - trainer - INFO -     test_accuracy  : 0.708055
2024-04-29 20:58:26,685 - trainer - INFO -     test_macro_f   : 0.697527
2024-04-29 20:58:26,685 - trainer - INFO -     test_precision : 0.733733
2024-04-29 20:58:26,685 - trainer - INFO -     test_recall    : 0.708055
2024-04-29 20:58:26,685 - trainer - INFO -     test_doc_entropy: 2.714807
2024-04-29 21:02:24,662 - trainer - INFO -     epoch          : 2
2024-04-29 21:02:24,662 - trainer - INFO -     loss           : 0.813573
2024-04-29 21:02:24,662 - trainer - INFO -     accuracy       : 0.758116
2024-04-29 21:02:24,662 - trainer - INFO -     macro_f        : 0.748922
2024-04-29 21:02:24,662 - trainer - INFO -     precision      : 0.780666
2024-04-29 21:02:24,662 - trainer - INFO -     recall         : 0.758116
2024-04-29 21:02:24,662 - trainer - INFO -     doc_entropy    : 2.493822
2024-04-29 21:02:24,662 - trainer - INFO -     val_loss       : 1.003368
2024-04-29 21:02:24,662 - trainer - INFO -     val_accuracy   : 0.70701
2024-04-29 21:02:24,662 - trainer - INFO -     val_macro_f    : 0.701124
2024-04-29 21:02:24,662 - trainer - INFO -     val_precision  : 0.741813
2024-04-29 21:02:24,662 - trainer - INFO -     val_recall     : 0.70701
2024-04-29 21:02:24,662 - trainer - INFO -     val_doc_entropy: 2.803227
2024-04-29 21:02:24,662 - trainer - INFO -     test_loss      : 0.99163
2024-04-29 21:02:24,662 - trainer - INFO -     test_accuracy  : 0.708703
2024-04-29 21:02:24,662 - trainer - INFO -     test_macro_f   : 0.702414
2024-04-29 21:02:24,662 - trainer - INFO -     test_precision : 0.743008
2024-04-29 21:02:24,662 - trainer - INFO -     test_recall    : 0.708703
2024-04-29 21:02:24,662 - trainer - INFO -     test_doc_entropy: 2.80763
2024-04-29 21:10:26,658 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=50, bias=False)
  (W_q): Linear(in_features=300, out_features=50, bias=False)
  (W_v): Linear(in_features=300, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,138,605
Freeze params: 0
2024-04-29 21:14:04,851 - trainer - INFO -     epoch          : 1
2024-04-29 21:14:04,851 - trainer - INFO -     loss           : 1.147329
2024-04-29 21:14:04,851 - trainer - INFO -     accuracy       : 0.670007
2024-04-29 21:14:04,851 - trainer - INFO -     macro_f        : 0.652282
2024-04-29 21:14:04,851 - trainer - INFO -     precision      : 0.684082
2024-04-29 21:14:04,851 - trainer - INFO -     recall         : 0.670007
2024-04-29 21:14:04,851 - trainer - INFO -     doc_entropy    : 2.176667
2024-04-29 21:14:04,851 - trainer - INFO -     val_loss       : 1.011653
2024-04-29 21:14:04,851 - trainer - INFO -     val_accuracy   : 0.701932
2024-04-29 21:14:04,851 - trainer - INFO -     val_macro_f    : 0.685503
2024-04-29 21:14:04,851 - trainer - INFO -     val_precision  : 0.71742
2024-04-29 21:14:04,851 - trainer - INFO -     val_recall     : 0.701932
2024-04-29 21:14:04,851 - trainer - INFO -     val_doc_entropy: 2.644082
2024-04-29 21:14:04,851 - trainer - INFO -     test_loss      : 1.005821
2024-04-29 21:14:04,851 - trainer - INFO -     test_accuracy  : 0.704321
2024-04-29 21:14:04,851 - trainer - INFO -     test_macro_f   : 0.688726
2024-04-29 21:14:04,851 - trainer - INFO -     test_precision : 0.721738
2024-04-29 21:14:04,851 - trainer - INFO -     test_recall    : 0.704321
2024-04-29 21:14:04,851 - trainer - INFO -     test_doc_entropy: 2.649688
2024-04-29 21:17:54,512 - trainer - INFO -     epoch          : 2
2024-04-29 21:17:54,512 - trainer - INFO -     loss           : 0.814806
2024-04-29 21:17:54,512 - trainer - INFO -     accuracy       : 0.754879
2024-04-29 21:17:54,512 - trainer - INFO -     macro_f        : 0.745846
2024-04-29 21:17:54,512 - trainer - INFO -     precision      : 0.779043
2024-04-29 21:17:54,512 - trainer - INFO -     recall         : 0.754879
2024-04-29 21:17:54,512 - trainer - INFO -     doc_entropy    : 2.250476
2024-04-29 21:17:54,512 - trainer - INFO -     val_loss       : 1.014332
2024-04-29 21:17:54,512 - trainer - INFO -     val_accuracy   : 0.705168
2024-04-29 21:17:54,512 - trainer - INFO -     val_macro_f    : 0.694157
2024-04-29 21:17:54,512 - trainer - INFO -     val_precision  : 0.729446
2024-04-29 21:17:54,512 - trainer - INFO -     val_recall     : 0.705168
2024-04-29 21:17:54,512 - trainer - INFO -     val_doc_entropy: 2.619609
2024-04-29 21:17:54,512 - trainer - INFO -     test_loss      : 1.005316
2024-04-29 21:17:54,512 - trainer - INFO -     test_accuracy  : 0.70935
2024-04-29 21:17:54,512 - trainer - INFO -     test_macro_f   : 0.698367
2024-04-29 21:17:54,512 - trainer - INFO -     test_precision : 0.732476
2024-04-29 21:17:54,512 - trainer - INFO -     test_recall    : 0.70935
2024-04-29 21:17:54,512 - trainer - INFO -     test_doc_entropy: 2.624072
2024-04-29 21:18:32,131 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=50, bias=False)
  (W_q): Linear(in_features=300, out_features=50, bias=False)
  (W_v): Linear(in_features=300, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,138,605
Freeze params: 0
2024-04-29 21:22:23,336 - trainer - INFO -     epoch          : 1
2024-04-29 21:22:23,336 - trainer - INFO -     loss           : 1.148722
2024-04-29 21:22:23,336 - trainer - INFO -     accuracy       : 0.671619
2024-04-29 21:22:23,336 - trainer - INFO -     macro_f        : 0.653232
2024-04-29 21:22:23,336 - trainer - INFO -     precision      : 0.684505
2024-04-29 21:22:23,336 - trainer - INFO -     recall         : 0.671619
2024-04-29 21:22:23,336 - trainer - INFO -     doc_entropy    : 2.308535
2024-04-29 21:22:23,336 - trainer - INFO -     val_loss       : 0.999664
2024-04-29 21:22:23,336 - trainer - INFO -     val_accuracy   : 0.707159
2024-04-29 21:22:23,336 - trainer - INFO -     val_macro_f    : 0.700412
2024-04-29 21:22:23,336 - trainer - INFO -     val_precision  : 0.739777
2024-04-29 21:22:23,336 - trainer - INFO -     val_recall     : 0.707159
2024-04-29 21:22:23,336 - trainer - INFO -     val_doc_entropy: 2.780839
2024-04-29 21:22:23,336 - trainer - INFO -     test_loss      : 1.001731
2024-04-29 21:22:23,336 - trainer - INFO -     test_accuracy  : 0.705616
2024-04-29 21:22:23,336 - trainer - INFO -     test_macro_f   : 0.695267
2024-04-29 21:22:23,336 - trainer - INFO -     test_precision : 0.731146
2024-04-29 21:22:23,336 - trainer - INFO -     test_recall    : 0.705616
2024-04-29 21:22:23,336 - trainer - INFO -     test_doc_entropy: 2.784155
2024-04-29 21:26:18,749 - trainer - INFO -     epoch          : 2
2024-04-29 21:26:18,749 - trainer - INFO -     loss           : 0.815415
2024-04-29 21:26:18,749 - trainer - INFO -     accuracy       : 0.755794
2024-04-29 21:26:18,749 - trainer - INFO -     macro_f        : 0.747322
2024-04-29 21:26:18,749 - trainer - INFO -     precision      : 0.781095
2024-04-29 21:26:18,749 - trainer - INFO -     recall         : 0.755794
2024-04-29 21:26:18,749 - trainer - INFO -     doc_entropy    : 2.453909
2024-04-29 21:26:18,749 - trainer - INFO -     val_loss       : 1.002019
2024-04-29 21:26:18,749 - trainer - INFO -     val_accuracy   : 0.708454
2024-04-29 21:26:18,749 - trainer - INFO -     val_macro_f    : 0.700202
2024-04-29 21:26:18,749 - trainer - INFO -     val_precision  : 0.738061
2024-04-29 21:26:18,749 - trainer - INFO -     val_recall     : 0.708454
2024-04-29 21:26:18,749 - trainer - INFO -     val_doc_entropy: 2.726844
2024-04-29 21:26:18,749 - trainer - INFO -     test_loss      : 0.98243
2024-04-29 21:26:18,749 - trainer - INFO -     test_accuracy  : 0.713831
2024-04-29 21:26:18,749 - trainer - INFO -     test_macro_f   : 0.704636
2024-04-29 21:26:18,749 - trainer - INFO -     test_precision : 0.742359
2024-04-29 21:26:18,749 - trainer - INFO -     test_recall    : 0.713831
2024-04-29 21:26:18,749 - trainer - INFO -     test_doc_entropy: 2.729881
2024-04-29 21:26:55,820 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=50, bias=False)
  (W_q): Linear(in_features=300, out_features=50, bias=False)
  (W_v): Linear(in_features=300, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,138,605
Freeze params: 0
2024-04-29 21:30:50,179 - trainer - INFO -     epoch          : 1
2024-04-29 21:30:50,179 - trainer - INFO -     loss           : 1.148355
2024-04-29 21:30:50,179 - trainer - INFO -     accuracy       : 0.670816
2024-04-29 21:30:50,179 - trainer - INFO -     macro_f        : 0.652489
2024-04-29 21:30:50,179 - trainer - INFO -     precision      : 0.683556
2024-04-29 21:30:50,179 - trainer - INFO -     recall         : 0.670816
2024-04-29 21:30:50,179 - trainer - INFO -     doc_entropy    : 2.27666
2024-04-29 21:30:50,179 - trainer - INFO -     val_loss       : 1.004722
2024-04-29 21:30:50,179 - trainer - INFO -     val_accuracy   : 0.706661
2024-04-29 21:30:50,179 - trainer - INFO -     val_macro_f    : 0.69468
2024-04-29 21:30:50,179 - trainer - INFO -     val_precision  : 0.728271
2024-04-29 21:30:50,179 - trainer - INFO -     val_recall     : 0.706661
2024-04-29 21:30:50,179 - trainer - INFO -     val_doc_entropy: 2.677616
2024-04-29 21:30:50,179 - trainer - INFO -     test_loss      : 1.003207
2024-04-29 21:30:50,179 - trainer - INFO -     test_accuracy  : 0.707358
2024-04-29 21:30:50,179 - trainer - INFO -     test_macro_f   : 0.696273
2024-04-29 21:30:50,179 - trainer - INFO -     test_precision : 0.731431
2024-04-29 21:30:50,179 - trainer - INFO -     test_recall    : 0.707358
2024-04-29 21:30:50,179 - trainer - INFO -     test_doc_entropy: 2.680476
2024-04-29 21:34:47,064 - trainer - INFO -     epoch          : 2
2024-04-29 21:34:47,064 - trainer - INFO -     loss           : 0.808996
2024-04-29 21:34:47,064 - trainer - INFO -     accuracy       : 0.757724
2024-04-29 21:34:47,064 - trainer - INFO -     macro_f        : 0.7489
2024-04-29 21:34:47,064 - trainer - INFO -     precision      : 0.781814
2024-04-29 21:34:47,064 - trainer - INFO -     recall         : 0.757724
2024-04-29 21:34:47,064 - trainer - INFO -     doc_entropy    : 2.416888
2024-04-29 21:34:47,064 - trainer - INFO -     val_loss       : 0.992919
2024-04-29 21:34:47,064 - trainer - INFO -     val_accuracy   : 0.709848
2024-04-29 21:34:47,064 - trainer - INFO -     val_macro_f    : 0.707551
2024-04-29 21:34:47,064 - trainer - INFO -     val_precision  : 0.752827
2024-04-29 21:34:47,064 - trainer - INFO -     val_recall     : 0.709848
2024-04-29 21:34:47,064 - trainer - INFO -     val_doc_entropy: 2.867964
2024-04-29 21:34:47,064 - trainer - INFO -     test_loss      : 0.98011
2024-04-29 21:34:47,064 - trainer - INFO -     test_accuracy  : 0.714229
2024-04-29 21:34:47,064 - trainer - INFO -     test_macro_f   : 0.710196
2024-04-29 21:34:47,064 - trainer - INFO -     test_precision : 0.750595
2024-04-29 21:34:47,064 - trainer - INFO -     test_recall    : 0.714229
2024-04-29 21:34:47,064 - trainer - INFO -     test_doc_entropy: 2.870502
2024-04-29 21:35:25,761 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=50, bias=False)
  (W_q): Linear(in_features=300, out_features=50, bias=False)
  (W_v): Linear(in_features=300, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,138,605
Freeze params: 0
2024-04-29 21:39:20,190 - trainer - INFO -     epoch          : 1
2024-04-29 21:39:20,190 - trainer - INFO -     loss           : 1.144584
2024-04-29 21:39:20,190 - trainer - INFO -     accuracy       : 0.670044
2024-04-29 21:39:20,190 - trainer - INFO -     macro_f        : 0.651646
2024-04-29 21:39:20,190 - trainer - INFO -     precision      : 0.682607
2024-04-29 21:39:20,190 - trainer - INFO -     recall         : 0.670044
2024-04-29 21:39:20,190 - trainer - INFO -     doc_entropy    : 2.277318
2024-04-29 21:39:20,190 - trainer - INFO -     val_loss       : 1.008496
2024-04-29 21:39:20,190 - trainer - INFO -     val_accuracy   : 0.707558
2024-04-29 21:39:20,190 - trainer - INFO -     val_macro_f    : 0.692368
2024-04-29 21:39:20,190 - trainer - INFO -     val_precision  : 0.722654
2024-04-29 21:39:20,190 - trainer - INFO -     val_recall     : 0.707558
2024-04-29 21:39:20,190 - trainer - INFO -     val_doc_entropy: 2.667689
2024-04-29 21:39:20,190 - trainer - INFO -     test_loss      : 1.001048
2024-04-29 21:39:20,190 - trainer - INFO -     test_accuracy  : 0.707757
2024-04-29 21:39:20,190 - trainer - INFO -     test_macro_f   : 0.692158
2024-04-29 21:39:20,190 - trainer - INFO -     test_precision : 0.72194
2024-04-29 21:39:20,190 - trainer - INFO -     test_recall    : 0.707757
2024-04-29 21:39:20,190 - trainer - INFO -     test_doc_entropy: 2.671115
2024-04-29 21:43:14,002 - trainer - INFO -     epoch          : 2
2024-04-29 21:43:14,002 - trainer - INFO -     loss           : 0.810189
2024-04-29 21:43:14,002 - trainer - INFO -     accuracy       : 0.757786
2024-04-29 21:43:14,002 - trainer - INFO -     macro_f        : 0.748553
2024-04-29 21:43:14,002 - trainer - INFO -     precision      : 0.780795
2024-04-29 21:43:14,002 - trainer - INFO -     recall         : 0.757786
2024-04-29 21:43:14,002 - trainer - INFO -     doc_entropy    : 2.386275
2024-04-29 21:43:14,002 - trainer - INFO -     val_loss       : 0.991614
2024-04-29 21:43:14,002 - trainer - INFO -     val_accuracy   : 0.708503
2024-04-29 21:43:14,002 - trainer - INFO -     val_macro_f    : 0.702854
2024-04-29 21:43:14,002 - trainer - INFO -     val_precision  : 0.744094
2024-04-29 21:43:14,002 - trainer - INFO -     val_recall     : 0.708503
2024-04-29 21:43:14,002 - trainer - INFO -     val_doc_entropy: 2.763141
2024-04-29 21:43:14,002 - trainer - INFO -     test_loss      : 0.982264
2024-04-29 21:43:14,002 - trainer - INFO -     test_accuracy  : 0.710943
2024-04-29 21:43:14,002 - trainer - INFO -     test_macro_f   : 0.704648
2024-04-29 21:43:14,002 - trainer - INFO -     test_precision : 0.743621
2024-04-29 21:43:14,002 - trainer - INFO -     test_recall    : 0.710943
2024-04-29 21:43:14,002 - trainer - INFO -     test_doc_entropy: 2.766163
2024-04-29 21:43:51,685 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=50, bias=False)
  (W_q): Linear(in_features=300, out_features=50, bias=False)
  (W_v): Linear(in_features=300, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,138,605
Freeze params: 0
2024-04-29 21:47:38,332 - trainer - INFO -     epoch          : 1
2024-04-29 21:47:38,332 - trainer - INFO -     loss           : 1.147949
2024-04-29 21:47:38,332 - trainer - INFO -     accuracy       : 0.670505
2024-04-29 21:47:38,332 - trainer - INFO -     macro_f        : 0.652399
2024-04-29 21:47:38,332 - trainer - INFO -     precision      : 0.684357
2024-04-29 21:47:38,332 - trainer - INFO -     recall         : 0.670505
2024-04-29 21:47:38,332 - trainer - INFO -     doc_entropy    : 2.354193
2024-04-29 21:47:38,332 - trainer - INFO -     val_loss       : 0.997723
2024-04-29 21:47:38,332 - trainer - INFO -     val_accuracy   : 0.706064
2024-04-29 21:47:38,332 - trainer - INFO -     val_macro_f    : 0.688744
2024-04-29 21:47:38,332 - trainer - INFO -     val_precision  : 0.718747
2024-04-29 21:47:38,332 - trainer - INFO -     val_recall     : 0.706064
2024-04-29 21:47:38,332 - trainer - INFO -     val_doc_entropy: 2.806971
2024-04-29 21:47:38,332 - trainer - INFO -     test_loss      : 0.994526
2024-04-29 21:47:38,332 - trainer - INFO -     test_accuracy  : 0.705765
2024-04-29 21:47:38,332 - trainer - INFO -     test_macro_f   : 0.687339
2024-04-29 21:47:38,332 - trainer - INFO -     test_precision : 0.716831
2024-04-29 21:47:38,332 - trainer - INFO -     test_recall    : 0.705765
2024-04-29 21:47:38,332 - trainer - INFO -     test_doc_entropy: 2.810691
2024-04-29 21:51:29,193 - trainer - INFO -     epoch          : 2
2024-04-29 21:51:29,193 - trainer - INFO -     loss           : 0.811834
2024-04-29 21:51:29,193 - trainer - INFO -     accuracy       : 0.756958
2024-04-29 21:51:29,193 - trainer - INFO -     macro_f        : 0.747765
2024-04-29 21:51:29,193 - trainer - INFO -     precision      : 0.78021
2024-04-29 21:51:29,193 - trainer - INFO -     recall         : 0.756958
2024-04-29 21:51:29,193 - trainer - INFO -     doc_entropy    : 2.45281
2024-04-29 21:51:29,193 - trainer - INFO -     val_loss       : 0.99924
2024-04-29 21:51:29,193 - trainer - INFO -     val_accuracy   : 0.710545
2024-04-29 21:51:29,193 - trainer - INFO -     val_macro_f    : 0.700106
2024-04-29 21:51:29,193 - trainer - INFO -     val_precision  : 0.736152
2024-04-29 21:51:29,193 - trainer - INFO -     val_recall     : 0.710545
2024-04-29 21:51:29,208 - trainer - INFO -     val_doc_entropy: 2.739803
2024-04-29 21:51:29,208 - trainer - INFO -     test_loss      : 0.998203
2024-04-29 21:51:29,208 - trainer - INFO -     test_accuracy  : 0.709748
2024-04-29 21:51:29,208 - trainer - INFO -     test_macro_f   : 0.700237
2024-04-29 21:51:29,208 - trainer - INFO -     test_precision : 0.737662
2024-04-29 21:51:29,208 - trainer - INFO -     test_recall    : 0.709748
2024-04-29 21:51:29,208 - trainer - INFO -     test_doc_entropy: 2.742816
2024-04-29 21:52:20,359 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=600, bias=True)
    (1): Tanh()
    (2): Linear(in_features=600, out_features=30, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=30, bias=False)
  (W_q): Linear(in_features=300, out_features=30, bias=False)
  (W_v): Linear(in_features=300, out_features=30, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((30, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,964,185
Freeze params: 0
2024-04-29 21:56:08,090 - trainer - INFO -     epoch          : 1
2024-04-29 21:56:08,090 - trainer - INFO -     loss           : 1.153703
2024-04-29 21:56:08,090 - trainer - INFO -     accuracy       : 0.66875
2024-04-29 21:56:08,090 - trainer - INFO -     macro_f        : 0.650436
2024-04-29 21:56:08,090 - trainer - INFO -     precision      : 0.681801
2024-04-29 21:56:08,090 - trainer - INFO -     recall         : 0.66875
2024-04-29 21:56:08,090 - trainer - INFO -     doc_entropy    : 2.247655
2024-04-29 21:56:08,090 - trainer - INFO -     val_loss       : 1.001944
2024-04-29 21:56:08,090 - trainer - INFO -     val_accuracy   : 0.706064
2024-04-29 21:56:08,090 - trainer - INFO -     val_macro_f    : 0.694023
2024-04-29 21:56:08,090 - trainer - INFO -     val_precision  : 0.727271
2024-04-29 21:56:08,090 - trainer - INFO -     val_recall     : 0.706064
2024-04-29 21:56:08,090 - trainer - INFO -     val_doc_entropy: 2.721184
2024-04-29 21:56:08,090 - trainer - INFO -     test_loss      : 0.992446
2024-04-29 21:56:08,090 - trainer - INFO -     test_accuracy  : 0.710843
2024-04-29 21:56:08,090 - trainer - INFO -     test_macro_f   : 0.698752
2024-04-29 21:56:08,090 - trainer - INFO -     test_precision : 0.732555
2024-04-29 21:56:08,090 - trainer - INFO -     test_recall    : 0.710843
2024-04-29 21:56:08,090 - trainer - INFO -     test_doc_entropy: 2.726031
2024-04-29 21:59:59,206 - trainer - INFO -     epoch          : 2
2024-04-29 21:59:59,206 - trainer - INFO -     loss           : 0.81208
2024-04-29 21:59:59,206 - trainer - INFO -     accuracy       : 0.755969
2024-04-29 21:59:59,206 - trainer - INFO -     macro_f        : 0.746723
2024-04-29 21:59:59,206 - trainer - INFO -     precision      : 0.77917
2024-04-29 21:59:59,206 - trainer - INFO -     recall         : 0.755969
2024-04-29 21:59:59,206 - trainer - INFO -     doc_entropy    : 2.416154
2024-04-29 21:59:59,206 - trainer - INFO -     val_loss       : 1.017561
2024-04-29 21:59:59,206 - trainer - INFO -     val_accuracy   : 0.70701
2024-04-29 21:59:59,206 - trainer - INFO -     val_macro_f    : 0.702514
2024-04-29 21:59:59,206 - trainer - INFO -     val_precision  : 0.745703
2024-04-29 21:59:59,206 - trainer - INFO -     val_recall     : 0.70701
2024-04-29 21:59:59,206 - trainer - INFO -     val_doc_entropy: 2.738233
2024-04-29 21:59:59,206 - trainer - INFO -     test_loss      : 1.001574
2024-04-29 21:59:59,206 - trainer - INFO -     test_accuracy  : 0.708304
2024-04-29 21:59:59,206 - trainer - INFO -     test_macro_f   : 0.703282
2024-04-29 21:59:59,206 - trainer - INFO -     test_precision : 0.747375
2024-04-29 21:59:59,206 - trainer - INFO -     test_recall    : 0.708304
2024-04-29 21:59:59,206 - trainer - INFO -     test_doc_entropy: 2.740992
2024-04-29 22:00:36,469 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=600, bias=True)
    (1): Tanh()
    (2): Linear(in_features=600, out_features=30, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=30, bias=False)
  (W_q): Linear(in_features=300, out_features=30, bias=False)
  (W_v): Linear(in_features=300, out_features=30, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((30, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,964,185
Freeze params: 0
2024-04-29 22:04:23,734 - trainer - INFO -     epoch          : 1
2024-04-29 22:04:23,734 - trainer - INFO -     loss           : 1.155103
2024-04-29 22:04:23,734 - trainer - INFO -     accuracy       : 0.668333
2024-04-29 22:04:23,734 - trainer - INFO -     macro_f        : 0.650021
2024-04-29 22:04:23,734 - trainer - INFO -     precision      : 0.681312
2024-04-29 22:04:23,734 - trainer - INFO -     recall         : 0.668333
2024-04-29 22:04:23,734 - trainer - INFO -     doc_entropy    : 2.220747
2024-04-29 22:04:23,734 - trainer - INFO -     val_loss       : 1.017896
2024-04-29 22:04:23,734 - trainer - INFO -     val_accuracy   : 0.70457
2024-04-29 22:04:23,734 - trainer - INFO -     val_macro_f    : 0.697735
2024-04-29 22:04:23,734 - trainer - INFO -     val_precision  : 0.738565
2024-04-29 22:04:23,734 - trainer - INFO -     val_recall     : 0.70457
2024-04-29 22:04:23,734 - trainer - INFO -     val_doc_entropy: 2.771048
2024-04-29 22:04:23,734 - trainer - INFO -     test_loss      : 1.020917
2024-04-29 22:04:23,734 - trainer - INFO -     test_accuracy  : 0.699442
2024-04-29 22:04:23,734 - trainer - INFO -     test_macro_f   : 0.692598
2024-04-29 22:04:23,734 - trainer - INFO -     test_precision : 0.732561
2024-04-29 22:04:23,734 - trainer - INFO -     test_recall    : 0.699442
2024-04-29 22:04:23,734 - trainer - INFO -     test_doc_entropy: 2.774895
2024-04-29 22:08:13,825 - trainer - INFO -     epoch          : 2
2024-04-29 22:08:13,825 - trainer - INFO -     loss           : 0.819212
2024-04-29 22:08:13,825 - trainer - INFO -     accuracy       : 0.755159
2024-04-29 22:08:13,825 - trainer - INFO -     macro_f        : 0.746505
2024-04-29 22:08:13,825 - trainer - INFO -     precision      : 0.779433
2024-04-29 22:08:13,825 - trainer - INFO -     recall         : 0.755159
2024-04-29 22:08:13,825 - trainer - INFO -     doc_entropy    : 2.396868
2024-04-29 22:08:13,825 - trainer - INFO -     val_loss       : 0.988813
2024-04-29 22:08:13,825 - trainer - INFO -     val_accuracy   : 0.711441
2024-04-29 22:08:13,825 - trainer - INFO -     val_macro_f    : 0.699387
2024-04-29 22:08:13,825 - trainer - INFO -     val_precision  : 0.732829
2024-04-29 22:08:13,825 - trainer - INFO -     val_recall     : 0.711441
2024-04-29 22:08:13,825 - trainer - INFO -     val_doc_entropy: 2.783139
2024-04-29 22:08:13,825 - trainer - INFO -     test_loss      : 0.985427
2024-04-29 22:08:13,825 - trainer - INFO -     test_accuracy  : 0.71169
2024-04-29 22:08:13,825 - trainer - INFO -     test_macro_f   : 0.701301
2024-04-29 22:08:13,825 - trainer - INFO -     test_precision : 0.735547
2024-04-29 22:08:13,825 - trainer - INFO -     test_recall    : 0.71169
2024-04-29 22:08:13,825 - trainer - INFO -     test_doc_entropy: 2.787126
2024-04-29 22:08:50,600 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=600, bias=True)
    (1): Tanh()
    (2): Linear(in_features=600, out_features=30, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=30, bias=False)
  (W_q): Linear(in_features=300, out_features=30, bias=False)
  (W_v): Linear(in_features=300, out_features=30, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((30, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,964,185
Freeze params: 0
2024-04-29 22:12:40,904 - trainer - INFO -     epoch          : 1
2024-04-29 22:12:40,904 - trainer - INFO -     loss           : 1.155704
2024-04-29 22:12:40,904 - trainer - INFO -     accuracy       : 0.668096
2024-04-29 22:12:40,904 - trainer - INFO -     macro_f        : 0.649168
2024-04-29 22:12:40,904 - trainer - INFO -     precision      : 0.680088
2024-04-29 22:12:40,904 - trainer - INFO -     recall         : 0.668096
2024-04-29 22:12:40,904 - trainer - INFO -     doc_entropy    : 2.296481
2024-04-29 22:12:40,904 - trainer - INFO -     val_loss       : 1.011031
2024-04-29 22:12:40,904 - trainer - INFO -     val_accuracy   : 0.705068
2024-04-29 22:12:40,904 - trainer - INFO -     val_macro_f    : 0.699362
2024-04-29 22:12:40,904 - trainer - INFO -     val_precision  : 0.740898
2024-04-29 22:12:40,904 - trainer - INFO -     val_recall     : 0.705068
2024-04-29 22:12:40,904 - trainer - INFO -     val_doc_entropy: 2.69447
2024-04-29 22:12:40,904 - trainer - INFO -     test_loss      : 1.006901
2024-04-29 22:12:40,904 - trainer - INFO -     test_accuracy  : 0.708255
2024-04-29 22:12:40,904 - trainer - INFO -     test_macro_f   : 0.702113
2024-04-29 22:12:40,904 - trainer - INFO -     test_precision : 0.742295
2024-04-29 22:12:40,904 - trainer - INFO -     test_recall    : 0.708255
2024-04-29 22:12:40,904 - trainer - INFO -     test_doc_entropy: 2.699055
2024-04-29 22:16:37,394 - trainer - INFO -     epoch          : 2
2024-04-29 22:16:37,394 - trainer - INFO -     loss           : 0.814056
2024-04-29 22:16:37,394 - trainer - INFO -     accuracy       : 0.755701
2024-04-29 22:16:37,394 - trainer - INFO -     macro_f        : 0.746801
2024-04-29 22:16:37,394 - trainer - INFO -     precision      : 0.779592
2024-04-29 22:16:37,394 - trainer - INFO -     recall         : 0.755701
2024-04-29 22:16:37,394 - trainer - INFO -     doc_entropy    : 2.450849
2024-04-29 22:16:37,394 - trainer - INFO -     val_loss       : 1.017521
2024-04-29 22:16:37,394 - trainer - INFO -     val_accuracy   : 0.704819
2024-04-29 22:16:37,394 - trainer - INFO -     val_macro_f    : 0.697873
2024-04-29 22:16:37,394 - trainer - INFO -     val_precision  : 0.738739
2024-04-29 22:16:37,394 - trainer - INFO -     val_recall     : 0.704819
2024-04-29 22:16:37,394 - trainer - INFO -     val_doc_entropy: 2.72197
2024-04-29 22:16:37,394 - trainer - INFO -     test_loss      : 1.00518
2024-04-29 22:16:37,394 - trainer - INFO -     test_accuracy  : 0.707508
2024-04-29 22:16:37,394 - trainer - INFO -     test_macro_f   : 0.699375
2024-04-29 22:16:37,410 - trainer - INFO -     test_precision : 0.736751
2024-04-29 22:16:37,410 - trainer - INFO -     test_recall    : 0.707508
2024-04-29 22:16:37,410 - trainer - INFO -     test_doc_entropy: 2.725389
2024-04-29 22:17:15,052 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=600, bias=True)
    (1): Tanh()
    (2): Linear(in_features=600, out_features=30, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=30, bias=False)
  (W_q): Linear(in_features=300, out_features=30, bias=False)
  (W_v): Linear(in_features=300, out_features=30, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((30, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,964,185
Freeze params: 0
2024-04-29 22:21:06,770 - trainer - INFO -     epoch          : 1
2024-04-29 22:21:06,770 - trainer - INFO -     loss           : 1.156335
2024-04-29 22:21:06,770 - trainer - INFO -     accuracy       : 0.668955
2024-04-29 22:21:06,770 - trainer - INFO -     macro_f        : 0.650945
2024-04-29 22:21:06,770 - trainer - INFO -     precision      : 0.682891
2024-04-29 22:21:06,770 - trainer - INFO -     recall         : 0.668955
2024-04-29 22:21:06,770 - trainer - INFO -     doc_entropy    : 2.325717
2024-04-29 22:21:06,770 - trainer - INFO -     val_loss       : 1.002992
2024-04-29 22:21:06,770 - trainer - INFO -     val_accuracy   : 0.705118
2024-04-29 22:21:06,770 - trainer - INFO -     val_macro_f    : 0.690107
2024-04-29 22:21:06,770 - trainer - INFO -     val_precision  : 0.719542
2024-04-29 22:21:06,770 - trainer - INFO -     val_recall     : 0.705118
2024-04-29 22:21:06,785 - trainer - INFO -     val_doc_entropy: 2.815492
2024-04-29 22:21:06,785 - trainer - INFO -     test_loss      : 1.001788
2024-04-29 22:21:06,785 - trainer - INFO -     test_accuracy  : 0.705715
2024-04-29 22:21:06,785 - trainer - INFO -     test_macro_f   : 0.693347
2024-04-29 22:21:06,785 - trainer - INFO -     test_precision : 0.726234
2024-04-29 22:21:06,785 - trainer - INFO -     test_recall    : 0.705715
2024-04-29 22:21:06,785 - trainer - INFO -     test_doc_entropy: 2.818827
2024-04-29 22:24:58,734 - trainer - INFO -     epoch          : 2
2024-04-29 22:24:58,734 - trainer - INFO -     loss           : 0.81689
2024-04-29 22:24:58,734 - trainer - INFO -     accuracy       : 0.755085
2024-04-29 22:24:58,750 - trainer - INFO -     macro_f        : 0.745642
2024-04-29 22:24:58,750 - trainer - INFO -     precision      : 0.778095
2024-04-29 22:24:58,750 - trainer - INFO -     recall         : 0.755085
2024-04-29 22:24:58,750 - trainer - INFO -     doc_entropy    : 2.495776
2024-04-29 22:24:58,750 - trainer - INFO -     val_loss       : 0.983033
2024-04-29 22:24:58,750 - trainer - INFO -     val_accuracy   : 0.715971
2024-04-29 22:24:58,750 - trainer - INFO -     val_macro_f    : 0.706383
2024-04-29 22:24:58,750 - trainer - INFO -     val_precision  : 0.742209
2024-04-29 22:24:58,750 - trainer - INFO -     val_recall     : 0.715971
2024-04-29 22:24:58,750 - trainer - INFO -     val_doc_entropy: 2.837004
2024-04-29 22:24:58,750 - trainer - INFO -     test_loss      : 0.978477
2024-04-29 22:24:58,750 - trainer - INFO -     test_accuracy  : 0.714328
2024-04-29 22:24:58,750 - trainer - INFO -     test_macro_f   : 0.706035
2024-04-29 22:24:58,750 - trainer - INFO -     test_precision : 0.744263
2024-04-29 22:24:58,750 - trainer - INFO -     test_recall    : 0.714328
2024-04-29 22:24:58,750 - trainer - INFO -     test_doc_entropy: 2.840656
2024-04-29 22:25:36,184 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=600, bias=True)
    (1): Tanh()
    (2): Linear(in_features=600, out_features=30, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=30, bias=False)
  (W_q): Linear(in_features=300, out_features=30, bias=False)
  (W_v): Linear(in_features=300, out_features=30, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((30, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,964,185
Freeze params: 0
2024-04-29 22:29:23,832 - trainer - INFO -     epoch          : 1
2024-04-29 22:29:23,832 - trainer - INFO -     loss           : 1.148133
2024-04-29 22:29:23,832 - trainer - INFO -     accuracy       : 0.670368
2024-04-29 22:29:23,832 - trainer - INFO -     macro_f        : 0.651535
2024-04-29 22:29:23,832 - trainer - INFO -     precision      : 0.682483
2024-04-29 22:29:23,832 - trainer - INFO -     recall         : 0.670368
2024-04-29 22:29:23,832 - trainer - INFO -     doc_entropy    : 2.220492
2024-04-29 22:29:23,832 - trainer - INFO -     val_loss       : 1.000251
2024-04-29 22:29:23,832 - trainer - INFO -     val_accuracy   : 0.706562
2024-04-29 22:29:23,832 - trainer - INFO -     val_macro_f    : 0.69536
2024-04-29 22:29:23,832 - trainer - INFO -     val_precision  : 0.730779
2024-04-29 22:29:23,832 - trainer - INFO -     val_recall     : 0.706562
2024-04-29 22:29:23,832 - trainer - INFO -     val_doc_entropy: 2.638573
2024-04-29 22:29:23,832 - trainer - INFO -     test_loss      : 1.002475
2024-04-29 22:29:23,832 - trainer - INFO -     test_accuracy  : 0.706412
2024-04-29 22:29:23,832 - trainer - INFO -     test_macro_f   : 0.695473
2024-04-29 22:29:23,832 - trainer - INFO -     test_precision : 0.731314
2024-04-29 22:29:23,832 - trainer - INFO -     test_recall    : 0.706412
2024-04-29 22:29:23,832 - trainer - INFO -     test_doc_entropy: 2.641838
2024-04-29 22:33:20,416 - trainer - INFO -     epoch          : 2
2024-04-29 22:33:20,432 - trainer - INFO -     loss           : 0.813855
2024-04-29 22:33:20,432 - trainer - INFO -     accuracy       : 0.75661
2024-04-29 22:33:20,432 - trainer - INFO -     macro_f        : 0.747732
2024-04-29 22:33:20,432 - trainer - INFO -     precision      : 0.780689
2024-04-29 22:33:20,432 - trainer - INFO -     recall         : 0.75661
2024-04-29 22:33:20,432 - trainer - INFO -     doc_entropy    : 2.343537
2024-04-29 22:33:20,432 - trainer - INFO -     val_loss       : 1.003208
2024-04-29 22:33:20,432 - trainer - INFO -     val_accuracy   : 0.712138
2024-04-29 22:33:20,432 - trainer - INFO -     val_macro_f    : 0.700945
2024-04-29 22:33:20,432 - trainer - INFO -     val_precision  : 0.735783
2024-04-29 22:33:20,432 - trainer - INFO -     val_recall     : 0.712138
2024-04-29 22:33:20,432 - trainer - INFO -     val_doc_entropy: 2.719796
2024-04-29 22:33:20,432 - trainer - INFO -     test_loss      : 0.997137
2024-04-29 22:33:20,432 - trainer - INFO -     test_accuracy  : 0.710346
2024-04-29 22:33:20,432 - trainer - INFO -     test_macro_f   : 0.699773
2024-04-29 22:33:20,432 - trainer - INFO -     test_precision : 0.734534
2024-04-29 22:33:20,432 - trainer - INFO -     test_recall    : 0.710346
2024-04-29 22:33:20,432 - trainer - INFO -     test_doc_entropy: 2.723463
2024-04-29 22:35:52,116 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=200, out_features=10, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=10, bias=False)
  (W_q): Linear(in_features=300, out_features=10, bias=False)
  (W_v): Linear(in_features=300, out_features=10, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((10, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,805,765
Freeze params: 0
2024-04-29 22:39:35,755 - trainer - INFO -     epoch          : 1
2024-04-29 22:39:35,755 - trainer - INFO -     loss           : 1.166238
2024-04-29 22:39:35,755 - trainer - INFO -     accuracy       : 0.666192
2024-04-29 22:39:35,755 - trainer - INFO -     macro_f        : 0.647309
2024-04-29 22:39:35,755 - trainer - INFO -     precision      : 0.678719
2024-04-29 22:39:35,755 - trainer - INFO -     recall         : 0.666192
2024-04-29 22:39:35,755 - trainer - INFO -     doc_entropy    : 2.482877
2024-04-29 22:39:35,755 - trainer - INFO -     val_loss       : 0.989896
2024-04-29 22:39:35,770 - trainer - INFO -     val_accuracy   : 0.708653
2024-04-29 22:39:35,770 - trainer - INFO -     val_macro_f    : 0.697515
2024-04-29 22:39:35,770 - trainer - INFO -     val_precision  : 0.731068
2024-04-29 22:39:35,770 - trainer - INFO -     val_recall     : 0.708653
2024-04-29 22:39:35,770 - trainer - INFO -     val_doc_entropy: 2.957077
2024-04-29 22:39:35,770 - trainer - INFO -     test_loss      : 0.995613
2024-04-29 22:39:35,770 - trainer - INFO -     test_accuracy  : 0.703873
2024-04-29 22:39:35,770 - trainer - INFO -     test_macro_f   : 0.691627
2024-04-29 22:39:35,770 - trainer - INFO -     test_precision : 0.724442
2024-04-29 22:39:35,770 - trainer - INFO -     test_recall    : 0.703873
2024-04-29 22:39:35,770 - trainer - INFO -     test_doc_entropy: 2.960294
2024-04-29 22:43:29,446 - trainer - INFO -     epoch          : 2
2024-04-29 22:43:29,446 - trainer - INFO -     loss           : 0.820075
2024-04-29 22:43:29,446 - trainer - INFO -     accuracy       : 0.754662
2024-04-29 22:43:29,446 - trainer - INFO -     macro_f        : 0.745414
2024-04-29 22:43:29,446 - trainer - INFO -     precision      : 0.777887
2024-04-29 22:43:29,446 - trainer - INFO -     recall         : 0.754662
2024-04-29 22:43:29,446 - trainer - INFO -     doc_entropy    : 2.643691
2024-04-29 22:43:29,446 - trainer - INFO -     val_loss       : 0.98458
2024-04-29 22:43:29,446 - trainer - INFO -     val_accuracy   : 0.713233
2024-04-29 22:43:29,446 - trainer - INFO -     val_macro_f    : 0.705747
2024-04-29 22:43:29,446 - trainer - INFO -     val_precision  : 0.743987
2024-04-29 22:43:29,446 - trainer - INFO -     val_recall     : 0.713233
2024-04-29 22:43:29,446 - trainer - INFO -     val_doc_entropy: 2.895412
2024-04-29 22:43:29,446 - trainer - INFO -     test_loss      : 0.976639
2024-04-29 22:43:29,446 - trainer - INFO -     test_accuracy  : 0.716519
2024-04-29 22:43:29,446 - trainer - INFO -     test_macro_f   : 0.709073
2024-04-29 22:43:29,446 - trainer - INFO -     test_precision : 0.746749
2024-04-29 22:43:29,446 - trainer - INFO -     test_recall    : 0.716519
2024-04-29 22:43:29,446 - trainer - INFO -     test_doc_entropy: 2.897103
2024-04-29 22:44:07,954 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=200, out_features=10, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=10, bias=False)
  (W_q): Linear(in_features=300, out_features=10, bias=False)
  (W_v): Linear(in_features=300, out_features=10, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((10, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,805,765
Freeze params: 0
2024-04-29 22:48:00,731 - trainer - INFO -     epoch          : 1
2024-04-29 22:48:00,731 - trainer - INFO -     loss           : 1.162985
2024-04-29 22:48:00,731 - trainer - INFO -     accuracy       : 0.666297
2024-04-29 22:48:00,731 - trainer - INFO -     macro_f        : 0.647607
2024-04-29 22:48:00,731 - trainer - INFO -     precision      : 0.678966
2024-04-29 22:48:00,731 - trainer - INFO -     recall         : 0.666297
2024-04-29 22:48:00,731 - trainer - INFO -     doc_entropy    : 2.418231
2024-04-29 22:48:00,731 - trainer - INFO -     val_loss       : 0.993251
2024-04-29 22:48:00,731 - trainer - INFO -     val_accuracy   : 0.708653
2024-04-29 22:48:00,731 - trainer - INFO -     val_macro_f    : 0.697968
2024-04-29 22:48:00,731 - trainer - INFO -     val_precision  : 0.733034
2024-04-29 22:48:00,731 - trainer - INFO -     val_recall     : 0.708653
2024-04-29 22:48:00,731 - trainer - INFO -     val_doc_entropy: 2.839179
2024-04-29 22:48:00,731 - trainer - INFO -     test_loss      : 0.991848
2024-04-29 22:48:00,731 - trainer - INFO -     test_accuracy  : 0.708703
2024-04-29 22:48:00,731 - trainer - INFO -     test_macro_f   : 0.698386
2024-04-29 22:48:00,731 - trainer - INFO -     test_precision : 0.733491
2024-04-29 22:48:00,731 - trainer - INFO -     test_recall    : 0.708703
2024-04-29 22:48:00,731 - trainer - INFO -     test_doc_entropy: 2.844995
2024-04-29 22:51:57,508 - trainer - INFO -     epoch          : 2
2024-04-29 22:51:57,508 - trainer - INFO -     loss           : 0.820807
2024-04-29 22:51:57,508 - trainer - INFO -     accuracy       : 0.754269
2024-04-29 22:51:57,508 - trainer - INFO -     macro_f        : 0.744868
2024-04-29 22:51:57,508 - trainer - INFO -     precision      : 0.777383
2024-04-29 22:51:57,508 - trainer - INFO -     recall         : 0.754269
2024-04-29 22:51:57,508 - trainer - INFO -     doc_entropy    : 2.578918
2024-04-29 22:51:57,508 - trainer - INFO -     val_loss       : 1.003116
2024-04-29 22:51:57,508 - trainer - INFO -     val_accuracy   : 0.707856
2024-04-29 22:51:57,524 - trainer - INFO -     val_macro_f    : 0.695471
2024-04-29 22:51:57,524 - trainer - INFO -     val_precision  : 0.730184
2024-04-29 22:51:57,524 - trainer - INFO -     val_recall     : 0.707856
2024-04-29 22:51:57,524 - trainer - INFO -     val_doc_entropy: 2.899671
2024-04-29 22:51:57,524 - trainer - INFO -     test_loss      : 0.989243
2024-04-29 22:51:57,524 - trainer - INFO -     test_accuracy  : 0.709051
2024-04-29 22:51:57,524 - trainer - INFO -     test_macro_f   : 0.695812
2024-04-29 22:51:57,524 - trainer - INFO -     test_precision : 0.730009
2024-04-29 22:51:57,524 - trainer - INFO -     test_recall    : 0.709051
2024-04-29 22:51:57,524 - trainer - INFO -     test_doc_entropy: 2.90333
2024-04-29 22:52:34,193 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=200, out_features=10, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=10, bias=False)
  (W_q): Linear(in_features=300, out_features=10, bias=False)
  (W_v): Linear(in_features=300, out_features=10, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((10, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,805,765
Freeze params: 0
2024-04-29 22:56:27,673 - trainer - INFO -     epoch          : 1
2024-04-29 22:56:27,673 - trainer - INFO -     loss           : 1.159686
2024-04-29 22:56:27,673 - trainer - INFO -     accuracy       : 0.66664
2024-04-29 22:56:27,673 - trainer - INFO -     macro_f        : 0.647759
2024-04-29 22:56:27,673 - trainer - INFO -     precision      : 0.679185
2024-04-29 22:56:27,673 - trainer - INFO -     recall         : 0.66664
2024-04-29 22:56:27,673 - trainer - INFO -     doc_entropy    : 2.507775
2024-04-29 22:56:27,673 - trainer - INFO -     val_loss       : 1.003652
2024-04-29 22:56:27,673 - trainer - INFO -     val_accuracy   : 0.705466
2024-04-29 22:56:27,673 - trainer - INFO -     val_macro_f    : 0.688867
2024-04-29 22:56:27,673 - trainer - INFO -     val_precision  : 0.718931
2024-04-29 22:56:27,673 - trainer - INFO -     val_recall     : 0.705466
2024-04-29 22:56:27,673 - trainer - INFO -     val_doc_entropy: 2.91343
2024-04-29 22:56:27,673 - trainer - INFO -     test_loss      : 1.002447
2024-04-29 22:56:27,673 - trainer - INFO -     test_accuracy  : 0.701135
2024-04-29 22:56:27,673 - trainer - INFO -     test_macro_f   : 0.684226
2024-04-29 22:56:27,673 - trainer - INFO -     test_precision : 0.714771
2024-04-29 22:56:27,673 - trainer - INFO -     test_recall    : 0.701135
2024-04-29 22:56:27,673 - trainer - INFO -     test_doc_entropy: 2.917389
2024-04-29 23:00:24,277 - trainer - INFO -     epoch          : 2
2024-04-29 23:00:24,277 - trainer - INFO -     loss           : 0.817012
2024-04-29 23:00:24,277 - trainer - INFO -     accuracy       : 0.755489
2024-04-29 23:00:24,277 - trainer - INFO -     macro_f        : 0.746374
2024-04-29 23:00:24,277 - trainer - INFO -     precision      : 0.779352
2024-04-29 23:00:24,277 - trainer - INFO -     recall         : 0.755489
2024-04-29 23:00:24,277 - trainer - INFO -     doc_entropy    : 2.682264
2024-04-29 23:00:24,277 - trainer - INFO -     val_loss       : 0.988948
2024-04-29 23:00:24,277 - trainer - INFO -     val_accuracy   : 0.710196
2024-04-29 23:00:24,277 - trainer - INFO -     val_macro_f    : 0.703436
2024-04-29 23:00:24,277 - trainer - INFO -     val_precision  : 0.742817
2024-04-29 23:00:24,277 - trainer - INFO -     val_recall     : 0.710196
2024-04-29 23:00:24,277 - trainer - INFO -     val_doc_entropy: 2.962407
2024-04-29 23:00:24,277 - trainer - INFO -     test_loss      : 0.981071
2024-04-29 23:00:24,277 - trainer - INFO -     test_accuracy  : 0.712835
2024-04-29 23:00:24,277 - trainer - INFO -     test_macro_f   : 0.704256
2024-04-29 23:00:24,277 - trainer - INFO -     test_precision : 0.741049
2024-04-29 23:00:24,277 - trainer - INFO -     test_recall    : 0.712835
2024-04-29 23:00:24,277 - trainer - INFO -     test_doc_entropy: 2.965866
2024-04-29 23:01:03,441 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=200, out_features=10, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=10, bias=False)
  (W_q): Linear(in_features=300, out_features=10, bias=False)
  (W_v): Linear(in_features=300, out_features=10, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((10, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,805,765
Freeze params: 0
2024-04-29 23:04:57,655 - trainer - INFO -     epoch          : 1
2024-04-29 23:04:57,655 - trainer - INFO -     loss           : 1.160365
2024-04-29 23:04:57,655 - trainer - INFO -     accuracy       : 0.666758
2024-04-29 23:04:57,655 - trainer - INFO -     macro_f        : 0.647866
2024-04-29 23:04:57,655 - trainer - INFO -     precision      : 0.678932
2024-04-29 23:04:57,655 - trainer - INFO -     recall         : 0.666758
2024-04-29 23:04:57,655 - trainer - INFO -     doc_entropy    : 2.470356
2024-04-29 23:04:57,655 - trainer - INFO -     val_loss       : 1.006923
2024-04-29 23:04:57,655 - trainer - INFO -     val_accuracy   : 0.705018
2024-04-29 23:04:57,655 - trainer - INFO -     val_macro_f    : 0.693517
2024-04-29 23:04:57,655 - trainer - INFO -     val_precision  : 0.729906
2024-04-29 23:04:57,655 - trainer - INFO -     val_recall     : 0.705018
2024-04-29 23:04:57,655 - trainer - INFO -     val_doc_entropy: 2.936063
2024-04-29 23:04:57,655 - trainer - INFO -     test_loss      : 1.007992
2024-04-29 23:04:57,655 - trainer - INFO -     test_accuracy  : 0.702927
2024-04-29 23:04:57,655 - trainer - INFO -     test_macro_f   : 0.690819
2024-04-29 23:04:57,655 - trainer - INFO -     test_precision : 0.72616
2024-04-29 23:04:57,655 - trainer - INFO -     test_recall    : 0.702927
2024-04-29 23:04:57,655 - trainer - INFO -     test_doc_entropy: 2.940259
2024-04-29 23:08:51,513 - trainer - INFO -     epoch          : 2
2024-04-29 23:08:51,513 - trainer - INFO -     loss           : 0.819298
2024-04-29 23:08:51,513 - trainer - INFO -     accuracy       : 0.753485
2024-04-29 23:08:51,513 - trainer - INFO -     macro_f        : 0.743875
2024-04-29 23:08:51,513 - trainer - INFO -     precision      : 0.776403
2024-04-29 23:08:51,513 - trainer - INFO -     recall         : 0.753485
2024-04-29 23:08:51,513 - trainer - INFO -     doc_entropy    : 2.604177
2024-04-29 23:08:51,513 - trainer - INFO -     val_loss       : 0.989199
2024-04-29 23:08:51,513 - trainer - INFO -     val_accuracy   : 0.712387
2024-04-29 23:08:51,513 - trainer - INFO -     val_macro_f    : 0.703097
2024-04-29 23:08:51,513 - trainer - INFO -     val_precision  : 0.739383
2024-04-29 23:08:51,513 - trainer - INFO -     val_recall     : 0.712387
2024-04-29 23:08:51,513 - trainer - INFO -     val_doc_entropy: 2.845323
2024-04-29 23:08:51,513 - trainer - INFO -     test_loss      : 0.981396
2024-04-29 23:08:51,513 - trainer - INFO -     test_accuracy  : 0.709101
2024-04-29 23:08:51,528 - trainer - INFO -     test_macro_f   : 0.700411
2024-04-29 23:08:51,528 - trainer - INFO -     test_precision : 0.737872
2024-04-29 23:08:51,528 - trainer - INFO -     test_recall    : 0.709101
2024-04-29 23:08:51,528 - trainer - INFO -     test_doc_entropy: 2.848914
2024-04-29 23:09:29,605 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=200, out_features=10, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=10, bias=False)
  (W_q): Linear(in_features=300, out_features=10, bias=False)
  (W_v): Linear(in_features=300, out_features=10, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((10, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 26,805,765
Freeze params: 0
2024-04-29 23:13:18,047 - trainer - INFO -     epoch          : 1
2024-04-29 23:13:18,047 - trainer - INFO -     loss           : 1.162864
2024-04-29 23:13:18,047 - trainer - INFO -     accuracy       : 0.666017
2024-04-29 23:13:18,047 - trainer - INFO -     macro_f        : 0.647302
2024-04-29 23:13:18,047 - trainer - INFO -     precision      : 0.678377
2024-04-29 23:13:18,047 - trainer - INFO -     recall         : 0.666017
2024-04-29 23:13:18,047 - trainer - INFO -     doc_entropy    : 2.43324
2024-04-29 23:13:18,047 - trainer - INFO -     val_loss       : 0.99951
2024-04-29 23:13:18,047 - trainer - INFO -     val_accuracy   : 0.709151
2024-04-29 23:13:18,047 - trainer - INFO -     val_macro_f    : 0.698542
2024-04-29 23:13:18,047 - trainer - INFO -     val_precision  : 0.733146
2024-04-29 23:13:18,047 - trainer - INFO -     val_recall     : 0.709151
2024-04-29 23:13:18,047 - trainer - INFO -     val_doc_entropy: 2.830243
2024-04-29 23:13:18,047 - trainer - INFO -     test_loss      : 0.993774
2024-04-29 23:13:18,047 - trainer - INFO -     test_accuracy  : 0.708802
2024-04-29 23:13:18,047 - trainer - INFO -     test_macro_f   : 0.69862
2024-04-29 23:13:18,047 - trainer - INFO -     test_precision : 0.733824
2024-04-29 23:13:18,047 - trainer - INFO -     test_recall    : 0.708802
2024-04-29 23:13:18,047 - trainer - INFO -     test_doc_entropy: 2.833871
2024-04-29 23:17:16,744 - trainer - INFO -     epoch          : 2
2024-04-29 23:17:16,744 - trainer - INFO -     loss           : 0.822398
2024-04-29 23:17:16,744 - trainer - INFO -     accuracy       : 0.753554
2024-04-29 23:17:16,744 - trainer - INFO -     macro_f        : 0.744507
2024-04-29 23:17:16,744 - trainer - INFO -     precision      : 0.777507
2024-04-29 23:17:16,744 - trainer - INFO -     recall         : 0.753554
2024-04-29 23:17:16,744 - trainer - INFO -     doc_entropy    : 2.587991
2024-04-29 23:17:16,744 - trainer - INFO -     val_loss       : 1.014067
2024-04-29 23:17:16,744 - trainer - INFO -     val_accuracy   : 0.704769
2024-04-29 23:17:16,744 - trainer - INFO -     val_macro_f    : 0.69621
2024-04-29 23:17:16,744 - trainer - INFO -     val_precision  : 0.733231
2024-04-29 23:17:16,744 - trainer - INFO -     val_recall     : 0.704769
2024-04-29 23:17:16,744 - trainer - INFO -     val_doc_entropy: 2.848858
2024-04-29 23:17:16,744 - trainer - INFO -     test_loss      : 1.003289
2024-04-29 23:17:16,744 - trainer - INFO -     test_accuracy  : 0.706313
2024-04-29 23:17:16,744 - trainer - INFO -     test_macro_f   : 0.69883
2024-04-29 23:17:16,744 - trainer - INFO -     test_precision : 0.739913
2024-04-29 23:17:16,744 - trainer - INFO -     test_recall    : 0.706313
2024-04-29 23:17:16,744 - trainer - INFO -     test_doc_entropy: 2.854178
2024-04-30 04:43:08,826 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,061,875
Freeze params: 0
2024-04-30 04:43:51,615 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,049,395
Freeze params: 0
2024-04-30 04:47:41,744 - trainer - INFO -     epoch          : 1
2024-04-30 04:47:41,744 - trainer - INFO -     loss           : 1.155993
2024-04-30 04:47:41,744 - trainer - INFO -     accuracy       : 0.668256
2024-04-30 04:47:41,744 - trainer - INFO -     macro_f        : 0.649765
2024-04-30 04:47:41,744 - trainer - INFO -     precision      : 0.680729
2024-04-30 04:47:41,744 - trainer - INFO -     recall         : 0.668256
2024-04-30 04:47:41,744 - trainer - INFO -     doc_entropy    : 2.310648
2024-04-30 04:47:41,744 - trainer - INFO -     val_loss       : 1.006433
2024-04-30 04:47:41,744 - trainer - INFO -     val_accuracy   : 0.705616
2024-04-30 04:47:41,744 - trainer - INFO -     val_macro_f    : 0.693296
2024-04-30 04:47:41,744 - trainer - INFO -     val_precision  : 0.725663
2024-04-30 04:47:41,744 - trainer - INFO -     val_recall     : 0.705616
2024-04-30 04:47:41,744 - trainer - INFO -     val_doc_entropy: 2.884072
2024-04-30 04:47:41,744 - trainer - INFO -     test_loss      : 1.006344
2024-04-30 04:47:41,744 - trainer - INFO -     test_accuracy  : 0.703674
2024-04-30 04:47:41,744 - trainer - INFO -     test_macro_f   : 0.692064
2024-04-30 04:47:41,744 - trainer - INFO -     test_precision : 0.724954
2024-04-30 04:47:41,744 - trainer - INFO -     test_recall    : 0.703674
2024-04-30 04:47:41,744 - trainer - INFO -     test_doc_entropy: 2.885702
2024-04-30 04:48:42,918 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): RNNBase(
    (attn): Linear(in_features=300, out_features=1, bias=True)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,785,496
Freeze params: 0
2024-04-30 04:58:20,392 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): RNNBase(
    (attn): Linear(in_features=300, out_features=1, bias=True)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,785,496
Freeze params: 0
2024-04-30 12:51:37,773 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=2400, bias=True)
    (1): Tanh()
    (2): Linear(in_features=2400, out_features=120, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=120, bias=False)
  (W_q): Linear(in_features=300, out_features=120, bias=False)
  (W_v): Linear(in_features=300, out_features=120, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((120, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,875,075
Freeze params: 0
2024-04-30 12:55:41,800 - trainer - INFO -     epoch          : 1
2024-04-30 12:55:41,800 - trainer - INFO -     loss           : 1.142683
2024-04-30 12:55:41,800 - trainer - INFO -     accuracy       : 0.67104
2024-04-30 12:55:41,800 - trainer - INFO -     macro_f        : 0.652863
2024-04-30 12:55:41,800 - trainer - INFO -     precision      : 0.684199
2024-04-30 12:55:41,800 - trainer - INFO -     recall         : 0.67104
2024-04-30 12:55:41,800 - trainer - INFO -     doc_entropy    : 2.327899
2024-04-30 12:55:41,800 - trainer - INFO -     val_loss       : 1.009038
2024-04-30 12:55:41,800 - trainer - INFO -     val_accuracy   : 0.706263
2024-04-30 12:55:41,800 - trainer - INFO -     val_macro_f    : 0.695266
2024-04-30 12:55:41,800 - trainer - INFO -     val_precision  : 0.730168
2024-04-30 12:55:41,800 - trainer - INFO -     val_recall     : 0.706263
2024-04-30 12:55:41,800 - trainer - INFO -     val_doc_entropy: 2.700108
2024-04-30 12:55:41,800 - trainer - INFO -     test_loss      : 1.001392
2024-04-30 12:55:41,800 - trainer - INFO -     test_accuracy  : 0.705815
2024-04-30 12:55:41,800 - trainer - INFO -     test_macro_f   : 0.693206
2024-04-30 12:55:41,800 - trainer - INFO -     test_precision : 0.726937
2024-04-30 12:55:41,800 - trainer - INFO -     test_recall    : 0.705815
2024-04-30 12:55:41,800 - trainer - INFO -     test_doc_entropy: 2.70481
2024-04-30 12:59:37,609 - trainer - INFO -     epoch          : 2
2024-04-30 12:59:37,609 - trainer - INFO -     loss           : 0.810387
2024-04-30 12:59:37,625 - trainer - INFO -     accuracy       : 0.75717
2024-04-30 12:59:37,625 - trainer - INFO -     macro_f        : 0.748428
2024-04-30 12:59:37,625 - trainer - INFO -     precision      : 0.781172
2024-04-30 12:59:37,625 - trainer - INFO -     recall         : 0.75717
2024-04-30 12:59:37,625 - trainer - INFO -     doc_entropy    : 2.486038
2024-04-30 12:59:37,625 - trainer - INFO -     val_loss       : 1.000614
2024-04-30 12:59:37,625 - trainer - INFO -     val_accuracy   : 0.708454
2024-04-30 12:59:37,625 - trainer - INFO -     val_macro_f    : 0.697577
2024-04-30 12:59:37,625 - trainer - INFO -     val_precision  : 0.733189
2024-04-30 12:59:37,625 - trainer - INFO -     val_recall     : 0.708454
2024-04-30 12:59:37,625 - trainer - INFO -     val_doc_entropy: 2.806715
2024-04-30 12:59:37,625 - trainer - INFO -     test_loss      : 0.999762
2024-04-30 12:59:37,625 - trainer - INFO -     test_accuracy  : 0.7093
2024-04-30 12:59:37,625 - trainer - INFO -     test_macro_f   : 0.697436
2024-04-30 12:59:37,625 - trainer - INFO -     test_precision : 0.730876
2024-04-30 12:59:37,625 - trainer - INFO -     test_recall    : 0.7093
2024-04-30 12:59:37,625 - trainer - INFO -     test_doc_entropy: 2.809384
2024-04-30 13:00:16,696 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=2400, bias=True)
    (1): Tanh()
    (2): Linear(in_features=2400, out_features=120, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=120, bias=False)
  (W_q): Linear(in_features=300, out_features=120, bias=False)
  (W_v): Linear(in_features=300, out_features=120, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((120, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,875,075
Freeze params: 0
2024-04-30 13:04:06,064 - trainer - INFO -     epoch          : 1
2024-04-30 13:04:06,064 - trainer - INFO -     loss           : 1.145607
2024-04-30 13:04:06,064 - trainer - INFO -     accuracy       : 0.670081
2024-04-30 13:04:06,064 - trainer - INFO -     macro_f        : 0.65248
2024-04-30 13:04:06,064 - trainer - INFO -     precision      : 0.685101
2024-04-30 13:04:06,064 - trainer - INFO -     recall         : 0.670081
2024-04-30 13:04:06,064 - trainer - INFO -     doc_entropy    : 2.343129
2024-04-30 13:04:06,064 - trainer - INFO -     val_loss       : 0.995533
2024-04-30 13:04:06,064 - trainer - INFO -     val_accuracy   : 0.707657
2024-04-30 13:04:06,064 - trainer - INFO -     val_macro_f    : 0.697678
2024-04-30 13:04:06,064 - trainer - INFO -     val_precision  : 0.73394
2024-04-30 13:04:06,064 - trainer - INFO -     val_recall     : 0.707657
2024-04-30 13:04:06,064 - trainer - INFO -     val_doc_entropy: 2.781748
2024-04-30 13:04:06,064 - trainer - INFO -     test_loss      : 1.001704
2024-04-30 13:04:06,064 - trainer - INFO -     test_accuracy  : 0.70691
2024-04-30 13:04:06,064 - trainer - INFO -     test_macro_f   : 0.697588
2024-04-30 13:04:06,079 - trainer - INFO -     test_precision : 0.734238
2024-04-30 13:04:06,079 - trainer - INFO -     test_recall    : 0.70691
2024-04-30 13:04:06,079 - trainer - INFO -     test_doc_entropy: 2.785937
2024-04-30 13:08:05,808 - trainer - INFO -     epoch          : 2
2024-04-30 13:08:05,808 - trainer - INFO -     loss           : 0.812798
2024-04-30 13:08:05,808 - trainer - INFO -     accuracy       : 0.756535
2024-04-30 13:08:05,808 - trainer - INFO -     macro_f        : 0.747511
2024-04-30 13:08:05,808 - trainer - INFO -     precision      : 0.77993
2024-04-30 13:08:05,808 - trainer - INFO -     recall         : 0.756535
2024-04-30 13:08:05,808 - trainer - INFO -     doc_entropy    : 2.523032
2024-04-30 13:08:05,808 - trainer - INFO -     val_loss       : 1.0122
2024-04-30 13:08:05,808 - trainer - INFO -     val_accuracy   : 0.706263
2024-04-30 13:08:05,808 - trainer - INFO -     val_macro_f    : 0.694775
2024-04-30 13:08:05,808 - trainer - INFO -     val_precision  : 0.728838
2024-04-30 13:08:05,808 - trainer - INFO -     val_recall     : 0.706263
2024-04-30 13:08:05,808 - trainer - INFO -     val_doc_entropy: 2.786458
2024-04-30 13:08:05,808 - trainer - INFO -     test_loss      : 1.006608
2024-04-30 13:08:05,808 - trainer - INFO -     test_accuracy  : 0.707607
2024-04-30 13:08:05,808 - trainer - INFO -     test_macro_f   : 0.697436
2024-04-30 13:08:05,808 - trainer - INFO -     test_precision : 0.732735
2024-04-30 13:08:05,808 - trainer - INFO -     test_recall    : 0.707607
2024-04-30 13:08:05,808 - trainer - INFO -     test_doc_entropy: 2.789744
2024-04-30 13:08:43,405 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=2400, bias=True)
    (1): Tanh()
    (2): Linear(in_features=2400, out_features=120, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=120, bias=False)
  (W_q): Linear(in_features=300, out_features=120, bias=False)
  (W_v): Linear(in_features=300, out_features=120, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((120, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,875,075
Freeze params: 0
2024-04-30 13:12:42,387 - trainer - INFO -     epoch          : 1
2024-04-30 13:12:42,387 - trainer - INFO -     loss           : 1.142949
2024-04-30 13:12:42,387 - trainer - INFO -     accuracy       : 0.672011
2024-04-30 13:12:42,387 - trainer - INFO -     macro_f        : 0.654046
2024-04-30 13:12:42,402 - trainer - INFO -     precision      : 0.685183
2024-04-30 13:12:42,402 - trainer - INFO -     recall         : 0.672011
2024-04-30 13:12:42,402 - trainer - INFO -     doc_entropy    : 2.463155
2024-04-30 13:12:42,402 - trainer - INFO -     val_loss       : 1.003545
2024-04-30 13:12:42,402 - trainer - INFO -     val_accuracy   : 0.702678
2024-04-30 13:12:42,402 - trainer - INFO -     val_macro_f    : 0.687707
2024-04-30 13:12:42,402 - trainer - INFO -     val_precision  : 0.718009
2024-04-30 13:12:42,402 - trainer - INFO -     val_recall     : 0.702678
2024-04-30 13:12:42,402 - trainer - INFO -     val_doc_entropy: 2.851507
2024-04-30 13:12:42,402 - trainer - INFO -     test_loss      : 1.003798
2024-04-30 13:12:42,402 - trainer - INFO -     test_accuracy  : 0.70467
2024-04-30 13:12:42,402 - trainer - INFO -     test_macro_f   : 0.691584
2024-04-30 13:12:42,402 - trainer - INFO -     test_precision : 0.723975
2024-04-30 13:12:42,402 - trainer - INFO -     test_recall    : 0.70467
2024-04-30 13:12:42,402 - trainer - INFO -     test_doc_entropy: 2.855986
2024-04-30 13:16:43,937 - trainer - INFO -     epoch          : 2
2024-04-30 13:16:43,952 - trainer - INFO -     loss           : 0.817935
2024-04-30 13:16:43,952 - trainer - INFO -     accuracy       : 0.755315
2024-04-30 13:16:43,952 - trainer - INFO -     macro_f        : 0.7463
2024-04-30 13:16:43,952 - trainer - INFO -     precision      : 0.779694
2024-04-30 13:16:43,952 - trainer - INFO -     recall         : 0.755315
2024-04-30 13:16:43,952 - trainer - INFO -     doc_entropy    : 2.626093
2024-04-30 13:16:43,952 - trainer - INFO -     val_loss       : 0.974882
2024-04-30 13:16:43,952 - trainer - INFO -     val_accuracy   : 0.713831
2024-04-30 13:16:43,952 - trainer - INFO -     val_macro_f    : 0.702868
2024-04-30 13:16:43,952 - trainer - INFO -     val_precision  : 0.737788
2024-04-30 13:16:43,952 - trainer - INFO -     val_recall     : 0.713831
2024-04-30 13:16:43,952 - trainer - INFO -     val_doc_entropy: 2.855776
2024-04-30 13:16:43,952 - trainer - INFO -     test_loss      : 0.969585
2024-04-30 13:16:43,952 - trainer - INFO -     test_accuracy  : 0.71393
2024-04-30 13:16:43,952 - trainer - INFO -     test_macro_f   : 0.702906
2024-04-30 13:16:43,952 - trainer - INFO -     test_precision : 0.736808
2024-04-30 13:16:43,952 - trainer - INFO -     test_recall    : 0.71393
2024-04-30 13:16:43,952 - trainer - INFO -     test_doc_entropy: 2.860943
2024-04-30 13:17:24,866 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=2400, bias=True)
    (1): Tanh()
    (2): Linear(in_features=2400, out_features=120, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=120, bias=False)
  (W_q): Linear(in_features=300, out_features=120, bias=False)
  (W_v): Linear(in_features=300, out_features=120, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((120, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,875,075
Freeze params: 0
2024-04-30 13:21:24,945 - trainer - INFO -     epoch          : 1
2024-04-30 13:21:24,945 - trainer - INFO -     loss           : 1.151745
2024-04-30 13:21:24,945 - trainer - INFO -     accuracy       : 0.668918
2024-04-30 13:21:24,945 - trainer - INFO -     macro_f        : 0.650639
2024-04-30 13:21:24,945 - trainer - INFO -     precision      : 0.681685
2024-04-30 13:21:24,945 - trainer - INFO -     recall         : 0.668918
2024-04-30 13:21:24,960 - trainer - INFO -     doc_entropy    : 2.309343
2024-04-30 13:21:24,960 - trainer - INFO -     val_loss       : 1.00501
2024-04-30 13:21:24,960 - trainer - INFO -     val_accuracy   : 0.707757
2024-04-30 13:21:24,960 - trainer - INFO -     val_macro_f    : 0.702358
2024-04-30 13:21:24,960 - trainer - INFO -     val_precision  : 0.742977
2024-04-30 13:21:24,960 - trainer - INFO -     val_recall     : 0.707757
2024-04-30 13:21:24,960 - trainer - INFO -     val_doc_entropy: 2.646465
2024-04-30 13:21:24,960 - trainer - INFO -     test_loss      : 1.009182
2024-04-30 13:21:24,960 - trainer - INFO -     test_accuracy  : 0.707508
2024-04-30 13:21:24,960 - trainer - INFO -     test_macro_f   : 0.700637
2024-04-30 13:21:24,960 - trainer - INFO -     test_precision : 0.738529
2024-04-30 13:21:24,960 - trainer - INFO -     test_recall    : 0.707508
2024-04-30 13:21:24,960 - trainer - INFO -     test_doc_entropy: 2.65044
2024-04-30 13:25:34,640 - trainer - INFO -     epoch          : 2
2024-04-30 13:25:34,640 - trainer - INFO -     loss           : 0.817899
2024-04-30 13:25:34,640 - trainer - INFO -     accuracy       : 0.75623
2024-04-30 13:25:34,640 - trainer - INFO -     macro_f        : 0.747555
2024-04-30 13:25:34,640 - trainer - INFO -     precision      : 0.780313
2024-04-30 13:25:34,640 - trainer - INFO -     recall         : 0.75623
2024-04-30 13:25:34,640 - trainer - INFO -     doc_entropy    : 2.448009
2024-04-30 13:25:34,640 - trainer - INFO -     val_loss       : 0.992592
2024-04-30 13:25:34,640 - trainer - INFO -     val_accuracy   : 0.713333
2024-04-30 13:25:34,640 - trainer - INFO -     val_macro_f    : 0.70851
2024-04-30 13:25:34,640 - trainer - INFO -     val_precision  : 0.747831
2024-04-30 13:25:34,640 - trainer - INFO -     val_recall     : 0.713333
2024-04-30 13:25:34,656 - trainer - INFO -     val_doc_entropy: 2.768396
2024-04-30 13:25:34,656 - trainer - INFO -     test_loss      : 0.989008
2024-04-30 13:25:34,656 - trainer - INFO -     test_accuracy  : 0.710993
2024-04-30 13:25:34,656 - trainer - INFO -     test_macro_f   : 0.706572
2024-04-30 13:25:34,656 - trainer - INFO -     test_precision : 0.748943
2024-04-30 13:25:34,656 - trainer - INFO -     test_recall    : 0.710993
2024-04-30 13:25:34,656 - trainer - INFO -     test_doc_entropy: 2.769926
2024-04-30 13:26:14,505 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=2400, bias=True)
    (1): Tanh()
    (2): Linear(in_features=2400, out_features=120, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=120, bias=False)
  (W_q): Linear(in_features=300, out_features=120, bias=False)
  (W_v): Linear(in_features=300, out_features=120, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((120, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,875,075
Freeze params: 0
2024-04-30 13:30:09,840 - trainer - INFO -     epoch          : 1
2024-04-30 13:30:09,840 - trainer - INFO -     loss           : 1.144605
2024-04-30 13:30:09,840 - trainer - INFO -     accuracy       : 0.671581
2024-04-30 13:30:09,840 - trainer - INFO -     macro_f        : 0.653233
2024-04-30 13:30:09,840 - trainer - INFO -     precision      : 0.684881
2024-04-30 13:30:09,840 - trainer - INFO -     recall         : 0.671581
2024-04-30 13:30:09,840 - trainer - INFO -     doc_entropy    : 2.363501
2024-04-30 13:30:09,840 - trainer - INFO -     val_loss       : 1.001249
2024-04-30 13:30:09,840 - trainer - INFO -     val_accuracy   : 0.707458
2024-04-30 13:30:09,840 - trainer - INFO -     val_macro_f    : 0.693563
2024-04-30 13:30:09,840 - trainer - INFO -     val_precision  : 0.725795
2024-04-30 13:30:09,840 - trainer - INFO -     val_recall     : 0.707458
2024-04-30 13:30:09,840 - trainer - INFO -     val_doc_entropy: 2.948435
2024-04-30 13:30:09,840 - trainer - INFO -     test_loss      : 0.999318
2024-04-30 13:30:09,840 - trainer - INFO -     test_accuracy  : 0.705865
2024-04-30 13:30:09,840 - trainer - INFO -     test_macro_f   : 0.691908
2024-04-30 13:30:09,840 - trainer - INFO -     test_precision : 0.724986
2024-04-30 13:30:09,840 - trainer - INFO -     test_recall    : 0.705865
2024-04-30 13:30:09,840 - trainer - INFO -     test_doc_entropy: 2.95303
2024-04-30 13:34:11,479 - trainer - INFO -     epoch          : 2
2024-04-30 13:34:11,479 - trainer - INFO -     loss           : 0.8136
2024-04-30 13:34:11,479 - trainer - INFO -     accuracy       : 0.756883
2024-04-30 13:34:11,479 - trainer - INFO -     macro_f        : 0.748503
2024-04-30 13:34:11,479 - trainer - INFO -     precision      : 0.781927
2024-04-30 13:34:11,479 - trainer - INFO -     recall         : 0.756883
2024-04-30 13:34:11,479 - trainer - INFO -     doc_entropy    : 2.550493
2024-04-30 13:34:11,479 - trainer - INFO -     val_loss       : 0.984144
2024-04-30 13:34:11,479 - trainer - INFO -     val_accuracy   : 0.716021
2024-04-30 13:34:11,479 - trainer - INFO -     val_macro_f    : 0.70371
2024-04-30 13:34:11,479 - trainer - INFO -     val_precision  : 0.736553
2024-04-30 13:34:11,479 - trainer - INFO -     val_recall     : 0.716021
2024-04-30 13:34:11,479 - trainer - INFO -     val_doc_entropy: 2.820874
2024-04-30 13:34:11,479 - trainer - INFO -     test_loss      : 0.983766
2024-04-30 13:34:11,494 - trainer - INFO -     test_accuracy  : 0.710644
2024-04-30 13:34:11,494 - trainer - INFO -     test_macro_f   : 0.69951
2024-04-30 13:34:11,494 - trainer - INFO -     test_precision : 0.73343
2024-04-30 13:34:11,494 - trainer - INFO -     test_recall    : 0.710644
2024-04-30 13:34:11,494 - trainer - INFO -     test_doc_entropy: 2.824347
2024-04-30 13:35:29,872 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=2800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=2800, out_features=140, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=140, bias=False)
  (W_q): Linear(in_features=300, out_features=140, bias=False)
  (W_v): Linear(in_features=300, out_features=140, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((140, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 28,121,495
Freeze params: 0
2024-04-30 13:39:33,293 - trainer - INFO -     epoch          : 1
2024-04-30 13:39:33,293 - trainer - INFO -     loss           : 1.14423
2024-04-30 13:39:33,293 - trainer - INFO -     accuracy       : 0.671146
2024-04-30 13:39:33,293 - trainer - INFO -     macro_f        : 0.653112
2024-04-30 13:39:33,293 - trainer - INFO -     precision      : 0.684441
2024-04-30 13:39:33,293 - trainer - INFO -     recall         : 0.671146
2024-04-30 13:39:33,293 - trainer - INFO -     doc_entropy    : 2.428306
2024-04-30 13:39:33,293 - trainer - INFO -     val_loss       : 1.009219
2024-04-30 13:39:33,293 - trainer - INFO -     val_accuracy   : 0.706412
2024-04-30 13:39:33,293 - trainer - INFO -     val_macro_f    : 0.693187
2024-04-30 13:39:33,293 - trainer - INFO -     val_precision  : 0.727417
2024-04-30 13:39:33,293 - trainer - INFO -     val_recall     : 0.706412
2024-04-30 13:39:33,293 - trainer - INFO -     val_doc_entropy: 2.81458
2024-04-30 13:39:33,293 - trainer - INFO -     test_loss      : 1.011446
2024-04-30 13:39:33,293 - trainer - INFO -     test_accuracy  : 0.702977
2024-04-30 13:39:33,293 - trainer - INFO -     test_macro_f   : 0.691377
2024-04-30 13:39:33,293 - trainer - INFO -     test_precision : 0.725741
2024-04-30 13:39:33,293 - trainer - INFO -     test_recall    : 0.702977
2024-04-30 13:39:33,293 - trainer - INFO -     test_doc_entropy: 2.820566
2024-04-30 13:43:35,960 - trainer - INFO -     epoch          : 2
2024-04-30 13:43:35,960 - trainer - INFO -     loss           : 0.815787
2024-04-30 13:43:35,960 - trainer - INFO -     accuracy       : 0.756018
2024-04-30 13:43:35,960 - trainer - INFO -     macro_f        : 0.746862
2024-04-30 13:43:35,960 - trainer - INFO -     precision      : 0.779206
2024-04-30 13:43:35,960 - trainer - INFO -     recall         : 0.756018
2024-04-30 13:43:35,960 - trainer - INFO -     doc_entropy    : 2.625468
2024-04-30 13:43:35,960 - trainer - INFO -     val_loss       : 0.995777
2024-04-30 13:43:35,960 - trainer - INFO -     val_accuracy   : 0.709748
2024-04-30 13:43:35,960 - trainer - INFO -     val_macro_f    : 0.701803
2024-04-30 13:43:35,960 - trainer - INFO -     val_precision  : 0.737585
2024-04-30 13:43:35,960 - trainer - INFO -     val_recall     : 0.709748
2024-04-30 13:43:35,960 - trainer - INFO -     val_doc_entropy: 2.911978
2024-04-30 13:43:35,960 - trainer - INFO -     test_loss      : 0.988128
2024-04-30 13:43:35,960 - trainer - INFO -     test_accuracy  : 0.711839
2024-04-30 13:43:35,960 - trainer - INFO -     test_macro_f   : 0.703751
2024-04-30 13:43:35,960 - trainer - INFO -     test_precision : 0.741405
2024-04-30 13:43:35,960 - trainer - INFO -     test_recall    : 0.711839
2024-04-30 13:43:35,960 - trainer - INFO -     test_doc_entropy: 2.916031
2024-04-30 13:44:17,206 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=2800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=2800, out_features=140, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=140, bias=False)
  (W_q): Linear(in_features=300, out_features=140, bias=False)
  (W_v): Linear(in_features=300, out_features=140, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((140, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 28,121,495
Freeze params: 0
2024-04-30 13:48:18,445 - trainer - INFO -     epoch          : 1
2024-04-30 13:48:18,445 - trainer - INFO -     loss           : 1.143159
2024-04-30 13:48:18,445 - trainer - INFO -     accuracy       : 0.671177
2024-04-30 13:48:18,445 - trainer - INFO -     macro_f        : 0.653129
2024-04-30 13:48:18,445 - trainer - INFO -     precision      : 0.684519
2024-04-30 13:48:18,445 - trainer - INFO -     recall         : 0.671177
2024-04-30 13:48:18,445 - trainer - INFO -     doc_entropy    : 2.32549
2024-04-30 13:48:18,445 - trainer - INFO -     val_loss       : 0.995474
2024-04-30 13:48:18,445 - trainer - INFO -     val_accuracy   : 0.705218
2024-04-30 13:48:18,445 - trainer - INFO -     val_macro_f    : 0.687797
2024-04-30 13:48:18,445 - trainer - INFO -     val_precision  : 0.717916
2024-04-30 13:48:18,445 - trainer - INFO -     val_recall     : 0.705218
2024-04-30 13:48:18,445 - trainer - INFO -     val_doc_entropy: 2.803168
2024-04-30 13:48:18,445 - trainer - INFO -     test_loss      : 0.996027
2024-04-30 13:48:18,445 - trainer - INFO -     test_accuracy  : 0.70696
2024-04-30 13:48:18,445 - trainer - INFO -     test_macro_f   : 0.689986
2024-04-30 13:48:18,445 - trainer - INFO -     test_precision : 0.72
2024-04-30 13:48:18,445 - trainer - INFO -     test_recall    : 0.70696
2024-04-30 13:48:18,445 - trainer - INFO -     test_doc_entropy: 2.807434
2024-04-30 13:52:22,396 - trainer - INFO -     epoch          : 2
2024-04-30 13:52:22,396 - trainer - INFO -     loss           : 0.813246
2024-04-30 13:52:22,396 - trainer - INFO -     accuracy       : 0.757083
2024-04-30 13:52:22,396 - trainer - INFO -     macro_f        : 0.747986
2024-04-30 13:52:22,396 - trainer - INFO -     precision      : 0.780291
2024-04-30 13:52:22,396 - trainer - INFO -     recall         : 0.757083
2024-04-30 13:52:22,396 - trainer - INFO -     doc_entropy    : 2.553731
2024-04-30 13:52:22,396 - trainer - INFO -     val_loss       : 0.995017
2024-04-30 13:52:22,396 - trainer - INFO -     val_accuracy   : 0.71169
2024-04-30 13:52:22,396 - trainer - INFO -     val_macro_f    : 0.705593
2024-04-30 13:52:22,396 - trainer - INFO -     val_precision  : 0.744045
2024-04-30 13:52:22,396 - trainer - INFO -     val_recall     : 0.71169
2024-04-30 13:52:22,396 - trainer - INFO -     val_doc_entropy: 2.859804
2024-04-30 13:52:22,396 - trainer - INFO -     test_loss      : 0.992024
2024-04-30 13:52:22,396 - trainer - INFO -     test_accuracy  : 0.712685
2024-04-30 13:52:22,396 - trainer - INFO -     test_macro_f   : 0.706879
2024-04-30 13:52:22,396 - trainer - INFO -     test_precision : 0.746967
2024-04-30 13:52:22,396 - trainer - INFO -     test_recall    : 0.712685
2024-04-30 13:52:22,396 - trainer - INFO -     test_doc_entropy: 2.863473
2024-04-30 13:53:01,044 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=2800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=2800, out_features=140, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=140, bias=False)
  (W_q): Linear(in_features=300, out_features=140, bias=False)
  (W_v): Linear(in_features=300, out_features=140, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((140, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 28,121,495
Freeze params: 0
2024-04-30 13:57:04,814 - trainer - INFO -     epoch          : 1
2024-04-30 13:57:04,814 - trainer - INFO -     loss           : 1.145943
2024-04-30 13:57:04,814 - trainer - INFO -     accuracy       : 0.669465
2024-04-30 13:57:04,830 - trainer - INFO -     macro_f        : 0.651583
2024-04-30 13:57:04,830 - trainer - INFO -     precision      : 0.68346
2024-04-30 13:57:04,830 - trainer - INFO -     recall         : 0.669465
2024-04-30 13:57:04,830 - trainer - INFO -     doc_entropy    : 2.370868
2024-04-30 13:57:04,830 - trainer - INFO -     val_loss       : 0.999598
2024-04-30 13:57:04,830 - trainer - INFO -     val_accuracy   : 0.705765
2024-04-30 13:57:04,830 - trainer - INFO -     val_macro_f    : 0.693178
2024-04-30 13:57:04,830 - trainer - INFO -     val_precision  : 0.72744
2024-04-30 13:57:04,830 - trainer - INFO -     val_recall     : 0.705765
2024-04-30 13:57:04,830 - trainer - INFO -     val_doc_entropy: 2.769495
2024-04-30 13:57:04,830 - trainer - INFO -     test_loss      : 1.004757
2024-04-30 13:57:04,830 - trainer - INFO -     test_accuracy  : 0.704222
2024-04-30 13:57:04,830 - trainer - INFO -     test_macro_f   : 0.692154
2024-04-30 13:57:04,830 - trainer - INFO -     test_precision : 0.724743
2024-04-30 13:57:04,830 - trainer - INFO -     test_recall    : 0.704222
2024-04-30 13:57:04,830 - trainer - INFO -     test_doc_entropy: 2.772318
2024-04-30 14:01:10,238 - trainer - INFO -     epoch          : 2
2024-04-30 14:01:10,238 - trainer - INFO -     loss           : 0.808375
2024-04-30 14:01:10,238 - trainer - INFO -     accuracy       : 0.756641
2024-04-30 14:01:10,238 - trainer - INFO -     macro_f        : 0.747654
2024-04-30 14:01:10,238 - trainer - INFO -     precision      : 0.780249
2024-04-30 14:01:10,238 - trainer - INFO -     recall         : 0.756641
2024-04-30 14:01:10,238 - trainer - INFO -     doc_entropy    : 2.567827
2024-04-30 14:01:10,238 - trainer - INFO -     val_loss       : 0.997445
2024-04-30 14:01:10,238 - trainer - INFO -     val_accuracy   : 0.709499
2024-04-30 14:01:10,238 - trainer - INFO -     val_macro_f    : 0.700378
2024-04-30 14:01:10,238 - trainer - INFO -     val_precision  : 0.738181
2024-04-30 14:01:10,238 - trainer - INFO -     val_recall     : 0.709499
2024-04-30 14:01:10,238 - trainer - INFO -     val_doc_entropy: 2.854663
2024-04-30 14:01:10,238 - trainer - INFO -     test_loss      : 0.988455
2024-04-30 14:01:10,238 - trainer - INFO -     test_accuracy  : 0.712387
2024-04-30 14:01:10,238 - trainer - INFO -     test_macro_f   : 0.704356
2024-04-30 14:01:10,238 - trainer - INFO -     test_precision : 0.742203
2024-04-30 14:01:10,238 - trainer - INFO -     test_recall    : 0.712387
2024-04-30 14:01:10,238 - trainer - INFO -     test_doc_entropy: 2.857946
2024-04-30 14:01:48,604 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=2800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=2800, out_features=140, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=140, bias=False)
  (W_q): Linear(in_features=300, out_features=140, bias=False)
  (W_v): Linear(in_features=300, out_features=140, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((140, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 28,121,495
Freeze params: 0
2024-04-30 14:05:50,115 - trainer - INFO -     epoch          : 1
2024-04-30 14:05:50,115 - trainer - INFO -     loss           : 1.145722
2024-04-30 14:05:50,115 - trainer - INFO -     accuracy       : 0.670417
2024-04-30 14:05:50,115 - trainer - INFO -     macro_f        : 0.652212
2024-04-30 14:05:50,115 - trainer - INFO -     precision      : 0.683889
2024-04-30 14:05:50,115 - trainer - INFO -     recall         : 0.670417
2024-04-30 14:05:50,115 - trainer - INFO -     doc_entropy    : 2.36952
2024-04-30 14:05:50,115 - trainer - INFO -     val_loss       : 1.012137
2024-04-30 14:05:50,115 - trainer - INFO -     val_accuracy   : 0.701384
2024-04-30 14:05:50,115 - trainer - INFO -     val_macro_f    : 0.683986
2024-04-30 14:05:50,115 - trainer - INFO -     val_precision  : 0.713407
2024-04-30 14:05:50,115 - trainer - INFO -     val_recall     : 0.701384
2024-04-30 14:05:50,115 - trainer - INFO -     val_doc_entropy: 2.857319
2024-04-30 14:05:50,115 - trainer - INFO -     test_loss      : 1.006668
2024-04-30 14:05:50,115 - trainer - INFO -     test_accuracy  : 0.700737
2024-04-30 14:05:50,115 - trainer - INFO -     test_macro_f   : 0.684224
2024-04-30 14:05:50,115 - trainer - INFO -     test_precision : 0.715456
2024-04-30 14:05:50,115 - trainer - INFO -     test_recall    : 0.700737
2024-04-30 14:05:50,115 - trainer - INFO -     test_doc_entropy: 2.861981
2024-04-30 14:09:54,727 - trainer - INFO -     epoch          : 2
2024-04-30 14:09:54,727 - trainer - INFO -     loss           : 0.818176
2024-04-30 14:09:54,727 - trainer - INFO -     accuracy       : 0.755502
2024-04-30 14:09:54,727 - trainer - INFO -     macro_f        : 0.746279
2024-04-30 14:09:54,727 - trainer - INFO -     precision      : 0.779016
2024-04-30 14:09:54,727 - trainer - INFO -     recall         : 0.755502
2024-04-30 14:09:54,727 - trainer - INFO -     doc_entropy    : 2.55016
2024-04-30 14:09:54,727 - trainer - INFO -     val_loss       : 0.987353
2024-04-30 14:09:54,727 - trainer - INFO -     val_accuracy   : 0.710943
2024-04-30 14:09:54,727 - trainer - INFO -     val_macro_f    : 0.698137
2024-04-30 14:09:54,727 - trainer - INFO -     val_precision  : 0.729051
2024-04-30 14:09:54,727 - trainer - INFO -     val_recall     : 0.710943
2024-04-30 14:09:54,727 - trainer - INFO -     val_doc_entropy: 2.867387
2024-04-30 14:09:54,727 - trainer - INFO -     test_loss      : 0.982575
2024-04-30 14:09:54,727 - trainer - INFO -     test_accuracy  : 0.715075
2024-04-30 14:09:54,727 - trainer - INFO -     test_macro_f   : 0.703219
2024-04-30 14:09:54,727 - trainer - INFO -     test_precision : 0.737524
2024-04-30 14:09:54,727 - trainer - INFO -     test_recall    : 0.715075
2024-04-30 14:09:54,727 - trainer - INFO -     test_doc_entropy: 2.871058
2024-04-30 14:10:34,439 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=2800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=2800, out_features=140, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=140, bias=False)
  (W_q): Linear(in_features=300, out_features=140, bias=False)
  (W_v): Linear(in_features=300, out_features=140, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((140, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 28,121,495
Freeze params: 0
2024-04-30 14:14:34,446 - trainer - INFO -     epoch          : 1
2024-04-30 14:14:34,446 - trainer - INFO -     loss           : 1.147625
2024-04-30 14:14:34,446 - trainer - INFO -     accuracy       : 0.670679
2024-04-30 14:14:34,446 - trainer - INFO -     macro_f        : 0.652312
2024-04-30 14:14:34,446 - trainer - INFO -     precision      : 0.683841
2024-04-30 14:14:34,446 - trainer - INFO -     recall         : 0.670679
2024-04-30 14:14:34,446 - trainer - INFO -     doc_entropy    : 2.359245
2024-04-30 14:14:34,446 - trainer - INFO -     val_loss       : 1.016139
2024-04-30 14:14:34,446 - trainer - INFO -     val_accuracy   : 0.703176
2024-04-30 14:14:34,446 - trainer - INFO -     val_macro_f    : 0.692453
2024-04-30 14:14:34,446 - trainer - INFO -     val_precision  : 0.730153
2024-04-30 14:14:34,446 - trainer - INFO -     val_recall     : 0.703176
2024-04-30 14:14:34,446 - trainer - INFO -     val_doc_entropy: 2.736221
2024-04-30 14:14:34,446 - trainer - INFO -     test_loss      : 1.014879
2024-04-30 14:14:34,446 - trainer - INFO -     test_accuracy  : 0.703276
2024-04-30 14:14:34,446 - trainer - INFO -     test_macro_f   : 0.693962
2024-04-30 14:14:34,446 - trainer - INFO -     test_precision : 0.73189
2024-04-30 14:14:34,446 - trainer - INFO -     test_recall    : 0.703276
2024-04-30 14:14:34,446 - trainer - INFO -     test_doc_entropy: 2.739874
2024-04-30 14:18:47,254 - trainer - INFO -     epoch          : 2
2024-04-30 14:18:47,254 - trainer - INFO -     loss           : 0.816023
2024-04-30 14:18:47,254 - trainer - INFO -     accuracy       : 0.756267
2024-04-30 14:18:47,254 - trainer - INFO -     macro_f        : 0.747242
2024-04-30 14:18:47,254 - trainer - INFO -     precision      : 0.779741
2024-04-30 14:18:47,254 - trainer - INFO -     recall         : 0.756267
2024-04-30 14:18:47,254 - trainer - INFO -     doc_entropy    : 2.528746
2024-04-30 14:18:47,254 - trainer - INFO -     val_loss       : 0.986503
2024-04-30 14:18:47,254 - trainer - INFO -     val_accuracy   : 0.712287
2024-04-30 14:18:47,254 - trainer - INFO -     val_macro_f    : 0.700255
2024-04-30 14:18:47,254 - trainer - INFO -     val_precision  : 0.734609
2024-04-30 14:18:47,254 - trainer - INFO -     val_recall     : 0.712287
2024-04-30 14:18:47,254 - trainer - INFO -     val_doc_entropy: 2.849603
2024-04-30 14:18:47,254 - trainer - INFO -     test_loss      : 0.992547
2024-04-30 14:18:47,254 - trainer - INFO -     test_accuracy  : 0.709997
2024-04-30 14:18:47,254 - trainer - INFO -     test_macro_f   : 0.698841
2024-04-30 14:18:47,254 - trainer - INFO -     test_precision : 0.734938
2024-04-30 14:18:47,254 - trainer - INFO -     test_recall    : 0.709997
2024-04-30 14:18:47,254 - trainer - INFO -     test_doc_entropy: 2.853266
2024-04-30 14:21:42,005 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=3200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=3200, out_features=160, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=160, bias=False)
  (W_q): Linear(in_features=300, out_features=160, bias=False)
  (W_v): Linear(in_features=300, out_features=160, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((160, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 28,383,915
Freeze params: 0
2024-04-30 14:25:39,987 - trainer - INFO -     epoch          : 1
2024-04-30 14:25:39,987 - trainer - INFO -     loss           : 1.143736
2024-04-30 14:25:39,987 - trainer - INFO -     accuracy       : 0.670523
2024-04-30 14:25:39,987 - trainer - INFO -     macro_f        : 0.652576
2024-04-30 14:25:39,987 - trainer - INFO -     precision      : 0.684514
2024-04-30 14:25:39,987 - trainer - INFO -     recall         : 0.670523
2024-04-30 14:25:39,987 - trainer - INFO -     doc_entropy    : 2.478511
2024-04-30 14:25:39,987 - trainer - INFO -     val_loss       : 1.010301
2024-04-30 14:25:39,987 - trainer - INFO -     val_accuracy   : 0.704769
2024-04-30 14:25:39,987 - trainer - INFO -     val_macro_f    : 0.693894
2024-04-30 14:25:39,987 - trainer - INFO -     val_precision  : 0.728981
2024-04-30 14:25:39,987 - trainer - INFO -     val_recall     : 0.704769
2024-04-30 14:25:39,987 - trainer - INFO -     val_doc_entropy: 2.864092
2024-04-30 14:25:39,987 - trainer - INFO -     test_loss      : 1.002032
2024-04-30 14:25:39,987 - trainer - INFO -     test_accuracy  : 0.705218
2024-04-30 14:25:39,987 - trainer - INFO -     test_macro_f   : 0.69678
2024-04-30 14:25:39,987 - trainer - INFO -     test_precision : 0.735508
2024-04-30 14:25:40,002 - trainer - INFO -     test_recall    : 0.705218
2024-04-30 14:25:40,002 - trainer - INFO -     test_doc_entropy: 2.868537
2024-04-30 14:29:47,692 - trainer - INFO -     epoch          : 2
2024-04-30 14:29:47,692 - trainer - INFO -     loss           : 0.812015
2024-04-30 14:29:47,692 - trainer - INFO -     accuracy       : 0.756641
2024-04-30 14:29:47,692 - trainer - INFO -     macro_f        : 0.747671
2024-04-30 14:29:47,692 - trainer - INFO -     precision      : 0.78034
2024-04-30 14:29:47,692 - trainer - INFO -     recall         : 0.756641
2024-04-30 14:29:47,692 - trainer - INFO -     doc_entropy    : 2.68282
2024-04-30 14:29:47,692 - trainer - INFO -     val_loss       : 0.996672
2024-04-30 14:29:47,692 - trainer - INFO -     val_accuracy   : 0.70935
2024-04-30 14:29:47,692 - trainer - INFO -     val_macro_f    : 0.702572
2024-04-30 14:29:47,692 - trainer - INFO -     val_precision  : 0.741936
2024-04-30 14:29:47,692 - trainer - INFO -     val_recall     : 0.70935
2024-04-30 14:29:47,692 - trainer - INFO -     val_doc_entropy: 2.901016
2024-04-30 14:29:47,692 - trainer - INFO -     test_loss      : 0.983304
2024-04-30 14:29:47,692 - trainer - INFO -     test_accuracy  : 0.71159
2024-04-30 14:29:47,692 - trainer - INFO -     test_macro_f   : 0.704241
2024-04-30 14:29:47,692 - trainer - INFO -     test_precision : 0.743584
2024-04-30 14:29:47,692 - trainer - INFO -     test_recall    : 0.71159
2024-04-30 14:29:47,692 - trainer - INFO -     test_doc_entropy: 2.905228
2024-04-30 14:30:28,096 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=3200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=3200, out_features=160, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=160, bias=False)
  (W_q): Linear(in_features=300, out_features=160, bias=False)
  (W_v): Linear(in_features=300, out_features=160, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((160, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 28,383,915
Freeze params: 0
2024-04-30 14:34:29,345 - trainer - INFO -     epoch          : 1
2024-04-30 14:34:29,345 - trainer - INFO -     loss           : 1.144296
2024-04-30 14:34:29,345 - trainer - INFO -     accuracy       : 0.670368
2024-04-30 14:34:29,345 - trainer - INFO -     macro_f        : 0.652672
2024-04-30 14:34:29,345 - trainer - INFO -     precision      : 0.684851
2024-04-30 14:34:29,345 - trainer - INFO -     recall         : 0.670368
2024-04-30 14:34:29,345 - trainer - INFO -     doc_entropy    : 2.440161
2024-04-30 14:34:29,345 - trainer - INFO -     val_loss       : 1.005863
2024-04-30 14:34:29,345 - trainer - INFO -     val_accuracy   : 0.704769
2024-04-30 14:34:29,345 - trainer - INFO -     val_macro_f    : 0.690501
2024-04-30 14:34:29,345 - trainer - INFO -     val_precision  : 0.721634
2024-04-30 14:34:29,345 - trainer - INFO -     val_recall     : 0.704769
2024-04-30 14:34:29,345 - trainer - INFO -     val_doc_entropy: 2.820142
2024-04-30 14:34:29,345 - trainer - INFO -     test_loss      : 1.006254
2024-04-30 14:34:29,345 - trainer - INFO -     test_accuracy  : 0.703077
2024-04-30 14:34:29,345 - trainer - INFO -     test_macro_f   : 0.688768
2024-04-30 14:34:29,345 - trainer - INFO -     test_precision : 0.721419
2024-04-30 14:34:29,345 - trainer - INFO -     test_recall    : 0.703077
2024-04-30 14:34:29,345 - trainer - INFO -     test_doc_entropy: 2.825341
2024-04-30 14:38:32,667 - trainer - INFO -     epoch          : 2
2024-04-30 14:38:32,667 - trainer - INFO -     loss           : 0.811484
2024-04-30 14:38:32,667 - trainer - INFO -     accuracy       : 0.75656
2024-04-30 14:38:32,667 - trainer - INFO -     macro_f        : 0.747578
2024-04-30 14:38:32,667 - trainer - INFO -     precision      : 0.780559
2024-04-30 14:38:32,667 - trainer - INFO -     recall         : 0.75656
2024-04-30 14:38:32,667 - trainer - INFO -     doc_entropy    : 2.627628
2024-04-30 14:38:32,667 - trainer - INFO -     val_loss       : 0.997016
2024-04-30 14:38:32,667 - trainer - INFO -     val_accuracy   : 0.708255
2024-04-30 14:38:32,667 - trainer - INFO -     val_macro_f    : 0.69872
2024-04-30 14:38:32,667 - trainer - INFO -     val_precision  : 0.733719
2024-04-30 14:38:32,667 - trainer - INFO -     val_recall     : 0.708255
2024-04-30 14:38:32,667 - trainer - INFO -     val_doc_entropy: 2.894842
2024-04-30 14:38:32,667 - trainer - INFO -     test_loss      : 0.990255
2024-04-30 14:38:32,667 - trainer - INFO -     test_accuracy  : 0.710296
2024-04-30 14:38:32,667 - trainer - INFO -     test_macro_f   : 0.700737
2024-04-30 14:38:32,667 - trainer - INFO -     test_precision : 0.735899
2024-04-30 14:38:32,667 - trainer - INFO -     test_recall    : 0.710296
2024-04-30 14:38:32,667 - trainer - INFO -     test_doc_entropy: 2.898952
2024-04-30 14:39:13,695 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=3200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=3200, out_features=160, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=160, bias=False)
  (W_q): Linear(in_features=300, out_features=160, bias=False)
  (W_v): Linear(in_features=300, out_features=160, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((160, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 28,383,915
Freeze params: 0
2024-04-30 14:43:18,660 - trainer - INFO -     epoch          : 1
2024-04-30 14:43:18,660 - trainer - INFO -     loss           : 1.14597
2024-04-30 14:43:18,660 - trainer - INFO -     accuracy       : 0.671438
2024-04-30 14:43:18,660 - trainer - INFO -     macro_f        : 0.653001
2024-04-30 14:43:18,660 - trainer - INFO -     precision      : 0.684327
2024-04-30 14:43:18,660 - trainer - INFO -     recall         : 0.671438
2024-04-30 14:43:18,660 - trainer - INFO -     doc_entropy    : 2.394567
2024-04-30 14:43:18,660 - trainer - INFO -     val_loss       : 1.006802
2024-04-30 14:43:18,660 - trainer - INFO -     val_accuracy   : 0.705218
2024-04-30 14:43:18,660 - trainer - INFO -     val_macro_f    : 0.689592
2024-04-30 14:43:18,660 - trainer - INFO -     val_precision  : 0.72132
2024-04-30 14:43:18,660 - trainer - INFO -     val_recall     : 0.705218
2024-04-30 14:43:18,660 - trainer - INFO -     val_doc_entropy: 2.763398
2024-04-30 14:43:18,660 - trainer - INFO -     test_loss      : 1.001601
2024-04-30 14:43:18,660 - trainer - INFO -     test_accuracy  : 0.705118
2024-04-30 14:43:18,660 - trainer - INFO -     test_macro_f   : 0.692393
2024-04-30 14:43:18,660 - trainer - INFO -     test_precision : 0.728414
2024-04-30 14:43:18,660 - trainer - INFO -     test_recall    : 0.705118
2024-04-30 14:43:18,660 - trainer - INFO -     test_doc_entropy: 2.768249
2024-04-30 14:47:23,465 - trainer - INFO -     epoch          : 2
2024-04-30 14:47:23,465 - trainer - INFO -     loss           : 0.81409
2024-04-30 14:47:23,465 - trainer - INFO -     accuracy       : 0.756865
2024-04-30 14:47:23,465 - trainer - INFO -     macro_f        : 0.748012
2024-04-30 14:47:23,465 - trainer - INFO -     precision      : 0.780591
2024-04-30 14:47:23,465 - trainer - INFO -     recall         : 0.756865
2024-04-30 14:47:23,465 - trainer - INFO -     doc_entropy    : 2.603613
2024-04-30 14:47:23,465 - trainer - INFO -     val_loss       : 1.006238
2024-04-30 14:47:23,465 - trainer - INFO -     val_accuracy   : 0.708603
2024-04-30 14:47:23,465 - trainer - INFO -     val_macro_f    : 0.701087
2024-04-30 14:47:23,465 - trainer - INFO -     val_precision  : 0.73858
2024-04-30 14:47:23,465 - trainer - INFO -     val_recall     : 0.708603
2024-04-30 14:47:23,465 - trainer - INFO -     val_doc_entropy: 2.81928
2024-04-30 14:47:23,465 - trainer - INFO -     test_loss      : 0.999509
2024-04-30 14:47:23,465 - trainer - INFO -     test_accuracy  : 0.707408
2024-04-30 14:47:23,465 - trainer - INFO -     test_macro_f   : 0.700455
2024-04-30 14:47:23,465 - trainer - INFO -     test_precision : 0.740099
2024-04-30 14:47:23,465 - trainer - INFO -     test_recall    : 0.707408
2024-04-30 14:47:23,465 - trainer - INFO -     test_doc_entropy: 2.823769
2024-04-30 14:48:03,956 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=3200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=3200, out_features=160, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=160, bias=False)
  (W_q): Linear(in_features=300, out_features=160, bias=False)
  (W_v): Linear(in_features=300, out_features=160, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((160, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 28,383,915
Freeze params: 0
2024-04-30 14:52:04,910 - trainer - INFO -     epoch          : 1
2024-04-30 14:52:04,910 - trainer - INFO -     loss           : 1.145315
2024-04-30 14:52:04,910 - trainer - INFO -     accuracy       : 0.67076
2024-04-30 14:52:04,910 - trainer - INFO -     macro_f        : 0.653035
2024-04-30 14:52:04,910 - trainer - INFO -     precision      : 0.684999
2024-04-30 14:52:04,910 - trainer - INFO -     recall         : 0.67076
2024-04-30 14:52:04,910 - trainer - INFO -     doc_entropy    : 2.432937
2024-04-30 14:52:04,910 - trainer - INFO -     val_loss       : 1.000083
2024-04-30 14:52:04,910 - trainer - INFO -     val_accuracy   : 0.708503
2024-04-30 14:52:04,910 - trainer - INFO -     val_macro_f    : 0.692414
2024-04-30 14:52:04,910 - trainer - INFO -     val_precision  : 0.722066
2024-04-30 14:52:04,910 - trainer - INFO -     val_recall     : 0.708503
2024-04-30 14:52:04,910 - trainer - INFO -     val_doc_entropy: 2.860222
2024-04-30 14:52:04,910 - trainer - INFO -     test_loss      : 0.99528
2024-04-30 14:52:04,910 - trainer - INFO -     test_accuracy  : 0.703973
2024-04-30 14:52:04,910 - trainer - INFO -     test_macro_f   : 0.69063
2024-04-30 14:52:04,910 - trainer - INFO -     test_precision : 0.722429
2024-04-30 14:52:04,910 - trainer - INFO -     test_recall    : 0.703973
2024-04-30 14:52:04,910 - trainer - INFO -     test_doc_entropy: 2.865959
2024-04-30 14:56:10,461 - trainer - INFO -     epoch          : 2
2024-04-30 14:56:10,461 - trainer - INFO -     loss           : 0.813761
2024-04-30 14:56:10,461 - trainer - INFO -     accuracy       : 0.75674
2024-04-30 14:56:10,461 - trainer - INFO -     macro_f        : 0.747531
2024-04-30 14:56:10,461 - trainer - INFO -     precision      : 0.779953
2024-04-30 14:56:10,461 - trainer - INFO -     recall         : 0.75674
2024-04-30 14:56:10,461 - trainer - INFO -     doc_entropy    : 2.6213
2024-04-30 14:56:10,461 - trainer - INFO -     val_loss       : 0.994483
2024-04-30 14:56:10,461 - trainer - INFO -     val_accuracy   : 0.711142
2024-04-30 14:56:10,461 - trainer - INFO -     val_macro_f    : 0.706266
2024-04-30 14:56:10,461 - trainer - INFO -     val_precision  : 0.746811
2024-04-30 14:56:10,461 - trainer - INFO -     val_recall     : 0.711142
2024-04-30 14:56:10,461 - trainer - INFO -     val_doc_entropy: 2.885546
2024-04-30 14:56:10,461 - trainer - INFO -     test_loss      : 0.988136
2024-04-30 14:56:10,461 - trainer - INFO -     test_accuracy  : 0.708952
2024-04-30 14:56:10,461 - trainer - INFO -     test_macro_f   : 0.70288
2024-04-30 14:56:10,461 - trainer - INFO -     test_precision : 0.74227
2024-04-30 14:56:10,461 - trainer - INFO -     test_recall    : 0.708952
2024-04-30 14:56:10,461 - trainer - INFO -     test_doc_entropy: 2.889609
2024-04-30 14:56:48,808 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=3200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=3200, out_features=160, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=160, bias=False)
  (W_q): Linear(in_features=300, out_features=160, bias=False)
  (W_v): Linear(in_features=300, out_features=160, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((160, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 28,383,915
Freeze params: 0
2024-04-30 15:00:56,882 - trainer - INFO -     epoch          : 1
2024-04-30 15:00:56,882 - trainer - INFO -     loss           : 1.143904
2024-04-30 15:00:56,882 - trainer - INFO -     accuracy       : 0.671326
2024-04-30 15:00:56,882 - trainer - INFO -     macro_f        : 0.653456
2024-04-30 15:00:56,882 - trainer - INFO -     precision      : 0.685394
2024-04-30 15:00:56,882 - trainer - INFO -     recall         : 0.671326
2024-04-30 15:00:56,882 - trainer - INFO -     doc_entropy    : 2.362021
2024-04-30 15:00:56,882 - trainer - INFO -     val_loss       : 0.998625
2024-04-30 15:00:56,882 - trainer - INFO -     val_accuracy   : 0.70701
2024-04-30 15:00:56,882 - trainer - INFO -     val_macro_f    : 0.69482
2024-04-30 15:00:56,882 - trainer - INFO -     val_precision  : 0.727171
2024-04-30 15:00:56,882 - trainer - INFO -     val_recall     : 0.70701
2024-04-30 15:00:56,882 - trainer - INFO -     val_doc_entropy: 2.7953
2024-04-30 15:00:56,882 - trainer - INFO -     test_loss      : 0.993657
2024-04-30 15:00:56,882 - trainer - INFO -     test_accuracy  : 0.705715
2024-04-30 15:00:56,882 - trainer - INFO -     test_macro_f   : 0.695845
2024-04-30 15:00:56,882 - trainer - INFO -     test_precision : 0.732431
2024-04-30 15:00:56,882 - trainer - INFO -     test_recall    : 0.705715
2024-04-30 15:00:56,882 - trainer - INFO -     test_doc_entropy: 2.798278
2024-04-30 15:05:00,858 - trainer - INFO -     epoch          : 2
2024-04-30 15:05:00,858 - trainer - INFO -     loss           : 0.814468
2024-04-30 15:05:00,858 - trainer - INFO -     accuracy       : 0.755359
2024-04-30 15:05:00,858 - trainer - INFO -     macro_f        : 0.746161
2024-04-30 15:05:00,858 - trainer - INFO -     precision      : 0.778774
2024-04-30 15:05:00,858 - trainer - INFO -     recall         : 0.755359
2024-04-30 15:05:00,858 - trainer - INFO -     doc_entropy    : 2.579437
2024-04-30 15:05:00,858 - trainer - INFO -     val_loss       : 0.991855
2024-04-30 15:05:00,858 - trainer - INFO -     val_accuracy   : 0.710993
2024-04-30 15:05:00,858 - trainer - INFO -     val_macro_f    : 0.70176
2024-04-30 15:05:00,858 - trainer - INFO -     val_precision  : 0.737181
2024-04-30 15:05:00,858 - trainer - INFO -     val_recall     : 0.710993
2024-04-30 15:05:00,858 - trainer - INFO -     val_doc_entropy: 2.862237
2024-04-30 15:05:00,858 - trainer - INFO -     test_loss      : 0.984492
2024-04-30 15:05:00,858 - trainer - INFO -     test_accuracy  : 0.714627
2024-04-30 15:05:00,858 - trainer - INFO -     test_macro_f   : 0.705621
2024-04-30 15:05:00,858 - trainer - INFO -     test_precision : 0.742323
2024-04-30 15:05:00,858 - trainer - INFO -     test_recall    : 0.714627
2024-04-30 15:05:00,858 - trainer - INFO -     test_doc_entropy: 2.86673
2024-04-30 23:12:34,927 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1200, out_features=60, bias=True)
  )
  (rnn): RNNBase(
    (attn): Linear(in_features=300, out_features=1, bias=True)
  )
  (W_k): Linear(in_features=300, out_features=60, bias=False)
  (W_q): Linear(in_features=300, out_features=60, bias=False)
  (W_v): Linear(in_features=300, out_features=60, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((60, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 25,967,916
Freeze params: 0
2024-04-30 23:24:21,096 - trainer - INFO -     epoch          : 1
2024-04-30 23:24:21,096 - trainer - INFO -     loss           : 1.203802
2024-04-30 23:24:21,096 - trainer - INFO -     accuracy       : 0.658609
2024-04-30 23:24:21,096 - trainer - INFO -     macro_f        : 0.638494
2024-04-30 23:24:21,096 - trainer - INFO -     precision      : 0.668432
2024-04-30 23:24:21,096 - trainer - INFO -     recall         : 0.658609
2024-04-30 23:24:21,096 - trainer - INFO -     doc_entropy    : 2.266187
2024-04-30 23:24:21,096 - trainer - INFO -     val_loss       : 1.108629
2024-04-30 23:24:21,096 - trainer - INFO -     val_accuracy   : 0.692235
2024-04-30 23:24:21,111 - trainer - INFO -     val_macro_f    : 0.684659
2024-04-30 23:24:21,111 - trainer - INFO -     val_precision  : 0.724297
2024-04-30 23:24:21,111 - trainer - INFO -     val_recall     : 0.692235
2024-04-30 23:24:21,111 - trainer - INFO -     val_doc_entropy: 2.264346
2024-04-30 23:24:21,111 - trainer - INFO -     test_loss      : 1.108478
2024-04-30 23:24:21,111 - trainer - INFO -     test_accuracy  : 0.69064
2024-04-30 23:24:21,111 - trainer - INFO -     test_macro_f   : 0.683457
2024-04-30 23:24:21,111 - trainer - INFO -     test_precision : 0.723466
2024-04-30 23:24:21,111 - trainer - INFO -     test_recall    : 0.69064
2024-04-30 23:24:21,111 - trainer - INFO -     test_doc_entropy: 2.268363
2024-05-01 00:09:53,347 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,049,395
Freeze params: 0
2024-05-01 00:13:40,913 - trainer - INFO -     epoch          : 1
2024-05-01 00:13:40,913 - trainer - INFO -     loss           : 1.150518
2024-05-01 00:13:40,913 - trainer - INFO -     accuracy       : 0.669714
2024-05-01 00:13:40,913 - trainer - INFO -     macro_f        : 0.651029
2024-05-01 00:13:40,913 - trainer - INFO -     precision      : 0.682352
2024-05-01 00:13:40,913 - trainer - INFO -     recall         : 0.669714
2024-05-01 00:13:40,913 - trainer - INFO -     doc_entropy    : 2.336203
2024-05-01 00:13:40,913 - trainer - INFO -     val_loss       : 1.013115
2024-05-01 00:13:40,913 - trainer - INFO -     val_accuracy   : 0.701583
2024-05-01 00:13:40,913 - trainer - INFO -     val_macro_f    : 0.693343
2024-05-01 00:13:40,913 - trainer - INFO -     val_precision  : 0.733032
2024-05-01 00:13:40,913 - trainer - INFO -     val_recall     : 0.701583
2024-05-01 00:13:40,913 - trainer - INFO -     val_doc_entropy: 2.824331
2024-05-01 00:13:40,913 - trainer - INFO -     test_loss      : 1.007178
2024-05-01 00:13:40,913 - trainer - INFO -     test_accuracy  : 0.706612
2024-05-01 00:13:40,913 - trainer - INFO -     test_macro_f   : 0.69874
2024-05-01 00:13:40,913 - trainer - INFO -     test_precision : 0.738251
2024-05-01 00:13:40,913 - trainer - INFO -     test_recall    : 0.706612
2024-05-01 00:13:40,913 - trainer - INFO -     test_doc_entropy: 2.828102
2024-05-01 00:17:27,607 - trainer - INFO -     epoch          : 2
2024-05-01 00:17:27,607 - trainer - INFO -     loss           : 0.813931
2024-05-01 00:17:27,607 - trainer - INFO -     accuracy       : 0.756454
2024-05-01 00:17:27,607 - trainer - INFO -     macro_f        : 0.747601
2024-05-01 00:17:27,607 - trainer - INFO -     precision      : 0.780355
2024-05-01 00:17:27,607 - trainer - INFO -     recall         : 0.756454
2024-05-01 00:17:27,607 - trainer - INFO -     doc_entropy    : 2.446461
2024-05-01 00:17:27,607 - trainer - INFO -     val_loss       : 0.999398
2024-05-01 00:17:27,607 - trainer - INFO -     val_accuracy   : 0.704919
2024-05-01 00:17:27,607 - trainer - INFO -     val_macro_f    : 0.699485
2024-05-01 00:17:27,607 - trainer - INFO -     val_precision  : 0.742789
2024-05-01 00:17:27,607 - trainer - INFO -     val_recall     : 0.704919
2024-05-01 00:17:27,607 - trainer - INFO -     val_doc_entropy: 2.790222
2024-05-01 00:17:27,607 - trainer - INFO -     test_loss      : 0.984703
2024-05-01 00:17:27,607 - trainer - INFO -     test_accuracy  : 0.710495
2024-05-01 00:17:27,607 - trainer - INFO -     test_macro_f   : 0.704288
2024-05-01 00:17:27,607 - trainer - INFO -     test_precision : 0.744216
2024-05-01 00:17:27,607 - trainer - INFO -     test_recall    : 0.710495
2024-05-01 00:17:27,607 - trainer - INFO -     test_doc_entropy: 2.792723
2024-05-01 00:21:38,395 - trainer - INFO -     epoch          : 3
2024-05-01 00:21:38,395 - trainer - INFO -     loss           : 0.592755
2024-05-01 00:21:38,395 - trainer - INFO -     accuracy       : 0.819102
2024-05-01 00:21:38,395 - trainer - INFO -     macro_f        : 0.8135
2024-05-01 00:21:38,395 - trainer - INFO -     precision      : 0.841881
2024-05-01 00:21:38,395 - trainer - INFO -     recall         : 0.819102
2024-05-01 00:21:38,395 - trainer - INFO -     doc_entropy    : 2.363436
2024-05-01 00:21:38,395 - trainer - INFO -     val_loss       : 1.083352
2024-05-01 00:21:38,395 - trainer - INFO -     val_accuracy   : 0.705267
2024-05-01 00:21:38,395 - trainer - INFO -     val_macro_f    : 0.695762
2024-05-01 00:21:38,395 - trainer - INFO -     val_precision  : 0.73138
2024-05-01 00:21:38,395 - trainer - INFO -     val_recall     : 0.705267
2024-05-01 00:21:38,395 - trainer - INFO -     val_doc_entropy: 2.7678
2024-05-01 00:21:38,395 - trainer - INFO -     test_loss      : 1.080556
2024-05-01 00:21:38,395 - trainer - INFO -     test_accuracy  : 0.706213
2024-05-01 00:21:38,395 - trainer - INFO -     test_macro_f   : 0.696492
2024-05-01 00:21:38,395 - trainer - INFO -     test_precision : 0.732968
2024-05-01 00:21:38,395 - trainer - INFO -     test_recall    : 0.706213
2024-05-01 00:21:38,395 - trainer - INFO -     test_doc_entropy: 2.770868
2024-05-01 00:24:26,921 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,049,395
Freeze params: 0
2024-05-01 00:28:16,913 - trainer - INFO -     epoch          : 1
2024-05-01 00:28:16,913 - trainer - INFO -     loss           : 1.15599
2024-05-01 00:28:16,913 - trainer - INFO -     accuracy       : 0.668252
2024-05-01 00:28:16,913 - trainer - INFO -     macro_f        : 0.64976
2024-05-01 00:28:16,913 - trainer - INFO -     precision      : 0.680722
2024-05-01 00:28:16,913 - trainer - INFO -     recall         : 0.668252
2024-05-01 00:28:16,913 - trainer - INFO -     doc_entropy    : 2.310653
2024-05-01 00:28:16,913 - trainer - INFO -     val_loss       : 1.008048
2024-05-01 00:28:16,913 - trainer - INFO -     val_accuracy   : 0.705616
2024-05-01 00:28:16,913 - trainer - INFO -     val_macro_f    : 0.693647
2024-05-01 00:28:16,913 - trainer - INFO -     val_precision  : 0.726153
2024-05-01 00:28:16,913 - trainer - INFO -     val_recall     : 0.705616
2024-05-01 00:28:16,913 - trainer - INFO -     val_doc_entropy: 2.876728
2024-05-01 00:28:16,913 - trainer - INFO -     test_loss      : 1.010519
2024-05-01 00:28:16,913 - trainer - INFO -     test_accuracy  : 0.703923
2024-05-01 00:28:16,913 - trainer - INFO -     test_macro_f   : 0.692144
2024-05-01 00:28:16,913 - trainer - INFO -     test_precision : 0.725422
2024-05-01 00:28:16,913 - trainer - INFO -     test_recall    : 0.703923
2024-05-01 00:28:16,913 - trainer - INFO -     test_doc_entropy: 2.878882
2024-05-01 00:32:13,380 - trainer - INFO -     epoch          : 2
2024-05-01 00:32:13,380 - trainer - INFO -     loss           : 0.820677
2024-05-01 00:32:13,380 - trainer - INFO -     accuracy       : 0.754543
2024-05-01 00:32:13,395 - trainer - INFO -     macro_f        : 0.745223
2024-05-01 00:32:13,395 - trainer - INFO -     precision      : 0.777613
2024-05-01 00:32:13,395 - trainer - INFO -     recall         : 0.754543
2024-05-01 00:32:13,395 - trainer - INFO -     doc_entropy    : 2.484276
2024-05-01 00:32:13,395 - trainer - INFO -     val_loss       : 1.003883
2024-05-01 00:32:13,395 - trainer - INFO -     val_accuracy   : 0.706811
2024-05-01 00:32:13,395 - trainer - INFO -     val_macro_f    : 0.699046
2024-05-01 00:32:13,395 - trainer - INFO -     val_precision  : 0.737666
2024-05-01 00:32:13,395 - trainer - INFO -     val_recall     : 0.706811
2024-05-01 00:32:13,395 - trainer - INFO -     val_doc_entropy: 2.808447
2024-05-01 00:32:13,395 - trainer - INFO -     test_loss      : 0.994319
2024-05-01 00:32:13,395 - trainer - INFO -     test_accuracy  : 0.705616
2024-05-01 00:32:13,395 - trainer - INFO -     test_macro_f   : 0.696916
2024-05-01 00:32:13,395 - trainer - INFO -     test_precision : 0.735738
2024-05-01 00:32:13,395 - trainer - INFO -     test_recall    : 0.705616
2024-05-01 00:32:13,395 - trainer - INFO -     test_doc_entropy: 2.809426
2024-05-01 00:36:13,596 - trainer - INFO -     epoch          : 3
2024-05-01 00:36:13,596 - trainer - INFO -     loss           : 0.59803
2024-05-01 00:36:13,612 - trainer - INFO -     accuracy       : 0.817838
2024-05-01 00:36:13,612 - trainer - INFO -     macro_f        : 0.812254
2024-05-01 00:36:13,612 - trainer - INFO -     precision      : 0.840704
2024-05-01 00:36:13,612 - trainer - INFO -     recall         : 0.817838
2024-05-01 00:36:13,612 - trainer - INFO -     doc_entropy    : 2.390933
2024-05-01 00:36:13,612 - trainer - INFO -     val_loss       : 1.082365
2024-05-01 00:36:13,612 - trainer - INFO -     val_accuracy   : 0.699193
2024-05-01 00:36:13,612 - trainer - INFO -     val_macro_f    : 0.691112
2024-05-01 00:36:13,612 - trainer - INFO -     val_precision  : 0.730423
2024-05-01 00:36:13,612 - trainer - INFO -     val_recall     : 0.699193
2024-05-01 00:36:13,612 - trainer - INFO -     val_doc_entropy: 2.804968
2024-05-01 00:36:13,612 - trainer - INFO -     test_loss      : 1.079281
2024-05-01 00:36:13,612 - trainer - INFO -     test_accuracy  : 0.69775
2024-05-01 00:36:13,612 - trainer - INFO -     test_macro_f   : 0.690348
2024-05-01 00:36:13,612 - trainer - INFO -     test_precision : 0.732374
2024-05-01 00:36:13,612 - trainer - INFO -     test_recall    : 0.69775
2024-05-01 00:36:13,612 - trainer - INFO -     test_doc_entropy: 2.807056
2024-05-01 00:36:50,391 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,049,395
Freeze params: 0
2024-05-01 00:38:57,092 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,049,395
Freeze params: 0
2024-05-01 00:42:51,327 - trainer - INFO -     epoch          : 1
2024-05-01 00:42:51,327 - trainer - INFO -     loss           : 1.153318
2024-05-01 00:42:51,327 - trainer - INFO -     accuracy       : 0.668557
2024-05-01 00:42:51,327 - trainer - INFO -     macro_f        : 0.650026
2024-05-01 00:42:51,327 - trainer - INFO -     precision      : 0.681365
2024-05-01 00:42:51,327 - trainer - INFO -     recall         : 0.668557
2024-05-01 00:42:51,327 - trainer - INFO -     doc_entropy    : 2.289192
2024-05-01 00:42:51,327 - trainer - INFO -     val_loss       : 1.000206
2024-05-01 00:42:51,327 - trainer - INFO -     val_accuracy   : 0.70706
2024-05-01 00:42:51,327 - trainer - INFO -     val_macro_f    : 0.693436
2024-05-01 00:42:51,327 - trainer - INFO -     val_precision  : 0.7267
2024-05-01 00:42:51,327 - trainer - INFO -     val_recall     : 0.70706
2024-05-01 00:42:51,327 - trainer - INFO -     val_doc_entropy: 2.607124
2024-05-01 00:42:51,327 - trainer - INFO -     test_loss      : 1.001061
2024-05-01 00:42:51,327 - trainer - INFO -     test_accuracy  : 0.704471
2024-05-01 00:42:51,327 - trainer - INFO -     test_macro_f   : 0.691237
2024-05-01 00:42:51,327 - trainer - INFO -     test_precision : 0.725167
2024-05-01 00:42:51,327 - trainer - INFO -     test_recall    : 0.704471
2024-05-01 00:42:51,327 - trainer - INFO -     test_doc_entropy: 2.612012
2024-05-01 00:46:54,122 - trainer - INFO -     epoch          : 2
2024-05-01 00:46:54,122 - trainer - INFO -     loss           : 0.815643
2024-05-01 00:46:54,122 - trainer - INFO -     accuracy       : 0.755913
2024-05-01 00:46:54,122 - trainer - INFO -     macro_f        : 0.746793
2024-05-01 00:46:54,122 - trainer - INFO -     precision      : 0.779499
2024-05-01 00:46:54,122 - trainer - INFO -     recall         : 0.755913
2024-05-01 00:46:54,122 - trainer - INFO -     doc_entropy    : 2.420715
2024-05-01 00:46:54,122 - trainer - INFO -     val_loss       : 0.994594
2024-05-01 00:46:54,122 - trainer - INFO -     val_accuracy   : 0.711839
2024-05-01 00:46:54,122 - trainer - INFO -     val_macro_f    : 0.699693
2024-05-01 00:46:54,122 - trainer - INFO -     val_precision  : 0.732445
2024-05-01 00:46:54,122 - trainer - INFO -     val_recall     : 0.711839
2024-05-01 00:46:54,122 - trainer - INFO -     val_doc_entropy: 2.767245
2024-05-01 00:46:54,122 - trainer - INFO -     test_loss      : 0.987878
2024-05-01 00:46:54,122 - trainer - INFO -     test_accuracy  : 0.711142
2024-05-01 00:46:54,122 - trainer - INFO -     test_macro_f   : 0.699507
2024-05-01 00:46:54,122 - trainer - INFO -     test_precision : 0.731739
2024-05-01 00:46:54,122 - trainer - INFO -     test_recall    : 0.711142
2024-05-01 00:46:54,122 - trainer - INFO -     test_doc_entropy: 2.770675
2024-05-01 00:51:03,008 - trainer - INFO -     epoch          : 3
2024-05-01 00:51:03,008 - trainer - INFO -     loss           : 0.592497
2024-05-01 00:51:03,008 - trainer - INFO -     accuracy       : 0.818996
2024-05-01 00:51:03,008 - trainer - INFO -     macro_f        : 0.813572
2024-05-01 00:51:03,008 - trainer - INFO -     precision      : 0.842355
2024-05-01 00:51:03,008 - trainer - INFO -     recall         : 0.818996
2024-05-01 00:51:03,008 - trainer - INFO -     doc_entropy    : 2.324615
2024-05-01 00:51:03,008 - trainer - INFO -     val_loss       : 1.085276
2024-05-01 00:51:03,008 - trainer - INFO -     val_accuracy   : 0.698248
2024-05-01 00:51:03,008 - trainer - INFO -     val_macro_f    : 0.691174
2024-05-01 00:51:03,008 - trainer - INFO -     val_precision  : 0.732033
2024-05-01 00:51:03,008 - trainer - INFO -     val_recall     : 0.698248
2024-05-01 00:51:03,008 - trainer - INFO -     val_doc_entropy: 2.706552
2024-05-01 00:51:03,008 - trainer - INFO -     test_loss      : 1.075056
2024-05-01 00:51:03,008 - trainer - INFO -     test_accuracy  : 0.703127
2024-05-01 00:51:03,008 - trainer - INFO -     test_macro_f   : 0.696826
2024-05-01 00:51:03,008 - trainer - INFO -     test_precision : 0.738183
2024-05-01 00:51:03,008 - trainer - INFO -     test_recall    : 0.703127
2024-05-01 00:51:03,008 - trainer - INFO -     test_doc_entropy: 2.709456
2024-05-01 00:51:41,820 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,049,395
Freeze params: 0
2024-05-01 00:53:15,782 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,049,395
Freeze params: 0
2024-05-01 00:57:17,869 - trainer - INFO -     epoch          : 1
2024-05-01 00:57:17,869 - trainer - INFO -     loss           : 1.152608
2024-05-01 00:57:17,869 - trainer - INFO -     accuracy       : 0.668874
2024-05-01 00:57:17,885 - trainer - INFO -     macro_f        : 0.650155
2024-05-01 00:57:17,885 - trainer - INFO -     precision      : 0.681327
2024-05-01 00:57:17,885 - trainer - INFO -     recall         : 0.668874
2024-05-01 00:57:17,885 - trainer - INFO -     doc_entropy    : 2.282242
2024-05-01 00:57:17,885 - trainer - INFO -     val_loss       : 1.025471
2024-05-01 00:57:17,885 - trainer - INFO -     val_accuracy   : 0.701085
2024-05-01 00:57:17,885 - trainer - INFO -     val_macro_f    : 0.688091
2024-05-01 00:57:17,885 - trainer - INFO -     val_precision  : 0.723102
2024-05-01 00:57:17,885 - trainer - INFO -     val_recall     : 0.701085
2024-05-01 00:57:17,885 - trainer - INFO -     val_doc_entropy: 2.774335
2024-05-01 00:57:17,885 - trainer - INFO -     test_loss      : 1.021722
2024-05-01 00:57:17,885 - trainer - INFO -     test_accuracy  : 0.699841
2024-05-01 00:57:17,885 - trainer - INFO -     test_macro_f   : 0.688587
2024-05-01 00:57:17,885 - trainer - INFO -     test_precision : 0.724525
2024-05-01 00:57:17,885 - trainer - INFO -     test_recall    : 0.699841
2024-05-01 00:57:17,885 - trainer - INFO -     test_doc_entropy: 2.778735
2024-05-01 01:01:22,570 - trainer - INFO -     epoch          : 2
2024-05-01 01:01:22,570 - trainer - INFO -     loss           : 0.818125
2024-05-01 01:01:22,570 - trainer - INFO -     accuracy       : 0.755496
2024-05-01 01:01:22,570 - trainer - INFO -     macro_f        : 0.746079
2024-05-01 01:01:22,570 - trainer - INFO -     precision      : 0.778349
2024-05-01 01:01:22,570 - trainer - INFO -     recall         : 0.755496
2024-05-01 01:01:22,570 - trainer - INFO -     doc_entropy    : 2.452751
2024-05-01 01:01:22,570 - trainer - INFO -     val_loss       : 1.003082
2024-05-01 01:01:22,570 - trainer - INFO -     val_accuracy   : 0.708553
2024-05-01 01:01:22,570 - trainer - INFO -     val_macro_f    : 0.702315
2024-05-01 01:01:22,570 - trainer - INFO -     val_precision  : 0.742109
2024-05-01 01:01:22,570 - trainer - INFO -     val_recall     : 0.708553
2024-05-01 01:01:22,570 - trainer - INFO -     val_doc_entropy: 2.800979
2024-05-01 01:01:22,570 - trainer - INFO -     test_loss      : 0.998602
2024-05-01 01:01:22,570 - trainer - INFO -     test_accuracy  : 0.707707
2024-05-01 01:01:22,570 - trainer - INFO -     test_macro_f   : 0.700096
2024-05-01 01:01:22,570 - trainer - INFO -     test_precision : 0.739848
2024-05-01 01:01:22,570 - trainer - INFO -     test_recall    : 0.707707
2024-05-01 01:01:22,570 - trainer - INFO -     test_doc_entropy: 2.80365
2024-05-01 01:05:24,719 - trainer - INFO -     epoch          : 3
2024-05-01 01:05:24,719 - trainer - INFO -     loss           : 0.595838
2024-05-01 01:05:24,719 - trainer - INFO -     accuracy       : 0.819058
2024-05-01 01:05:24,719 - trainer - INFO -     macro_f        : 0.813296
2024-05-01 01:05:24,719 - trainer - INFO -     precision      : 0.841146
2024-05-01 01:05:24,719 - trainer - INFO -     recall         : 0.819058
2024-05-01 01:05:24,719 - trainer - INFO -     doc_entropy    : 2.356623
2024-05-01 01:05:24,719 - trainer - INFO -     val_loss       : 1.091229
2024-05-01 01:05:24,719 - trainer - INFO -     val_accuracy   : 0.695509
2024-05-01 01:05:24,719 - trainer - INFO -     val_macro_f    : 0.691778
2024-05-01 01:05:24,719 - trainer - INFO -     val_precision  : 0.736105
2024-05-01 01:05:24,719 - trainer - INFO -     val_recall     : 0.695509
2024-05-01 01:05:24,719 - trainer - INFO -     val_doc_entropy: 2.727348
2024-05-01 01:05:24,719 - trainer - INFO -     test_loss      : 1.071202
2024-05-01 01:05:24,719 - trainer - INFO -     test_accuracy  : 0.700687
2024-05-01 01:05:24,719 - trainer - INFO -     test_macro_f   : 0.695884
2024-05-01 01:05:24,719 - trainer - INFO -     test_precision : 0.737968
2024-05-01 01:05:24,719 - trainer - INFO -     test_recall    : 0.700687
2024-05-01 01:05:24,719 - trainer - INFO -     test_doc_entropy: 2.729969
2024-05-01 01:06:01,742 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,049,395
Freeze params: 0
2024-05-01 01:10:34,141 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,049,395
Freeze params: 0
2024-05-01 01:14:18,941 - trainer - INFO -     epoch          : 1
2024-05-01 01:14:18,941 - trainer - INFO -     loss           : 1.15599
2024-05-01 01:14:18,941 - trainer - INFO -     accuracy       : 0.668252
2024-05-01 01:14:18,941 - trainer - INFO -     macro_f        : 0.64976
2024-05-01 01:14:18,941 - trainer - INFO -     precision      : 0.680722
2024-05-01 01:14:18,941 - trainer - INFO -     recall         : 0.668252
2024-05-01 01:14:18,941 - trainer - INFO -     doc_entropy    : 2.310653
2024-05-01 01:14:18,941 - trainer - INFO -     val_loss       : 1.008048
2024-05-01 01:14:18,941 - trainer - INFO -     val_accuracy   : 0.705616
2024-05-01 01:14:18,941 - trainer - INFO -     val_macro_f    : 0.693647
2024-05-01 01:14:18,941 - trainer - INFO -     val_precision  : 0.726153
2024-05-01 01:14:18,941 - trainer - INFO -     val_recall     : 0.705616
2024-05-01 01:14:18,941 - trainer - INFO -     val_doc_entropy: 2.876728
2024-05-01 01:14:18,941 - trainer - INFO -     test_loss      : 1.010519
2024-05-01 01:14:18,941 - trainer - INFO -     test_accuracy  : 0.703923
2024-05-01 01:14:18,941 - trainer - INFO -     test_macro_f   : 0.692144
2024-05-01 01:14:18,941 - trainer - INFO -     test_precision : 0.725422
2024-05-01 01:14:18,941 - trainer - INFO -     test_recall    : 0.703923
2024-05-01 01:14:18,941 - trainer - INFO -     test_doc_entropy: 2.878882
2024-05-01 01:18:22,922 - trainer - INFO -     epoch          : 2
2024-05-01 01:18:22,922 - trainer - INFO -     loss           : 0.820677
2024-05-01 01:18:22,922 - trainer - INFO -     accuracy       : 0.754543
2024-05-01 01:18:22,922 - trainer - INFO -     macro_f        : 0.745223
2024-05-01 01:18:22,922 - trainer - INFO -     precision      : 0.777613
2024-05-01 01:18:22,922 - trainer - INFO -     recall         : 0.754543
2024-05-01 01:18:22,922 - trainer - INFO -     doc_entropy    : 2.484276
2024-05-01 01:18:22,922 - trainer - INFO -     val_loss       : 1.003883
2024-05-01 01:18:22,922 - trainer - INFO -     val_accuracy   : 0.706811
2024-05-01 01:18:22,922 - trainer - INFO -     val_macro_f    : 0.699046
2024-05-01 01:18:22,922 - trainer - INFO -     val_precision  : 0.737666
2024-05-01 01:18:22,922 - trainer - INFO -     val_recall     : 0.706811
2024-05-01 01:18:22,922 - trainer - INFO -     val_doc_entropy: 2.808447
2024-05-01 01:18:22,922 - trainer - INFO -     test_loss      : 0.994319
2024-05-01 01:18:22,922 - trainer - INFO -     test_accuracy  : 0.705616
2024-05-01 01:18:22,922 - trainer - INFO -     test_macro_f   : 0.696916
2024-05-01 01:18:22,922 - trainer - INFO -     test_precision : 0.735738
2024-05-01 01:18:22,922 - trainer - INFO -     test_recall    : 0.705616
2024-05-01 01:18:22,922 - trainer - INFO -     test_doc_entropy: 2.809426
2024-05-01 01:19:04,450 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,049,395
Freeze params: 0
2024-05-01 01:30:43,733 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(768, 768)
    (lstm): LSTM(768, 768)
  )
  (W_k): Linear(in_features=768, out_features=50, bias=False)
  (W_q): Linear(in_features=768, out_features=50, bias=False)
  (W_v): Linear(in_features=768, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 41,388,053
Freeze params: 0
2024-05-01 01:33:43,747 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(768, 768)
    (lstm): LSTM(768, 768)
  )
  (W_k): Linear(in_features=768, out_features=50, bias=False)
  (W_q): Linear(in_features=768, out_features=50, bias=False)
  (W_v): Linear(in_features=768, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 41,388,053
Freeze params: 0
2024-05-01 01:49:31,105 - trainer - INFO -     epoch          : 1
2024-05-01 01:49:31,121 - trainer - INFO -     loss           : 1.141611
2024-05-01 01:49:31,121 - trainer - INFO -     accuracy       : 0.668102
2024-05-01 01:49:31,121 - trainer - INFO -     macro_f        : 0.651302
2024-05-01 01:49:31,121 - trainer - INFO -     precision      : 0.683359
2024-05-01 01:49:31,121 - trainer - INFO -     recall         : 0.668102
2024-05-01 01:49:31,121 - trainer - INFO -     doc_entropy    : 2.625517
2024-05-01 01:49:31,121 - trainer - INFO -     val_loss       : 0.965355
2024-05-01 01:49:31,121 - trainer - INFO -     val_accuracy   : 0.712685
2024-05-01 01:49:31,121 - trainer - INFO -     val_macro_f    : 0.698415
2024-05-01 01:49:31,121 - trainer - INFO -     val_precision  : 0.730775
2024-05-01 01:49:31,121 - trainer - INFO -     val_recall     : 0.712685
2024-05-01 01:49:31,121 - trainer - INFO -     val_doc_entropy: 2.896179
2024-05-01 01:49:31,121 - trainer - INFO -     test_loss      : 0.956413
2024-05-01 01:49:31,121 - trainer - INFO -     test_accuracy  : 0.710445
2024-05-01 01:49:31,121 - trainer - INFO -     test_macro_f   : 0.696048
2024-05-01 01:49:31,121 - trainer - INFO -     test_precision : 0.728065
2024-05-01 01:49:31,121 - trainer - INFO -     test_recall    : 0.710445
2024-05-01 01:49:31,121 - trainer - INFO -     test_doc_entropy: 2.897726
2024-05-01 02:05:09,480 - trainer - INFO -     epoch          : 2
2024-05-01 02:05:09,496 - trainer - INFO -     loss           : 0.789562
2024-05-01 02:05:09,496 - trainer - INFO -     accuracy       : 0.759821
2024-05-01 02:05:09,496 - trainer - INFO -     macro_f        : 0.751724
2024-05-01 02:05:09,496 - trainer - INFO -     precision      : 0.784949
2024-05-01 02:05:09,496 - trainer - INFO -     recall         : 0.759821
2024-05-01 02:05:09,496 - trainer - INFO -     doc_entropy    : 2.593135
2024-05-01 02:05:09,496 - trainer - INFO -     val_loss       : 0.943793
2024-05-01 02:05:09,496 - trainer - INFO -     val_accuracy   : 0.720153
2024-05-01 02:05:09,496 - trainer - INFO -     val_macro_f    : 0.710111
2024-05-01 02:05:09,496 - trainer - INFO -     val_precision  : 0.743606
2024-05-01 02:05:09,496 - trainer - INFO -     val_recall     : 0.720153
2024-05-01 02:05:09,496 - trainer - INFO -     val_doc_entropy: 2.823629
2024-05-01 02:05:09,496 - trainer - INFO -     test_loss      : 0.936306
2024-05-01 02:05:09,496 - trainer - INFO -     test_accuracy  : 0.721348
2024-05-01 02:05:09,496 - trainer - INFO -     test_macro_f   : 0.711364
2024-05-01 02:05:09,496 - trainer - INFO -     test_precision : 0.745751
2024-05-01 02:05:09,496 - trainer - INFO -     test_recall    : 0.721348
2024-05-01 02:05:09,496 - trainer - INFO -     test_doc_entropy: 2.825874
2024-05-01 02:06:49,655 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(768, 768)
    (lstm): LSTM(768, 768)
  )
  (W_k): Linear(in_features=768, out_features=50, bias=False)
  (W_q): Linear(in_features=768, out_features=50, bias=False)
  (W_v): Linear(in_features=768, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 41,388,053
Freeze params: 0
2024-05-01 02:22:29,017 - trainer - INFO -     epoch          : 1
2024-05-01 02:22:29,033 - trainer - INFO -     loss           : 1.143942
2024-05-01 02:22:29,033 - trainer - INFO -     accuracy       : 0.666926
2024-05-01 02:22:29,033 - trainer - INFO -     macro_f        : 0.649733
2024-05-01 02:22:29,033 - trainer - INFO -     precision      : 0.681925
2024-05-01 02:22:29,033 - trainer - INFO -     recall         : 0.666926
2024-05-01 02:22:29,033 - trainer - INFO -     doc_entropy    : 2.582094
2024-05-01 02:22:29,033 - trainer - INFO -     val_loss       : 0.965655
2024-05-01 02:22:29,033 - trainer - INFO -     val_accuracy   : 0.713631
2024-05-01 02:22:29,033 - trainer - INFO -     val_macro_f    : 0.698127
2024-05-01 02:22:29,033 - trainer - INFO -     val_precision  : 0.730721
2024-05-01 02:22:29,033 - trainer - INFO -     val_recall     : 0.713631
2024-05-01 02:22:29,033 - trainer - INFO -     val_doc_entropy: 2.855117
2024-05-01 02:22:29,033 - trainer - INFO -     test_loss      : 0.9632
2024-05-01 02:22:29,033 - trainer - INFO -     test_accuracy  : 0.707707
2024-05-01 02:22:29,033 - trainer - INFO -     test_macro_f   : 0.691948
2024-05-01 02:22:29,033 - trainer - INFO -     test_precision : 0.724168
2024-05-01 02:22:29,033 - trainer - INFO -     test_recall    : 0.707707
2024-05-01 02:22:29,033 - trainer - INFO -     test_doc_entropy: 2.856935
2024-05-01 02:38:09,639 - trainer - INFO -     epoch          : 2
2024-05-01 02:38:09,639 - trainer - INFO -     loss           : 0.788948
2024-05-01 02:38:09,639 - trainer - INFO -     accuracy       : 0.760201
2024-05-01 02:38:09,639 - trainer - INFO -     macro_f        : 0.752138
2024-05-01 02:38:09,639 - trainer - INFO -     precision      : 0.785164
2024-05-01 02:38:09,639 - trainer - INFO -     recall         : 0.760201
2024-05-01 02:38:09,639 - trainer - INFO -     doc_entropy    : 2.559693
2024-05-01 02:38:09,639 - trainer - INFO -     val_loss       : 0.941747
2024-05-01 02:38:09,639 - trainer - INFO -     val_accuracy   : 0.720402
2024-05-01 02:38:09,639 - trainer - INFO -     val_macro_f    : 0.712051
2024-05-01 02:38:09,639 - trainer - INFO -     val_precision  : 0.748358
2024-05-01 02:38:09,639 - trainer - INFO -     val_recall     : 0.720402
2024-05-01 02:38:09,639 - trainer - INFO -     val_doc_entropy: 2.824202
2024-05-01 02:38:09,639 - trainer - INFO -     test_loss      : 0.937784
2024-05-01 02:38:09,639 - trainer - INFO -     test_accuracy  : 0.719954
2024-05-01 02:38:09,639 - trainer - INFO -     test_macro_f   : 0.712377
2024-05-01 02:38:09,639 - trainer - INFO -     test_precision : 0.750423
2024-05-01 02:38:09,639 - trainer - INFO -     test_recall    : 0.719954
2024-05-01 02:38:09,639 - trainer - INFO -     test_doc_entropy: 2.825195
2024-05-01 02:39:47,775 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(768, 768)
    (lstm): LSTM(768, 768)
  )
  (W_k): Linear(in_features=768, out_features=50, bias=False)
  (W_q): Linear(in_features=768, out_features=50, bias=False)
  (W_v): Linear(in_features=768, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 41,388,053
Freeze params: 0
2024-05-01 02:55:30,625 - trainer - INFO -     epoch          : 1
2024-05-01 02:55:30,625 - trainer - INFO -     loss           : 1.145226
2024-05-01 02:55:30,625 - trainer - INFO -     accuracy       : 0.66771
2024-05-01 02:55:30,625 - trainer - INFO -     macro_f        : 0.650444
2024-05-01 02:55:30,625 - trainer - INFO -     precision      : 0.68285
2024-05-01 02:55:30,625 - trainer - INFO -     recall         : 0.66771
2024-05-01 02:55:30,625 - trainer - INFO -     doc_entropy    : 2.620157
2024-05-01 02:55:30,625 - trainer - INFO -     val_loss       : 0.966342
2024-05-01 02:55:30,625 - trainer - INFO -     val_accuracy   : 0.711192
2024-05-01 02:55:30,625 - trainer - INFO -     val_macro_f    : 0.703915
2024-05-01 02:55:30,625 - trainer - INFO -     val_precision  : 0.741938
2024-05-01 02:55:30,625 - trainer - INFO -     val_recall     : 0.711192
2024-05-01 02:55:30,625 - trainer - INFO -     val_doc_entropy: 2.892865
2024-05-01 02:55:30,625 - trainer - INFO -     test_loss      : 0.959918
2024-05-01 02:55:30,625 - trainer - INFO -     test_accuracy  : 0.709599
2024-05-01 02:55:30,625 - trainer - INFO -     test_macro_f   : 0.703568
2024-05-01 02:55:30,625 - trainer - INFO -     test_precision : 0.742462
2024-05-01 02:55:30,625 - trainer - INFO -     test_recall    : 0.709599
2024-05-01 02:55:30,640 - trainer - INFO -     test_doc_entropy: 2.895991
2024-05-01 03:11:02,705 - trainer - INFO -     epoch          : 2
2024-05-01 03:11:02,705 - trainer - INFO -     loss           : 0.789485
2024-05-01 03:11:02,705 - trainer - INFO -     accuracy       : 0.760082
2024-05-01 03:11:02,705 - trainer - INFO -     macro_f        : 0.751803
2024-05-01 03:11:02,705 - trainer - INFO -     precision      : 0.78493
2024-05-01 03:11:02,705 - trainer - INFO -     recall         : 0.760082
2024-05-01 03:11:02,705 - trainer - INFO -     doc_entropy    : 2.597382
2024-05-01 03:11:02,705 - trainer - INFO -     val_loss       : 0.955225
2024-05-01 03:11:02,705 - trainer - INFO -     val_accuracy   : 0.717365
2024-05-01 03:11:02,705 - trainer - INFO -     val_macro_f    : 0.711899
2024-05-01 03:11:02,705 - trainer - INFO -     val_precision  : 0.753028
2024-05-01 03:11:02,705 - trainer - INFO -     val_recall     : 0.717365
2024-05-01 03:11:02,705 - trainer - INFO -     val_doc_entropy: 2.856534
2024-05-01 03:11:02,705 - trainer - INFO -     test_loss      : 0.950042
2024-05-01 03:11:02,721 - trainer - INFO -     test_accuracy  : 0.721298
2024-05-01 03:11:02,721 - trainer - INFO -     test_macro_f   : 0.716019
2024-05-01 03:11:02,721 - trainer - INFO -     test_precision : 0.757117
2024-05-01 03:11:02,721 - trainer - INFO -     test_recall    : 0.721298
2024-05-01 03:11:02,721 - trainer - INFO -     test_doc_entropy: 2.859611
2024-05-01 03:12:40,056 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(768, 768)
    (lstm): LSTM(768, 768)
  )
  (W_k): Linear(in_features=768, out_features=50, bias=False)
  (W_q): Linear(in_features=768, out_features=50, bias=False)
  (W_v): Linear(in_features=768, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 41,388,053
Freeze params: 0
2024-05-01 13:22:42,550 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(768, 768)
    (lstm): LSTM(768, 768)
  )
  (W_k): Linear(in_features=768, out_features=50, bias=False)
  (W_q): Linear(in_features=768, out_features=50, bias=False)
  (W_v): Linear(in_features=768, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 41,388,053
Freeze params: 0
2024-05-01 13:37:48,780 - trainer - INFO -     epoch          : 1
2024-05-01 13:37:48,780 - trainer - INFO -     loss           : 1.14342
2024-05-01 13:37:48,780 - trainer - INFO -     accuracy       : 0.668494
2024-05-01 13:37:48,780 - trainer - INFO -     macro_f        : 0.651101
2024-05-01 13:37:48,780 - trainer - INFO -     precision      : 0.683085
2024-05-01 13:37:48,780 - trainer - INFO -     recall         : 0.668494
2024-05-01 13:37:48,780 - trainer - INFO -     doc_entropy    : 2.638451
2024-05-01 13:37:48,780 - trainer - INFO -     val_loss       : 0.970405
2024-05-01 13:37:48,780 - trainer - INFO -     val_accuracy   : 0.710993
2024-05-01 13:37:48,780 - trainer - INFO -     val_macro_f    : 0.700784
2024-05-01 13:37:48,780 - trainer - INFO -     val_precision  : 0.73607
2024-05-01 13:37:48,780 - trainer - INFO -     val_recall     : 0.710993
2024-05-01 13:37:48,780 - trainer - INFO -     val_doc_entropy: 2.88638
2024-05-01 13:37:48,780 - trainer - INFO -     test_loss      : 0.959112
2024-05-01 13:37:48,780 - trainer - INFO -     test_accuracy  : 0.710196
2024-05-01 13:37:48,780 - trainer - INFO -     test_macro_f   : 0.700629
2024-05-01 13:37:48,780 - trainer - INFO -     test_precision : 0.736391
2024-05-01 13:37:48,780 - trainer - INFO -     test_recall    : 0.710196
2024-05-01 13:37:48,780 - trainer - INFO -     test_doc_entropy: 2.886848
2024-05-01 13:53:00,544 - trainer - INFO -     epoch          : 2
2024-05-01 13:53:00,544 - trainer - INFO -     loss           : 0.791552
2024-05-01 13:53:00,544 - trainer - INFO -     accuracy       : 0.760549
2024-05-01 13:53:00,544 - trainer - INFO -     macro_f        : 0.752382
2024-05-01 13:53:00,544 - trainer - INFO -     precision      : 0.785142
2024-05-01 13:53:00,544 - trainer - INFO -     recall         : 0.760549
2024-05-01 13:53:00,544 - trainer - INFO -     doc_entropy    : 2.609156
2024-05-01 13:53:00,544 - trainer - INFO -     val_loss       : 0.943767
2024-05-01 13:53:00,544 - trainer - INFO -     val_accuracy   : 0.722742
2024-05-01 13:53:00,544 - trainer - INFO -     val_macro_f    : 0.712665
2024-05-01 13:53:00,544 - trainer - INFO -     val_precision  : 0.748329
2024-05-01 13:53:00,544 - trainer - INFO -     val_recall     : 0.722742
2024-05-01 13:53:00,544 - trainer - INFO -     val_doc_entropy: 2.826493
2024-05-01 13:53:00,544 - trainer - INFO -     test_loss      : 0.942858
2024-05-01 13:53:00,544 - trainer - INFO -     test_accuracy  : 0.721597
2024-05-01 13:53:00,544 - trainer - INFO -     test_macro_f   : 0.711381
2024-05-01 13:53:00,544 - trainer - INFO -     test_precision : 0.745049
2024-05-01 13:53:00,544 - trainer - INFO -     test_recall    : 0.721597
2024-05-01 13:53:00,544 - trainer - INFO -     test_doc_entropy: 2.826774
2024-05-01 13:54:36,696 - train - INFO - BiAttentionClassifyModel(
  (embedding): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(768, 768)
    (lstm): LSTM(768, 768)
  )
  (W_k): Linear(in_features=768, out_features=50, bias=False)
  (W_q): Linear(in_features=768, out_features=50, bias=False)
  (W_v): Linear(in_features=768, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 41,388,053
Freeze params: 0
2024-05-01 14:09:48,814 - trainer - INFO -     epoch          : 1
2024-05-01 14:09:49,675 - trainer - INFO -     loss           : 1.142689
2024-05-01 14:09:49,675 - trainer - INFO -     accuracy       : 0.669167
2024-05-01 14:09:49,675 - trainer - INFO -     macro_f        : 0.652417
2024-05-01 14:09:49,675 - trainer - INFO -     precision      : 0.68513
2024-05-01 14:09:49,675 - trainer - INFO -     recall         : 0.669167
2024-05-01 14:09:49,675 - trainer - INFO -     doc_entropy    : 2.627142
2024-05-01 14:09:49,675 - trainer - INFO -     val_loss       : 0.965932
2024-05-01 14:09:49,675 - trainer - INFO -     val_accuracy   : 0.714776
2024-05-01 14:09:49,675 - trainer - INFO -     val_macro_f    : 0.701953
2024-05-01 14:09:49,691 - trainer - INFO -     val_precision  : 0.733142
2024-05-01 14:09:49,691 - trainer - INFO -     val_recall     : 0.714776
2024-05-01 14:09:49,691 - trainer - INFO -     val_doc_entropy: 2.889577
2024-05-01 14:09:49,691 - trainer - INFO -     test_loss      : 0.955455
2024-05-01 14:09:49,691 - trainer - INFO -     test_accuracy  : 0.713582
2024-05-01 14:09:49,691 - trainer - INFO -     test_macro_f   : 0.702043
2024-05-01 14:09:49,691 - trainer - INFO -     test_precision : 0.736431
2024-05-01 14:09:49,691 - trainer - INFO -     test_recall    : 0.713582
2024-05-01 14:09:49,691 - trainer - INFO -     test_doc_entropy: 2.892255
2024-05-01 14:25:00,367 - trainer - INFO -     epoch          : 2
2024-05-01 14:25:00,367 - trainer - INFO -     loss           : 0.788529
2024-05-01 14:25:00,367 - trainer - INFO -     accuracy       : 0.761719
2024-05-01 14:25:00,367 - trainer - INFO -     macro_f        : 0.75341
2024-05-01 14:25:00,367 - trainer - INFO -     precision      : 0.785902
2024-05-01 14:25:00,367 - trainer - INFO -     recall         : 0.761719
2024-05-01 14:25:00,367 - trainer - INFO -     doc_entropy    : 2.596673
2024-05-01 14:25:00,367 - trainer - INFO -     val_loss       : 0.949124
2024-05-01 14:25:00,367 - trainer - INFO -     val_accuracy   : 0.719407
2024-05-01 14:25:00,367 - trainer - INFO -     val_macro_f    : 0.716488
2024-05-01 14:25:00,367 - trainer - INFO -     val_precision  : 0.760468
2024-05-01 14:25:00,367 - trainer - INFO -     val_recall     : 0.719407
2024-05-01 14:25:00,367 - trainer - INFO -     val_doc_entropy: 2.840108
2024-05-01 14:25:00,367 - trainer - INFO -     test_loss      : 0.944046
2024-05-01 14:25:00,367 - trainer - INFO -     test_accuracy  : 0.719257
2024-05-01 14:25:00,367 - trainer - INFO -     test_macro_f   : 0.717291
2024-05-01 14:25:00,367 - trainer - INFO -     test_precision : 0.763251
2024-05-01 14:25:00,367 - trainer - INFO -     test_recall    : 0.719257
2024-05-01 14:25:00,367 - trainer - INFO -     test_doc_entropy: 2.843034
2024-05-01 14:28:03,846 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(768, 768)
    (lstm): LSTM(768, 768)
  )
  (W_k): Linear(in_features=768, out_features=50, bias=False)
  (W_q): Linear(in_features=768, out_features=50, bias=False)
  (W_v): Linear(in_features=768, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 40,795,925
Freeze params: 0
2024-05-01 14:43:04,756 - trainer - INFO -     epoch          : 1
2024-05-01 14:43:04,756 - trainer - INFO -     loss           : 1.125066
2024-05-01 14:43:04,756 - trainer - INFO -     accuracy       : 0.671245
2024-05-01 14:43:04,756 - trainer - INFO -     macro_f        : 0.655045
2024-05-01 14:43:04,756 - trainer - INFO -     precision      : 0.687676
2024-05-01 14:43:04,756 - trainer - INFO -     recall         : 0.671245
2024-05-01 14:43:04,756 - trainer - INFO -     doc_entropy    : 2.548638
2024-05-01 14:43:04,756 - trainer - INFO -     val_loss       : 0.955377
2024-05-01 14:43:04,756 - trainer - INFO -     val_accuracy   : 0.716071
2024-05-01 14:43:04,756 - trainer - INFO -     val_macro_f    : 0.701603
2024-05-01 14:43:04,756 - trainer - INFO -     val_precision  : 0.731979
2024-05-01 14:43:04,756 - trainer - INFO -     val_recall     : 0.716071
2024-05-01 14:43:04,756 - trainer - INFO -     val_doc_entropy: 2.803732
2024-05-01 14:43:04,756 - trainer - INFO -     test_loss      : 0.948998
2024-05-01 14:43:04,756 - trainer - INFO -     test_accuracy  : 0.712188
2024-05-01 14:43:04,756 - trainer - INFO -     test_macro_f   : 0.698116
2024-05-01 14:43:04,756 - trainer - INFO -     test_precision : 0.730975
2024-05-01 14:43:04,756 - trainer - INFO -     test_recall    : 0.712188
2024-05-01 14:43:04,756 - trainer - INFO -     test_doc_entropy: 2.804978
2024-05-01 14:58:12,944 - trainer - INFO -     epoch          : 2
2024-05-01 14:58:12,944 - trainer - INFO -     loss           : 0.774911
2024-05-01 14:58:12,944 - trainer - INFO -     accuracy       : 0.764551
2024-05-01 14:58:12,944 - trainer - INFO -     macro_f        : 0.756658
2024-05-01 14:58:12,944 - trainer - INFO -     precision      : 0.789679
2024-05-01 14:58:12,944 - trainer - INFO -     recall         : 0.764551
2024-05-01 14:58:12,944 - trainer - INFO -     doc_entropy    : 2.502539
2024-05-01 14:58:12,944 - trainer - INFO -     val_loss       : 0.930996
2024-05-01 14:58:12,944 - trainer - INFO -     val_accuracy   : 0.726924
2024-05-01 14:58:12,944 - trainer - INFO -     val_macro_f    : 0.715596
2024-05-01 14:58:12,944 - trainer - INFO -     val_precision  : 0.747095
2024-05-01 14:58:12,944 - trainer - INFO -     val_recall     : 0.726924
2024-05-01 14:58:12,944 - trainer - INFO -     val_doc_entropy: 2.749132
2024-05-01 14:58:12,944 - trainer - INFO -     test_loss      : 0.926534
2024-05-01 14:58:12,944 - trainer - INFO -     test_accuracy  : 0.721746
2024-05-01 14:58:12,944 - trainer - INFO -     test_macro_f   : 0.710015
2024-05-01 14:58:12,944 - trainer - INFO -     test_precision : 0.74416
2024-05-01 14:58:12,944 - trainer - INFO -     test_recall    : 0.721746
2024-05-01 14:58:12,944 - trainer - INFO -     test_doc_entropy: 2.74862
2024-05-01 14:59:48,312 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(768, 768)
    (lstm): LSTM(768, 768)
  )
  (W_k): Linear(in_features=768, out_features=50, bias=False)
  (W_q): Linear(in_features=768, out_features=50, bias=False)
  (W_v): Linear(in_features=768, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 40,795,925
Freeze params: 0
2024-05-01 15:14:56,083 - trainer - INFO -     epoch          : 1
2024-05-01 15:14:56,083 - trainer - INFO -     loss           : 1.126867
2024-05-01 15:14:56,083 - trainer - INFO -     accuracy       : 0.671799
2024-05-01 15:14:56,083 - trainer - INFO -     macro_f        : 0.65534
2024-05-01 15:14:56,083 - trainer - INFO -     precision      : 0.688112
2024-05-01 15:14:56,083 - trainer - INFO -     recall         : 0.671799
2024-05-01 15:14:56,083 - trainer - INFO -     doc_entropy    : 2.489522
2024-05-01 15:14:56,083 - trainer - INFO -     val_loss       : 0.95534
2024-05-01 15:14:56,083 - trainer - INFO -     val_accuracy   : 0.716619
2024-05-01 15:14:56,083 - trainer - INFO -     val_macro_f    : 0.702434
2024-05-01 15:14:56,083 - trainer - INFO -     val_precision  : 0.735164
2024-05-01 15:14:56,083 - trainer - INFO -     val_recall     : 0.716619
2024-05-01 15:14:56,083 - trainer - INFO -     val_doc_entropy: 2.788405
2024-05-01 15:14:56,083 - trainer - INFO -     test_loss      : 0.948285
2024-05-01 15:14:56,083 - trainer - INFO -     test_accuracy  : 0.712735
2024-05-01 15:14:56,083 - trainer - INFO -     test_macro_f   : 0.698208
2024-05-01 15:14:56,083 - trainer - INFO -     test_precision : 0.731161
2024-05-01 15:14:56,083 - trainer - INFO -     test_recall    : 0.712735
2024-05-01 15:14:56,083 - trainer - INFO -     test_doc_entropy: 2.79056
2024-05-01 15:30:04,558 - trainer - INFO -     epoch          : 2
2024-05-01 15:30:04,558 - trainer - INFO -     loss           : 0.777563
2024-05-01 15:30:04,558 - trainer - INFO -     accuracy       : 0.763879
2024-05-01 15:30:04,558 - trainer - INFO -     macro_f        : 0.755877
2024-05-01 15:30:04,558 - trainer - INFO -     precision      : 0.788271
2024-05-01 15:30:04,558 - trainer - INFO -     recall         : 0.763879
2024-05-01 15:30:04,558 - trainer - INFO -     doc_entropy    : 2.456304
2024-05-01 15:30:04,558 - trainer - INFO -     val_loss       : 0.934847
2024-05-01 15:30:04,558 - trainer - INFO -     val_accuracy   : 0.724286
2024-05-01 15:30:04,573 - trainer - INFO -     val_macro_f    : 0.71624
2024-05-01 15:30:04,573 - trainer - INFO -     val_precision  : 0.7525
2024-05-01 15:30:04,573 - trainer - INFO -     val_recall     : 0.724286
2024-05-01 15:30:04,573 - trainer - INFO -     val_doc_entropy: 2.706261
2024-05-01 15:30:04,573 - trainer - INFO -     test_loss      : 0.923109
2024-05-01 15:30:04,573 - trainer - INFO -     test_accuracy  : 0.723937
2024-05-01 15:30:04,573 - trainer - INFO -     test_macro_f   : 0.717366
2024-05-01 15:30:04,573 - trainer - INFO -     test_precision : 0.754973
2024-05-01 15:30:04,573 - trainer - INFO -     test_recall    : 0.723937
2024-05-01 15:30:04,573 - trainer - INFO -     test_doc_entropy: 2.705336
2024-05-01 15:31:39,065 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(768, 768)
    (lstm): LSTM(768, 768)
  )
  (W_k): Linear(in_features=768, out_features=50, bias=False)
  (W_q): Linear(in_features=768, out_features=50, bias=False)
  (W_v): Linear(in_features=768, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 40,795,925
Freeze params: 0
2024-05-01 15:46:44,836 - trainer - INFO -     epoch          : 1
2024-05-01 15:46:44,836 - trainer - INFO -     loss           : 1.126598
2024-05-01 15:46:44,836 - trainer - INFO -     accuracy       : 0.672359
2024-05-01 15:46:44,836 - trainer - INFO -     macro_f        : 0.655487
2024-05-01 15:46:44,836 - trainer - INFO -     precision      : 0.688344
2024-05-01 15:46:44,836 - trainer - INFO -     recall         : 0.672359
2024-05-01 15:46:44,836 - trainer - INFO -     doc_entropy    : 2.52837
2024-05-01 15:46:44,836 - trainer - INFO -     val_loss       : 0.956143
2024-05-01 15:46:44,836 - trainer - INFO -     val_accuracy   : 0.713183
2024-05-01 15:46:44,836 - trainer - INFO -     val_macro_f    : 0.705273
2024-05-01 15:46:44,836 - trainer - INFO -     val_precision  : 0.741927
2024-05-01 15:46:44,836 - trainer - INFO -     val_recall     : 0.713183
2024-05-01 15:46:44,836 - trainer - INFO -     val_doc_entropy: 2.831215
2024-05-01 15:46:44,836 - trainer - INFO -     test_loss      : 0.949983
2024-05-01 15:46:44,836 - trainer - INFO -     test_accuracy  : 0.709897
2024-05-01 15:46:44,836 - trainer - INFO -     test_macro_f   : 0.703854
2024-05-01 15:46:44,836 - trainer - INFO -     test_precision : 0.74288
2024-05-01 15:46:44,836 - trainer - INFO -     test_recall    : 0.709897
2024-05-01 15:46:44,836 - trainer - INFO -     test_doc_entropy: 2.832269
2024-05-01 16:01:52,696 - trainer - INFO -     epoch          : 2
2024-05-01 16:01:52,711 - trainer - INFO -     loss           : 0.775047
2024-05-01 16:01:52,711 - trainer - INFO -     accuracy       : 0.764209
2024-05-01 16:01:52,711 - trainer - INFO -     macro_f        : 0.756011
2024-05-01 16:01:52,711 - trainer - INFO -     precision      : 0.788704
2024-05-01 16:01:52,711 - trainer - INFO -     recall         : 0.764209
2024-05-01 16:01:52,711 - trainer - INFO -     doc_entropy    : 2.459924
2024-05-01 16:01:52,711 - trainer - INFO -     val_loss       : 0.944786
2024-05-01 16:01:52,711 - trainer - INFO -     val_accuracy   : 0.722045
2024-05-01 16:01:52,711 - trainer - INFO -     val_macro_f    : 0.715681
2024-05-01 16:01:52,711 - trainer - INFO -     val_precision  : 0.754701
2024-05-01 16:01:52,711 - trainer - INFO -     val_recall     : 0.722045
2024-05-01 16:01:52,711 - trainer - INFO -     val_doc_entropy: 2.726884
2024-05-01 16:01:52,711 - trainer - INFO -     test_loss      : 0.937594
2024-05-01 16:01:52,711 - trainer - INFO -     test_accuracy  : 0.722892
2024-05-01 16:01:52,711 - trainer - INFO -     test_macro_f   : 0.717141
2024-05-01 16:01:52,711 - trainer - INFO -     test_precision : 0.756437
2024-05-01 16:01:52,711 - trainer - INFO -     test_recall    : 0.722892
2024-05-01 16:01:52,711 - trainer - INFO -     test_doc_entropy: 2.72634
2024-05-01 16:03:28,542 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(768, 768)
    (lstm): LSTM(768, 768)
  )
  (W_k): Linear(in_features=768, out_features=50, bias=False)
  (W_q): Linear(in_features=768, out_features=50, bias=False)
  (W_v): Linear(in_features=768, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 40,795,925
Freeze params: 0
2024-05-01 16:18:34,877 - trainer - INFO -     epoch          : 1
2024-05-01 16:18:34,877 - trainer - INFO -     loss           : 1.125856
2024-05-01 16:18:34,877 - trainer - INFO -     accuracy       : 0.672123
2024-05-01 16:18:34,877 - trainer - INFO -     macro_f        : 0.655363
2024-05-01 16:18:34,877 - trainer - INFO -     precision      : 0.687793
2024-05-01 16:18:34,877 - trainer - INFO -     recall         : 0.672123
2024-05-01 16:18:34,877 - trainer - INFO -     doc_entropy    : 2.541666
2024-05-01 16:18:34,877 - trainer - INFO -     val_loss       : 0.954735
2024-05-01 16:18:34,877 - trainer - INFO -     val_accuracy   : 0.716569
2024-05-01 16:18:34,877 - trainer - INFO -     val_macro_f    : 0.706492
2024-05-01 16:18:34,877 - trainer - INFO -     val_precision  : 0.740329
2024-05-01 16:18:34,877 - trainer - INFO -     val_recall     : 0.716569
2024-05-01 16:18:34,877 - trainer - INFO -     val_doc_entropy: 2.78208
2024-05-01 16:18:34,877 - trainer - INFO -     test_loss      : 0.940885
2024-05-01 16:18:34,877 - trainer - INFO -     test_accuracy  : 0.714528
2024-05-01 16:18:34,877 - trainer - INFO -     test_macro_f   : 0.705393
2024-05-01 16:18:34,877 - trainer - INFO -     test_precision : 0.74137
2024-05-01 16:18:34,877 - trainer - INFO -     test_recall    : 0.714528
2024-05-01 16:18:34,877 - trainer - INFO -     test_doc_entropy: 2.783814
2024-05-01 16:33:42,019 - trainer - INFO -     epoch          : 2
2024-05-01 16:33:42,019 - trainer - INFO -     loss           : 0.773432
2024-05-01 16:33:42,019 - trainer - INFO -     accuracy       : 0.764987
2024-05-01 16:33:42,019 - trainer - INFO -     macro_f        : 0.756981
2024-05-01 16:33:42,019 - trainer - INFO -     precision      : 0.789595
2024-05-01 16:33:42,019 - trainer - INFO -     recall         : 0.764987
2024-05-01 16:33:42,019 - trainer - INFO -     doc_entropy    : 2.505815
2024-05-01 16:33:42,019 - trainer - INFO -     val_loss       : 0.933944
2024-05-01 16:33:42,019 - trainer - INFO -     val_accuracy   : 0.72568
2024-05-01 16:33:42,019 - trainer - INFO -     val_macro_f    : 0.716449
2024-05-01 16:33:42,019 - trainer - INFO -     val_precision  : 0.751211
2024-05-01 16:33:42,019 - trainer - INFO -     val_recall     : 0.72568
2024-05-01 16:33:42,019 - trainer - INFO -     val_doc_entropy: 2.745465
2024-05-01 16:33:42,019 - trainer - INFO -     test_loss      : 0.931509
2024-05-01 16:33:42,019 - trainer - INFO -     test_accuracy  : 0.721995
2024-05-01 16:33:42,019 - trainer - INFO -     test_macro_f   : 0.712443
2024-05-01 16:33:42,019 - trainer - INFO -     test_precision : 0.747162
2024-05-01 16:33:42,019 - trainer - INFO -     test_recall    : 0.721995
2024-05-01 16:33:42,019 - trainer - INFO -     test_doc_entropy: 2.745396
2024-05-01 16:35:16,387 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(768, 768)
    (lstm): LSTM(768, 768)
  )
  (W_k): Linear(in_features=768, out_features=50, bias=False)
  (W_q): Linear(in_features=768, out_features=50, bias=False)
  (W_v): Linear(in_features=768, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 40,795,925
Freeze params: 0
2024-05-01 16:50:22,706 - trainer - INFO -     epoch          : 1
2024-05-01 16:50:22,706 - trainer - INFO -     loss           : 1.125623
2024-05-01 16:50:22,706 - trainer - INFO -     accuracy       : 0.672135
2024-05-01 16:50:22,706 - trainer - INFO -     macro_f        : 0.655796
2024-05-01 16:50:22,706 - trainer - INFO -     precision      : 0.688552
2024-05-01 16:50:22,706 - trainer - INFO -     recall         : 0.672135
2024-05-01 16:50:22,706 - trainer - INFO -     doc_entropy    : 2.531634
2024-05-01 16:50:22,706 - trainer - INFO -     val_loss       : 0.94956
2024-05-01 16:50:22,706 - trainer - INFO -     val_accuracy   : 0.717515
2024-05-01 16:50:22,706 - trainer - INFO -     val_macro_f    : 0.705308
2024-05-01 16:50:22,706 - trainer - INFO -     val_precision  : 0.736169
2024-05-01 16:50:22,706 - trainer - INFO -     val_recall     : 0.717515
2024-05-01 16:50:22,706 - trainer - INFO -     val_doc_entropy: 2.798837
2024-05-01 16:50:22,706 - trainer - INFO -     test_loss      : 0.942203
2024-05-01 16:50:22,706 - trainer - INFO -     test_accuracy  : 0.715025
2024-05-01 16:50:22,706 - trainer - INFO -     test_macro_f   : 0.704113
2024-05-01 16:50:22,706 - trainer - INFO -     test_precision : 0.738496
2024-05-01 16:50:22,706 - trainer - INFO -     test_recall    : 0.715025
2024-05-01 16:50:22,706 - trainer - INFO -     test_doc_entropy: 2.800532
2024-05-01 17:05:32,148 - trainer - INFO -     epoch          : 2
2024-05-01 17:05:32,148 - trainer - INFO -     loss           : 0.773522
2024-05-01 17:05:32,148 - trainer - INFO -     accuracy       : 0.764358
2024-05-01 17:05:32,148 - trainer - INFO -     macro_f        : 0.755915
2024-05-01 17:05:32,148 - trainer - INFO -     precision      : 0.787875
2024-05-01 17:05:32,148 - trainer - INFO -     recall         : 0.764358
2024-05-01 17:05:32,148 - trainer - INFO -     doc_entropy    : 2.47934
2024-05-01 17:05:32,148 - trainer - INFO -     val_loss       : 0.93185
2024-05-01 17:05:32,148 - trainer - INFO -     val_accuracy   : 0.723439
2024-05-01 17:05:32,148 - trainer - INFO -     val_macro_f    : 0.719098
2024-05-01 17:05:32,148 - trainer - INFO -     val_precision  : 0.759635
2024-05-01 17:05:32,148 - trainer - INFO -     val_recall     : 0.723439
2024-05-01 17:05:32,148 - trainer - INFO -     val_doc_entropy: 2.69272
2024-05-01 17:05:32,148 - trainer - INFO -     test_loss      : 0.928748
2024-05-01 17:05:32,148 - trainer - INFO -     test_accuracy  : 0.720303
2024-05-01 17:05:32,148 - trainer - INFO -     test_macro_f   : 0.716965
2024-05-01 17:05:32,148 - trainer - INFO -     test_precision : 0.759411
2024-05-01 17:05:32,148 - trainer - INFO -     test_recall    : 0.720303
2024-05-01 17:05:32,148 - trainer - INFO -     test_doc_entropy: 2.692315
2024-05-01 17:08:02,309 - train - INFO - BiAttentionClassifyModel(
  (embedding): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(768, 768)
    (lstm): LSTM(768, 768)
  )
  (W_k): Linear(in_features=768, out_features=50, bias=False)
  (W_q): Linear(in_features=768, out_features=50, bias=False)
  (W_v): Linear(in_features=768, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 56,551,445
Freeze params: 0
2024-05-01 17:23:48,709 - trainer - INFO -     epoch          : 1
2024-05-01 17:23:48,709 - trainer - INFO -     loss           : 1.180169
2024-05-01 17:23:48,709 - trainer - INFO -     accuracy       : 0.656576
2024-05-01 17:23:48,709 - trainer - INFO -     macro_f        : 0.637316
2024-05-01 17:23:48,709 - trainer - INFO -     precision      : 0.668136
2024-05-01 17:23:48,709 - trainer - INFO -     recall         : 0.656576
2024-05-01 17:23:48,709 - trainer - INFO -     doc_entropy    : 2.745197
2024-05-01 17:23:48,709 - trainer - INFO -     val_loss       : 0.992748
2024-05-01 17:23:48,709 - trainer - INFO -     val_accuracy   : 0.706313
2024-05-01 17:23:48,709 - trainer - INFO -     val_macro_f    : 0.691505
2024-05-01 17:23:48,709 - trainer - INFO -     val_precision  : 0.722212
2024-05-01 17:23:48,709 - trainer - INFO -     val_recall     : 0.706313
2024-05-01 17:23:48,709 - trainer - INFO -     val_doc_entropy: 2.914072
2024-05-01 17:23:48,709 - trainer - INFO -     test_loss      : 0.995862
2024-05-01 17:23:48,709 - trainer - INFO -     test_accuracy  : 0.704321
2024-05-01 17:23:48,709 - trainer - INFO -     test_macro_f   : 0.690208
2024-05-01 17:23:48,709 - trainer - INFO -     test_precision : 0.723143
2024-05-01 17:23:48,709 - trainer - INFO -     test_recall    : 0.704321
2024-05-01 17:23:48,709 - trainer - INFO -     test_doc_entropy: 2.916281
2024-05-01 17:39:38,795 - trainer - INFO -     epoch          : 2
2024-05-01 17:39:38,795 - trainer - INFO -     loss           : 0.870064
2024-05-01 17:39:38,795 - trainer - INFO -     accuracy       : 0.737497
2024-05-01 17:39:38,795 - trainer - INFO -     macro_f        : 0.727894
2024-05-01 17:39:38,795 - trainer - INFO -     precision      : 0.762328
2024-05-01 17:39:38,795 - trainer - INFO -     recall         : 0.737497
2024-05-01 17:39:38,795 - trainer - INFO -     doc_entropy    : 2.692869
2024-05-01 17:39:38,795 - trainer - INFO -     val_loss       : 0.937665
2024-05-01 17:39:38,795 - trainer - INFO -     val_accuracy   : 0.721746
2024-05-01 17:39:38,795 - trainer - INFO -     val_macro_f    : 0.709822
2024-05-01 17:39:38,795 - trainer - INFO -     val_precision  : 0.741734
2024-05-01 17:39:38,795 - trainer - INFO -     val_recall     : 0.721746
2024-05-01 17:39:38,795 - trainer - INFO -     val_doc_entropy: 2.904473
2024-05-01 17:39:38,795 - trainer - INFO -     test_loss      : 0.941732
2024-05-01 17:39:38,795 - trainer - INFO -     test_accuracy  : 0.720452
2024-05-01 17:39:38,795 - trainer - INFO -     test_macro_f   : 0.708743
2024-05-01 17:39:38,795 - trainer - INFO -     test_precision : 0.744478
2024-05-01 17:39:38,795 - trainer - INFO -     test_recall    : 0.720452
2024-05-01 17:39:38,795 - trainer - INFO -     test_doc_entropy: 2.90583
2024-05-01 17:41:13,880 - train - INFO - BiAttentionClassifyModel(
  (embedding): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(768, 768)
    (lstm): LSTM(768, 768)
  )
  (W_k): Linear(in_features=768, out_features=50, bias=False)
  (W_q): Linear(in_features=768, out_features=50, bias=False)
  (W_v): Linear(in_features=768, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 56,551,445
Freeze params: 0
2024-05-01 17:57:00,545 - trainer - INFO -     epoch          : 1
2024-05-01 17:57:00,545 - trainer - INFO -     loss           : 1.175687
2024-05-01 17:57:00,545 - trainer - INFO -     accuracy       : 0.657914
2024-05-01 17:57:00,545 - trainer - INFO -     macro_f        : 0.638689
2024-05-01 17:57:00,545 - trainer - INFO -     precision      : 0.669701
2024-05-01 17:57:00,545 - trainer - INFO -     recall         : 0.657914
2024-05-01 17:57:00,545 - trainer - INFO -     doc_entropy    : 2.791086
2024-05-01 17:57:00,545 - trainer - INFO -     val_loss       : 0.991379
2024-05-01 17:57:00,545 - trainer - INFO -     val_accuracy   : 0.703425
2024-05-01 17:57:00,545 - trainer - INFO -     val_macro_f    : 0.689222
2024-05-01 17:57:00,545 - trainer - INFO -     val_precision  : 0.723422
2024-05-01 17:57:00,545 - trainer - INFO -     val_recall     : 0.703425
2024-05-01 17:57:00,545 - trainer - INFO -     val_doc_entropy: 3.006285
2024-05-01 17:57:00,545 - trainer - INFO -     test_loss      : 0.993831
2024-05-01 17:57:00,545 - trainer - INFO -     test_accuracy  : 0.705317
2024-05-01 17:57:00,560 - trainer - INFO -     test_macro_f   : 0.691496
2024-05-01 17:57:00,560 - trainer - INFO -     test_precision : 0.727452
2024-05-01 17:57:00,560 - trainer - INFO -     test_recall    : 0.705317
2024-05-01 17:57:00,560 - trainer - INFO -     test_doc_entropy: 3.008992
2024-05-01 18:12:48,904 - trainer - INFO -     epoch          : 2
2024-05-01 18:12:48,904 - trainer - INFO -     loss           : 0.867231
2024-05-01 18:12:48,904 - trainer - INFO -     accuracy       : 0.737746
2024-05-01 18:12:48,904 - trainer - INFO -     macro_f        : 0.728255
2024-05-01 18:12:48,904 - trainer - INFO -     precision      : 0.762336
2024-05-01 18:12:48,904 - trainer - INFO -     recall         : 0.737746
2024-05-01 18:12:48,904 - trainer - INFO -     doc_entropy    : 2.789016
2024-05-01 18:12:48,904 - trainer - INFO -     val_loss       : 0.946536
2024-05-01 18:12:48,904 - trainer - INFO -     val_accuracy   : 0.720452
2024-05-01 18:12:48,904 - trainer - INFO -     val_macro_f    : 0.709966
2024-05-01 18:12:48,904 - trainer - INFO -     val_precision  : 0.744741
2024-05-01 18:12:48,904 - trainer - INFO -     val_recall     : 0.720452
2024-05-01 18:12:48,904 - trainer - INFO -     val_doc_entropy: 2.961483
2024-05-01 18:12:48,904 - trainer - INFO -     test_loss      : 0.949692
2024-05-01 18:12:48,904 - trainer - INFO -     test_accuracy  : 0.719606
2024-05-01 18:12:48,904 - trainer - INFO -     test_macro_f   : 0.710339
2024-05-01 18:12:48,904 - trainer - INFO -     test_precision : 0.747431
2024-05-01 18:12:48,904 - trainer - INFO -     test_recall    : 0.719606
2024-05-01 18:12:48,904 - trainer - INFO -     test_doc_entropy: 2.962504
2024-05-01 18:14:24,123 - train - INFO - BiAttentionClassifyModel(
  (embedding): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(768, 768)
    (lstm): LSTM(768, 768)
  )
  (W_k): Linear(in_features=768, out_features=50, bias=False)
  (W_q): Linear(in_features=768, out_features=50, bias=False)
  (W_v): Linear(in_features=768, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 56,551,445
Freeze params: 0
2024-05-01 18:30:10,683 - trainer - INFO -     epoch          : 1
2024-05-01 18:30:10,683 - trainer - INFO -     loss           : 1.178396
2024-05-01 18:30:10,683 - trainer - INFO -     accuracy       : 0.657995
2024-05-01 18:30:10,683 - trainer - INFO -     macro_f        : 0.639089
2024-05-01 18:30:10,683 - trainer - INFO -     precision      : 0.670688
2024-05-01 18:30:10,683 - trainer - INFO -     recall         : 0.657995
2024-05-01 18:30:10,683 - trainer - INFO -     doc_entropy    : 2.778893
2024-05-01 18:30:10,683 - trainer - INFO -     val_loss       : 1.001013
2024-05-01 18:30:10,683 - trainer - INFO -     val_accuracy   : 0.698795
2024-05-01 18:30:10,683 - trainer - INFO -     val_macro_f    : 0.69156
2024-05-01 18:30:10,683 - trainer - INFO -     val_precision  : 0.7319
2024-05-01 18:30:10,683 - trainer - INFO -     val_recall     : 0.698795
2024-05-01 18:30:10,683 - trainer - INFO -     val_doc_entropy: 2.968149
2024-05-01 18:30:10,683 - trainer - INFO -     test_loss      : 0.999873
2024-05-01 18:30:10,683 - trainer - INFO -     test_accuracy  : 0.701036
2024-05-01 18:30:10,683 - trainer - INFO -     test_macro_f   : 0.695768
2024-05-01 18:30:10,683 - trainer - INFO -     test_precision : 0.737735
2024-05-01 18:30:10,683 - trainer - INFO -     test_recall    : 0.701036
2024-05-01 18:30:10,683 - trainer - INFO -     test_doc_entropy: 2.96919
2024-05-01 18:46:00,315 - trainer - INFO -     epoch          : 2
2024-05-01 18:46:00,315 - trainer - INFO -     loss           : 0.865478
2024-05-01 18:46:00,315 - trainer - INFO -     accuracy       : 0.739065
2024-05-01 18:46:00,315 - trainer - INFO -     macro_f        : 0.729315
2024-05-01 18:46:00,315 - trainer - INFO -     precision      : 0.763501
2024-05-01 18:46:00,315 - trainer - INFO -     recall         : 0.739065
2024-05-01 18:46:00,315 - trainer - INFO -     doc_entropy    : 2.738368
2024-05-01 18:46:00,315 - trainer - INFO -     val_loss       : 0.942393
2024-05-01 18:46:00,315 - trainer - INFO -     val_accuracy   : 0.720801
2024-05-01 18:46:00,315 - trainer - INFO -     val_macro_f    : 0.713492
2024-05-01 18:46:00,315 - trainer - INFO -     val_precision  : 0.750955
2024-05-01 18:46:00,315 - trainer - INFO -     val_recall     : 0.720801
2024-05-01 18:46:00,315 - trainer - INFO -     val_doc_entropy: 2.898368
2024-05-01 18:46:00,315 - trainer - INFO -     test_loss      : 0.952046
2024-05-01 18:46:00,315 - trainer - INFO -     test_accuracy  : 0.719904
2024-05-01 18:46:00,315 - trainer - INFO -     test_macro_f   : 0.713646
2024-05-01 18:46:00,315 - trainer - INFO -     test_precision : 0.754007
2024-05-01 18:46:00,315 - trainer - INFO -     test_recall    : 0.719904
2024-05-01 18:46:00,330 - trainer - INFO -     test_doc_entropy: 2.899006
2024-05-01 18:47:35,539 - train - INFO - BiAttentionClassifyModel(
  (embedding): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(768, 768)
    (lstm): LSTM(768, 768)
  )
  (W_k): Linear(in_features=768, out_features=50, bias=False)
  (W_q): Linear(in_features=768, out_features=50, bias=False)
  (W_v): Linear(in_features=768, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 56,551,445
Freeze params: 0
2024-05-01 19:03:24,635 - trainer - INFO -     epoch          : 1
2024-05-01 19:03:24,635 - trainer - INFO -     loss           : 1.180514
2024-05-01 19:03:24,635 - trainer - INFO -     accuracy       : 0.657441
2024-05-01 19:03:24,635 - trainer - INFO -     macro_f        : 0.638007
2024-05-01 19:03:24,635 - trainer - INFO -     precision      : 0.668818
2024-05-01 19:03:24,635 - trainer - INFO -     recall         : 0.657441
2024-05-01 19:03:24,635 - trainer - INFO -     doc_entropy    : 2.760247
2024-05-01 19:03:24,635 - trainer - INFO -     val_loss       : 0.999498
2024-05-01 19:03:24,635 - trainer - INFO -     val_accuracy   : 0.704321
2024-05-01 19:03:24,635 - trainer - INFO -     val_macro_f    : 0.693008
2024-05-01 19:03:24,635 - trainer - INFO -     val_precision  : 0.727986
2024-05-01 19:03:24,635 - trainer - INFO -     val_recall     : 0.704321
2024-05-01 19:03:24,635 - trainer - INFO -     val_doc_entropy: 2.936265
2024-05-01 19:03:24,635 - trainer - INFO -     test_loss      : 0.999367
2024-05-01 19:03:24,635 - trainer - INFO -     test_accuracy  : 0.698994
2024-05-01 19:03:24,635 - trainer - INFO -     test_macro_f   : 0.689646
2024-05-01 19:03:24,635 - trainer - INFO -     test_precision : 0.727872
2024-05-01 19:03:24,635 - trainer - INFO -     test_recall    : 0.698994
2024-05-01 19:03:24,635 - trainer - INFO -     test_doc_entropy: 2.937589
2024-05-01 19:19:16,686 - trainer - INFO -     epoch          : 2
2024-05-01 19:19:16,686 - trainer - INFO -     loss           : 0.867611
2024-05-01 19:19:16,686 - trainer - INFO -     accuracy       : 0.738461
2024-05-01 19:19:16,686 - trainer - INFO -     macro_f        : 0.729282
2024-05-01 19:19:16,686 - trainer - INFO -     precision      : 0.763484
2024-05-01 19:19:16,686 - trainer - INFO -     recall         : 0.738461
2024-05-01 19:19:16,686 - trainer - INFO -     doc_entropy    : 2.748447
2024-05-01 19:19:16,686 - trainer - INFO -     val_loss       : 0.94397
2024-05-01 19:19:16,686 - trainer - INFO -     val_accuracy   : 0.720253
2024-05-01 19:19:16,686 - trainer - INFO -     val_macro_f    : 0.709142
2024-05-01 19:19:16,686 - trainer - INFO -     val_precision  : 0.741935
2024-05-01 19:19:16,686 - trainer - INFO -     val_recall     : 0.720253
2024-05-01 19:19:16,686 - trainer - INFO -     val_doc_entropy: 2.93435
2024-05-01 19:19:16,686 - trainer - INFO -     test_loss      : 0.947593
2024-05-01 19:19:16,686 - trainer - INFO -     test_accuracy  : 0.718062
2024-05-01 19:19:16,686 - trainer - INFO -     test_macro_f   : 0.708544
2024-05-01 19:19:16,686 - trainer - INFO -     test_precision : 0.74379
2024-05-01 19:19:16,686 - trainer - INFO -     test_recall    : 0.718062
2024-05-01 19:19:16,686 - trainer - INFO -     test_doc_entropy: 2.934581
2024-05-01 19:20:52,789 - train - INFO - BiAttentionClassifyModel(
  (embedding): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(768, 768)
    (lstm): LSTM(768, 768)
  )
  (W_k): Linear(in_features=768, out_features=50, bias=False)
  (W_q): Linear(in_features=768, out_features=50, bias=False)
  (W_v): Linear(in_features=768, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 56,551,445
Freeze params: 0
2024-05-01 19:36:41,809 - trainer - INFO -     epoch          : 1
2024-05-01 19:36:41,809 - trainer - INFO -     loss           : 1.176934
2024-05-01 19:36:41,809 - trainer - INFO -     accuracy       : 0.658343
2024-05-01 19:36:41,809 - trainer - INFO -     macro_f        : 0.639774
2024-05-01 19:36:41,809 - trainer - INFO -     precision      : 0.671623
2024-05-01 19:36:41,809 - trainer - INFO -     recall         : 0.658343
2024-05-01 19:36:41,809 - trainer - INFO -     doc_entropy    : 2.74242
2024-05-01 19:36:41,809 - trainer - INFO -     val_loss       : 0.993822
2024-05-01 19:36:41,809 - trainer - INFO -     val_accuracy   : 0.703425
2024-05-01 19:36:41,809 - trainer - INFO -     val_macro_f    : 0.689585
2024-05-01 19:36:41,809 - trainer - INFO -     val_precision  : 0.720529
2024-05-01 19:36:41,809 - trainer - INFO -     val_recall     : 0.703425
2024-05-01 19:36:41,809 - trainer - INFO -     val_doc_entropy: 2.938017
2024-05-01 19:36:41,809 - trainer - INFO -     test_loss      : 0.996966
2024-05-01 19:36:41,809 - trainer - INFO -     test_accuracy  : 0.701832
2024-05-01 19:36:41,809 - trainer - INFO -     test_macro_f   : 0.689532
2024-05-01 19:36:41,809 - trainer - INFO -     test_precision : 0.722675
2024-05-01 19:36:41,809 - trainer - INFO -     test_recall    : 0.701832
2024-05-01 19:36:41,809 - trainer - INFO -     test_doc_entropy: 2.939312
2024-05-01 19:52:26,998 - trainer - INFO -     epoch          : 2
2024-05-01 19:52:26,998 - trainer - INFO -     loss           : 0.866827
2024-05-01 19:52:27,013 - trainer - INFO -     accuracy       : 0.738884
2024-05-01 19:52:27,013 - trainer - INFO -     macro_f        : 0.729322
2024-05-01 19:52:27,013 - trainer - INFO -     precision      : 0.762834
2024-05-01 19:52:27,013 - trainer - INFO -     recall         : 0.738884
2024-05-01 19:52:27,013 - trainer - INFO -     doc_entropy    : 2.722882
2024-05-01 19:52:27,013 - trainer - INFO -     val_loss       : 0.947071
2024-05-01 19:52:27,013 - trainer - INFO -     val_accuracy   : 0.717067
2024-05-01 19:52:27,013 - trainer - INFO -     val_macro_f    : 0.710241
2024-05-01 19:52:27,013 - trainer - INFO -     val_precision  : 0.749593
2024-05-01 19:52:27,013 - trainer - INFO -     val_recall     : 0.717067
2024-05-01 19:52:27,013 - trainer - INFO -     val_doc_entropy: 2.883972
2024-05-01 19:52:27,013 - trainer - INFO -     test_loss      : 0.948298
2024-05-01 19:52:27,013 - trainer - INFO -     test_accuracy  : 0.719108
2024-05-01 19:52:27,013 - trainer - INFO -     test_macro_f   : 0.714522
2024-05-01 19:52:27,013 - trainer - INFO -     test_precision : 0.75645
2024-05-01 19:52:27,013 - trainer - INFO -     test_recall    : 0.719108
2024-05-01 19:52:27,013 - trainer - INFO -     test_doc_entropy: 2.883164
2024-05-02 09:55:53,881 - train - INFO - BiAttentionClassifyModel(
  (embedding): XLNetModel(
    (word_embedding): Embedding(32000, 768)
    (layer): ModuleList(
      (0): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): GELUActivation()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(768, 768)
    (lstm): LSTM(768, 768)
  )
  (W_k): Linear(in_features=768, out_features=50, bias=False)
  (W_q): Linear(in_features=768, out_features=50, bias=False)
  (W_v): Linear(in_features=768, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 42,127,637
Freeze params: 0
2024-05-02 10:14:22,281 - trainer - INFO -     epoch          : 1
2024-05-02 10:14:22,281 - trainer - INFO -     loss           : 1.194166
2024-05-02 10:14:22,281 - trainer - INFO -     accuracy       : 0.654833
2024-05-02 10:14:22,281 - trainer - INFO -     macro_f        : 0.635512
2024-05-02 10:14:22,281 - trainer - INFO -     precision      : 0.666863
2024-05-02 10:14:22,281 - trainer - INFO -     recall         : 0.654833
2024-05-02 10:14:22,281 - trainer - INFO -     doc_entropy    : 2.717158
2024-05-02 10:14:22,281 - trainer - INFO -     val_loss       : 0.995548
2024-05-02 10:14:22,281 - trainer - INFO -     val_accuracy   : 0.706363
2024-05-02 10:14:22,281 - trainer - INFO -     val_macro_f    : 0.690379
2024-05-02 10:14:22,281 - trainer - INFO -     val_precision  : 0.721
2024-05-02 10:14:22,281 - trainer - INFO -     val_recall     : 0.706363
2024-05-02 10:14:22,281 - trainer - INFO -     val_doc_entropy: 2.910939
2024-05-02 10:14:22,281 - trainer - INFO -     test_loss      : 0.994571
2024-05-02 10:14:22,281 - trainer - INFO -     test_accuracy  : 0.704969
2024-05-02 10:14:22,281 - trainer - INFO -     test_macro_f   : 0.689237
2024-05-02 10:14:22,281 - trainer - INFO -     test_precision : 0.720461
2024-05-02 10:14:22,281 - trainer - INFO -     test_recall    : 0.704969
2024-05-02 10:14:22,281 - trainer - INFO -     test_doc_entropy: 2.913551
2024-05-02 10:33:06,249 - trainer - INFO -     epoch          : 2
2024-05-02 10:33:06,249 - trainer - INFO -     loss           : 0.845475
2024-05-02 10:33:06,249 - trainer - INFO -     accuracy       : 0.744822
2024-05-02 10:33:06,249 - trainer - INFO -     macro_f        : 0.735464
2024-05-02 10:33:06,249 - trainer - INFO -     precision      : 0.76876
2024-05-02 10:33:06,249 - trainer - INFO -     recall         : 0.744822
2024-05-02 10:33:06,249 - trainer - INFO -     doc_entropy    : 2.725746
2024-05-02 10:33:06,249 - trainer - INFO -     val_loss       : 0.964161
2024-05-02 10:33:06,249 - trainer - INFO -     val_accuracy   : 0.721099
2024-05-02 10:33:06,249 - trainer - INFO -     val_macro_f    : 0.711597
2024-05-02 10:33:06,249 - trainer - INFO -     val_precision  : 0.748097
2024-05-02 10:33:06,249 - trainer - INFO -     val_recall     : 0.721099
2024-05-02 10:33:06,249 - trainer - INFO -     val_doc_entropy: 2.928924
2024-05-02 10:33:06,249 - trainer - INFO -     test_loss      : 0.97335
2024-05-02 10:33:06,249 - trainer - INFO -     test_accuracy  : 0.716419
2024-05-02 10:33:06,249 - trainer - INFO -     test_macro_f   : 0.706964
2024-05-02 10:33:06,249 - trainer - INFO -     test_precision : 0.743592
2024-05-02 10:33:06,249 - trainer - INFO -     test_recall    : 0.716419
2024-05-02 10:33:06,249 - trainer - INFO -     test_doc_entropy: 2.930869
2024-05-02 10:35:07,441 - train - INFO - BiAttentionClassifyModel(
  (embedding): XLNetModel(
    (word_embedding): Embedding(32000, 768)
    (layer): ModuleList(
      (0): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): GELUActivation()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(768, 768)
    (lstm): LSTM(768, 768)
  )
  (W_k): Linear(in_features=768, out_features=50, bias=False)
  (W_q): Linear(in_features=768, out_features=50, bias=False)
  (W_v): Linear(in_features=768, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 42,127,637
Freeze params: 0
2024-05-02 10:53:51,216 - trainer - INFO -     epoch          : 1
2024-05-02 10:53:51,216 - trainer - INFO -     loss           : 1.194306
2024-05-02 10:53:51,216 - trainer - INFO -     accuracy       : 0.65568
2024-05-02 10:53:51,216 - trainer - INFO -     macro_f        : 0.635899
2024-05-02 10:53:51,216 - trainer - INFO -     precision      : 0.666221
2024-05-02 10:53:51,216 - trainer - INFO -     recall         : 0.65568
2024-05-02 10:53:51,216 - trainer - INFO -     doc_entropy    : 2.732817
2024-05-02 10:53:51,216 - trainer - INFO -     val_loss       : 0.999877
2024-05-02 10:53:51,216 - trainer - INFO -     val_accuracy   : 0.709748
2024-05-02 10:53:51,216 - trainer - INFO -     val_macro_f    : 0.698588
2024-05-02 10:53:51,216 - trainer - INFO -     val_precision  : 0.733255
2024-05-02 10:53:51,216 - trainer - INFO -     val_recall     : 0.709748
2024-05-02 10:53:51,216 - trainer - INFO -     val_doc_entropy: 2.914382
2024-05-02 10:53:51,216 - trainer - INFO -     test_loss      : 1.002523
2024-05-02 10:53:51,216 - trainer - INFO -     test_accuracy  : 0.700388
2024-05-02 10:53:51,216 - trainer - INFO -     test_macro_f   : 0.688696
2024-05-02 10:53:51,216 - trainer - INFO -     test_precision : 0.724528
2024-05-02 10:53:51,216 - trainer - INFO -     test_recall    : 0.700388
2024-05-02 10:53:51,216 - trainer - INFO -     test_doc_entropy: 2.916147
2024-05-02 11:12:39,452 - trainer - INFO -     epoch          : 2
2024-05-02 11:12:39,452 - trainer - INFO -     loss           : 0.845712
2024-05-02 11:12:39,452 - trainer - INFO -     accuracy       : 0.744305
2024-05-02 11:12:39,452 - trainer - INFO -     macro_f        : 0.734781
2024-05-02 11:12:39,452 - trainer - INFO -     precision      : 0.768081
2024-05-02 11:12:39,452 - trainer - INFO -     recall         : 0.744305
2024-05-02 11:12:39,452 - trainer - INFO -     doc_entropy    : 2.751511
2024-05-02 11:12:39,452 - trainer - INFO -     val_loss       : 0.951717
2024-05-02 11:12:39,452 - trainer - INFO -     val_accuracy   : 0.721199
2024-05-02 11:12:39,452 - trainer - INFO -     val_macro_f    : 0.710632
2024-05-02 11:12:39,452 - trainer - INFO -     val_precision  : 0.745129
2024-05-02 11:12:39,452 - trainer - INFO -     val_recall     : 0.721199
2024-05-02 11:12:39,452 - trainer - INFO -     val_doc_entropy: 2.970651
2024-05-02 11:12:39,452 - trainer - INFO -     test_loss      : 0.952618
2024-05-02 11:12:39,452 - trainer - INFO -     test_accuracy  : 0.721697
2024-05-02 11:12:39,452 - trainer - INFO -     test_macro_f   : 0.71364
2024-05-02 11:12:39,452 - trainer - INFO -     test_precision : 0.749095
2024-05-02 11:12:39,452 - trainer - INFO -     test_recall    : 0.721697
2024-05-02 11:12:39,452 - trainer - INFO -     test_doc_entropy: 2.970924
2024-05-02 11:14:41,266 - train - INFO - BiAttentionClassifyModel(
  (embedding): XLNetModel(
    (word_embedding): Embedding(32000, 768)
    (layer): ModuleList(
      (0): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): GELUActivation()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(768, 768)
    (lstm): LSTM(768, 768)
  )
  (W_k): Linear(in_features=768, out_features=50, bias=False)
  (W_q): Linear(in_features=768, out_features=50, bias=False)
  (W_v): Linear(in_features=768, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 42,127,637
Freeze params: 0
2024-05-02 11:33:27,567 - trainer - INFO -     epoch          : 1
2024-05-02 11:33:27,567 - trainer - INFO -     loss           : 1.194844
2024-05-02 11:33:27,567 - trainer - INFO -     accuracy       : 0.654728
2024-05-02 11:33:27,567 - trainer - INFO -     macro_f        : 0.63492
2024-05-02 11:33:27,567 - trainer - INFO -     precision      : 0.66546
2024-05-02 11:33:27,567 - trainer - INFO -     recall         : 0.654728
2024-05-02 11:33:27,567 - trainer - INFO -     doc_entropy    : 2.726378
2024-05-02 11:33:27,567 - trainer - INFO -     val_loss       : 0.997447
2024-05-02 11:33:27,567 - trainer - INFO -     val_accuracy   : 0.710097
2024-05-02 11:33:27,567 - trainer - INFO -     val_macro_f    : 0.694665
2024-05-02 11:33:27,567 - trainer - INFO -     val_precision  : 0.725655
2024-05-02 11:33:27,567 - trainer - INFO -     val_recall     : 0.710097
2024-05-02 11:33:27,567 - trainer - INFO -     val_doc_entropy: 2.902478
2024-05-02 11:33:27,567 - trainer - INFO -     test_loss      : 0.996091
2024-05-02 11:33:27,567 - trainer - INFO -     test_accuracy  : 0.706761
2024-05-02 11:33:27,567 - trainer - INFO -     test_macro_f   : 0.691858
2024-05-02 11:33:27,567 - trainer - INFO -     test_precision : 0.723284
2024-05-02 11:33:27,567 - trainer - INFO -     test_recall    : 0.706761
2024-05-02 11:33:27,567 - trainer - INFO -     test_doc_entropy: 2.903745
2024-05-02 11:52:16,466 - trainer - INFO -     epoch          : 2
2024-05-02 11:52:16,466 - trainer - INFO -     loss           : 0.846158
2024-05-02 11:52:16,466 - trainer - INFO -     accuracy       : 0.744436
2024-05-02 11:52:16,466 - trainer - INFO -     macro_f        : 0.734984
2024-05-02 11:52:16,466 - trainer - INFO -     precision      : 0.7682
2024-05-02 11:52:16,466 - trainer - INFO -     recall         : 0.744436
2024-05-02 11:52:16,466 - trainer - INFO -     doc_entropy    : 2.75786
2024-05-02 11:52:16,466 - trainer - INFO -     val_loss       : 0.958151
2024-05-02 11:52:16,466 - trainer - INFO -     val_accuracy   : 0.722244
2024-05-02 11:52:16,466 - trainer - INFO -     val_macro_f    : 0.714632
2024-05-02 11:52:16,466 - trainer - INFO -     val_precision  : 0.750806
2024-05-02 11:52:16,466 - trainer - INFO -     val_recall     : 0.722244
2024-05-02 11:52:16,466 - trainer - INFO -     val_doc_entropy: 2.916231
2024-05-02 11:52:16,466 - trainer - INFO -     test_loss      : 0.962604
2024-05-02 11:52:16,466 - trainer - INFO -     test_accuracy  : 0.718212
2024-05-02 11:52:16,466 - trainer - INFO -     test_macro_f   : 0.711004
2024-05-02 11:52:16,466 - trainer - INFO -     test_precision : 0.749063
2024-05-02 11:52:16,466 - trainer - INFO -     test_recall    : 0.718212
2024-05-02 11:52:16,466 - trainer - INFO -     test_doc_entropy: 2.917089
2024-05-02 11:54:16,988 - train - INFO - BiAttentionClassifyModel(
  (embedding): XLNetModel(
    (word_embedding): Embedding(32000, 768)
    (layer): ModuleList(
      (0): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): GELUActivation()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(768, 768)
    (lstm): LSTM(768, 768)
  )
  (W_k): Linear(in_features=768, out_features=50, bias=False)
  (W_q): Linear(in_features=768, out_features=50, bias=False)
  (W_v): Linear(in_features=768, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 42,127,637
Freeze params: 0
2024-05-02 12:13:06,341 - trainer - INFO -     epoch          : 1
2024-05-02 12:13:06,341 - trainer - INFO -     loss           : 1.192163
2024-05-02 12:13:06,356 - trainer - INFO -     accuracy       : 0.654609
2024-05-02 12:13:06,356 - trainer - INFO -     macro_f        : 0.634644
2024-05-02 12:13:06,356 - trainer - INFO -     precision      : 0.665095
2024-05-02 12:13:06,356 - trainer - INFO -     recall         : 0.654609
2024-05-02 12:13:06,356 - trainer - INFO -     doc_entropy    : 2.727949
2024-05-02 12:13:06,356 - trainer - INFO -     val_loss       : 1.003755
2024-05-02 12:13:06,356 - trainer - INFO -     val_accuracy   : 0.70462
2024-05-02 12:13:06,356 - trainer - INFO -     val_macro_f    : 0.695675
2024-05-02 12:13:06,356 - trainer - INFO -     val_precision  : 0.734396
2024-05-02 12:13:06,356 - trainer - INFO -     val_recall     : 0.70462
2024-05-02 12:13:06,356 - trainer - INFO -     val_doc_entropy: 2.903964
2024-05-02 12:13:06,356 - trainer - INFO -     test_loss      : 1.00273
2024-05-02 12:13:06,356 - trainer - INFO -     test_accuracy  : 0.701782
2024-05-02 12:13:06,356 - trainer - INFO -     test_macro_f   : 0.694861
2024-05-02 12:13:06,356 - trainer - INFO -     test_precision : 0.733565
2024-05-02 12:13:06,356 - trainer - INFO -     test_recall    : 0.701782
2024-05-02 12:13:06,356 - trainer - INFO -     test_doc_entropy: 2.903828
2024-05-02 12:31:56,511 - trainer - INFO -     epoch          : 2
2024-05-02 12:31:56,511 - trainer - INFO -     loss           : 0.843311
2024-05-02 12:31:56,511 - trainer - INFO -     accuracy       : 0.745475
2024-05-02 12:31:56,511 - trainer - INFO -     macro_f        : 0.736078
2024-05-02 12:31:56,511 - trainer - INFO -     precision      : 0.769227
2024-05-02 12:31:56,511 - trainer - INFO -     recall         : 0.745475
2024-05-02 12:31:56,511 - trainer - INFO -     doc_entropy    : 2.726925
2024-05-02 12:31:56,511 - trainer - INFO -     val_loss       : 0.959675
2024-05-02 12:31:56,511 - trainer - INFO -     val_accuracy   : 0.720452
2024-05-02 12:31:56,511 - trainer - INFO -     val_macro_f    : 0.712124
2024-05-02 12:31:56,511 - trainer - INFO -     val_precision  : 0.751172
2024-05-02 12:31:56,511 - trainer - INFO -     val_recall     : 0.720452
2024-05-02 12:31:56,511 - trainer - INFO -     val_doc_entropy: 2.912005
2024-05-02 12:31:56,511 - trainer - INFO -     test_loss      : 0.963025
2024-05-02 12:31:56,511 - trainer - INFO -     test_accuracy  : 0.717465
2024-05-02 12:31:56,511 - trainer - INFO -     test_macro_f   : 0.709689
2024-05-02 12:31:56,511 - trainer - INFO -     test_precision : 0.74812
2024-05-02 12:31:56,511 - trainer - INFO -     test_recall    : 0.717465
2024-05-02 12:31:56,511 - trainer - INFO -     test_doc_entropy: 2.91168
2024-05-02 12:34:01,666 - train - INFO - BiAttentionClassifyModel(
  (embedding): XLNetModel(
    (word_embedding): Embedding(32000, 768)
    (layer): ModuleList(
      (0): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): GELUActivation()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (classifier): Linear(in_features=768, out_features=26, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(768, 768)
    (lstm): LSTM(768, 768)
  )
  (W_k): Linear(in_features=768, out_features=50, bias=False)
  (W_q): Linear(in_features=768, out_features=50, bias=False)
  (W_v): Linear(in_features=768, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 42,127,637
Freeze params: 0
2024-05-02 12:52:47,275 - trainer - INFO -     epoch          : 1
2024-05-02 12:52:47,275 - trainer - INFO -     loss           : 1.191743
2024-05-02 12:52:47,275 - trainer - INFO -     accuracy       : 0.655306
2024-05-02 12:52:47,275 - trainer - INFO -     macro_f        : 0.635979
2024-05-02 12:52:47,275 - trainer - INFO -     precision      : 0.66693
2024-05-02 12:52:47,275 - trainer - INFO -     recall         : 0.655306
2024-05-02 12:52:47,275 - trainer - INFO -     doc_entropy    : 2.721159
2024-05-02 12:52:47,275 - trainer - INFO -     val_loss       : 0.999738
2024-05-02 12:52:47,275 - trainer - INFO -     val_accuracy   : 0.706114
2024-05-02 12:52:47,275 - trainer - INFO -     val_macro_f    : 0.694611
2024-05-02 12:52:47,275 - trainer - INFO -     val_precision  : 0.732499
2024-05-02 12:52:47,275 - trainer - INFO -     val_recall     : 0.706114
2024-05-02 12:52:47,275 - trainer - INFO -     val_doc_entropy: 2.93005
2024-05-02 12:52:47,275 - trainer - INFO -     test_loss      : 0.996377
2024-05-02 12:52:47,275 - trainer - INFO -     test_accuracy  : 0.705516
2024-05-02 12:52:47,275 - trainer - INFO -     test_macro_f   : 0.693228
2024-05-02 12:52:47,275 - trainer - INFO -     test_precision : 0.73169
2024-05-02 12:52:47,275 - trainer - INFO -     test_recall    : 0.705516
2024-05-02 12:52:47,275 - trainer - INFO -     test_doc_entropy: 2.929685
2024-05-02 13:11:33,826 - trainer - INFO -     epoch          : 2
2024-05-02 13:11:33,826 - trainer - INFO -     loss           : 0.843423
2024-05-02 13:11:33,826 - trainer - INFO -     accuracy       : 0.744704
2024-05-02 13:11:33,826 - trainer - INFO -     macro_f        : 0.735193
2024-05-02 13:11:33,826 - trainer - INFO -     precision      : 0.768921
2024-05-02 13:11:33,826 - trainer - INFO -     recall         : 0.744704
2024-05-02 13:11:33,826 - trainer - INFO -     doc_entropy    : 2.769534
2024-05-02 13:11:33,826 - trainer - INFO -     val_loss       : 0.955309
2024-05-02 13:11:33,826 - trainer - INFO -     val_accuracy   : 0.721746
2024-05-02 13:11:33,826 - trainer - INFO -     val_macro_f    : 0.713478
2024-05-02 13:11:33,826 - trainer - INFO -     val_precision  : 0.750007
2024-05-02 13:11:33,826 - trainer - INFO -     val_recall     : 0.721746
2024-05-02 13:11:33,826 - trainer - INFO -     val_doc_entropy: 2.924062
2024-05-02 13:11:33,826 - trainer - INFO -     test_loss      : 0.961864
2024-05-02 13:11:33,826 - trainer - INFO -     test_accuracy  : 0.71851
2024-05-02 13:11:33,826 - trainer - INFO -     test_macro_f   : 0.711569
2024-05-02 13:11:33,826 - trainer - INFO -     test_precision : 0.749505
2024-05-02 13:11:33,826 - trainer - INFO -     test_recall    : 0.71851
2024-05-02 13:11:33,826 - trainer - INFO -     test_doc_entropy: 2.926257
2024-05-03 02:02:59,891 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=50, bias=False)
  (W_q): Linear(in_features=300, out_features=50, bias=False)
  (W_v): Linear(in_features=300, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,138,605
Freeze params: 0
2024-05-03 02:05:26,472 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=50, bias=False)
  (W_q): Linear(in_features=300, out_features=50, bias=False)
  (W_v): Linear(in_features=300, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,138,605
Freeze params: 0
2024-05-03 02:09:05,669 - trainer - INFO -     epoch          : 1
2024-05-03 02:09:05,669 - trainer - INFO -     loss           : 1.147137
2024-05-03 02:09:05,669 - trainer - INFO -     accuracy       : 0.671202
2024-05-03 02:09:05,669 - trainer - INFO -     macro_f        : 0.653383
2024-05-03 02:09:05,669 - trainer - INFO -     precision      : 0.685136
2024-05-03 02:09:05,669 - trainer - INFO -     recall         : 0.671202
2024-05-03 02:09:05,669 - trainer - INFO -     doc_entropy    : 2.431388
2024-05-03 02:09:05,669 - trainer - INFO -     val_loss       : 1.009709
2024-05-03 02:09:05,669 - trainer - INFO -     val_accuracy   : 0.70228
2024-05-03 02:09:05,669 - trainer - INFO -     val_macro_f    : 0.688161
2024-05-03 02:09:05,669 - trainer - INFO -     val_precision  : 0.722572
2024-05-03 02:09:05,669 - trainer - INFO -     val_recall     : 0.70228
2024-05-03 02:09:05,669 - trainer - INFO -     val_doc_entropy: 2.665228
2024-05-03 02:09:05,669 - trainer - INFO -     test_loss      : 1.001569
2024-05-03 02:09:05,669 - trainer - INFO -     test_accuracy  : 0.705267
2024-05-03 02:09:05,669 - trainer - INFO -     test_macro_f   : 0.691095
2024-05-03 02:09:05,669 - trainer - INFO -     test_precision : 0.72524
2024-05-03 02:09:05,669 - trainer - INFO -     test_recall    : 0.705267
2024-05-03 02:09:05,669 - trainer - INFO -     test_doc_entropy: 2.667715
2024-05-03 02:12:46,181 - trainer - INFO -     epoch          : 2
2024-05-03 02:12:46,181 - trainer - INFO -     loss           : 0.801048
2024-05-03 02:12:46,181 - trainer - INFO -     accuracy       : 0.76035
2024-05-03 02:12:46,181 - trainer - INFO -     macro_f        : 0.751465
2024-05-03 02:12:46,181 - trainer - INFO -     precision      : 0.784149
2024-05-03 02:12:46,181 - trainer - INFO -     recall         : 0.76035
2024-05-03 02:12:46,181 - trainer - INFO -     doc_entropy    : 1.971008
2024-05-03 02:12:46,181 - trainer - INFO -     val_loss       : 1.011529
2024-05-03 02:12:46,181 - trainer - INFO -     val_accuracy   : 0.704819
2024-05-03 02:12:46,181 - trainer - INFO -     val_macro_f    : 0.693679
2024-05-03 02:12:46,181 - trainer - INFO -     val_precision  : 0.729334
2024-05-03 02:12:46,181 - trainer - INFO -     val_recall     : 0.704819
2024-05-03 02:12:46,181 - trainer - INFO -     val_doc_entropy: 2.439413
2024-05-03 02:12:46,181 - trainer - INFO -     test_loss      : 1.004455
2024-05-03 02:12:46,181 - trainer - INFO -     test_accuracy  : 0.710395
2024-05-03 02:12:46,181 - trainer - INFO -     test_macro_f   : 0.699451
2024-05-03 02:12:46,181 - trainer - INFO -     test_precision : 0.733392
2024-05-03 02:12:46,181 - trainer - INFO -     test_recall    : 0.710395
2024-05-03 02:12:46,181 - trainer - INFO -     test_doc_entropy: 2.442626
2024-05-03 02:13:23,275 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=50, bias=False)
  (W_q): Linear(in_features=300, out_features=50, bias=False)
  (W_v): Linear(in_features=300, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,138,605
Freeze params: 0
2024-05-03 02:13:48,471 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=50, bias=False)
  (W_q): Linear(in_features=300, out_features=50, bias=False)
  (W_v): Linear(in_features=300, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,138,605
Freeze params: 0
2024-05-03 02:17:29,122 - trainer - INFO -     epoch          : 1
2024-05-03 02:17:29,122 - trainer - INFO -     loss           : 1.147137
2024-05-03 02:17:29,122 - trainer - INFO -     accuracy       : 0.671202
2024-05-03 02:17:29,122 - trainer - INFO -     macro_f        : 0.653383
2024-05-03 02:17:29,122 - trainer - INFO -     precision      : 0.685136
2024-05-03 02:17:29,122 - trainer - INFO -     recall         : 0.671202
2024-05-03 02:17:29,122 - trainer - INFO -     doc_entropy    : 2.431388
2024-05-03 02:17:29,122 - trainer - INFO -     val_loss       : 1.009709
2024-05-03 02:17:29,122 - trainer - INFO -     val_accuracy   : 0.70228
2024-05-03 02:17:29,122 - trainer - INFO -     val_macro_f    : 0.688161
2024-05-03 02:17:29,122 - trainer - INFO -     val_precision  : 0.722572
2024-05-03 02:17:29,122 - trainer - INFO -     val_recall     : 0.70228
2024-05-03 02:17:29,122 - trainer - INFO -     val_doc_entropy: 2.665228
2024-05-03 02:17:29,122 - trainer - INFO -     test_loss      : 1.001569
2024-05-03 02:17:29,122 - trainer - INFO -     test_accuracy  : 0.705267
2024-05-03 02:17:29,122 - trainer - INFO -     test_macro_f   : 0.691095
2024-05-03 02:17:29,122 - trainer - INFO -     test_precision : 0.72524
2024-05-03 02:17:29,122 - trainer - INFO -     test_recall    : 0.705267
2024-05-03 02:17:29,122 - trainer - INFO -     test_doc_entropy: 2.667715
2024-05-03 02:21:11,413 - trainer - INFO -     epoch          : 2
2024-05-03 02:21:11,413 - trainer - INFO -     loss           : 0.801048
2024-05-03 02:21:11,413 - trainer - INFO -     accuracy       : 0.76035
2024-05-03 02:21:11,413 - trainer - INFO -     macro_f        : 0.751465
2024-05-03 02:21:11,413 - trainer - INFO -     precision      : 0.784149
2024-05-03 02:21:11,413 - trainer - INFO -     recall         : 0.76035
2024-05-03 02:21:11,413 - trainer - INFO -     doc_entropy    : 1.971008
2024-05-03 02:21:11,413 - trainer - INFO -     val_loss       : 1.011529
2024-05-03 02:21:11,413 - trainer - INFO -     val_accuracy   : 0.704819
2024-05-03 02:21:11,413 - trainer - INFO -     val_macro_f    : 0.693679
2024-05-03 02:21:11,413 - trainer - INFO -     val_precision  : 0.729334
2024-05-03 02:21:11,413 - trainer - INFO -     val_recall     : 0.704819
2024-05-03 02:21:11,413 - trainer - INFO -     val_doc_entropy: 2.439413
2024-05-03 02:21:11,413 - trainer - INFO -     test_loss      : 1.004455
2024-05-03 02:21:11,413 - trainer - INFO -     test_accuracy  : 0.710395
2024-05-03 02:21:11,413 - trainer - INFO -     test_macro_f   : 0.699451
2024-05-03 02:21:11,413 - trainer - INFO -     test_precision : 0.733392
2024-05-03 02:21:11,413 - trainer - INFO -     test_recall    : 0.710395
2024-05-03 02:21:11,413 - trainer - INFO -     test_doc_entropy: 2.442626
2024-05-03 02:21:51,779 - gensim.corpora.dictionary - INFO - adding document #0 to Dictionary<0 unique tokens: []>
2024-05-03 02:21:52,201 - gensim.corpora.dictionary - INFO - adding document #10000 to Dictionary<23643 unique tokens: ['-', '.', '?', 'a', 'africa']...>
2024-05-03 02:21:52,607 - gensim.corpora.dictionary - INFO - adding document #20000 to Dictionary<32428 unique tokens: ['-', '.', '?', 'a', 'africa']...>
2024-05-03 02:21:52,623 - gensim.corpora.dictionary - INFO - built Dictionary<32503 unique tokens: ['-', '.', '?', 'a', 'africa']...> from 20086 documents (total 703102 corpus positions)
2024-05-03 02:21:52,639 - gensim.utils - INFO - Dictionary lifecycle event {'msg': "built Dictionary<32503 unique tokens: ['-', '.', '?', 'a', 'africa']...> from 20086 documents (total 703102 corpus positions)", 'datetime': '2024-05-03T02:21:52.623406', 'gensim': '4.3.2', 'python': '3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}
2024-05-03 02:21:52,654 - gensim.topic_coherence.probability_estimation - INFO - using ParallelWordOccurrenceAccumulator<processes=11, batch_size=64> to estimate probabilities from sliding windows
2024-05-03 02:22:53,906 - gensim.topic_coherence.text_analysis - INFO - 1 batches submitted to accumulate stats from 64 documents (1358 virtual)
2024-05-03 02:22:53,921 - gensim.topic_coherence.text_analysis - INFO - 2 batches submitted to accumulate stats from 128 documents (3142 virtual)
2024-05-03 02:22:53,921 - gensim.topic_coherence.text_analysis - INFO - 3 batches submitted to accumulate stats from 192 documents (4707 virtual)
2024-05-03 02:22:53,921 - gensim.topic_coherence.text_analysis - INFO - 4 batches submitted to accumulate stats from 256 documents (6346 virtual)
2024-05-03 02:22:53,921 - gensim.topic_coherence.text_analysis - INFO - 5 batches submitted to accumulate stats from 320 documents (7961 virtual)
2024-05-03 02:22:53,921 - gensim.topic_coherence.text_analysis - INFO - 6 batches submitted to accumulate stats from 384 documents (9298 virtual)
2024-05-03 02:22:53,921 - gensim.topic_coherence.text_analysis - INFO - 7 batches submitted to accumulate stats from 448 documents (11371 virtual)
2024-05-03 02:22:53,921 - gensim.topic_coherence.text_analysis - INFO - 8 batches submitted to accumulate stats from 512 documents (13011 virtual)
2024-05-03 02:22:53,921 - gensim.topic_coherence.text_analysis - INFO - 9 batches submitted to accumulate stats from 576 documents (14534 virtual)
2024-05-03 02:22:53,921 - gensim.topic_coherence.text_analysis - INFO - 10 batches submitted to accumulate stats from 640 documents (16161 virtual)
2024-05-03 02:22:53,921 - gensim.topic_coherence.text_analysis - INFO - 11 batches submitted to accumulate stats from 704 documents (17689 virtual)
2024-05-03 02:22:53,937 - gensim.topic_coherence.text_analysis - INFO - 12 batches submitted to accumulate stats from 768 documents (19256 virtual)
2024-05-03 02:22:53,937 - gensim.topic_coherence.text_analysis - INFO - 13 batches submitted to accumulate stats from 832 documents (21175 virtual)
2024-05-03 02:22:53,937 - gensim.topic_coherence.text_analysis - INFO - 14 batches submitted to accumulate stats from 896 documents (22850 virtual)
2024-05-03 02:22:53,937 - gensim.topic_coherence.text_analysis - INFO - 15 batches submitted to accumulate stats from 960 documents (24681 virtual)
2024-05-03 02:22:53,953 - gensim.topic_coherence.text_analysis - INFO - 16 batches submitted to accumulate stats from 1024 documents (26222 virtual)
2024-05-03 02:22:53,953 - gensim.topic_coherence.text_analysis - INFO - 17 batches submitted to accumulate stats from 1088 documents (27953 virtual)
2024-05-03 02:22:53,953 - gensim.topic_coherence.text_analysis - INFO - 18 batches submitted to accumulate stats from 1152 documents (29594 virtual)
2024-05-03 02:22:53,953 - gensim.topic_coherence.text_analysis - INFO - 19 batches submitted to accumulate stats from 1216 documents (31239 virtual)
2024-05-03 02:22:53,953 - gensim.topic_coherence.text_analysis - INFO - 20 batches submitted to accumulate stats from 1280 documents (32901 virtual)
2024-05-03 02:22:53,953 - gensim.topic_coherence.text_analysis - INFO - 21 batches submitted to accumulate stats from 1344 documents (34481 virtual)
2024-05-03 02:22:53,953 - gensim.topic_coherence.text_analysis - INFO - 22 batches submitted to accumulate stats from 1408 documents (36151 virtual)
2024-05-03 02:22:53,968 - gensim.topic_coherence.text_analysis - INFO - 23 batches submitted to accumulate stats from 1472 documents (37837 virtual)
2024-05-03 02:22:53,968 - gensim.topic_coherence.text_analysis - INFO - 24 batches submitted to accumulate stats from 1536 documents (39453 virtual)
2024-05-03 02:22:53,968 - gensim.topic_coherence.text_analysis - INFO - 25 batches submitted to accumulate stats from 1600 documents (41046 virtual)
2024-05-03 02:22:53,984 - gensim.topic_coherence.text_analysis - INFO - 26 batches submitted to accumulate stats from 1664 documents (42892 virtual)
2024-05-03 02:22:53,984 - gensim.topic_coherence.text_analysis - INFO - 27 batches submitted to accumulate stats from 1728 documents (44445 virtual)
2024-05-03 02:22:53,984 - gensim.topic_coherence.text_analysis - INFO - 28 batches submitted to accumulate stats from 1792 documents (46073 virtual)
2024-05-03 02:22:53,984 - gensim.topic_coherence.text_analysis - INFO - 29 batches submitted to accumulate stats from 1856 documents (47643 virtual)
2024-05-03 02:22:53,984 - gensim.topic_coherence.text_analysis - INFO - 30 batches submitted to accumulate stats from 1920 documents (49252 virtual)
2024-05-03 02:22:53,984 - gensim.topic_coherence.text_analysis - INFO - 31 batches submitted to accumulate stats from 1984 documents (50774 virtual)
2024-05-03 02:22:53,999 - gensim.topic_coherence.text_analysis - INFO - 32 batches submitted to accumulate stats from 2048 documents (52387 virtual)
2024-05-03 02:22:53,999 - gensim.topic_coherence.text_analysis - INFO - 33 batches submitted to accumulate stats from 2112 documents (53815 virtual)
2024-05-03 02:22:54,015 - gensim.topic_coherence.text_analysis - INFO - 34 batches submitted to accumulate stats from 2176 documents (55540 virtual)
2024-05-03 02:22:54,015 - gensim.topic_coherence.text_analysis - INFO - 35 batches submitted to accumulate stats from 2240 documents (57348 virtual)
2024-05-03 02:22:54,015 - gensim.topic_coherence.text_analysis - INFO - 36 batches submitted to accumulate stats from 2304 documents (59258 virtual)
2024-05-03 02:22:54,015 - gensim.topic_coherence.text_analysis - INFO - 37 batches submitted to accumulate stats from 2368 documents (60957 virtual)
2024-05-03 02:22:54,031 - gensim.topic_coherence.text_analysis - INFO - 38 batches submitted to accumulate stats from 2432 documents (62425 virtual)
2024-05-03 02:22:54,031 - gensim.topic_coherence.text_analysis - INFO - 39 batches submitted to accumulate stats from 2496 documents (64029 virtual)
2024-05-03 02:22:54,031 - gensim.topic_coherence.text_analysis - INFO - 40 batches submitted to accumulate stats from 2560 documents (65725 virtual)
2024-05-03 02:22:54,031 - gensim.topic_coherence.text_analysis - INFO - 41 batches submitted to accumulate stats from 2624 documents (67346 virtual)
2024-05-03 02:22:54,046 - gensim.topic_coherence.text_analysis - INFO - 42 batches submitted to accumulate stats from 2688 documents (68863 virtual)
2024-05-03 02:22:54,046 - gensim.topic_coherence.text_analysis - INFO - 43 batches submitted to accumulate stats from 2752 documents (70539 virtual)
2024-05-03 02:22:54,046 - gensim.topic_coherence.text_analysis - INFO - 44 batches submitted to accumulate stats from 2816 documents (71922 virtual)
2024-05-03 02:22:54,062 - gensim.topic_coherence.text_analysis - INFO - 45 batches submitted to accumulate stats from 2880 documents (73294 virtual)
2024-05-03 02:22:54,062 - gensim.topic_coherence.text_analysis - INFO - 46 batches submitted to accumulate stats from 2944 documents (75084 virtual)
2024-05-03 02:22:54,062 - gensim.topic_coherence.text_analysis - INFO - 47 batches submitted to accumulate stats from 3008 documents (76769 virtual)
2024-05-03 02:22:54,062 - gensim.topic_coherence.text_analysis - INFO - 48 batches submitted to accumulate stats from 3072 documents (78312 virtual)
2024-05-03 02:22:54,078 - gensim.topic_coherence.text_analysis - INFO - 49 batches submitted to accumulate stats from 3136 documents (80039 virtual)
2024-05-03 02:22:54,083 - gensim.topic_coherence.text_analysis - INFO - 50 batches submitted to accumulate stats from 3200 documents (81572 virtual)
2024-05-03 02:22:54,083 - gensim.topic_coherence.text_analysis - INFO - 51 batches submitted to accumulate stats from 3264 documents (83189 virtual)
2024-05-03 02:22:54,083 - gensim.topic_coherence.text_analysis - INFO - 52 batches submitted to accumulate stats from 3328 documents (84783 virtual)
2024-05-03 02:22:54,109 - gensim.topic_coherence.text_analysis - INFO - 53 batches submitted to accumulate stats from 3392 documents (86570 virtual)
2024-05-03 02:22:54,109 - gensim.topic_coherence.text_analysis - INFO - 54 batches submitted to accumulate stats from 3456 documents (88371 virtual)
2024-05-03 02:22:54,109 - gensim.topic_coherence.text_analysis - INFO - 55 batches submitted to accumulate stats from 3520 documents (90295 virtual)
2024-05-03 02:22:54,125 - gensim.topic_coherence.text_analysis - INFO - 56 batches submitted to accumulate stats from 3584 documents (92118 virtual)
2024-05-03 02:22:54,125 - gensim.topic_coherence.text_analysis - INFO - 57 batches submitted to accumulate stats from 3648 documents (93914 virtual)
2024-05-03 02:22:54,125 - gensim.topic_coherence.text_analysis - INFO - 58 batches submitted to accumulate stats from 3712 documents (95729 virtual)
2024-05-03 02:22:54,125 - gensim.topic_coherence.text_analysis - INFO - 59 batches submitted to accumulate stats from 3776 documents (97293 virtual)
2024-05-03 02:22:54,140 - gensim.topic_coherence.text_analysis - INFO - 60 batches submitted to accumulate stats from 3840 documents (98908 virtual)
2024-05-03 02:22:54,140 - gensim.topic_coherence.text_analysis - INFO - 61 batches submitted to accumulate stats from 3904 documents (100586 virtual)
2024-05-03 02:22:54,140 - gensim.topic_coherence.text_analysis - INFO - 62 batches submitted to accumulate stats from 3968 documents (102208 virtual)
2024-05-03 02:22:54,140 - gensim.topic_coherence.text_analysis - INFO - 63 batches submitted to accumulate stats from 4032 documents (103862 virtual)
2024-05-03 02:22:54,156 - gensim.topic_coherence.text_analysis - INFO - 64 batches submitted to accumulate stats from 4096 documents (105500 virtual)
2024-05-03 02:22:54,156 - gensim.topic_coherence.text_analysis - INFO - 65 batches submitted to accumulate stats from 4160 documents (106974 virtual)
2024-05-03 02:22:54,172 - gensim.topic_coherence.text_analysis - INFO - 66 batches submitted to accumulate stats from 4224 documents (108587 virtual)
2024-05-03 02:22:54,172 - gensim.topic_coherence.text_analysis - INFO - 67 batches submitted to accumulate stats from 4288 documents (110059 virtual)
2024-05-03 02:22:54,172 - gensim.topic_coherence.text_analysis - INFO - 68 batches submitted to accumulate stats from 4352 documents (111905 virtual)
2024-05-03 02:22:54,172 - gensim.topic_coherence.text_analysis - INFO - 69 batches submitted to accumulate stats from 4416 documents (113549 virtual)
2024-05-03 02:22:54,172 - gensim.topic_coherence.text_analysis - INFO - 70 batches submitted to accumulate stats from 4480 documents (115163 virtual)
2024-05-03 02:22:54,172 - gensim.topic_coherence.text_analysis - INFO - 71 batches submitted to accumulate stats from 4544 documents (117083 virtual)
2024-05-03 02:22:54,172 - gensim.topic_coherence.text_analysis - INFO - 72 batches submitted to accumulate stats from 4608 documents (118654 virtual)
2024-05-03 02:22:54,187 - gensim.topic_coherence.text_analysis - INFO - 73 batches submitted to accumulate stats from 4672 documents (120224 virtual)
2024-05-03 02:22:54,187 - gensim.topic_coherence.text_analysis - INFO - 74 batches submitted to accumulate stats from 4736 documents (121786 virtual)
2024-05-03 02:22:54,187 - gensim.topic_coherence.text_analysis - INFO - 75 batches submitted to accumulate stats from 4800 documents (123233 virtual)
2024-05-03 02:22:54,187 - gensim.topic_coherence.text_analysis - INFO - 76 batches submitted to accumulate stats from 4864 documents (124761 virtual)
2024-05-03 02:22:54,187 - gensim.topic_coherence.text_analysis - INFO - 77 batches submitted to accumulate stats from 4928 documents (126221 virtual)
2024-05-03 02:22:54,187 - gensim.topic_coherence.text_analysis - INFO - 78 batches submitted to accumulate stats from 4992 documents (127857 virtual)
2024-05-03 02:22:54,187 - gensim.topic_coherence.text_analysis - INFO - 79 batches submitted to accumulate stats from 5056 documents (129432 virtual)
2024-05-03 02:22:54,203 - gensim.topic_coherence.text_analysis - INFO - 80 batches submitted to accumulate stats from 5120 documents (130948 virtual)
2024-05-03 02:22:54,203 - gensim.topic_coherence.text_analysis - INFO - 81 batches submitted to accumulate stats from 5184 documents (132913 virtual)
2024-05-03 02:22:54,203 - gensim.topic_coherence.text_analysis - INFO - 82 batches submitted to accumulate stats from 5248 documents (134700 virtual)
2024-05-03 02:22:54,203 - gensim.topic_coherence.text_analysis - INFO - 83 batches submitted to accumulate stats from 5312 documents (136417 virtual)
2024-05-03 02:22:54,203 - gensim.topic_coherence.text_analysis - INFO - 84 batches submitted to accumulate stats from 5376 documents (138141 virtual)
2024-05-03 02:22:54,203 - gensim.topic_coherence.text_analysis - INFO - 85 batches submitted to accumulate stats from 5440 documents (139767 virtual)
2024-05-03 02:22:54,219 - gensim.topic_coherence.text_analysis - INFO - 86 batches submitted to accumulate stats from 5504 documents (141151 virtual)
2024-05-03 02:22:54,219 - gensim.topic_coherence.text_analysis - INFO - 87 batches submitted to accumulate stats from 5568 documents (142647 virtual)
2024-05-03 02:22:54,219 - gensim.topic_coherence.text_analysis - INFO - 88 batches submitted to accumulate stats from 5632 documents (144175 virtual)
2024-05-03 02:22:54,234 - gensim.topic_coherence.text_analysis - INFO - 89 batches submitted to accumulate stats from 5696 documents (145825 virtual)
2024-05-03 02:22:54,250 - gensim.topic_coherence.text_analysis - INFO - 90 batches submitted to accumulate stats from 5760 documents (147317 virtual)
2024-05-03 02:22:54,265 - gensim.topic_coherence.text_analysis - INFO - 91 batches submitted to accumulate stats from 5824 documents (149158 virtual)
2024-05-03 02:22:54,265 - gensim.topic_coherence.text_analysis - INFO - 92 batches submitted to accumulate stats from 5888 documents (150755 virtual)
2024-05-03 02:22:54,265 - gensim.topic_coherence.text_analysis - INFO - 93 batches submitted to accumulate stats from 5952 documents (152237 virtual)
2024-05-03 02:22:54,265 - gensim.topic_coherence.text_analysis - INFO - 94 batches submitted to accumulate stats from 6016 documents (154013 virtual)
2024-05-03 02:22:54,265 - gensim.topic_coherence.text_analysis - INFO - 95 batches submitted to accumulate stats from 6080 documents (155593 virtual)
2024-05-03 02:22:54,265 - gensim.topic_coherence.text_analysis - INFO - 96 batches submitted to accumulate stats from 6144 documents (157114 virtual)
2024-05-03 02:22:54,265 - gensim.topic_coherence.text_analysis - INFO - 97 batches submitted to accumulate stats from 6208 documents (158783 virtual)
2024-05-03 02:22:54,265 - gensim.topic_coherence.text_analysis - INFO - 98 batches submitted to accumulate stats from 6272 documents (160467 virtual)
2024-05-03 02:22:54,281 - gensim.topic_coherence.text_analysis - INFO - 99 batches submitted to accumulate stats from 6336 documents (162287 virtual)
2024-05-03 02:22:54,281 - gensim.topic_coherence.text_analysis - INFO - 100 batches submitted to accumulate stats from 6400 documents (163932 virtual)
2024-05-03 02:22:54,281 - gensim.topic_coherence.text_analysis - INFO - 101 batches submitted to accumulate stats from 6464 documents (165414 virtual)
2024-05-03 02:22:54,297 - gensim.topic_coherence.text_analysis - INFO - 102 batches submitted to accumulate stats from 6528 documents (166934 virtual)
2024-05-03 02:22:54,297 - gensim.topic_coherence.text_analysis - INFO - 103 batches submitted to accumulate stats from 6592 documents (168399 virtual)
2024-05-03 02:22:54,297 - gensim.topic_coherence.text_analysis - INFO - 104 batches submitted to accumulate stats from 6656 documents (170596 virtual)
2024-05-03 02:22:54,297 - gensim.topic_coherence.text_analysis - INFO - 105 batches submitted to accumulate stats from 6720 documents (172319 virtual)
2024-05-03 02:22:54,297 - gensim.topic_coherence.text_analysis - INFO - 106 batches submitted to accumulate stats from 6784 documents (173973 virtual)
2024-05-03 02:22:54,297 - gensim.topic_coherence.text_analysis - INFO - 107 batches submitted to accumulate stats from 6848 documents (175817 virtual)
2024-05-03 02:22:54,297 - gensim.topic_coherence.text_analysis - INFO - 108 batches submitted to accumulate stats from 6912 documents (177402 virtual)
2024-05-03 02:22:54,297 - gensim.topic_coherence.text_analysis - INFO - 109 batches submitted to accumulate stats from 6976 documents (179106 virtual)
2024-05-03 02:22:54,312 - gensim.topic_coherence.text_analysis - INFO - 110 batches submitted to accumulate stats from 7040 documents (181089 virtual)
2024-05-03 02:22:54,312 - gensim.topic_coherence.text_analysis - INFO - 111 batches submitted to accumulate stats from 7104 documents (182660 virtual)
2024-05-03 02:22:54,312 - gensim.topic_coherence.text_analysis - INFO - 112 batches submitted to accumulate stats from 7168 documents (184289 virtual)
2024-05-03 02:22:54,328 - gensim.topic_coherence.text_analysis - INFO - 113 batches submitted to accumulate stats from 7232 documents (185825 virtual)
2024-05-03 02:22:54,328 - gensim.topic_coherence.text_analysis - INFO - 114 batches submitted to accumulate stats from 7296 documents (187420 virtual)
2024-05-03 02:22:54,328 - gensim.topic_coherence.text_analysis - INFO - 115 batches submitted to accumulate stats from 7360 documents (189102 virtual)
2024-05-03 02:22:54,328 - gensim.topic_coherence.text_analysis - INFO - 116 batches submitted to accumulate stats from 7424 documents (190745 virtual)
2024-05-03 02:22:54,328 - gensim.topic_coherence.text_analysis - INFO - 117 batches submitted to accumulate stats from 7488 documents (192238 virtual)
2024-05-03 02:22:54,328 - gensim.topic_coherence.text_analysis - INFO - 118 batches submitted to accumulate stats from 7552 documents (194107 virtual)
2024-05-03 02:22:54,328 - gensim.topic_coherence.text_analysis - INFO - 119 batches submitted to accumulate stats from 7616 documents (195570 virtual)
2024-05-03 02:22:54,344 - gensim.topic_coherence.text_analysis - INFO - 120 batches submitted to accumulate stats from 7680 documents (197064 virtual)
2024-05-03 02:22:54,344 - gensim.topic_coherence.text_analysis - INFO - 121 batches submitted to accumulate stats from 7744 documents (198821 virtual)
2024-05-03 02:22:54,359 - gensim.topic_coherence.text_analysis - INFO - 122 batches submitted to accumulate stats from 7808 documents (200394 virtual)
2024-05-03 02:22:54,359 - gensim.topic_coherence.text_analysis - INFO - 123 batches submitted to accumulate stats from 7872 documents (202352 virtual)
2024-05-03 02:22:54,359 - gensim.topic_coherence.text_analysis - INFO - 124 batches submitted to accumulate stats from 7936 documents (204181 virtual)
2024-05-03 02:22:54,359 - gensim.topic_coherence.text_analysis - INFO - 125 batches submitted to accumulate stats from 8000 documents (206063 virtual)
2024-05-03 02:22:54,359 - gensim.topic_coherence.text_analysis - INFO - 126 batches submitted to accumulate stats from 8064 documents (207766 virtual)
2024-05-03 02:22:54,359 - gensim.topic_coherence.text_analysis - INFO - 127 batches submitted to accumulate stats from 8128 documents (209460 virtual)
2024-05-03 02:22:54,375 - gensim.topic_coherence.text_analysis - INFO - 128 batches submitted to accumulate stats from 8192 documents (211022 virtual)
2024-05-03 02:22:54,375 - gensim.topic_coherence.text_analysis - INFO - 129 batches submitted to accumulate stats from 8256 documents (212632 virtual)
2024-05-03 02:22:54,375 - gensim.topic_coherence.text_analysis - INFO - 130 batches submitted to accumulate stats from 8320 documents (214210 virtual)
2024-05-03 02:22:54,390 - gensim.topic_coherence.text_analysis - INFO - 131 batches submitted to accumulate stats from 8384 documents (215651 virtual)
2024-05-03 02:22:54,390 - gensim.topic_coherence.text_analysis - INFO - 132 batches submitted to accumulate stats from 8448 documents (217300 virtual)
2024-05-03 02:22:54,390 - gensim.topic_coherence.text_analysis - INFO - 133 batches submitted to accumulate stats from 8512 documents (219035 virtual)
2024-05-03 02:22:54,390 - gensim.topic_coherence.text_analysis - INFO - 134 batches submitted to accumulate stats from 8576 documents (220675 virtual)
2024-05-03 02:22:54,406 - gensim.topic_coherence.text_analysis - INFO - 135 batches submitted to accumulate stats from 8640 documents (222562 virtual)
2024-05-03 02:22:54,406 - gensim.topic_coherence.text_analysis - INFO - 136 batches submitted to accumulate stats from 8704 documents (224243 virtual)
2024-05-03 02:22:54,406 - gensim.topic_coherence.text_analysis - INFO - 137 batches submitted to accumulate stats from 8768 documents (225942 virtual)
2024-05-03 02:22:54,406 - gensim.topic_coherence.text_analysis - INFO - 138 batches submitted to accumulate stats from 8832 documents (227774 virtual)
2024-05-03 02:22:54,406 - gensim.topic_coherence.text_analysis - INFO - 139 batches submitted to accumulate stats from 8896 documents (229378 virtual)
2024-05-03 02:22:54,422 - gensim.topic_coherence.text_analysis - INFO - 140 batches submitted to accumulate stats from 8960 documents (231026 virtual)
2024-05-03 02:22:54,422 - gensim.topic_coherence.text_analysis - INFO - 141 batches submitted to accumulate stats from 9024 documents (232664 virtual)
2024-05-03 02:22:54,437 - gensim.topic_coherence.text_analysis - INFO - 142 batches submitted to accumulate stats from 9088 documents (234376 virtual)
2024-05-03 02:22:54,437 - gensim.topic_coherence.text_analysis - INFO - 143 batches submitted to accumulate stats from 9152 documents (236034 virtual)
2024-05-03 02:22:54,437 - gensim.topic_coherence.text_analysis - INFO - 144 batches submitted to accumulate stats from 9216 documents (237753 virtual)
2024-05-03 02:22:54,437 - gensim.topic_coherence.text_analysis - INFO - 145 batches submitted to accumulate stats from 9280 documents (239260 virtual)
2024-05-03 02:22:54,453 - gensim.topic_coherence.text_analysis - INFO - 146 batches submitted to accumulate stats from 9344 documents (240889 virtual)
2024-05-03 02:22:54,453 - gensim.topic_coherence.text_analysis - INFO - 147 batches submitted to accumulate stats from 9408 documents (242607 virtual)
2024-05-03 02:22:54,453 - gensim.topic_coherence.text_analysis - INFO - 148 batches submitted to accumulate stats from 9472 documents (244168 virtual)
2024-05-03 02:22:54,453 - gensim.topic_coherence.text_analysis - INFO - 149 batches submitted to accumulate stats from 9536 documents (245827 virtual)
2024-05-03 02:22:54,453 - gensim.topic_coherence.text_analysis - INFO - 150 batches submitted to accumulate stats from 9600 documents (247340 virtual)
2024-05-03 02:22:54,469 - gensim.topic_coherence.text_analysis - INFO - 151 batches submitted to accumulate stats from 9664 documents (248935 virtual)
2024-05-03 02:22:54,484 - gensim.topic_coherence.text_analysis - INFO - 152 batches submitted to accumulate stats from 9728 documents (250682 virtual)
2024-05-03 02:22:54,484 - gensim.topic_coherence.text_analysis - INFO - 153 batches submitted to accumulate stats from 9792 documents (252525 virtual)
2024-05-03 02:22:54,500 - gensim.topic_coherence.text_analysis - INFO - 154 batches submitted to accumulate stats from 9856 documents (253964 virtual)
2024-05-03 02:22:54,531 - gensim.topic_coherence.text_analysis - INFO - 155 batches submitted to accumulate stats from 9920 documents (255761 virtual)
2024-05-03 02:22:54,531 - gensim.topic_coherence.text_analysis - INFO - 156 batches submitted to accumulate stats from 9984 documents (257656 virtual)
2024-05-03 02:22:54,531 - gensim.topic_coherence.text_analysis - INFO - 157 batches submitted to accumulate stats from 10048 documents (259320 virtual)
2024-05-03 02:22:54,531 - gensim.topic_coherence.text_analysis - INFO - 158 batches submitted to accumulate stats from 10112 documents (261193 virtual)
2024-05-03 02:22:54,531 - gensim.topic_coherence.text_analysis - INFO - 159 batches submitted to accumulate stats from 10176 documents (262723 virtual)
2024-05-03 02:22:54,547 - gensim.topic_coherence.text_analysis - INFO - 160 batches submitted to accumulate stats from 10240 documents (264223 virtual)
2024-05-03 02:22:54,547 - gensim.topic_coherence.text_analysis - INFO - 161 batches submitted to accumulate stats from 10304 documents (265694 virtual)
2024-05-03 02:22:54,547 - gensim.topic_coherence.text_analysis - INFO - 162 batches submitted to accumulate stats from 10368 documents (267448 virtual)
2024-05-03 02:22:54,547 - gensim.topic_coherence.text_analysis - INFO - 163 batches submitted to accumulate stats from 10432 documents (269251 virtual)
2024-05-03 02:22:54,547 - gensim.topic_coherence.text_analysis - INFO - 164 batches submitted to accumulate stats from 10496 documents (270973 virtual)
2024-05-03 02:22:54,562 - gensim.topic_coherence.text_analysis - INFO - 165 batches submitted to accumulate stats from 10560 documents (272683 virtual)
2024-05-03 02:22:54,562 - gensim.topic_coherence.text_analysis - INFO - 166 batches submitted to accumulate stats from 10624 documents (274294 virtual)
2024-05-03 02:22:54,562 - gensim.topic_coherence.text_analysis - INFO - 167 batches submitted to accumulate stats from 10688 documents (276045 virtual)
2024-05-03 02:22:54,562 - gensim.topic_coherence.text_analysis - INFO - 168 batches submitted to accumulate stats from 10752 documents (277496 virtual)
2024-05-03 02:22:54,562 - gensim.topic_coherence.text_analysis - INFO - 169 batches submitted to accumulate stats from 10816 documents (279131 virtual)
2024-05-03 02:22:54,578 - gensim.topic_coherence.text_analysis - INFO - 170 batches submitted to accumulate stats from 10880 documents (280812 virtual)
2024-05-03 02:22:54,578 - gensim.topic_coherence.text_analysis - INFO - 171 batches submitted to accumulate stats from 10944 documents (282408 virtual)
2024-05-03 02:22:54,578 - gensim.topic_coherence.text_analysis - INFO - 172 batches submitted to accumulate stats from 11008 documents (284125 virtual)
2024-05-03 02:22:54,578 - gensim.topic_coherence.text_analysis - INFO - 173 batches submitted to accumulate stats from 11072 documents (285575 virtual)
2024-05-03 02:22:54,578 - gensim.topic_coherence.text_analysis - INFO - 174 batches submitted to accumulate stats from 11136 documents (287185 virtual)
2024-05-03 02:22:54,578 - gensim.topic_coherence.text_analysis - INFO - 175 batches submitted to accumulate stats from 11200 documents (289145 virtual)
2024-05-03 02:22:54,594 - gensim.topic_coherence.text_analysis - INFO - 176 batches submitted to accumulate stats from 11264 documents (290900 virtual)
2024-05-03 02:22:54,594 - gensim.topic_coherence.text_analysis - INFO - 177 batches submitted to accumulate stats from 11328 documents (292686 virtual)
2024-05-03 02:22:54,594 - gensim.topic_coherence.text_analysis - INFO - 178 batches submitted to accumulate stats from 11392 documents (294505 virtual)
2024-05-03 02:22:54,609 - gensim.topic_coherence.text_analysis - INFO - 179 batches submitted to accumulate stats from 11456 documents (296236 virtual)
2024-05-03 02:22:54,609 - gensim.topic_coherence.text_analysis - INFO - 180 batches submitted to accumulate stats from 11520 documents (297647 virtual)
2024-05-03 02:22:54,609 - gensim.topic_coherence.text_analysis - INFO - 181 batches submitted to accumulate stats from 11584 documents (299327 virtual)
2024-05-03 02:22:54,609 - gensim.topic_coherence.text_analysis - INFO - 182 batches submitted to accumulate stats from 11648 documents (300853 virtual)
2024-05-03 02:22:54,609 - gensim.topic_coherence.text_analysis - INFO - 183 batches submitted to accumulate stats from 11712 documents (302601 virtual)
2024-05-03 02:22:54,625 - gensim.topic_coherence.text_analysis - INFO - 184 batches submitted to accumulate stats from 11776 documents (304181 virtual)
2024-05-03 02:22:54,625 - gensim.topic_coherence.text_analysis - INFO - 185 batches submitted to accumulate stats from 11840 documents (305710 virtual)
2024-05-03 02:22:54,640 - gensim.topic_coherence.text_analysis - INFO - 186 batches submitted to accumulate stats from 11904 documents (307265 virtual)
2024-05-03 02:22:54,640 - gensim.topic_coherence.text_analysis - INFO - 187 batches submitted to accumulate stats from 11968 documents (309148 virtual)
2024-05-03 02:22:54,640 - gensim.topic_coherence.text_analysis - INFO - 188 batches submitted to accumulate stats from 12032 documents (310818 virtual)
2024-05-03 02:22:54,640 - gensim.topic_coherence.text_analysis - INFO - 189 batches submitted to accumulate stats from 12096 documents (312472 virtual)
2024-05-03 02:22:54,640 - gensim.topic_coherence.text_analysis - INFO - 190 batches submitted to accumulate stats from 12160 documents (314119 virtual)
2024-05-03 02:22:54,640 - gensim.topic_coherence.text_analysis - INFO - 191 batches submitted to accumulate stats from 12224 documents (316054 virtual)
2024-05-03 02:22:54,656 - gensim.topic_coherence.text_analysis - INFO - 192 batches submitted to accumulate stats from 12288 documents (317685 virtual)
2024-05-03 02:22:54,656 - gensim.topic_coherence.text_analysis - INFO - 193 batches submitted to accumulate stats from 12352 documents (319351 virtual)
2024-05-03 02:22:54,656 - gensim.topic_coherence.text_analysis - INFO - 194 batches submitted to accumulate stats from 12416 documents (321100 virtual)
2024-05-03 02:22:54,672 - gensim.topic_coherence.text_analysis - INFO - 195 batches submitted to accumulate stats from 12480 documents (323099 virtual)
2024-05-03 02:22:54,672 - gensim.topic_coherence.text_analysis - INFO - 196 batches submitted to accumulate stats from 12544 documents (324548 virtual)
2024-05-03 02:22:54,672 - gensim.topic_coherence.text_analysis - INFO - 197 batches submitted to accumulate stats from 12608 documents (326231 virtual)
2024-05-03 02:22:54,672 - gensim.topic_coherence.text_analysis - INFO - 198 batches submitted to accumulate stats from 12672 documents (327851 virtual)
2024-05-03 02:22:54,672 - gensim.topic_coherence.text_analysis - INFO - 199 batches submitted to accumulate stats from 12736 documents (329386 virtual)
2024-05-03 02:22:54,672 - gensim.topic_coherence.text_analysis - INFO - 200 batches submitted to accumulate stats from 12800 documents (331113 virtual)
2024-05-03 02:22:54,672 - gensim.topic_coherence.text_analysis - INFO - 201 batches submitted to accumulate stats from 12864 documents (332712 virtual)
2024-05-03 02:22:54,688 - gensim.topic_coherence.text_analysis - INFO - 202 batches submitted to accumulate stats from 12928 documents (334135 virtual)
2024-05-03 02:22:54,688 - gensim.topic_coherence.text_analysis - INFO - 203 batches submitted to accumulate stats from 12992 documents (335914 virtual)
2024-05-03 02:22:54,703 - gensim.topic_coherence.text_analysis - INFO - 204 batches submitted to accumulate stats from 13056 documents (337641 virtual)
2024-05-03 02:22:54,703 - gensim.topic_coherence.text_analysis - INFO - 205 batches submitted to accumulate stats from 13120 documents (339449 virtual)
2024-05-03 02:22:54,703 - gensim.topic_coherence.text_analysis - INFO - 206 batches submitted to accumulate stats from 13184 documents (341168 virtual)
2024-05-03 02:22:54,703 - gensim.topic_coherence.text_analysis - INFO - 207 batches submitted to accumulate stats from 13248 documents (342833 virtual)
2024-05-03 02:22:54,703 - gensim.topic_coherence.text_analysis - INFO - 208 batches submitted to accumulate stats from 13312 documents (344704 virtual)
2024-05-03 02:22:54,719 - gensim.topic_coherence.text_analysis - INFO - 209 batches submitted to accumulate stats from 13376 documents (346650 virtual)
2024-05-03 02:22:54,719 - gensim.topic_coherence.text_analysis - INFO - 210 batches submitted to accumulate stats from 13440 documents (348531 virtual)
2024-05-03 02:22:54,719 - gensim.topic_coherence.text_analysis - INFO - 211 batches submitted to accumulate stats from 13504 documents (350342 virtual)
2024-05-03 02:22:54,719 - gensim.topic_coherence.text_analysis - INFO - 212 batches submitted to accumulate stats from 13568 documents (352068 virtual)
2024-05-03 02:22:54,735 - gensim.topic_coherence.text_analysis - INFO - 213 batches submitted to accumulate stats from 13632 documents (353789 virtual)
2024-05-03 02:22:54,735 - gensim.topic_coherence.text_analysis - INFO - 214 batches submitted to accumulate stats from 13696 documents (355216 virtual)
2024-05-03 02:22:54,735 - gensim.topic_coherence.text_analysis - INFO - 215 batches submitted to accumulate stats from 13760 documents (356990 virtual)
2024-05-03 02:22:54,735 - gensim.topic_coherence.text_analysis - INFO - 216 batches submitted to accumulate stats from 13824 documents (358762 virtual)
2024-05-03 02:22:54,735 - gensim.topic_coherence.text_analysis - INFO - 217 batches submitted to accumulate stats from 13888 documents (360320 virtual)
2024-05-03 02:22:54,735 - gensim.topic_coherence.text_analysis - INFO - 218 batches submitted to accumulate stats from 13952 documents (361867 virtual)
2024-05-03 02:22:54,735 - gensim.topic_coherence.text_analysis - INFO - 219 batches submitted to accumulate stats from 14016 documents (363519 virtual)
2024-05-03 02:22:54,750 - gensim.topic_coherence.text_analysis - INFO - 220 batches submitted to accumulate stats from 14080 documents (365141 virtual)
2024-05-03 02:22:54,750 - gensim.topic_coherence.text_analysis - INFO - 221 batches submitted to accumulate stats from 14144 documents (366934 virtual)
2024-05-03 02:22:54,750 - gensim.topic_coherence.text_analysis - INFO - 222 batches submitted to accumulate stats from 14208 documents (368448 virtual)
2024-05-03 02:22:54,750 - gensim.topic_coherence.text_analysis - INFO - 223 batches submitted to accumulate stats from 14272 documents (370012 virtual)
2024-05-03 02:22:54,750 - gensim.topic_coherence.text_analysis - INFO - 224 batches submitted to accumulate stats from 14336 documents (371704 virtual)
2024-05-03 02:22:54,750 - gensim.topic_coherence.text_analysis - INFO - 225 batches submitted to accumulate stats from 14400 documents (373331 virtual)
2024-05-03 02:22:54,766 - gensim.topic_coherence.text_analysis - INFO - 226 batches submitted to accumulate stats from 14464 documents (375174 virtual)
2024-05-03 02:22:54,766 - gensim.topic_coherence.text_analysis - INFO - 227 batches submitted to accumulate stats from 14528 documents (377135 virtual)
2024-05-03 02:22:54,766 - gensim.topic_coherence.text_analysis - INFO - 228 batches submitted to accumulate stats from 14592 documents (378860 virtual)
2024-05-03 02:22:54,766 - gensim.topic_coherence.text_analysis - INFO - 229 batches submitted to accumulate stats from 14656 documents (380581 virtual)
2024-05-03 02:22:54,766 - gensim.topic_coherence.text_analysis - INFO - 230 batches submitted to accumulate stats from 14720 documents (382278 virtual)
2024-05-03 02:22:54,782 - gensim.topic_coherence.text_analysis - INFO - 231 batches submitted to accumulate stats from 14784 documents (383970 virtual)
2024-05-03 02:22:54,782 - gensim.topic_coherence.text_analysis - INFO - 232 batches submitted to accumulate stats from 14848 documents (385680 virtual)
2024-05-03 02:22:54,782 - gensim.topic_coherence.text_analysis - INFO - 233 batches submitted to accumulate stats from 14912 documents (387378 virtual)
2024-05-03 02:22:54,782 - gensim.topic_coherence.text_analysis - INFO - 234 batches submitted to accumulate stats from 14976 documents (388908 virtual)
2024-05-03 02:22:54,782 - gensim.topic_coherence.text_analysis - INFO - 235 batches submitted to accumulate stats from 15040 documents (390473 virtual)
2024-05-03 02:22:54,782 - gensim.topic_coherence.text_analysis - INFO - 236 batches submitted to accumulate stats from 15104 documents (391966 virtual)
2024-05-03 02:22:54,797 - gensim.topic_coherence.text_analysis - INFO - 237 batches submitted to accumulate stats from 15168 documents (393446 virtual)
2024-05-03 02:22:54,797 - gensim.topic_coherence.text_analysis - INFO - 238 batches submitted to accumulate stats from 15232 documents (394979 virtual)
2024-05-03 02:22:54,797 - gensim.topic_coherence.text_analysis - INFO - 239 batches submitted to accumulate stats from 15296 documents (396707 virtual)
2024-05-03 02:22:54,797 - gensim.topic_coherence.text_analysis - INFO - 240 batches submitted to accumulate stats from 15360 documents (398434 virtual)
2024-05-03 02:22:54,797 - gensim.topic_coherence.text_analysis - INFO - 241 batches submitted to accumulate stats from 15424 documents (400092 virtual)
2024-05-03 02:22:54,813 - gensim.topic_coherence.text_analysis - INFO - 242 batches submitted to accumulate stats from 15488 documents (402054 virtual)
2024-05-03 02:22:54,813 - gensim.topic_coherence.text_analysis - INFO - 243 batches submitted to accumulate stats from 15552 documents (403465 virtual)
2024-05-03 02:22:54,828 - gensim.topic_coherence.text_analysis - INFO - 244 batches submitted to accumulate stats from 15616 documents (405289 virtual)
2024-05-03 02:22:54,828 - gensim.topic_coherence.text_analysis - INFO - 245 batches submitted to accumulate stats from 15680 documents (406750 virtual)
2024-05-03 02:22:54,828 - gensim.topic_coherence.text_analysis - INFO - 246 batches submitted to accumulate stats from 15744 documents (408306 virtual)
2024-05-03 02:22:54,828 - gensim.topic_coherence.text_analysis - INFO - 247 batches submitted to accumulate stats from 15808 documents (410097 virtual)
2024-05-03 02:22:54,828 - gensim.topic_coherence.text_analysis - INFO - 248 batches submitted to accumulate stats from 15872 documents (411915 virtual)
2024-05-03 02:22:54,828 - gensim.topic_coherence.text_analysis - INFO - 249 batches submitted to accumulate stats from 15936 documents (413538 virtual)
2024-05-03 02:22:54,844 - gensim.topic_coherence.text_analysis - INFO - 250 batches submitted to accumulate stats from 16000 documents (415307 virtual)
2024-05-03 02:22:54,844 - gensim.topic_coherence.text_analysis - INFO - 251 batches submitted to accumulate stats from 16064 documents (416993 virtual)
2024-05-03 02:22:54,844 - gensim.topic_coherence.text_analysis - INFO - 252 batches submitted to accumulate stats from 16128 documents (418582 virtual)
2024-05-03 02:22:54,860 - gensim.topic_coherence.text_analysis - INFO - 253 batches submitted to accumulate stats from 16192 documents (420282 virtual)
2024-05-03 02:22:54,875 - gensim.topic_coherence.text_analysis - INFO - 254 batches submitted to accumulate stats from 16256 documents (421859 virtual)
2024-05-03 02:22:54,875 - gensim.topic_coherence.text_analysis - INFO - 255 batches submitted to accumulate stats from 16320 documents (423441 virtual)
2024-05-03 02:22:54,907 - gensim.topic_coherence.text_analysis - INFO - 256 batches submitted to accumulate stats from 16384 documents (425169 virtual)
2024-05-03 02:22:54,907 - gensim.topic_coherence.text_analysis - INFO - 257 batches submitted to accumulate stats from 16448 documents (426804 virtual)
2024-05-03 02:22:54,907 - gensim.topic_coherence.text_analysis - INFO - 258 batches submitted to accumulate stats from 16512 documents (428503 virtual)
2024-05-03 02:22:54,907 - gensim.topic_coherence.text_analysis - INFO - 259 batches submitted to accumulate stats from 16576 documents (430063 virtual)
2024-05-03 02:22:54,922 - gensim.topic_coherence.text_analysis - INFO - 260 batches submitted to accumulate stats from 16640 documents (431672 virtual)
2024-05-03 02:22:54,922 - gensim.topic_coherence.text_analysis - INFO - 261 batches submitted to accumulate stats from 16704 documents (433298 virtual)
2024-05-03 02:22:54,922 - gensim.topic_coherence.text_analysis - INFO - 262 batches submitted to accumulate stats from 16768 documents (434951 virtual)
2024-05-03 02:22:54,922 - gensim.topic_coherence.text_analysis - INFO - 263 batches submitted to accumulate stats from 16832 documents (436505 virtual)
2024-05-03 02:22:54,922 - gensim.topic_coherence.text_analysis - INFO - 264 batches submitted to accumulate stats from 16896 documents (438134 virtual)
2024-05-03 02:22:54,922 - gensim.topic_coherence.text_analysis - INFO - 265 batches submitted to accumulate stats from 16960 documents (439831 virtual)
2024-05-03 02:22:54,922 - gensim.topic_coherence.text_analysis - INFO - 266 batches submitted to accumulate stats from 17024 documents (441666 virtual)
2024-05-03 02:22:54,938 - gensim.topic_coherence.text_analysis - INFO - 267 batches submitted to accumulate stats from 17088 documents (443389 virtual)
2024-05-03 02:22:54,938 - gensim.topic_coherence.text_analysis - INFO - 268 batches submitted to accumulate stats from 17152 documents (445008 virtual)
2024-05-03 02:22:54,938 - gensim.topic_coherence.text_analysis - INFO - 269 batches submitted to accumulate stats from 17216 documents (446691 virtual)
2024-05-03 02:22:54,938 - gensim.topic_coherence.text_analysis - INFO - 270 batches submitted to accumulate stats from 17280 documents (448258 virtual)
2024-05-03 02:22:54,938 - gensim.topic_coherence.text_analysis - INFO - 271 batches submitted to accumulate stats from 17344 documents (450177 virtual)
2024-05-03 02:22:54,938 - gensim.topic_coherence.text_analysis - INFO - 272 batches submitted to accumulate stats from 17408 documents (451838 virtual)
2024-05-03 02:22:54,953 - gensim.topic_coherence.text_analysis - INFO - 273 batches submitted to accumulate stats from 17472 documents (453765 virtual)
2024-05-03 02:22:54,953 - gensim.topic_coherence.text_analysis - INFO - 274 batches submitted to accumulate stats from 17536 documents (455302 virtual)
2024-05-03 02:22:54,953 - gensim.topic_coherence.text_analysis - INFO - 275 batches submitted to accumulate stats from 17600 documents (457026 virtual)
2024-05-03 02:22:54,953 - gensim.topic_coherence.text_analysis - INFO - 276 batches submitted to accumulate stats from 17664 documents (458678 virtual)
2024-05-03 02:22:54,953 - gensim.topic_coherence.text_analysis - INFO - 277 batches submitted to accumulate stats from 17728 documents (460363 virtual)
2024-05-03 02:22:54,953 - gensim.topic_coherence.text_analysis - INFO - 278 batches submitted to accumulate stats from 17792 documents (462290 virtual)
2024-05-03 02:22:54,969 - gensim.topic_coherence.text_analysis - INFO - 279 batches submitted to accumulate stats from 17856 documents (464043 virtual)
2024-05-03 02:22:54,969 - gensim.topic_coherence.text_analysis - INFO - 280 batches submitted to accumulate stats from 17920 documents (465601 virtual)
2024-05-03 02:22:54,969 - gensim.topic_coherence.text_analysis - INFO - 281 batches submitted to accumulate stats from 17984 documents (467208 virtual)
2024-05-03 02:22:54,969 - gensim.topic_coherence.text_analysis - INFO - 282 batches submitted to accumulate stats from 18048 documents (468985 virtual)
2024-05-03 02:22:54,969 - gensim.topic_coherence.text_analysis - INFO - 283 batches submitted to accumulate stats from 18112 documents (470540 virtual)
2024-05-03 02:22:54,969 - gensim.topic_coherence.text_analysis - INFO - 284 batches submitted to accumulate stats from 18176 documents (472187 virtual)
2024-05-03 02:22:54,985 - gensim.topic_coherence.text_analysis - INFO - 285 batches submitted to accumulate stats from 18240 documents (473657 virtual)
2024-05-03 02:22:54,985 - gensim.topic_coherence.text_analysis - INFO - 286 batches submitted to accumulate stats from 18304 documents (475433 virtual)
2024-05-03 02:22:54,985 - gensim.topic_coherence.text_analysis - INFO - 287 batches submitted to accumulate stats from 18368 documents (476949 virtual)
2024-05-03 02:22:55,000 - gensim.topic_coherence.text_analysis - INFO - 288 batches submitted to accumulate stats from 18432 documents (478519 virtual)
2024-05-03 02:22:55,000 - gensim.topic_coherence.text_analysis - INFO - 289 batches submitted to accumulate stats from 18496 documents (480346 virtual)
2024-05-03 02:22:55,000 - gensim.topic_coherence.text_analysis - INFO - 290 batches submitted to accumulate stats from 18560 documents (482113 virtual)
2024-05-03 02:22:55,000 - gensim.topic_coherence.text_analysis - INFO - 291 batches submitted to accumulate stats from 18624 documents (483726 virtual)
2024-05-03 02:22:55,000 - gensim.topic_coherence.text_analysis - INFO - 292 batches submitted to accumulate stats from 18688 documents (485454 virtual)
2024-05-03 02:22:55,000 - gensim.topic_coherence.text_analysis - INFO - 293 batches submitted to accumulate stats from 18752 documents (487120 virtual)
2024-05-03 02:22:55,016 - gensim.topic_coherence.text_analysis - INFO - 294 batches submitted to accumulate stats from 18816 documents (488702 virtual)
2024-05-03 02:22:55,016 - gensim.topic_coherence.text_analysis - INFO - 295 batches submitted to accumulate stats from 18880 documents (490511 virtual)
2024-05-03 02:22:55,016 - gensim.topic_coherence.text_analysis - INFO - 296 batches submitted to accumulate stats from 18944 documents (492226 virtual)
2024-05-03 02:22:55,032 - gensim.topic_coherence.text_analysis - INFO - 297 batches submitted to accumulate stats from 19008 documents (494165 virtual)
2024-05-03 02:22:55,032 - gensim.topic_coherence.text_analysis - INFO - 298 batches submitted to accumulate stats from 19072 documents (495678 virtual)
2024-05-03 02:22:55,032 - gensim.topic_coherence.text_analysis - INFO - 299 batches submitted to accumulate stats from 19136 documents (497503 virtual)
2024-05-03 02:22:55,032 - gensim.topic_coherence.text_analysis - INFO - 300 batches submitted to accumulate stats from 19200 documents (499131 virtual)
2024-05-03 02:22:55,032 - gensim.topic_coherence.text_analysis - INFO - 301 batches submitted to accumulate stats from 19264 documents (500986 virtual)
2024-05-03 02:22:55,032 - gensim.topic_coherence.text_analysis - INFO - 302 batches submitted to accumulate stats from 19328 documents (502687 virtual)
2024-05-03 02:22:55,047 - gensim.topic_coherence.text_analysis - INFO - 303 batches submitted to accumulate stats from 19392 documents (504106 virtual)
2024-05-03 02:22:55,047 - gensim.topic_coherence.text_analysis - INFO - 304 batches submitted to accumulate stats from 19456 documents (505909 virtual)
2024-05-03 02:22:55,047 - gensim.topic_coherence.text_analysis - INFO - 305 batches submitted to accumulate stats from 19520 documents (507546 virtual)
2024-05-03 02:22:55,063 - gensim.topic_coherence.text_analysis - INFO - 306 batches submitted to accumulate stats from 19584 documents (509312 virtual)
2024-05-03 02:22:55,063 - gensim.topic_coherence.text_analysis - INFO - 307 batches submitted to accumulate stats from 19648 documents (510954 virtual)
2024-05-03 02:22:55,063 - gensim.topic_coherence.text_analysis - INFO - 308 batches submitted to accumulate stats from 19712 documents (512465 virtual)
2024-05-03 02:22:55,063 - gensim.topic_coherence.text_analysis - INFO - 309 batches submitted to accumulate stats from 19776 documents (514132 virtual)
2024-05-03 02:22:55,063 - gensim.topic_coherence.text_analysis - INFO - 310 batches submitted to accumulate stats from 19840 documents (515846 virtual)
2024-05-03 02:22:55,063 - gensim.topic_coherence.text_analysis - INFO - 311 batches submitted to accumulate stats from 19904 documents (517436 virtual)
2024-05-03 02:22:55,078 - gensim.topic_coherence.text_analysis - INFO - 312 batches submitted to accumulate stats from 19968 documents (519076 virtual)
2024-05-03 02:22:55,078 - gensim.topic_coherence.text_analysis - INFO - 313 batches submitted to accumulate stats from 20032 documents (520797 virtual)
2024-05-03 02:22:55,078 - gensim.topic_coherence.text_analysis - INFO - 314 batches submitted to accumulate stats from 20096 documents (522328 virtual)
2024-05-03 02:22:55,469 - gensim.topic_coherence.text_analysis - INFO - 11 accumulators retrieved from output queue
2024-05-03 02:22:55,594 - gensim.topic_coherence.text_analysis - INFO - accumulated word occurrence stats for 523696 virtual documents
2024-05-03 02:22:56,657 - gensim.corpora.dictionary - INFO - adding document #0 to Dictionary<0 unique tokens: []>
2024-05-03 02:22:57,063 - gensim.corpora.dictionary - INFO - adding document #10000 to Dictionary<23643 unique tokens: ['-', '.', '?', 'a', 'africa']...>
2024-05-03 02:22:57,454 - gensim.corpora.dictionary - INFO - adding document #20000 to Dictionary<32428 unique tokens: ['-', '.', '?', 'a', 'africa']...>
2024-05-03 02:22:57,469 - gensim.corpora.dictionary - INFO - built Dictionary<32503 unique tokens: ['-', '.', '?', 'a', 'africa']...> from 20086 documents (total 703102 corpus positions)
2024-05-03 02:22:57,469 - gensim.utils - INFO - Dictionary lifecycle event {'msg': "built Dictionary<32503 unique tokens: ['-', '.', '?', 'a', 'africa']...> from 20086 documents (total 703102 corpus positions)", 'datetime': '2024-05-03T02:22:57.469930', 'gensim': '4.3.2', 'python': '3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}
2024-05-03 02:22:57,485 - gensim.topic_coherence.probability_estimation - INFO - using ParallelWordOccurrenceAccumulator<processes=11, batch_size=64> to estimate probabilities from sliding windows
2024-05-03 02:23:50,351 - gensim.topic_coherence.text_analysis - INFO - 11 accumulators retrieved from output queue
2024-05-03 02:23:50,445 - gensim.topic_coherence.text_analysis - INFO - accumulated word occurrence stats for 21620 virtual documents
2024-05-03 02:23:55,114 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=50, bias=False)
  (W_q): Linear(in_features=300, out_features=50, bias=False)
  (W_v): Linear(in_features=300, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,138,605
Freeze params: 0
2024-05-03 02:25:26,552 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=50, bias=False)
  (W_q): Linear(in_features=300, out_features=50, bias=False)
  (W_v): Linear(in_features=300, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,138,605
Freeze params: 0
2024-05-03 02:29:03,115 - trainer - INFO -     epoch          : 1
2024-05-03 02:29:03,115 - trainer - INFO -     loss           : 1.147455
2024-05-03 02:29:03,115 - trainer - INFO -     accuracy       : 0.671444
2024-05-03 02:29:03,115 - trainer - INFO -     macro_f        : 0.653313
2024-05-03 02:29:03,115 - trainer - INFO -     precision      : 0.684793
2024-05-03 02:29:03,115 - trainer - INFO -     recall         : 0.671444
2024-05-03 02:29:03,115 - trainer - INFO -     doc_entropy    : 2.422342
2024-05-03 02:29:03,115 - trainer - INFO -     val_loss       : 0.997176
2024-05-03 02:29:03,115 - trainer - INFO -     val_accuracy   : 0.707856
2024-05-03 02:29:03,115 - trainer - INFO -     val_macro_f    : 0.700245
2024-05-03 02:29:03,115 - trainer - INFO -     val_precision  : 0.738896
2024-05-03 02:29:03,115 - trainer - INFO -     val_recall     : 0.707856
2024-05-03 02:29:03,115 - trainer - INFO -     val_doc_entropy: 2.649725
2024-05-03 02:29:03,115 - trainer - INFO -     test_loss      : 0.999256
2024-05-03 02:29:03,115 - trainer - INFO -     test_accuracy  : 0.704969
2024-05-03 02:29:03,115 - trainer - INFO -     test_macro_f   : 0.694396
2024-05-03 02:29:03,115 - trainer - INFO -     test_precision : 0.730612
2024-05-03 02:29:03,115 - trainer - INFO -     test_recall    : 0.704969
2024-05-03 02:29:03,115 - trainer - INFO -     test_doc_entropy: 2.651051
2024-05-03 02:32:49,176 - trainer - INFO -     epoch          : 2
2024-05-03 02:32:49,176 - trainer - INFO -     loss           : 0.80504
2024-05-03 02:32:49,176 - trainer - INFO -     accuracy       : 0.759174
2024-05-03 02:32:49,176 - trainer - INFO -     macro_f        : 0.750513
2024-05-03 02:32:49,176 - trainer - INFO -     precision      : 0.783754
2024-05-03 02:32:49,176 - trainer - INFO -     recall         : 0.759174
2024-05-03 02:32:49,176 - trainer - INFO -     doc_entropy    : 2.023295
2024-05-03 02:32:49,176 - trainer - INFO -     val_loss       : 1.004548
2024-05-03 02:32:49,176 - trainer - INFO -     val_accuracy   : 0.708902
2024-05-03 02:32:49,176 - trainer - INFO -     val_macro_f    : 0.700584
2024-05-03 02:32:49,176 - trainer - INFO -     val_precision  : 0.738377
2024-05-03 02:32:49,176 - trainer - INFO -     val_recall     : 0.708902
2024-05-03 02:32:49,176 - trainer - INFO -     val_doc_entropy: 2.446161
2024-05-03 02:32:49,176 - trainer - INFO -     test_loss      : 0.996567
2024-05-03 02:32:49,176 - trainer - INFO -     test_accuracy  : 0.711092
2024-05-03 02:32:49,176 - trainer - INFO -     test_macro_f   : 0.70309
2024-05-03 02:32:49,176 - trainer - INFO -     test_precision : 0.742568
2024-05-03 02:32:49,176 - trainer - INFO -     test_recall    : 0.711092
2024-05-03 02:32:49,176 - trainer - INFO -     test_doc_entropy: 2.448555
2024-05-03 02:33:29,001 - gensim.corpora.dictionary - INFO - adding document #0 to Dictionary<0 unique tokens: []>
2024-05-03 02:33:29,392 - gensim.corpora.dictionary - INFO - adding document #10000 to Dictionary<23643 unique tokens: ['-', '.', '?', 'a', 'africa']...>
2024-05-03 02:33:29,814 - gensim.corpora.dictionary - INFO - adding document #20000 to Dictionary<32428 unique tokens: ['-', '.', '?', 'a', 'africa']...>
2024-05-03 02:33:29,814 - gensim.corpora.dictionary - INFO - built Dictionary<32503 unique tokens: ['-', '.', '?', 'a', 'africa']...> from 20086 documents (total 703102 corpus positions)
2024-05-03 02:33:29,829 - gensim.utils - INFO - Dictionary lifecycle event {'msg': "built Dictionary<32503 unique tokens: ['-', '.', '?', 'a', 'africa']...> from 20086 documents (total 703102 corpus positions)", 'datetime': '2024-05-03T02:33:29.814312', 'gensim': '4.3.2', 'python': '3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}
2024-05-03 02:33:29,845 - gensim.topic_coherence.probability_estimation - INFO - using ParallelWordOccurrenceAccumulator<processes=11, batch_size=64> to estimate probabilities from sliding windows
2024-05-03 02:34:31,124 - gensim.topic_coherence.text_analysis - INFO - 1 batches submitted to accumulate stats from 64 documents (1358 virtual)
2024-05-03 02:34:31,140 - gensim.topic_coherence.text_analysis - INFO - 2 batches submitted to accumulate stats from 128 documents (3142 virtual)
2024-05-03 02:34:31,140 - gensim.topic_coherence.text_analysis - INFO - 3 batches submitted to accumulate stats from 192 documents (4707 virtual)
2024-05-03 02:34:31,140 - gensim.topic_coherence.text_analysis - INFO - 4 batches submitted to accumulate stats from 256 documents (6346 virtual)
2024-05-03 02:34:31,140 - gensim.topic_coherence.text_analysis - INFO - 5 batches submitted to accumulate stats from 320 documents (7961 virtual)
2024-05-03 02:34:31,140 - gensim.topic_coherence.text_analysis - INFO - 6 batches submitted to accumulate stats from 384 documents (9298 virtual)
2024-05-03 02:34:31,156 - gensim.topic_coherence.text_analysis - INFO - 7 batches submitted to accumulate stats from 448 documents (11371 virtual)
2024-05-03 02:34:31,156 - gensim.topic_coherence.text_analysis - INFO - 8 batches submitted to accumulate stats from 512 documents (13011 virtual)
2024-05-03 02:34:31,156 - gensim.topic_coherence.text_analysis - INFO - 9 batches submitted to accumulate stats from 576 documents (14534 virtual)
2024-05-03 02:34:31,156 - gensim.topic_coherence.text_analysis - INFO - 10 batches submitted to accumulate stats from 640 documents (16161 virtual)
2024-05-03 02:34:31,171 - gensim.topic_coherence.text_analysis - INFO - 11 batches submitted to accumulate stats from 704 documents (17689 virtual)
2024-05-03 02:34:31,171 - gensim.topic_coherence.text_analysis - INFO - 12 batches submitted to accumulate stats from 768 documents (19256 virtual)
2024-05-03 02:34:31,187 - gensim.topic_coherence.text_analysis - INFO - 13 batches submitted to accumulate stats from 832 documents (21175 virtual)
2024-05-03 02:34:31,187 - gensim.topic_coherence.text_analysis - INFO - 14 batches submitted to accumulate stats from 896 documents (22850 virtual)
2024-05-03 02:34:31,203 - gensim.topic_coherence.text_analysis - INFO - 15 batches submitted to accumulate stats from 960 documents (24681 virtual)
2024-05-03 02:34:31,203 - gensim.topic_coherence.text_analysis - INFO - 16 batches submitted to accumulate stats from 1024 documents (26222 virtual)
2024-05-03 02:34:31,203 - gensim.topic_coherence.text_analysis - INFO - 17 batches submitted to accumulate stats from 1088 documents (27953 virtual)
2024-05-03 02:34:31,203 - gensim.topic_coherence.text_analysis - INFO - 18 batches submitted to accumulate stats from 1152 documents (29594 virtual)
2024-05-03 02:34:31,203 - gensim.topic_coherence.text_analysis - INFO - 19 batches submitted to accumulate stats from 1216 documents (31239 virtual)
2024-05-03 02:34:31,218 - gensim.topic_coherence.text_analysis - INFO - 20 batches submitted to accumulate stats from 1280 documents (32901 virtual)
2024-05-03 02:34:31,218 - gensim.topic_coherence.text_analysis - INFO - 21 batches submitted to accumulate stats from 1344 documents (34481 virtual)
2024-05-03 02:34:31,218 - gensim.topic_coherence.text_analysis - INFO - 22 batches submitted to accumulate stats from 1408 documents (36151 virtual)
2024-05-03 02:34:31,234 - gensim.topic_coherence.text_analysis - INFO - 23 batches submitted to accumulate stats from 1472 documents (37837 virtual)
2024-05-03 02:34:31,234 - gensim.topic_coherence.text_analysis - INFO - 24 batches submitted to accumulate stats from 1536 documents (39453 virtual)
2024-05-03 02:34:31,249 - gensim.topic_coherence.text_analysis - INFO - 25 batches submitted to accumulate stats from 1600 documents (41046 virtual)
2024-05-03 02:34:31,249 - gensim.topic_coherence.text_analysis - INFO - 26 batches submitted to accumulate stats from 1664 documents (42892 virtual)
2024-05-03 02:34:31,249 - gensim.topic_coherence.text_analysis - INFO - 27 batches submitted to accumulate stats from 1728 documents (44445 virtual)
2024-05-03 02:34:31,249 - gensim.topic_coherence.text_analysis - INFO - 28 batches submitted to accumulate stats from 1792 documents (46073 virtual)
2024-05-03 02:34:31,249 - gensim.topic_coherence.text_analysis - INFO - 29 batches submitted to accumulate stats from 1856 documents (47643 virtual)
2024-05-03 02:34:31,249 - gensim.topic_coherence.text_analysis - INFO - 30 batches submitted to accumulate stats from 1920 documents (49252 virtual)
2024-05-03 02:34:31,265 - gensim.topic_coherence.text_analysis - INFO - 31 batches submitted to accumulate stats from 1984 documents (50774 virtual)
2024-05-03 02:34:31,265 - gensim.topic_coherence.text_analysis - INFO - 32 batches submitted to accumulate stats from 2048 documents (52387 virtual)
2024-05-03 02:34:31,281 - gensim.topic_coherence.text_analysis - INFO - 33 batches submitted to accumulate stats from 2112 documents (53815 virtual)
2024-05-03 02:34:31,281 - gensim.topic_coherence.text_analysis - INFO - 34 batches submitted to accumulate stats from 2176 documents (55540 virtual)
2024-05-03 02:34:31,281 - gensim.topic_coherence.text_analysis - INFO - 35 batches submitted to accumulate stats from 2240 documents (57348 virtual)
2024-05-03 02:34:31,281 - gensim.topic_coherence.text_analysis - INFO - 36 batches submitted to accumulate stats from 2304 documents (59258 virtual)
2024-05-03 02:34:31,296 - gensim.topic_coherence.text_analysis - INFO - 37 batches submitted to accumulate stats from 2368 documents (60957 virtual)
2024-05-03 02:34:31,296 - gensim.topic_coherence.text_analysis - INFO - 38 batches submitted to accumulate stats from 2432 documents (62425 virtual)
2024-05-03 02:34:31,296 - gensim.topic_coherence.text_analysis - INFO - 39 batches submitted to accumulate stats from 2496 documents (64029 virtual)
2024-05-03 02:34:31,296 - gensim.topic_coherence.text_analysis - INFO - 40 batches submitted to accumulate stats from 2560 documents (65725 virtual)
2024-05-03 02:34:31,296 - gensim.topic_coherence.text_analysis - INFO - 41 batches submitted to accumulate stats from 2624 documents (67346 virtual)
2024-05-03 02:34:31,296 - gensim.topic_coherence.text_analysis - INFO - 42 batches submitted to accumulate stats from 2688 documents (68863 virtual)
2024-05-03 02:34:31,312 - gensim.topic_coherence.text_analysis - INFO - 43 batches submitted to accumulate stats from 2752 documents (70539 virtual)
2024-05-03 02:34:31,312 - gensim.topic_coherence.text_analysis - INFO - 44 batches submitted to accumulate stats from 2816 documents (71922 virtual)
2024-05-03 02:34:31,312 - gensim.topic_coherence.text_analysis - INFO - 45 batches submitted to accumulate stats from 2880 documents (73294 virtual)
2024-05-03 02:34:31,312 - gensim.topic_coherence.text_analysis - INFO - 46 batches submitted to accumulate stats from 2944 documents (75084 virtual)
2024-05-03 02:34:31,328 - gensim.topic_coherence.text_analysis - INFO - 47 batches submitted to accumulate stats from 3008 documents (76769 virtual)
2024-05-03 02:34:31,328 - gensim.topic_coherence.text_analysis - INFO - 48 batches submitted to accumulate stats from 3072 documents (78312 virtual)
2024-05-03 02:34:31,328 - gensim.topic_coherence.text_analysis - INFO - 49 batches submitted to accumulate stats from 3136 documents (80039 virtual)
2024-05-03 02:34:31,328 - gensim.topic_coherence.text_analysis - INFO - 50 batches submitted to accumulate stats from 3200 documents (81572 virtual)
2024-05-03 02:34:31,328 - gensim.topic_coherence.text_analysis - INFO - 51 batches submitted to accumulate stats from 3264 documents (83189 virtual)
2024-05-03 02:34:31,343 - gensim.topic_coherence.text_analysis - INFO - 52 batches submitted to accumulate stats from 3328 documents (84783 virtual)
2024-05-03 02:34:31,343 - gensim.topic_coherence.text_analysis - INFO - 53 batches submitted to accumulate stats from 3392 documents (86570 virtual)
2024-05-03 02:34:31,343 - gensim.topic_coherence.text_analysis - INFO - 54 batches submitted to accumulate stats from 3456 documents (88371 virtual)
2024-05-03 02:34:31,343 - gensim.topic_coherence.text_analysis - INFO - 55 batches submitted to accumulate stats from 3520 documents (90295 virtual)
2024-05-03 02:34:31,343 - gensim.topic_coherence.text_analysis - INFO - 56 batches submitted to accumulate stats from 3584 documents (92118 virtual)
2024-05-03 02:34:31,343 - gensim.topic_coherence.text_analysis - INFO - 57 batches submitted to accumulate stats from 3648 documents (93914 virtual)
2024-05-03 02:34:31,359 - gensim.topic_coherence.text_analysis - INFO - 58 batches submitted to accumulate stats from 3712 documents (95729 virtual)
2024-05-03 02:34:31,359 - gensim.topic_coherence.text_analysis - INFO - 59 batches submitted to accumulate stats from 3776 documents (97293 virtual)
2024-05-03 02:34:31,359 - gensim.topic_coherence.text_analysis - INFO - 60 batches submitted to accumulate stats from 3840 documents (98908 virtual)
2024-05-03 02:34:31,359 - gensim.topic_coherence.text_analysis - INFO - 61 batches submitted to accumulate stats from 3904 documents (100586 virtual)
2024-05-03 02:34:31,359 - gensim.topic_coherence.text_analysis - INFO - 62 batches submitted to accumulate stats from 3968 documents (102208 virtual)
2024-05-03 02:34:31,374 - gensim.topic_coherence.text_analysis - INFO - 63 batches submitted to accumulate stats from 4032 documents (103862 virtual)
2024-05-03 02:34:31,374 - gensim.topic_coherence.text_analysis - INFO - 64 batches submitted to accumulate stats from 4096 documents (105500 virtual)
2024-05-03 02:34:31,374 - gensim.topic_coherence.text_analysis - INFO - 65 batches submitted to accumulate stats from 4160 documents (106974 virtual)
2024-05-03 02:34:31,390 - gensim.topic_coherence.text_analysis - INFO - 66 batches submitted to accumulate stats from 4224 documents (108587 virtual)
2024-05-03 02:34:31,406 - gensim.topic_coherence.text_analysis - INFO - 67 batches submitted to accumulate stats from 4288 documents (110059 virtual)
2024-05-03 02:34:31,406 - gensim.topic_coherence.text_analysis - INFO - 68 batches submitted to accumulate stats from 4352 documents (111905 virtual)
2024-05-03 02:34:31,406 - gensim.topic_coherence.text_analysis - INFO - 69 batches submitted to accumulate stats from 4416 documents (113549 virtual)
2024-05-03 02:34:31,406 - gensim.topic_coherence.text_analysis - INFO - 70 batches submitted to accumulate stats from 4480 documents (115163 virtual)
2024-05-03 02:34:31,406 - gensim.topic_coherence.text_analysis - INFO - 71 batches submitted to accumulate stats from 4544 documents (117083 virtual)
2024-05-03 02:34:31,406 - gensim.topic_coherence.text_analysis - INFO - 72 batches submitted to accumulate stats from 4608 documents (118654 virtual)
2024-05-03 02:34:31,421 - gensim.topic_coherence.text_analysis - INFO - 73 batches submitted to accumulate stats from 4672 documents (120224 virtual)
2024-05-03 02:34:31,421 - gensim.topic_coherence.text_analysis - INFO - 74 batches submitted to accumulate stats from 4736 documents (121786 virtual)
2024-05-03 02:34:31,421 - gensim.topic_coherence.text_analysis - INFO - 75 batches submitted to accumulate stats from 4800 documents (123233 virtual)
2024-05-03 02:34:31,421 - gensim.topic_coherence.text_analysis - INFO - 76 batches submitted to accumulate stats from 4864 documents (124761 virtual)
2024-05-03 02:34:31,421 - gensim.topic_coherence.text_analysis - INFO - 77 batches submitted to accumulate stats from 4928 documents (126221 virtual)
2024-05-03 02:34:31,421 - gensim.topic_coherence.text_analysis - INFO - 78 batches submitted to accumulate stats from 4992 documents (127857 virtual)
2024-05-03 02:34:31,437 - gensim.topic_coherence.text_analysis - INFO - 79 batches submitted to accumulate stats from 5056 documents (129432 virtual)
2024-05-03 02:34:31,437 - gensim.topic_coherence.text_analysis - INFO - 80 batches submitted to accumulate stats from 5120 documents (130948 virtual)
2024-05-03 02:34:31,453 - gensim.topic_coherence.text_analysis - INFO - 81 batches submitted to accumulate stats from 5184 documents (132913 virtual)
2024-05-03 02:34:31,453 - gensim.topic_coherence.text_analysis - INFO - 82 batches submitted to accumulate stats from 5248 documents (134700 virtual)
2024-05-03 02:34:31,453 - gensim.topic_coherence.text_analysis - INFO - 83 batches submitted to accumulate stats from 5312 documents (136417 virtual)
2024-05-03 02:34:31,453 - gensim.topic_coherence.text_analysis - INFO - 84 batches submitted to accumulate stats from 5376 documents (138141 virtual)
2024-05-03 02:34:31,468 - gensim.topic_coherence.text_analysis - INFO - 85 batches submitted to accumulate stats from 5440 documents (139767 virtual)
2024-05-03 02:34:31,468 - gensim.topic_coherence.text_analysis - INFO - 86 batches submitted to accumulate stats from 5504 documents (141151 virtual)
2024-05-03 02:34:31,468 - gensim.topic_coherence.text_analysis - INFO - 87 batches submitted to accumulate stats from 5568 documents (142647 virtual)
2024-05-03 02:34:31,484 - gensim.topic_coherence.text_analysis - INFO - 88 batches submitted to accumulate stats from 5632 documents (144175 virtual)
2024-05-03 02:34:31,484 - gensim.topic_coherence.text_analysis - INFO - 89 batches submitted to accumulate stats from 5696 documents (145825 virtual)
2024-05-03 02:34:31,484 - gensim.topic_coherence.text_analysis - INFO - 90 batches submitted to accumulate stats from 5760 documents (147317 virtual)
2024-05-03 02:34:31,484 - gensim.topic_coherence.text_analysis - INFO - 91 batches submitted to accumulate stats from 5824 documents (149158 virtual)
2024-05-03 02:34:31,484 - gensim.topic_coherence.text_analysis - INFO - 92 batches submitted to accumulate stats from 5888 documents (150755 virtual)
2024-05-03 02:34:31,499 - gensim.topic_coherence.text_analysis - INFO - 93 batches submitted to accumulate stats from 5952 documents (152237 virtual)
2024-05-03 02:34:31,499 - gensim.topic_coherence.text_analysis - INFO - 94 batches submitted to accumulate stats from 6016 documents (154013 virtual)
2024-05-03 02:34:31,515 - gensim.topic_coherence.text_analysis - INFO - 95 batches submitted to accumulate stats from 6080 documents (155593 virtual)
2024-05-03 02:34:31,515 - gensim.topic_coherence.text_analysis - INFO - 96 batches submitted to accumulate stats from 6144 documents (157114 virtual)
2024-05-03 02:34:31,531 - gensim.topic_coherence.text_analysis - INFO - 97 batches submitted to accumulate stats from 6208 documents (158783 virtual)
2024-05-03 02:34:31,546 - gensim.topic_coherence.text_analysis - INFO - 98 batches submitted to accumulate stats from 6272 documents (160467 virtual)
2024-05-03 02:34:31,546 - gensim.topic_coherence.text_analysis - INFO - 99 batches submitted to accumulate stats from 6336 documents (162287 virtual)
2024-05-03 02:34:31,546 - gensim.topic_coherence.text_analysis - INFO - 100 batches submitted to accumulate stats from 6400 documents (163932 virtual)
2024-05-03 02:34:31,546 - gensim.topic_coherence.text_analysis - INFO - 101 batches submitted to accumulate stats from 6464 documents (165414 virtual)
2024-05-03 02:34:31,546 - gensim.topic_coherence.text_analysis - INFO - 102 batches submitted to accumulate stats from 6528 documents (166934 virtual)
2024-05-03 02:34:31,546 - gensim.topic_coherence.text_analysis - INFO - 103 batches submitted to accumulate stats from 6592 documents (168399 virtual)
2024-05-03 02:34:31,546 - gensim.topic_coherence.text_analysis - INFO - 104 batches submitted to accumulate stats from 6656 documents (170596 virtual)
2024-05-03 02:34:31,562 - gensim.topic_coherence.text_analysis - INFO - 105 batches submitted to accumulate stats from 6720 documents (172319 virtual)
2024-05-03 02:34:31,562 - gensim.topic_coherence.text_analysis - INFO - 106 batches submitted to accumulate stats from 6784 documents (173973 virtual)
2024-05-03 02:34:31,562 - gensim.topic_coherence.text_analysis - INFO - 107 batches submitted to accumulate stats from 6848 documents (175817 virtual)
2024-05-03 02:34:31,562 - gensim.topic_coherence.text_analysis - INFO - 108 batches submitted to accumulate stats from 6912 documents (177402 virtual)
2024-05-03 02:34:31,578 - gensim.topic_coherence.text_analysis - INFO - 109 batches submitted to accumulate stats from 6976 documents (179106 virtual)
2024-05-03 02:34:31,578 - gensim.topic_coherence.text_analysis - INFO - 110 batches submitted to accumulate stats from 7040 documents (181089 virtual)
2024-05-03 02:34:31,578 - gensim.topic_coherence.text_analysis - INFO - 111 batches submitted to accumulate stats from 7104 documents (182660 virtual)
2024-05-03 02:34:31,578 - gensim.topic_coherence.text_analysis - INFO - 112 batches submitted to accumulate stats from 7168 documents (184289 virtual)
2024-05-03 02:34:31,578 - gensim.topic_coherence.text_analysis - INFO - 113 batches submitted to accumulate stats from 7232 documents (185825 virtual)
2024-05-03 02:34:31,593 - gensim.topic_coherence.text_analysis - INFO - 114 batches submitted to accumulate stats from 7296 documents (187420 virtual)
2024-05-03 02:34:31,593 - gensim.topic_coherence.text_analysis - INFO - 115 batches submitted to accumulate stats from 7360 documents (189102 virtual)
2024-05-03 02:34:31,593 - gensim.topic_coherence.text_analysis - INFO - 116 batches submitted to accumulate stats from 7424 documents (190745 virtual)
2024-05-03 02:34:31,609 - gensim.topic_coherence.text_analysis - INFO - 117 batches submitted to accumulate stats from 7488 documents (192238 virtual)
2024-05-03 02:34:31,609 - gensim.topic_coherence.text_analysis - INFO - 118 batches submitted to accumulate stats from 7552 documents (194107 virtual)
2024-05-03 02:34:31,609 - gensim.topic_coherence.text_analysis - INFO - 119 batches submitted to accumulate stats from 7616 documents (195570 virtual)
2024-05-03 02:34:31,609 - gensim.topic_coherence.text_analysis - INFO - 120 batches submitted to accumulate stats from 7680 documents (197064 virtual)
2024-05-03 02:34:31,624 - gensim.topic_coherence.text_analysis - INFO - 121 batches submitted to accumulate stats from 7744 documents (198821 virtual)
2024-05-03 02:34:31,624 - gensim.topic_coherence.text_analysis - INFO - 122 batches submitted to accumulate stats from 7808 documents (200394 virtual)
2024-05-03 02:34:31,624 - gensim.topic_coherence.text_analysis - INFO - 123 batches submitted to accumulate stats from 7872 documents (202352 virtual)
2024-05-03 02:34:31,640 - gensim.topic_coherence.text_analysis - INFO - 124 batches submitted to accumulate stats from 7936 documents (204181 virtual)
2024-05-03 02:34:31,640 - gensim.topic_coherence.text_analysis - INFO - 125 batches submitted to accumulate stats from 8000 documents (206063 virtual)
2024-05-03 02:34:31,640 - gensim.topic_coherence.text_analysis - INFO - 126 batches submitted to accumulate stats from 8064 documents (207766 virtual)
2024-05-03 02:34:31,640 - gensim.topic_coherence.text_analysis - INFO - 127 batches submitted to accumulate stats from 8128 documents (209460 virtual)
2024-05-03 02:34:31,656 - gensim.topic_coherence.text_analysis - INFO - 128 batches submitted to accumulate stats from 8192 documents (211022 virtual)
2024-05-03 02:34:31,656 - gensim.topic_coherence.text_analysis - INFO - 129 batches submitted to accumulate stats from 8256 documents (212632 virtual)
2024-05-03 02:34:31,656 - gensim.topic_coherence.text_analysis - INFO - 130 batches submitted to accumulate stats from 8320 documents (214210 virtual)
2024-05-03 02:34:31,656 - gensim.topic_coherence.text_analysis - INFO - 131 batches submitted to accumulate stats from 8384 documents (215651 virtual)
2024-05-03 02:34:31,656 - gensim.topic_coherence.text_analysis - INFO - 132 batches submitted to accumulate stats from 8448 documents (217300 virtual)
2024-05-03 02:34:31,671 - gensim.topic_coherence.text_analysis - INFO - 133 batches submitted to accumulate stats from 8512 documents (219035 virtual)
2024-05-03 02:34:31,671 - gensim.topic_coherence.text_analysis - INFO - 134 batches submitted to accumulate stats from 8576 documents (220675 virtual)
2024-05-03 02:34:31,671 - gensim.topic_coherence.text_analysis - INFO - 135 batches submitted to accumulate stats from 8640 documents (222562 virtual)
2024-05-03 02:34:31,687 - gensim.topic_coherence.text_analysis - INFO - 136 batches submitted to accumulate stats from 8704 documents (224243 virtual)
2024-05-03 02:34:31,703 - gensim.topic_coherence.text_analysis - INFO - 137 batches submitted to accumulate stats from 8768 documents (225942 virtual)
2024-05-03 02:34:31,703 - gensim.topic_coherence.text_analysis - INFO - 138 batches submitted to accumulate stats from 8832 documents (227774 virtual)
2024-05-03 02:34:31,703 - gensim.topic_coherence.text_analysis - INFO - 139 batches submitted to accumulate stats from 8896 documents (229378 virtual)
2024-05-03 02:34:31,703 - gensim.topic_coherence.text_analysis - INFO - 140 batches submitted to accumulate stats from 8960 documents (231026 virtual)
2024-05-03 02:34:31,703 - gensim.topic_coherence.text_analysis - INFO - 141 batches submitted to accumulate stats from 9024 documents (232664 virtual)
2024-05-03 02:34:31,703 - gensim.topic_coherence.text_analysis - INFO - 142 batches submitted to accumulate stats from 9088 documents (234376 virtual)
2024-05-03 02:34:31,703 - gensim.topic_coherence.text_analysis - INFO - 143 batches submitted to accumulate stats from 9152 documents (236034 virtual)
2024-05-03 02:34:31,703 - gensim.topic_coherence.text_analysis - INFO - 144 batches submitted to accumulate stats from 9216 documents (237753 virtual)
2024-05-03 02:34:31,718 - gensim.topic_coherence.text_analysis - INFO - 145 batches submitted to accumulate stats from 9280 documents (239260 virtual)
2024-05-03 02:34:31,718 - gensim.topic_coherence.text_analysis - INFO - 146 batches submitted to accumulate stats from 9344 documents (240889 virtual)
2024-05-03 02:34:31,718 - gensim.topic_coherence.text_analysis - INFO - 147 batches submitted to accumulate stats from 9408 documents (242607 virtual)
2024-05-03 02:34:31,734 - gensim.topic_coherence.text_analysis - INFO - 148 batches submitted to accumulate stats from 9472 documents (244168 virtual)
2024-05-03 02:34:31,750 - gensim.topic_coherence.text_analysis - INFO - 149 batches submitted to accumulate stats from 9536 documents (245827 virtual)
2024-05-03 02:34:31,750 - gensim.topic_coherence.text_analysis - INFO - 150 batches submitted to accumulate stats from 9600 documents (247340 virtual)
2024-05-03 02:34:31,750 - gensim.topic_coherence.text_analysis - INFO - 151 batches submitted to accumulate stats from 9664 documents (248935 virtual)
2024-05-03 02:34:31,750 - gensim.topic_coherence.text_analysis - INFO - 152 batches submitted to accumulate stats from 9728 documents (250682 virtual)
2024-05-03 02:34:31,750 - gensim.topic_coherence.text_analysis - INFO - 153 batches submitted to accumulate stats from 9792 documents (252525 virtual)
2024-05-03 02:34:31,765 - gensim.topic_coherence.text_analysis - INFO - 154 batches submitted to accumulate stats from 9856 documents (253964 virtual)
2024-05-03 02:34:31,765 - gensim.topic_coherence.text_analysis - INFO - 155 batches submitted to accumulate stats from 9920 documents (255761 virtual)
2024-05-03 02:34:31,781 - gensim.topic_coherence.text_analysis - INFO - 156 batches submitted to accumulate stats from 9984 documents (257656 virtual)
2024-05-03 02:34:31,781 - gensim.topic_coherence.text_analysis - INFO - 157 batches submitted to accumulate stats from 10048 documents (259320 virtual)
2024-05-03 02:34:31,781 - gensim.topic_coherence.text_analysis - INFO - 158 batches submitted to accumulate stats from 10112 documents (261193 virtual)
2024-05-03 02:34:31,781 - gensim.topic_coherence.text_analysis - INFO - 159 batches submitted to accumulate stats from 10176 documents (262723 virtual)
2024-05-03 02:34:31,781 - gensim.topic_coherence.text_analysis - INFO - 160 batches submitted to accumulate stats from 10240 documents (264223 virtual)
2024-05-03 02:34:31,796 - gensim.topic_coherence.text_analysis - INFO - 161 batches submitted to accumulate stats from 10304 documents (265694 virtual)
2024-05-03 02:34:31,796 - gensim.topic_coherence.text_analysis - INFO - 162 batches submitted to accumulate stats from 10368 documents (267448 virtual)
2024-05-03 02:34:31,796 - gensim.topic_coherence.text_analysis - INFO - 163 batches submitted to accumulate stats from 10432 documents (269251 virtual)
2024-05-03 02:34:31,796 - gensim.topic_coherence.text_analysis - INFO - 164 batches submitted to accumulate stats from 10496 documents (270973 virtual)
2024-05-03 02:34:31,796 - gensim.topic_coherence.text_analysis - INFO - 165 batches submitted to accumulate stats from 10560 documents (272683 virtual)
2024-05-03 02:34:31,812 - gensim.topic_coherence.text_analysis - INFO - 166 batches submitted to accumulate stats from 10624 documents (274294 virtual)
2024-05-03 02:34:31,812 - gensim.topic_coherence.text_analysis - INFO - 167 batches submitted to accumulate stats from 10688 documents (276045 virtual)
2024-05-03 02:34:31,812 - gensim.topic_coherence.text_analysis - INFO - 168 batches submitted to accumulate stats from 10752 documents (277496 virtual)
2024-05-03 02:34:31,828 - gensim.topic_coherence.text_analysis - INFO - 169 batches submitted to accumulate stats from 10816 documents (279131 virtual)
2024-05-03 02:34:31,828 - gensim.topic_coherence.text_analysis - INFO - 170 batches submitted to accumulate stats from 10880 documents (280812 virtual)
2024-05-03 02:34:31,828 - gensim.topic_coherence.text_analysis - INFO - 171 batches submitted to accumulate stats from 10944 documents (282408 virtual)
2024-05-03 02:34:31,843 - gensim.topic_coherence.text_analysis - INFO - 172 batches submitted to accumulate stats from 11008 documents (284125 virtual)
2024-05-03 02:34:31,843 - gensim.topic_coherence.text_analysis - INFO - 173 batches submitted to accumulate stats from 11072 documents (285575 virtual)
2024-05-03 02:34:31,843 - gensim.topic_coherence.text_analysis - INFO - 174 batches submitted to accumulate stats from 11136 documents (287185 virtual)
2024-05-03 02:34:31,843 - gensim.topic_coherence.text_analysis - INFO - 175 batches submitted to accumulate stats from 11200 documents (289145 virtual)
2024-05-03 02:34:31,859 - gensim.topic_coherence.text_analysis - INFO - 176 batches submitted to accumulate stats from 11264 documents (290900 virtual)
2024-05-03 02:34:31,859 - gensim.topic_coherence.text_analysis - INFO - 177 batches submitted to accumulate stats from 11328 documents (292686 virtual)
2024-05-03 02:34:31,859 - gensim.topic_coherence.text_analysis - INFO - 178 batches submitted to accumulate stats from 11392 documents (294505 virtual)
2024-05-03 02:34:31,859 - gensim.topic_coherence.text_analysis - INFO - 179 batches submitted to accumulate stats from 11456 documents (296236 virtual)
2024-05-03 02:34:31,859 - gensim.topic_coherence.text_analysis - INFO - 180 batches submitted to accumulate stats from 11520 documents (297647 virtual)
2024-05-03 02:34:31,874 - gensim.topic_coherence.text_analysis - INFO - 181 batches submitted to accumulate stats from 11584 documents (299327 virtual)
2024-05-03 02:34:31,874 - gensim.topic_coherence.text_analysis - INFO - 182 batches submitted to accumulate stats from 11648 documents (300853 virtual)
2024-05-03 02:34:31,874 - gensim.topic_coherence.text_analysis - INFO - 183 batches submitted to accumulate stats from 11712 documents (302601 virtual)
2024-05-03 02:34:31,890 - gensim.topic_coherence.text_analysis - INFO - 184 batches submitted to accumulate stats from 11776 documents (304181 virtual)
2024-05-03 02:34:31,890 - gensim.topic_coherence.text_analysis - INFO - 185 batches submitted to accumulate stats from 11840 documents (305710 virtual)
2024-05-03 02:34:31,890 - gensim.topic_coherence.text_analysis - INFO - 186 batches submitted to accumulate stats from 11904 documents (307265 virtual)
2024-05-03 02:34:31,890 - gensim.topic_coherence.text_analysis - INFO - 187 batches submitted to accumulate stats from 11968 documents (309148 virtual)
2024-05-03 02:34:31,890 - gensim.topic_coherence.text_analysis - INFO - 188 batches submitted to accumulate stats from 12032 documents (310818 virtual)
2024-05-03 02:34:31,890 - gensim.topic_coherence.text_analysis - INFO - 189 batches submitted to accumulate stats from 12096 documents (312472 virtual)
2024-05-03 02:34:31,906 - gensim.topic_coherence.text_analysis - INFO - 190 batches submitted to accumulate stats from 12160 documents (314119 virtual)
2024-05-03 02:34:31,906 - gensim.topic_coherence.text_analysis - INFO - 191 batches submitted to accumulate stats from 12224 documents (316054 virtual)
2024-05-03 02:34:31,921 - gensim.topic_coherence.text_analysis - INFO - 192 batches submitted to accumulate stats from 12288 documents (317685 virtual)
2024-05-03 02:34:31,921 - gensim.topic_coherence.text_analysis - INFO - 193 batches submitted to accumulate stats from 12352 documents (319351 virtual)
2024-05-03 02:34:31,921 - gensim.topic_coherence.text_analysis - INFO - 194 batches submitted to accumulate stats from 12416 documents (321100 virtual)
2024-05-03 02:34:31,921 - gensim.topic_coherence.text_analysis - INFO - 195 batches submitted to accumulate stats from 12480 documents (323099 virtual)
2024-05-03 02:34:31,937 - gensim.topic_coherence.text_analysis - INFO - 196 batches submitted to accumulate stats from 12544 documents (324548 virtual)
2024-05-03 02:34:31,937 - gensim.topic_coherence.text_analysis - INFO - 197 batches submitted to accumulate stats from 12608 documents (326231 virtual)
2024-05-03 02:34:31,937 - gensim.topic_coherence.text_analysis - INFO - 198 batches submitted to accumulate stats from 12672 documents (327851 virtual)
2024-05-03 02:34:31,953 - gensim.topic_coherence.text_analysis - INFO - 199 batches submitted to accumulate stats from 12736 documents (329386 virtual)
2024-05-03 02:34:31,953 - gensim.topic_coherence.text_analysis - INFO - 200 batches submitted to accumulate stats from 12800 documents (331113 virtual)
2024-05-03 02:34:31,953 - gensim.topic_coherence.text_analysis - INFO - 201 batches submitted to accumulate stats from 12864 documents (332712 virtual)
2024-05-03 02:34:31,953 - gensim.topic_coherence.text_analysis - INFO - 202 batches submitted to accumulate stats from 12928 documents (334135 virtual)
2024-05-03 02:34:31,953 - gensim.topic_coherence.text_analysis - INFO - 203 batches submitted to accumulate stats from 12992 documents (335914 virtual)
2024-05-03 02:34:31,968 - gensim.topic_coherence.text_analysis - INFO - 204 batches submitted to accumulate stats from 13056 documents (337641 virtual)
2024-05-03 02:34:31,968 - gensim.topic_coherence.text_analysis - INFO - 205 batches submitted to accumulate stats from 13120 documents (339449 virtual)
2024-05-03 02:34:31,984 - gensim.topic_coherence.text_analysis - INFO - 206 batches submitted to accumulate stats from 13184 documents (341168 virtual)
2024-05-03 02:34:31,984 - gensim.topic_coherence.text_analysis - INFO - 207 batches submitted to accumulate stats from 13248 documents (342833 virtual)
2024-05-03 02:34:31,999 - gensim.topic_coherence.text_analysis - INFO - 208 batches submitted to accumulate stats from 13312 documents (344704 virtual)
2024-05-03 02:34:32,015 - gensim.topic_coherence.text_analysis - INFO - 209 batches submitted to accumulate stats from 13376 documents (346650 virtual)
2024-05-03 02:34:32,015 - gensim.topic_coherence.text_analysis - INFO - 210 batches submitted to accumulate stats from 13440 documents (348531 virtual)
2024-05-03 02:34:32,015 - gensim.topic_coherence.text_analysis - INFO - 211 batches submitted to accumulate stats from 13504 documents (350342 virtual)
2024-05-03 02:34:32,031 - gensim.topic_coherence.text_analysis - INFO - 212 batches submitted to accumulate stats from 13568 documents (352068 virtual)
2024-05-03 02:34:32,046 - gensim.topic_coherence.text_analysis - INFO - 213 batches submitted to accumulate stats from 13632 documents (353789 virtual)
2024-05-03 02:34:32,046 - gensim.topic_coherence.text_analysis - INFO - 214 batches submitted to accumulate stats from 13696 documents (355216 virtual)
2024-05-03 02:34:32,078 - gensim.topic_coherence.text_analysis - INFO - 215 batches submitted to accumulate stats from 13760 documents (356990 virtual)
2024-05-03 02:34:32,093 - gensim.topic_coherence.text_analysis - INFO - 216 batches submitted to accumulate stats from 13824 documents (358762 virtual)
2024-05-03 02:34:32,093 - gensim.topic_coherence.text_analysis - INFO - 217 batches submitted to accumulate stats from 13888 documents (360320 virtual)
2024-05-03 02:34:32,093 - gensim.topic_coherence.text_analysis - INFO - 218 batches submitted to accumulate stats from 13952 documents (361867 virtual)
2024-05-03 02:34:32,109 - gensim.topic_coherence.text_analysis - INFO - 219 batches submitted to accumulate stats from 14016 documents (363519 virtual)
2024-05-03 02:34:32,109 - gensim.topic_coherence.text_analysis - INFO - 220 batches submitted to accumulate stats from 14080 documents (365141 virtual)
2024-05-03 02:34:32,109 - gensim.topic_coherence.text_analysis - INFO - 221 batches submitted to accumulate stats from 14144 documents (366934 virtual)
2024-05-03 02:34:32,109 - gensim.topic_coherence.text_analysis - INFO - 222 batches submitted to accumulate stats from 14208 documents (368448 virtual)
2024-05-03 02:34:32,140 - gensim.topic_coherence.text_analysis - INFO - 223 batches submitted to accumulate stats from 14272 documents (370012 virtual)
2024-05-03 02:34:32,140 - gensim.topic_coherence.text_analysis - INFO - 224 batches submitted to accumulate stats from 14336 documents (371704 virtual)
2024-05-03 02:34:32,140 - gensim.topic_coherence.text_analysis - INFO - 225 batches submitted to accumulate stats from 14400 documents (373331 virtual)
2024-05-03 02:34:32,140 - gensim.topic_coherence.text_analysis - INFO - 226 batches submitted to accumulate stats from 14464 documents (375174 virtual)
2024-05-03 02:34:32,140 - gensim.topic_coherence.text_analysis - INFO - 227 batches submitted to accumulate stats from 14528 documents (377135 virtual)
2024-05-03 02:34:32,156 - gensim.topic_coherence.text_analysis - INFO - 228 batches submitted to accumulate stats from 14592 documents (378860 virtual)
2024-05-03 02:34:32,171 - gensim.topic_coherence.text_analysis - INFO - 229 batches submitted to accumulate stats from 14656 documents (380581 virtual)
2024-05-03 02:34:32,171 - gensim.topic_coherence.text_analysis - INFO - 230 batches submitted to accumulate stats from 14720 documents (382278 virtual)
2024-05-03 02:34:32,171 - gensim.topic_coherence.text_analysis - INFO - 231 batches submitted to accumulate stats from 14784 documents (383970 virtual)
2024-05-03 02:34:32,171 - gensim.topic_coherence.text_analysis - INFO - 232 batches submitted to accumulate stats from 14848 documents (385680 virtual)
2024-05-03 02:34:32,171 - gensim.topic_coherence.text_analysis - INFO - 233 batches submitted to accumulate stats from 14912 documents (387378 virtual)
2024-05-03 02:34:32,171 - gensim.topic_coherence.text_analysis - INFO - 234 batches submitted to accumulate stats from 14976 documents (388908 virtual)
2024-05-03 02:34:32,187 - gensim.topic_coherence.text_analysis - INFO - 235 batches submitted to accumulate stats from 15040 documents (390473 virtual)
2024-05-03 02:34:32,187 - gensim.topic_coherence.text_analysis - INFO - 236 batches submitted to accumulate stats from 15104 documents (391966 virtual)
2024-05-03 02:34:32,203 - gensim.topic_coherence.text_analysis - INFO - 237 batches submitted to accumulate stats from 15168 documents (393446 virtual)
2024-05-03 02:34:32,203 - gensim.topic_coherence.text_analysis - INFO - 238 batches submitted to accumulate stats from 15232 documents (394979 virtual)
2024-05-03 02:34:32,203 - gensim.topic_coherence.text_analysis - INFO - 239 batches submitted to accumulate stats from 15296 documents (396707 virtual)
2024-05-03 02:34:32,203 - gensim.topic_coherence.text_analysis - INFO - 240 batches submitted to accumulate stats from 15360 documents (398434 virtual)
2024-05-03 02:34:32,218 - gensim.topic_coherence.text_analysis - INFO - 241 batches submitted to accumulate stats from 15424 documents (400092 virtual)
2024-05-03 02:34:32,234 - gensim.topic_coherence.text_analysis - INFO - 242 batches submitted to accumulate stats from 15488 documents (402054 virtual)
2024-05-03 02:34:32,234 - gensim.topic_coherence.text_analysis - INFO - 243 batches submitted to accumulate stats from 15552 documents (403465 virtual)
2024-05-03 02:34:32,234 - gensim.topic_coherence.text_analysis - INFO - 244 batches submitted to accumulate stats from 15616 documents (405289 virtual)
2024-05-03 02:34:32,249 - gensim.topic_coherence.text_analysis - INFO - 245 batches submitted to accumulate stats from 15680 documents (406750 virtual)
2024-05-03 02:34:32,249 - gensim.topic_coherence.text_analysis - INFO - 246 batches submitted to accumulate stats from 15744 documents (408306 virtual)
2024-05-03 02:34:32,249 - gensim.topic_coherence.text_analysis - INFO - 247 batches submitted to accumulate stats from 15808 documents (410097 virtual)
2024-05-03 02:34:32,265 - gensim.topic_coherence.text_analysis - INFO - 248 batches submitted to accumulate stats from 15872 documents (411915 virtual)
2024-05-03 02:34:32,265 - gensim.topic_coherence.text_analysis - INFO - 249 batches submitted to accumulate stats from 15936 documents (413538 virtual)
2024-05-03 02:34:32,281 - gensim.topic_coherence.text_analysis - INFO - 250 batches submitted to accumulate stats from 16000 documents (415307 virtual)
2024-05-03 02:34:32,281 - gensim.topic_coherence.text_analysis - INFO - 251 batches submitted to accumulate stats from 16064 documents (416993 virtual)
2024-05-03 02:34:32,281 - gensim.topic_coherence.text_analysis - INFO - 252 batches submitted to accumulate stats from 16128 documents (418582 virtual)
2024-05-03 02:34:32,296 - gensim.topic_coherence.text_analysis - INFO - 253 batches submitted to accumulate stats from 16192 documents (420282 virtual)
2024-05-03 02:34:32,296 - gensim.topic_coherence.text_analysis - INFO - 254 batches submitted to accumulate stats from 16256 documents (421859 virtual)
2024-05-03 02:34:32,296 - gensim.topic_coherence.text_analysis - INFO - 255 batches submitted to accumulate stats from 16320 documents (423441 virtual)
2024-05-03 02:34:32,296 - gensim.topic_coherence.text_analysis - INFO - 256 batches submitted to accumulate stats from 16384 documents (425169 virtual)
2024-05-03 02:34:32,312 - gensim.topic_coherence.text_analysis - INFO - 257 batches submitted to accumulate stats from 16448 documents (426804 virtual)
2024-05-03 02:34:32,328 - gensim.topic_coherence.text_analysis - INFO - 258 batches submitted to accumulate stats from 16512 documents (428503 virtual)
2024-05-03 02:34:32,328 - gensim.topic_coherence.text_analysis - INFO - 259 batches submitted to accumulate stats from 16576 documents (430063 virtual)
2024-05-03 02:34:32,328 - gensim.topic_coherence.text_analysis - INFO - 260 batches submitted to accumulate stats from 16640 documents (431672 virtual)
2024-05-03 02:34:32,328 - gensim.topic_coherence.text_analysis - INFO - 261 batches submitted to accumulate stats from 16704 documents (433298 virtual)
2024-05-03 02:34:32,344 - gensim.topic_coherence.text_analysis - INFO - 262 batches submitted to accumulate stats from 16768 documents (434951 virtual)
2024-05-03 02:34:32,344 - gensim.topic_coherence.text_analysis - INFO - 263 batches submitted to accumulate stats from 16832 documents (436505 virtual)
2024-05-03 02:34:32,344 - gensim.topic_coherence.text_analysis - INFO - 264 batches submitted to accumulate stats from 16896 documents (438134 virtual)
2024-05-03 02:34:32,344 - gensim.topic_coherence.text_analysis - INFO - 265 batches submitted to accumulate stats from 16960 documents (439831 virtual)
2024-05-03 02:34:32,344 - gensim.topic_coherence.text_analysis - INFO - 266 batches submitted to accumulate stats from 17024 documents (441666 virtual)
2024-05-03 02:34:32,359 - gensim.topic_coherence.text_analysis - INFO - 267 batches submitted to accumulate stats from 17088 documents (443389 virtual)
2024-05-03 02:34:32,359 - gensim.topic_coherence.text_analysis - INFO - 268 batches submitted to accumulate stats from 17152 documents (445008 virtual)
2024-05-03 02:34:32,359 - gensim.topic_coherence.text_analysis - INFO - 269 batches submitted to accumulate stats from 17216 documents (446691 virtual)
2024-05-03 02:34:32,359 - gensim.topic_coherence.text_analysis - INFO - 270 batches submitted to accumulate stats from 17280 documents (448258 virtual)
2024-05-03 02:34:32,359 - gensim.topic_coherence.text_analysis - INFO - 271 batches submitted to accumulate stats from 17344 documents (450177 virtual)
2024-05-03 02:34:32,359 - gensim.topic_coherence.text_analysis - INFO - 272 batches submitted to accumulate stats from 17408 documents (451838 virtual)
2024-05-03 02:34:32,359 - gensim.topic_coherence.text_analysis - INFO - 273 batches submitted to accumulate stats from 17472 documents (453765 virtual)
2024-05-03 02:34:32,375 - gensim.topic_coherence.text_analysis - INFO - 274 batches submitted to accumulate stats from 17536 documents (455302 virtual)
2024-05-03 02:34:32,375 - gensim.topic_coherence.text_analysis - INFO - 275 batches submitted to accumulate stats from 17600 documents (457026 virtual)
2024-05-03 02:34:32,375 - gensim.topic_coherence.text_analysis - INFO - 276 batches submitted to accumulate stats from 17664 documents (458678 virtual)
2024-05-03 02:34:32,375 - gensim.topic_coherence.text_analysis - INFO - 277 batches submitted to accumulate stats from 17728 documents (460363 virtual)
2024-05-03 02:34:32,375 - gensim.topic_coherence.text_analysis - INFO - 278 batches submitted to accumulate stats from 17792 documents (462290 virtual)
2024-05-03 02:34:32,375 - gensim.topic_coherence.text_analysis - INFO - 279 batches submitted to accumulate stats from 17856 documents (464043 virtual)
2024-05-03 02:34:32,375 - gensim.topic_coherence.text_analysis - INFO - 280 batches submitted to accumulate stats from 17920 documents (465601 virtual)
2024-05-03 02:34:32,391 - gensim.topic_coherence.text_analysis - INFO - 281 batches submitted to accumulate stats from 17984 documents (467208 virtual)
2024-05-03 02:34:32,391 - gensim.topic_coherence.text_analysis - INFO - 282 batches submitted to accumulate stats from 18048 documents (468985 virtual)
2024-05-03 02:34:32,391 - gensim.topic_coherence.text_analysis - INFO - 283 batches submitted to accumulate stats from 18112 documents (470540 virtual)
2024-05-03 02:34:32,391 - gensim.topic_coherence.text_analysis - INFO - 284 batches submitted to accumulate stats from 18176 documents (472187 virtual)
2024-05-03 02:34:32,391 - gensim.topic_coherence.text_analysis - INFO - 285 batches submitted to accumulate stats from 18240 documents (473657 virtual)
2024-05-03 02:34:32,391 - gensim.topic_coherence.text_analysis - INFO - 286 batches submitted to accumulate stats from 18304 documents (475433 virtual)
2024-05-03 02:34:32,391 - gensim.topic_coherence.text_analysis - INFO - 287 batches submitted to accumulate stats from 18368 documents (476949 virtual)
2024-05-03 02:34:32,391 - gensim.topic_coherence.text_analysis - INFO - 288 batches submitted to accumulate stats from 18432 documents (478519 virtual)
2024-05-03 02:34:32,406 - gensim.topic_coherence.text_analysis - INFO - 289 batches submitted to accumulate stats from 18496 documents (480346 virtual)
2024-05-03 02:34:32,406 - gensim.topic_coherence.text_analysis - INFO - 290 batches submitted to accumulate stats from 18560 documents (482113 virtual)
2024-05-03 02:34:32,422 - gensim.topic_coherence.text_analysis - INFO - 291 batches submitted to accumulate stats from 18624 documents (483726 virtual)
2024-05-03 02:34:32,422 - gensim.topic_coherence.text_analysis - INFO - 292 batches submitted to accumulate stats from 18688 documents (485454 virtual)
2024-05-03 02:34:32,422 - gensim.topic_coherence.text_analysis - INFO - 293 batches submitted to accumulate stats from 18752 documents (487120 virtual)
2024-05-03 02:34:32,422 - gensim.topic_coherence.text_analysis - INFO - 294 batches submitted to accumulate stats from 18816 documents (488702 virtual)
2024-05-03 02:34:32,453 - gensim.topic_coherence.text_analysis - INFO - 295 batches submitted to accumulate stats from 18880 documents (490511 virtual)
2024-05-03 02:34:32,453 - gensim.topic_coherence.text_analysis - INFO - 296 batches submitted to accumulate stats from 18944 documents (492226 virtual)
2024-05-03 02:34:32,469 - gensim.topic_coherence.text_analysis - INFO - 297 batches submitted to accumulate stats from 19008 documents (494165 virtual)
2024-05-03 02:34:32,484 - gensim.topic_coherence.text_analysis - INFO - 298 batches submitted to accumulate stats from 19072 documents (495678 virtual)
2024-05-03 02:34:32,484 - gensim.topic_coherence.text_analysis - INFO - 299 batches submitted to accumulate stats from 19136 documents (497503 virtual)
2024-05-03 02:34:32,484 - gensim.topic_coherence.text_analysis - INFO - 300 batches submitted to accumulate stats from 19200 documents (499131 virtual)
2024-05-03 02:34:32,484 - gensim.topic_coherence.text_analysis - INFO - 301 batches submitted to accumulate stats from 19264 documents (500986 virtual)
2024-05-03 02:34:32,484 - gensim.topic_coherence.text_analysis - INFO - 302 batches submitted to accumulate stats from 19328 documents (502687 virtual)
2024-05-03 02:34:32,500 - gensim.topic_coherence.text_analysis - INFO - 303 batches submitted to accumulate stats from 19392 documents (504106 virtual)
2024-05-03 02:34:32,500 - gensim.topic_coherence.text_analysis - INFO - 304 batches submitted to accumulate stats from 19456 documents (505909 virtual)
2024-05-03 02:34:32,500 - gensim.topic_coherence.text_analysis - INFO - 305 batches submitted to accumulate stats from 19520 documents (507546 virtual)
2024-05-03 02:34:32,500 - gensim.topic_coherence.text_analysis - INFO - 306 batches submitted to accumulate stats from 19584 documents (509312 virtual)
2024-05-03 02:34:32,500 - gensim.topic_coherence.text_analysis - INFO - 307 batches submitted to accumulate stats from 19648 documents (510954 virtual)
2024-05-03 02:34:32,500 - gensim.topic_coherence.text_analysis - INFO - 308 batches submitted to accumulate stats from 19712 documents (512465 virtual)
2024-05-03 02:34:32,500 - gensim.topic_coherence.text_analysis - INFO - 309 batches submitted to accumulate stats from 19776 documents (514132 virtual)
2024-05-03 02:34:32,516 - gensim.topic_coherence.text_analysis - INFO - 310 batches submitted to accumulate stats from 19840 documents (515846 virtual)
2024-05-03 02:34:32,516 - gensim.topic_coherence.text_analysis - INFO - 311 batches submitted to accumulate stats from 19904 documents (517436 virtual)
2024-05-03 02:34:32,516 - gensim.topic_coherence.text_analysis - INFO - 312 batches submitted to accumulate stats from 19968 documents (519076 virtual)
2024-05-03 02:34:32,516 - gensim.topic_coherence.text_analysis - INFO - 313 batches submitted to accumulate stats from 20032 documents (520797 virtual)
2024-05-03 02:34:32,516 - gensim.topic_coherence.text_analysis - INFO - 314 batches submitted to accumulate stats from 20096 documents (522328 virtual)
2024-05-03 02:34:32,906 - gensim.topic_coherence.text_analysis - INFO - 11 accumulators retrieved from output queue
2024-05-03 02:34:33,109 - gensim.topic_coherence.text_analysis - INFO - accumulated word occurrence stats for 523696 virtual documents
2024-05-03 02:34:34,188 - gensim.corpora.dictionary - INFO - adding document #0 to Dictionary<0 unique tokens: []>
2024-05-03 02:34:34,563 - gensim.corpora.dictionary - INFO - adding document #10000 to Dictionary<23643 unique tokens: ['-', '.', '?', 'a', 'africa']...>
2024-05-03 02:34:34,922 - gensim.corpora.dictionary - INFO - adding document #20000 to Dictionary<32428 unique tokens: ['-', '.', '?', 'a', 'africa']...>
2024-05-03 02:34:34,922 - gensim.corpora.dictionary - INFO - built Dictionary<32503 unique tokens: ['-', '.', '?', 'a', 'africa']...> from 20086 documents (total 703102 corpus positions)
2024-05-03 02:34:34,938 - gensim.utils - INFO - Dictionary lifecycle event {'msg': "built Dictionary<32503 unique tokens: ['-', '.', '?', 'a', 'africa']...> from 20086 documents (total 703102 corpus positions)", 'datetime': '2024-05-03T02:34:34.938738', 'gensim': '4.3.2', 'python': '3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}
2024-05-03 02:34:34,954 - gensim.topic_coherence.probability_estimation - INFO - using ParallelWordOccurrenceAccumulator<processes=11, batch_size=64> to estimate probabilities from sliding windows
2024-05-03 02:35:28,957 - gensim.topic_coherence.text_analysis - INFO - 11 accumulators retrieved from output queue
2024-05-03 02:35:29,098 - gensim.topic_coherence.text_analysis - INFO - accumulated word occurrence stats for 21620 virtual documents
2024-05-03 02:35:33,741 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=50, bias=False)
  (W_q): Linear(in_features=300, out_features=50, bias=False)
  (W_v): Linear(in_features=300, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,138,605
Freeze params: 0
2024-05-03 02:39:18,103 - trainer - INFO -     epoch          : 1
2024-05-03 02:39:18,103 - trainer - INFO -     loss           : 1.150567
2024-05-03 02:39:18,103 - trainer - INFO -     accuracy       : 0.669764
2024-05-03 02:39:18,103 - trainer - INFO -     macro_f        : 0.65158
2024-05-03 02:39:18,103 - trainer - INFO -     precision      : 0.682956
2024-05-03 02:39:18,103 - trainer - INFO -     recall         : 0.669764
2024-05-03 02:39:18,103 - trainer - INFO -     doc_entropy    : 2.471753
2024-05-03 02:39:18,103 - trainer - INFO -     val_loss       : 1.004176
2024-05-03 02:39:18,103 - trainer - INFO -     val_accuracy   : 0.708553
2024-05-03 02:39:18,103 - trainer - INFO -     val_macro_f    : 0.698049
2024-05-03 02:39:18,103 - trainer - INFO -     val_precision  : 0.733104
2024-05-03 02:39:18,103 - trainer - INFO -     val_recall     : 0.708553
2024-05-03 02:39:18,103 - trainer - INFO -     val_doc_entropy: 2.645558
2024-05-03 02:39:18,103 - trainer - INFO -     test_loss      : 1.002743
2024-05-03 02:39:18,103 - trainer - INFO -     test_accuracy  : 0.706512
2024-05-03 02:39:18,103 - trainer - INFO -     test_macro_f   : 0.697454
2024-05-03 02:39:18,103 - trainer - INFO -     test_precision : 0.733363
2024-05-03 02:39:18,103 - trainer - INFO -     test_recall    : 0.706512
2024-05-03 02:39:18,103 - trainer - INFO -     test_doc_entropy: 2.648375
2024-05-03 02:43:03,966 - trainer - INFO -     epoch          : 2
2024-05-03 02:43:03,966 - trainer - INFO -     loss           : 0.800808
2024-05-03 02:43:03,966 - trainer - INFO -     accuracy       : 0.760792
2024-05-03 02:43:03,966 - trainer - INFO -     macro_f        : 0.752048
2024-05-03 02:43:03,966 - trainer - INFO -     precision      : 0.784744
2024-05-03 02:43:03,966 - trainer - INFO -     recall         : 0.760792
2024-05-03 02:43:03,966 - trainer - INFO -     doc_entropy    : 2.038472
2024-05-03 02:43:03,966 - trainer - INFO -     val_loss       : 1.015592
2024-05-03 02:43:03,966 - trainer - INFO -     val_accuracy   : 0.706761
2024-05-03 02:43:03,966 - trainer - INFO -     val_macro_f    : 0.706953
2024-05-03 02:43:03,966 - trainer - INFO -     val_precision  : 0.754938
2024-05-03 02:43:03,966 - trainer - INFO -     val_recall     : 0.706761
2024-05-03 02:43:03,966 - trainer - INFO -     val_doc_entropy: 2.483353
2024-05-03 02:43:03,966 - trainer - INFO -     test_loss      : 1.00113
2024-05-03 02:43:03,966 - trainer - INFO -     test_accuracy  : 0.709748
2024-05-03 02:43:03,966 - trainer - INFO -     test_macro_f   : 0.708567
2024-05-03 02:43:03,966 - trainer - INFO -     test_precision : 0.752007
2024-05-03 02:43:03,966 - trainer - INFO -     test_recall    : 0.709748
2024-05-03 02:43:03,966 - trainer - INFO -     test_doc_entropy: 2.485888
2024-05-03 02:43:45,870 - gensim.corpora.dictionary - INFO - adding document #0 to Dictionary<0 unique tokens: []>
2024-05-03 02:43:46,292 - gensim.corpora.dictionary - INFO - adding document #10000 to Dictionary<23643 unique tokens: ['-', '.', '?', 'a', 'africa']...>
2024-05-03 02:43:46,730 - gensim.corpora.dictionary - INFO - adding document #20000 to Dictionary<32428 unique tokens: ['-', '.', '?', 'a', 'africa']...>
2024-05-03 02:43:46,746 - gensim.corpora.dictionary - INFO - built Dictionary<32503 unique tokens: ['-', '.', '?', 'a', 'africa']...> from 20086 documents (total 703102 corpus positions)
2024-05-03 02:43:46,746 - gensim.utils - INFO - Dictionary lifecycle event {'msg': "built Dictionary<32503 unique tokens: ['-', '.', '?', 'a', 'africa']...> from 20086 documents (total 703102 corpus positions)", 'datetime': '2024-05-03T02:43:46.746438', 'gensim': '4.3.2', 'python': '3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}
2024-05-03 02:43:46,762 - gensim.topic_coherence.probability_estimation - INFO - using ParallelWordOccurrenceAccumulator<processes=11, batch_size=64> to estimate probabilities from sliding windows
2024-05-03 02:44:47,250 - gensim.topic_coherence.text_analysis - INFO - 1 batches submitted to accumulate stats from 64 documents (1358 virtual)
2024-05-03 02:44:47,265 - gensim.topic_coherence.text_analysis - INFO - 2 batches submitted to accumulate stats from 128 documents (3142 virtual)
2024-05-03 02:44:47,265 - gensim.topic_coherence.text_analysis - INFO - 3 batches submitted to accumulate stats from 192 documents (4707 virtual)
2024-05-03 02:44:47,265 - gensim.topic_coherence.text_analysis - INFO - 4 batches submitted to accumulate stats from 256 documents (6346 virtual)
2024-05-03 02:44:47,265 - gensim.topic_coherence.text_analysis - INFO - 5 batches submitted to accumulate stats from 320 documents (7961 virtual)
2024-05-03 02:44:47,265 - gensim.topic_coherence.text_analysis - INFO - 6 batches submitted to accumulate stats from 384 documents (9298 virtual)
2024-05-03 02:44:47,265 - gensim.topic_coherence.text_analysis - INFO - 7 batches submitted to accumulate stats from 448 documents (11371 virtual)
2024-05-03 02:44:47,265 - gensim.topic_coherence.text_analysis - INFO - 8 batches submitted to accumulate stats from 512 documents (13011 virtual)
2024-05-03 02:44:47,265 - gensim.topic_coherence.text_analysis - INFO - 9 batches submitted to accumulate stats from 576 documents (14534 virtual)
2024-05-03 02:44:47,265 - gensim.topic_coherence.text_analysis - INFO - 10 batches submitted to accumulate stats from 640 documents (16161 virtual)
2024-05-03 02:44:47,281 - gensim.topic_coherence.text_analysis - INFO - 11 batches submitted to accumulate stats from 704 documents (17689 virtual)
2024-05-03 02:44:47,281 - gensim.topic_coherence.text_analysis - INFO - 12 batches submitted to accumulate stats from 768 documents (19256 virtual)
2024-05-03 02:44:47,281 - gensim.topic_coherence.text_analysis - INFO - 13 batches submitted to accumulate stats from 832 documents (21175 virtual)
2024-05-03 02:44:47,281 - gensim.topic_coherence.text_analysis - INFO - 14 batches submitted to accumulate stats from 896 documents (22850 virtual)
2024-05-03 02:44:47,281 - gensim.topic_coherence.text_analysis - INFO - 15 batches submitted to accumulate stats from 960 documents (24681 virtual)
2024-05-03 02:44:47,281 - gensim.topic_coherence.text_analysis - INFO - 16 batches submitted to accumulate stats from 1024 documents (26222 virtual)
2024-05-03 02:44:47,281 - gensim.topic_coherence.text_analysis - INFO - 17 batches submitted to accumulate stats from 1088 documents (27953 virtual)
2024-05-03 02:44:47,281 - gensim.topic_coherence.text_analysis - INFO - 18 batches submitted to accumulate stats from 1152 documents (29594 virtual)
2024-05-03 02:44:47,281 - gensim.topic_coherence.text_analysis - INFO - 19 batches submitted to accumulate stats from 1216 documents (31239 virtual)
2024-05-03 02:44:47,297 - gensim.topic_coherence.text_analysis - INFO - 20 batches submitted to accumulate stats from 1280 documents (32901 virtual)
2024-05-03 02:44:47,297 - gensim.topic_coherence.text_analysis - INFO - 21 batches submitted to accumulate stats from 1344 documents (34481 virtual)
2024-05-03 02:44:47,297 - gensim.topic_coherence.text_analysis - INFO - 22 batches submitted to accumulate stats from 1408 documents (36151 virtual)
2024-05-03 02:44:47,297 - gensim.topic_coherence.text_analysis - INFO - 23 batches submitted to accumulate stats from 1472 documents (37837 virtual)
2024-05-03 02:44:47,297 - gensim.topic_coherence.text_analysis - INFO - 24 batches submitted to accumulate stats from 1536 documents (39453 virtual)
2024-05-03 02:44:47,312 - gensim.topic_coherence.text_analysis - INFO - 25 batches submitted to accumulate stats from 1600 documents (41046 virtual)
2024-05-03 02:44:47,312 - gensim.topic_coherence.text_analysis - INFO - 26 batches submitted to accumulate stats from 1664 documents (42892 virtual)
2024-05-03 02:44:47,312 - gensim.topic_coherence.text_analysis - INFO - 27 batches submitted to accumulate stats from 1728 documents (44445 virtual)
2024-05-03 02:44:47,312 - gensim.topic_coherence.text_analysis - INFO - 28 batches submitted to accumulate stats from 1792 documents (46073 virtual)
2024-05-03 02:44:47,312 - gensim.topic_coherence.text_analysis - INFO - 29 batches submitted to accumulate stats from 1856 documents (47643 virtual)
2024-05-03 02:44:47,312 - gensim.topic_coherence.text_analysis - INFO - 30 batches submitted to accumulate stats from 1920 documents (49252 virtual)
2024-05-03 02:44:47,312 - gensim.topic_coherence.text_analysis - INFO - 31 batches submitted to accumulate stats from 1984 documents (50774 virtual)
2024-05-03 02:44:47,312 - gensim.topic_coherence.text_analysis - INFO - 32 batches submitted to accumulate stats from 2048 documents (52387 virtual)
2024-05-03 02:44:47,328 - gensim.topic_coherence.text_analysis - INFO - 33 batches submitted to accumulate stats from 2112 documents (53815 virtual)
2024-05-03 02:44:47,328 - gensim.topic_coherence.text_analysis - INFO - 34 batches submitted to accumulate stats from 2176 documents (55540 virtual)
2024-05-03 02:44:47,328 - gensim.topic_coherence.text_analysis - INFO - 35 batches submitted to accumulate stats from 2240 documents (57348 virtual)
2024-05-03 02:44:47,328 - gensim.topic_coherence.text_analysis - INFO - 36 batches submitted to accumulate stats from 2304 documents (59258 virtual)
2024-05-03 02:44:47,328 - gensim.topic_coherence.text_analysis - INFO - 37 batches submitted to accumulate stats from 2368 documents (60957 virtual)
2024-05-03 02:44:47,343 - gensim.topic_coherence.text_analysis - INFO - 38 batches submitted to accumulate stats from 2432 documents (62425 virtual)
2024-05-03 02:44:47,343 - gensim.topic_coherence.text_analysis - INFO - 39 batches submitted to accumulate stats from 2496 documents (64029 virtual)
2024-05-03 02:44:47,343 - gensim.topic_coherence.text_analysis - INFO - 40 batches submitted to accumulate stats from 2560 documents (65725 virtual)
2024-05-03 02:44:47,343 - gensim.topic_coherence.text_analysis - INFO - 41 batches submitted to accumulate stats from 2624 documents (67346 virtual)
2024-05-03 02:44:47,343 - gensim.topic_coherence.text_analysis - INFO - 42 batches submitted to accumulate stats from 2688 documents (68863 virtual)
2024-05-03 02:44:47,343 - gensim.topic_coherence.text_analysis - INFO - 43 batches submitted to accumulate stats from 2752 documents (70539 virtual)
2024-05-03 02:44:47,359 - gensim.topic_coherence.text_analysis - INFO - 44 batches submitted to accumulate stats from 2816 documents (71922 virtual)
2024-05-03 02:44:47,359 - gensim.topic_coherence.text_analysis - INFO - 45 batches submitted to accumulate stats from 2880 documents (73294 virtual)
2024-05-03 02:44:47,359 - gensim.topic_coherence.text_analysis - INFO - 46 batches submitted to accumulate stats from 2944 documents (75084 virtual)
2024-05-03 02:44:47,359 - gensim.topic_coherence.text_analysis - INFO - 47 batches submitted to accumulate stats from 3008 documents (76769 virtual)
2024-05-03 02:44:47,359 - gensim.topic_coherence.text_analysis - INFO - 48 batches submitted to accumulate stats from 3072 documents (78312 virtual)
2024-05-03 02:44:47,359 - gensim.topic_coherence.text_analysis - INFO - 49 batches submitted to accumulate stats from 3136 documents (80039 virtual)
2024-05-03 02:44:47,359 - gensim.topic_coherence.text_analysis - INFO - 50 batches submitted to accumulate stats from 3200 documents (81572 virtual)
2024-05-03 02:44:47,359 - gensim.topic_coherence.text_analysis - INFO - 51 batches submitted to accumulate stats from 3264 documents (83189 virtual)
2024-05-03 02:44:47,375 - gensim.topic_coherence.text_analysis - INFO - 52 batches submitted to accumulate stats from 3328 documents (84783 virtual)
2024-05-03 02:44:47,375 - gensim.topic_coherence.text_analysis - INFO - 53 batches submitted to accumulate stats from 3392 documents (86570 virtual)
2024-05-03 02:44:47,375 - gensim.topic_coherence.text_analysis - INFO - 54 batches submitted to accumulate stats from 3456 documents (88371 virtual)
2024-05-03 02:44:47,375 - gensim.topic_coherence.text_analysis - INFO - 55 batches submitted to accumulate stats from 3520 documents (90295 virtual)
2024-05-03 02:44:47,375 - gensim.topic_coherence.text_analysis - INFO - 56 batches submitted to accumulate stats from 3584 documents (92118 virtual)
2024-05-03 02:44:47,375 - gensim.topic_coherence.text_analysis - INFO - 57 batches submitted to accumulate stats from 3648 documents (93914 virtual)
2024-05-03 02:44:47,390 - gensim.topic_coherence.text_analysis - INFO - 58 batches submitted to accumulate stats from 3712 documents (95729 virtual)
2024-05-03 02:44:47,390 - gensim.topic_coherence.text_analysis - INFO - 59 batches submitted to accumulate stats from 3776 documents (97293 virtual)
2024-05-03 02:44:47,390 - gensim.topic_coherence.text_analysis - INFO - 60 batches submitted to accumulate stats from 3840 documents (98908 virtual)
2024-05-03 02:44:47,390 - gensim.topic_coherence.text_analysis - INFO - 61 batches submitted to accumulate stats from 3904 documents (100586 virtual)
2024-05-03 02:44:47,390 - gensim.topic_coherence.text_analysis - INFO - 62 batches submitted to accumulate stats from 3968 documents (102208 virtual)
2024-05-03 02:44:47,390 - gensim.topic_coherence.text_analysis - INFO - 63 batches submitted to accumulate stats from 4032 documents (103862 virtual)
2024-05-03 02:44:47,390 - gensim.topic_coherence.text_analysis - INFO - 64 batches submitted to accumulate stats from 4096 documents (105500 virtual)
2024-05-03 02:44:47,406 - gensim.topic_coherence.text_analysis - INFO - 65 batches submitted to accumulate stats from 4160 documents (106974 virtual)
2024-05-03 02:44:47,406 - gensim.topic_coherence.text_analysis - INFO - 66 batches submitted to accumulate stats from 4224 documents (108587 virtual)
2024-05-03 02:44:47,406 - gensim.topic_coherence.text_analysis - INFO - 67 batches submitted to accumulate stats from 4288 documents (110059 virtual)
2024-05-03 02:44:47,406 - gensim.topic_coherence.text_analysis - INFO - 68 batches submitted to accumulate stats from 4352 documents (111905 virtual)
2024-05-03 02:44:47,406 - gensim.topic_coherence.text_analysis - INFO - 69 batches submitted to accumulate stats from 4416 documents (113549 virtual)
2024-05-03 02:44:47,406 - gensim.topic_coherence.text_analysis - INFO - 70 batches submitted to accumulate stats from 4480 documents (115163 virtual)
2024-05-03 02:44:47,422 - gensim.topic_coherence.text_analysis - INFO - 71 batches submitted to accumulate stats from 4544 documents (117083 virtual)
2024-05-03 02:44:47,422 - gensim.topic_coherence.text_analysis - INFO - 72 batches submitted to accumulate stats from 4608 documents (118654 virtual)
2024-05-03 02:44:47,422 - gensim.topic_coherence.text_analysis - INFO - 73 batches submitted to accumulate stats from 4672 documents (120224 virtual)
2024-05-03 02:44:47,422 - gensim.topic_coherence.text_analysis - INFO - 74 batches submitted to accumulate stats from 4736 documents (121786 virtual)
2024-05-03 02:44:47,422 - gensim.topic_coherence.text_analysis - INFO - 75 batches submitted to accumulate stats from 4800 documents (123233 virtual)
2024-05-03 02:44:47,422 - gensim.topic_coherence.text_analysis - INFO - 76 batches submitted to accumulate stats from 4864 documents (124761 virtual)
2024-05-03 02:44:47,422 - gensim.topic_coherence.text_analysis - INFO - 77 batches submitted to accumulate stats from 4928 documents (126221 virtual)
2024-05-03 02:44:47,437 - gensim.topic_coherence.text_analysis - INFO - 78 batches submitted to accumulate stats from 4992 documents (127857 virtual)
2024-05-03 02:44:47,437 - gensim.topic_coherence.text_analysis - INFO - 79 batches submitted to accumulate stats from 5056 documents (129432 virtual)
2024-05-03 02:44:47,437 - gensim.topic_coherence.text_analysis - INFO - 80 batches submitted to accumulate stats from 5120 documents (130948 virtual)
2024-05-03 02:44:47,437 - gensim.topic_coherence.text_analysis - INFO - 81 batches submitted to accumulate stats from 5184 documents (132913 virtual)
2024-05-03 02:44:47,437 - gensim.topic_coherence.text_analysis - INFO - 82 batches submitted to accumulate stats from 5248 documents (134700 virtual)
2024-05-03 02:44:47,437 - gensim.topic_coherence.text_analysis - INFO - 83 batches submitted to accumulate stats from 5312 documents (136417 virtual)
2024-05-03 02:44:47,453 - gensim.topic_coherence.text_analysis - INFO - 84 batches submitted to accumulate stats from 5376 documents (138141 virtual)
2024-05-03 02:44:47,453 - gensim.topic_coherence.text_analysis - INFO - 85 batches submitted to accumulate stats from 5440 documents (139767 virtual)
2024-05-03 02:44:47,453 - gensim.topic_coherence.text_analysis - INFO - 86 batches submitted to accumulate stats from 5504 documents (141151 virtual)
2024-05-03 02:44:47,453 - gensim.topic_coherence.text_analysis - INFO - 87 batches submitted to accumulate stats from 5568 documents (142647 virtual)
2024-05-03 02:44:47,453 - gensim.topic_coherence.text_analysis - INFO - 88 batches submitted to accumulate stats from 5632 documents (144175 virtual)
2024-05-03 02:44:47,453 - gensim.topic_coherence.text_analysis - INFO - 89 batches submitted to accumulate stats from 5696 documents (145825 virtual)
2024-05-03 02:44:47,453 - gensim.topic_coherence.text_analysis - INFO - 90 batches submitted to accumulate stats from 5760 documents (147317 virtual)
2024-05-03 02:44:47,468 - gensim.topic_coherence.text_analysis - INFO - 91 batches submitted to accumulate stats from 5824 documents (149158 virtual)
2024-05-03 02:44:47,468 - gensim.topic_coherence.text_analysis - INFO - 92 batches submitted to accumulate stats from 5888 documents (150755 virtual)
2024-05-03 02:44:47,468 - gensim.topic_coherence.text_analysis - INFO - 93 batches submitted to accumulate stats from 5952 documents (152237 virtual)
2024-05-03 02:44:47,468 - gensim.topic_coherence.text_analysis - INFO - 94 batches submitted to accumulate stats from 6016 documents (154013 virtual)
2024-05-03 02:44:47,468 - gensim.topic_coherence.text_analysis - INFO - 95 batches submitted to accumulate stats from 6080 documents (155593 virtual)
2024-05-03 02:44:47,484 - gensim.topic_coherence.text_analysis - INFO - 96 batches submitted to accumulate stats from 6144 documents (157114 virtual)
2024-05-03 02:44:47,484 - gensim.topic_coherence.text_analysis - INFO - 97 batches submitted to accumulate stats from 6208 documents (158783 virtual)
2024-05-03 02:44:47,484 - gensim.topic_coherence.text_analysis - INFO - 98 batches submitted to accumulate stats from 6272 documents (160467 virtual)
2024-05-03 02:44:47,484 - gensim.topic_coherence.text_analysis - INFO - 99 batches submitted to accumulate stats from 6336 documents (162287 virtual)
2024-05-03 02:44:47,484 - gensim.topic_coherence.text_analysis - INFO - 100 batches submitted to accumulate stats from 6400 documents (163932 virtual)
2024-05-03 02:44:47,484 - gensim.topic_coherence.text_analysis - INFO - 101 batches submitted to accumulate stats from 6464 documents (165414 virtual)
2024-05-03 02:44:47,484 - gensim.topic_coherence.text_analysis - INFO - 102 batches submitted to accumulate stats from 6528 documents (166934 virtual)
2024-05-03 02:44:47,500 - gensim.topic_coherence.text_analysis - INFO - 103 batches submitted to accumulate stats from 6592 documents (168399 virtual)
2024-05-03 02:44:47,500 - gensim.topic_coherence.text_analysis - INFO - 104 batches submitted to accumulate stats from 6656 documents (170596 virtual)
2024-05-03 02:44:47,500 - gensim.topic_coherence.text_analysis - INFO - 105 batches submitted to accumulate stats from 6720 documents (172319 virtual)
2024-05-03 02:44:47,500 - gensim.topic_coherence.text_analysis - INFO - 106 batches submitted to accumulate stats from 6784 documents (173973 virtual)
2024-05-03 02:44:47,500 - gensim.topic_coherence.text_analysis - INFO - 107 batches submitted to accumulate stats from 6848 documents (175817 virtual)
2024-05-03 02:44:47,500 - gensim.topic_coherence.text_analysis - INFO - 108 batches submitted to accumulate stats from 6912 documents (177402 virtual)
2024-05-03 02:44:47,500 - gensim.topic_coherence.text_analysis - INFO - 109 batches submitted to accumulate stats from 6976 documents (179106 virtual)
2024-05-03 02:44:47,515 - gensim.topic_coherence.text_analysis - INFO - 110 batches submitted to accumulate stats from 7040 documents (181089 virtual)
2024-05-03 02:44:47,515 - gensim.topic_coherence.text_analysis - INFO - 111 batches submitted to accumulate stats from 7104 documents (182660 virtual)
2024-05-03 02:44:47,515 - gensim.topic_coherence.text_analysis - INFO - 112 batches submitted to accumulate stats from 7168 documents (184289 virtual)
2024-05-03 02:44:47,515 - gensim.topic_coherence.text_analysis - INFO - 113 batches submitted to accumulate stats from 7232 documents (185825 virtual)
2024-05-03 02:44:47,515 - gensim.topic_coherence.text_analysis - INFO - 114 batches submitted to accumulate stats from 7296 documents (187420 virtual)
2024-05-03 02:44:47,515 - gensim.topic_coherence.text_analysis - INFO - 115 batches submitted to accumulate stats from 7360 documents (189102 virtual)
2024-05-03 02:44:47,515 - gensim.topic_coherence.text_analysis - INFO - 116 batches submitted to accumulate stats from 7424 documents (190745 virtual)
2024-05-03 02:44:47,531 - gensim.topic_coherence.text_analysis - INFO - 117 batches submitted to accumulate stats from 7488 documents (192238 virtual)
2024-05-03 02:44:47,531 - gensim.topic_coherence.text_analysis - INFO - 118 batches submitted to accumulate stats from 7552 documents (194107 virtual)
2024-05-03 02:44:47,531 - gensim.topic_coherence.text_analysis - INFO - 119 batches submitted to accumulate stats from 7616 documents (195570 virtual)
2024-05-03 02:44:47,531 - gensim.topic_coherence.text_analysis - INFO - 120 batches submitted to accumulate stats from 7680 documents (197064 virtual)
2024-05-03 02:44:47,547 - gensim.topic_coherence.text_analysis - INFO - 121 batches submitted to accumulate stats from 7744 documents (198821 virtual)
2024-05-03 02:44:47,547 - gensim.topic_coherence.text_analysis - INFO - 122 batches submitted to accumulate stats from 7808 documents (200394 virtual)
2024-05-03 02:44:47,547 - gensim.topic_coherence.text_analysis - INFO - 123 batches submitted to accumulate stats from 7872 documents (202352 virtual)
2024-05-03 02:44:47,547 - gensim.topic_coherence.text_analysis - INFO - 124 batches submitted to accumulate stats from 7936 documents (204181 virtual)
2024-05-03 02:44:47,547 - gensim.topic_coherence.text_analysis - INFO - 125 batches submitted to accumulate stats from 8000 documents (206063 virtual)
2024-05-03 02:44:47,547 - gensim.topic_coherence.text_analysis - INFO - 126 batches submitted to accumulate stats from 8064 documents (207766 virtual)
2024-05-03 02:44:47,547 - gensim.topic_coherence.text_analysis - INFO - 127 batches submitted to accumulate stats from 8128 documents (209460 virtual)
2024-05-03 02:44:47,562 - gensim.topic_coherence.text_analysis - INFO - 128 batches submitted to accumulate stats from 8192 documents (211022 virtual)
2024-05-03 02:44:47,562 - gensim.topic_coherence.text_analysis - INFO - 129 batches submitted to accumulate stats from 8256 documents (212632 virtual)
2024-05-03 02:44:47,562 - gensim.topic_coherence.text_analysis - INFO - 130 batches submitted to accumulate stats from 8320 documents (214210 virtual)
2024-05-03 02:44:47,562 - gensim.topic_coherence.text_analysis - INFO - 131 batches submitted to accumulate stats from 8384 documents (215651 virtual)
2024-05-03 02:44:47,578 - gensim.topic_coherence.text_analysis - INFO - 132 batches submitted to accumulate stats from 8448 documents (217300 virtual)
2024-05-03 02:44:47,578 - gensim.topic_coherence.text_analysis - INFO - 133 batches submitted to accumulate stats from 8512 documents (219035 virtual)
2024-05-03 02:44:47,593 - gensim.topic_coherence.text_analysis - INFO - 134 batches submitted to accumulate stats from 8576 documents (220675 virtual)
2024-05-03 02:44:47,609 - gensim.topic_coherence.text_analysis - INFO - 135 batches submitted to accumulate stats from 8640 documents (222562 virtual)
2024-05-03 02:44:47,609 - gensim.topic_coherence.text_analysis - INFO - 136 batches submitted to accumulate stats from 8704 documents (224243 virtual)
2024-05-03 02:44:47,609 - gensim.topic_coherence.text_analysis - INFO - 137 batches submitted to accumulate stats from 8768 documents (225942 virtual)
2024-05-03 02:44:47,609 - gensim.topic_coherence.text_analysis - INFO - 138 batches submitted to accumulate stats from 8832 documents (227774 virtual)
2024-05-03 02:44:47,609 - gensim.topic_coherence.text_analysis - INFO - 139 batches submitted to accumulate stats from 8896 documents (229378 virtual)
2024-05-03 02:44:47,609 - gensim.topic_coherence.text_analysis - INFO - 140 batches submitted to accumulate stats from 8960 documents (231026 virtual)
2024-05-03 02:44:47,609 - gensim.topic_coherence.text_analysis - INFO - 141 batches submitted to accumulate stats from 9024 documents (232664 virtual)
2024-05-03 02:44:47,625 - gensim.topic_coherence.text_analysis - INFO - 142 batches submitted to accumulate stats from 9088 documents (234376 virtual)
2024-05-03 02:44:47,625 - gensim.topic_coherence.text_analysis - INFO - 143 batches submitted to accumulate stats from 9152 documents (236034 virtual)
2024-05-03 02:44:47,625 - gensim.topic_coherence.text_analysis - INFO - 144 batches submitted to accumulate stats from 9216 documents (237753 virtual)
2024-05-03 02:44:47,625 - gensim.topic_coherence.text_analysis - INFO - 145 batches submitted to accumulate stats from 9280 documents (239260 virtual)
2024-05-03 02:44:47,640 - gensim.topic_coherence.text_analysis - INFO - 146 batches submitted to accumulate stats from 9344 documents (240889 virtual)
2024-05-03 02:44:47,640 - gensim.topic_coherence.text_analysis - INFO - 147 batches submitted to accumulate stats from 9408 documents (242607 virtual)
2024-05-03 02:44:47,640 - gensim.topic_coherence.text_analysis - INFO - 148 batches submitted to accumulate stats from 9472 documents (244168 virtual)
2024-05-03 02:44:47,640 - gensim.topic_coherence.text_analysis - INFO - 149 batches submitted to accumulate stats from 9536 documents (245827 virtual)
2024-05-03 02:44:47,640 - gensim.topic_coherence.text_analysis - INFO - 150 batches submitted to accumulate stats from 9600 documents (247340 virtual)
2024-05-03 02:44:47,656 - gensim.topic_coherence.text_analysis - INFO - 151 batches submitted to accumulate stats from 9664 documents (248935 virtual)
2024-05-03 02:44:47,656 - gensim.topic_coherence.text_analysis - INFO - 152 batches submitted to accumulate stats from 9728 documents (250682 virtual)
2024-05-03 02:44:47,656 - gensim.topic_coherence.text_analysis - INFO - 153 batches submitted to accumulate stats from 9792 documents (252525 virtual)
2024-05-03 02:44:47,656 - gensim.topic_coherence.text_analysis - INFO - 154 batches submitted to accumulate stats from 9856 documents (253964 virtual)
2024-05-03 02:44:47,656 - gensim.topic_coherence.text_analysis - INFO - 155 batches submitted to accumulate stats from 9920 documents (255761 virtual)
2024-05-03 02:44:47,656 - gensim.topic_coherence.text_analysis - INFO - 156 batches submitted to accumulate stats from 9984 documents (257656 virtual)
2024-05-03 02:44:47,672 - gensim.topic_coherence.text_analysis - INFO - 157 batches submitted to accumulate stats from 10048 documents (259320 virtual)
2024-05-03 02:44:47,672 - gensim.topic_coherence.text_analysis - INFO - 158 batches submitted to accumulate stats from 10112 documents (261193 virtual)
2024-05-03 02:44:47,672 - gensim.topic_coherence.text_analysis - INFO - 159 batches submitted to accumulate stats from 10176 documents (262723 virtual)
2024-05-03 02:44:47,672 - gensim.topic_coherence.text_analysis - INFO - 160 batches submitted to accumulate stats from 10240 documents (264223 virtual)
2024-05-03 02:44:47,687 - gensim.topic_coherence.text_analysis - INFO - 161 batches submitted to accumulate stats from 10304 documents (265694 virtual)
2024-05-03 02:44:47,687 - gensim.topic_coherence.text_analysis - INFO - 162 batches submitted to accumulate stats from 10368 documents (267448 virtual)
2024-05-03 02:44:47,687 - gensim.topic_coherence.text_analysis - INFO - 163 batches submitted to accumulate stats from 10432 documents (269251 virtual)
2024-05-03 02:44:47,687 - gensim.topic_coherence.text_analysis - INFO - 164 batches submitted to accumulate stats from 10496 documents (270973 virtual)
2024-05-03 02:44:47,687 - gensim.topic_coherence.text_analysis - INFO - 165 batches submitted to accumulate stats from 10560 documents (272683 virtual)
2024-05-03 02:44:47,687 - gensim.topic_coherence.text_analysis - INFO - 166 batches submitted to accumulate stats from 10624 documents (274294 virtual)
2024-05-03 02:44:47,687 - gensim.topic_coherence.text_analysis - INFO - 167 batches submitted to accumulate stats from 10688 documents (276045 virtual)
2024-05-03 02:44:47,703 - gensim.topic_coherence.text_analysis - INFO - 168 batches submitted to accumulate stats from 10752 documents (277496 virtual)
2024-05-03 02:44:47,703 - gensim.topic_coherence.text_analysis - INFO - 169 batches submitted to accumulate stats from 10816 documents (279131 virtual)
2024-05-03 02:44:47,703 - gensim.topic_coherence.text_analysis - INFO - 170 batches submitted to accumulate stats from 10880 documents (280812 virtual)
2024-05-03 02:44:47,703 - gensim.topic_coherence.text_analysis - INFO - 171 batches submitted to accumulate stats from 10944 documents (282408 virtual)
2024-05-03 02:44:47,703 - gensim.topic_coherence.text_analysis - INFO - 172 batches submitted to accumulate stats from 11008 documents (284125 virtual)
2024-05-03 02:44:47,718 - gensim.topic_coherence.text_analysis - INFO - 173 batches submitted to accumulate stats from 11072 documents (285575 virtual)
2024-05-03 02:44:47,718 - gensim.topic_coherence.text_analysis - INFO - 174 batches submitted to accumulate stats from 11136 documents (287185 virtual)
2024-05-03 02:44:47,718 - gensim.topic_coherence.text_analysis - INFO - 175 batches submitted to accumulate stats from 11200 documents (289145 virtual)
2024-05-03 02:44:47,718 - gensim.topic_coherence.text_analysis - INFO - 176 batches submitted to accumulate stats from 11264 documents (290900 virtual)
2024-05-03 02:44:47,718 - gensim.topic_coherence.text_analysis - INFO - 177 batches submitted to accumulate stats from 11328 documents (292686 virtual)
2024-05-03 02:44:47,734 - gensim.topic_coherence.text_analysis - INFO - 178 batches submitted to accumulate stats from 11392 documents (294505 virtual)
2024-05-03 02:44:47,734 - gensim.topic_coherence.text_analysis - INFO - 179 batches submitted to accumulate stats from 11456 documents (296236 virtual)
2024-05-03 02:44:47,734 - gensim.topic_coherence.text_analysis - INFO - 180 batches submitted to accumulate stats from 11520 documents (297647 virtual)
2024-05-03 02:44:47,734 - gensim.topic_coherence.text_analysis - INFO - 181 batches submitted to accumulate stats from 11584 documents (299327 virtual)
2024-05-03 02:44:47,734 - gensim.topic_coherence.text_analysis - INFO - 182 batches submitted to accumulate stats from 11648 documents (300853 virtual)
2024-05-03 02:44:47,750 - gensim.topic_coherence.text_analysis - INFO - 183 batches submitted to accumulate stats from 11712 documents (302601 virtual)
2024-05-03 02:44:47,750 - gensim.topic_coherence.text_analysis - INFO - 184 batches submitted to accumulate stats from 11776 documents (304181 virtual)
2024-05-03 02:44:47,750 - gensim.topic_coherence.text_analysis - INFO - 185 batches submitted to accumulate stats from 11840 documents (305710 virtual)
2024-05-03 02:44:47,750 - gensim.topic_coherence.text_analysis - INFO - 186 batches submitted to accumulate stats from 11904 documents (307265 virtual)
2024-05-03 02:44:47,750 - gensim.topic_coherence.text_analysis - INFO - 187 batches submitted to accumulate stats from 11968 documents (309148 virtual)
2024-05-03 02:44:47,750 - gensim.topic_coherence.text_analysis - INFO - 188 batches submitted to accumulate stats from 12032 documents (310818 virtual)
2024-05-03 02:44:47,765 - gensim.topic_coherence.text_analysis - INFO - 189 batches submitted to accumulate stats from 12096 documents (312472 virtual)
2024-05-03 02:44:47,765 - gensim.topic_coherence.text_analysis - INFO - 190 batches submitted to accumulate stats from 12160 documents (314119 virtual)
2024-05-03 02:44:47,765 - gensim.topic_coherence.text_analysis - INFO - 191 batches submitted to accumulate stats from 12224 documents (316054 virtual)
2024-05-03 02:44:47,765 - gensim.topic_coherence.text_analysis - INFO - 192 batches submitted to accumulate stats from 12288 documents (317685 virtual)
2024-05-03 02:44:47,765 - gensim.topic_coherence.text_analysis - INFO - 193 batches submitted to accumulate stats from 12352 documents (319351 virtual)
2024-05-03 02:44:47,781 - gensim.topic_coherence.text_analysis - INFO - 194 batches submitted to accumulate stats from 12416 documents (321100 virtual)
2024-05-03 02:44:47,781 - gensim.topic_coherence.text_analysis - INFO - 195 batches submitted to accumulate stats from 12480 documents (323099 virtual)
2024-05-03 02:44:47,781 - gensim.topic_coherence.text_analysis - INFO - 196 batches submitted to accumulate stats from 12544 documents (324548 virtual)
2024-05-03 02:44:47,781 - gensim.topic_coherence.text_analysis - INFO - 197 batches submitted to accumulate stats from 12608 documents (326231 virtual)
2024-05-03 02:44:47,781 - gensim.topic_coherence.text_analysis - INFO - 198 batches submitted to accumulate stats from 12672 documents (327851 virtual)
2024-05-03 02:44:47,797 - gensim.topic_coherence.text_analysis - INFO - 199 batches submitted to accumulate stats from 12736 documents (329386 virtual)
2024-05-03 02:44:47,797 - gensim.topic_coherence.text_analysis - INFO - 200 batches submitted to accumulate stats from 12800 documents (331113 virtual)
2024-05-03 02:44:47,797 - gensim.topic_coherence.text_analysis - INFO - 201 batches submitted to accumulate stats from 12864 documents (332712 virtual)
2024-05-03 02:44:47,797 - gensim.topic_coherence.text_analysis - INFO - 202 batches submitted to accumulate stats from 12928 documents (334135 virtual)
2024-05-03 02:44:47,797 - gensim.topic_coherence.text_analysis - INFO - 203 batches submitted to accumulate stats from 12992 documents (335914 virtual)
2024-05-03 02:44:47,797 - gensim.topic_coherence.text_analysis - INFO - 204 batches submitted to accumulate stats from 13056 documents (337641 virtual)
2024-05-03 02:44:47,812 - gensim.topic_coherence.text_analysis - INFO - 205 batches submitted to accumulate stats from 13120 documents (339449 virtual)
2024-05-03 02:44:47,812 - gensim.topic_coherence.text_analysis - INFO - 206 batches submitted to accumulate stats from 13184 documents (341168 virtual)
2024-05-03 02:44:47,812 - gensim.topic_coherence.text_analysis - INFO - 207 batches submitted to accumulate stats from 13248 documents (342833 virtual)
2024-05-03 02:44:47,812 - gensim.topic_coherence.text_analysis - INFO - 208 batches submitted to accumulate stats from 13312 documents (344704 virtual)
2024-05-03 02:44:47,812 - gensim.topic_coherence.text_analysis - INFO - 209 batches submitted to accumulate stats from 13376 documents (346650 virtual)
2024-05-03 02:44:47,828 - gensim.topic_coherence.text_analysis - INFO - 210 batches submitted to accumulate stats from 13440 documents (348531 virtual)
2024-05-03 02:44:47,828 - gensim.topic_coherence.text_analysis - INFO - 211 batches submitted to accumulate stats from 13504 documents (350342 virtual)
2024-05-03 02:44:47,828 - gensim.topic_coherence.text_analysis - INFO - 212 batches submitted to accumulate stats from 13568 documents (352068 virtual)
2024-05-03 02:44:47,828 - gensim.topic_coherence.text_analysis - INFO - 213 batches submitted to accumulate stats from 13632 documents (353789 virtual)
2024-05-03 02:44:47,828 - gensim.topic_coherence.text_analysis - INFO - 214 batches submitted to accumulate stats from 13696 documents (355216 virtual)
2024-05-03 02:44:47,828 - gensim.topic_coherence.text_analysis - INFO - 215 batches submitted to accumulate stats from 13760 documents (356990 virtual)
2024-05-03 02:44:47,828 - gensim.topic_coherence.text_analysis - INFO - 216 batches submitted to accumulate stats from 13824 documents (358762 virtual)
2024-05-03 02:44:47,843 - gensim.topic_coherence.text_analysis - INFO - 217 batches submitted to accumulate stats from 13888 documents (360320 virtual)
2024-05-03 02:44:47,843 - gensim.topic_coherence.text_analysis - INFO - 218 batches submitted to accumulate stats from 13952 documents (361867 virtual)
2024-05-03 02:44:47,843 - gensim.topic_coherence.text_analysis - INFO - 219 batches submitted to accumulate stats from 14016 documents (363519 virtual)
2024-05-03 02:44:47,843 - gensim.topic_coherence.text_analysis - INFO - 220 batches submitted to accumulate stats from 14080 documents (365141 virtual)
2024-05-03 02:44:47,843 - gensim.topic_coherence.text_analysis - INFO - 221 batches submitted to accumulate stats from 14144 documents (366934 virtual)
2024-05-03 02:44:47,859 - gensim.topic_coherence.text_analysis - INFO - 222 batches submitted to accumulate stats from 14208 documents (368448 virtual)
2024-05-03 02:44:47,859 - gensim.topic_coherence.text_analysis - INFO - 223 batches submitted to accumulate stats from 14272 documents (370012 virtual)
2024-05-03 02:44:47,859 - gensim.topic_coherence.text_analysis - INFO - 224 batches submitted to accumulate stats from 14336 documents (371704 virtual)
2024-05-03 02:44:47,859 - gensim.topic_coherence.text_analysis - INFO - 225 batches submitted to accumulate stats from 14400 documents (373331 virtual)
2024-05-03 02:44:47,859 - gensim.topic_coherence.text_analysis - INFO - 226 batches submitted to accumulate stats from 14464 documents (375174 virtual)
2024-05-03 02:44:47,859 - gensim.topic_coherence.text_analysis - INFO - 227 batches submitted to accumulate stats from 14528 documents (377135 virtual)
2024-05-03 02:44:47,875 - gensim.topic_coherence.text_analysis - INFO - 228 batches submitted to accumulate stats from 14592 documents (378860 virtual)
2024-05-03 02:44:47,875 - gensim.topic_coherence.text_analysis - INFO - 229 batches submitted to accumulate stats from 14656 documents (380581 virtual)
2024-05-03 02:44:47,875 - gensim.topic_coherence.text_analysis - INFO - 230 batches submitted to accumulate stats from 14720 documents (382278 virtual)
2024-05-03 02:44:47,875 - gensim.topic_coherence.text_analysis - INFO - 231 batches submitted to accumulate stats from 14784 documents (383970 virtual)
2024-05-03 02:44:47,875 - gensim.topic_coherence.text_analysis - INFO - 232 batches submitted to accumulate stats from 14848 documents (385680 virtual)
2024-05-03 02:44:47,875 - gensim.topic_coherence.text_analysis - INFO - 233 batches submitted to accumulate stats from 14912 documents (387378 virtual)
2024-05-03 02:44:47,875 - gensim.topic_coherence.text_analysis - INFO - 234 batches submitted to accumulate stats from 14976 documents (388908 virtual)
2024-05-03 02:44:47,890 - gensim.topic_coherence.text_analysis - INFO - 235 batches submitted to accumulate stats from 15040 documents (390473 virtual)
2024-05-03 02:44:47,890 - gensim.topic_coherence.text_analysis - INFO - 236 batches submitted to accumulate stats from 15104 documents (391966 virtual)
2024-05-03 02:44:47,890 - gensim.topic_coherence.text_analysis - INFO - 237 batches submitted to accumulate stats from 15168 documents (393446 virtual)
2024-05-03 02:44:47,890 - gensim.topic_coherence.text_analysis - INFO - 238 batches submitted to accumulate stats from 15232 documents (394979 virtual)
2024-05-03 02:44:47,890 - gensim.topic_coherence.text_analysis - INFO - 239 batches submitted to accumulate stats from 15296 documents (396707 virtual)
2024-05-03 02:44:47,890 - gensim.topic_coherence.text_analysis - INFO - 240 batches submitted to accumulate stats from 15360 documents (398434 virtual)
2024-05-03 02:44:47,906 - gensim.topic_coherence.text_analysis - INFO - 241 batches submitted to accumulate stats from 15424 documents (400092 virtual)
2024-05-03 02:44:47,906 - gensim.topic_coherence.text_analysis - INFO - 242 batches submitted to accumulate stats from 15488 documents (402054 virtual)
2024-05-03 02:44:47,906 - gensim.topic_coherence.text_analysis - INFO - 243 batches submitted to accumulate stats from 15552 documents (403465 virtual)
2024-05-03 02:44:47,906 - gensim.topic_coherence.text_analysis - INFO - 244 batches submitted to accumulate stats from 15616 documents (405289 virtual)
2024-05-03 02:44:47,906 - gensim.topic_coherence.text_analysis - INFO - 245 batches submitted to accumulate stats from 15680 documents (406750 virtual)
2024-05-03 02:44:47,906 - gensim.topic_coherence.text_analysis - INFO - 246 batches submitted to accumulate stats from 15744 documents (408306 virtual)
2024-05-03 02:44:47,922 - gensim.topic_coherence.text_analysis - INFO - 247 batches submitted to accumulate stats from 15808 documents (410097 virtual)
2024-05-03 02:44:47,922 - gensim.topic_coherence.text_analysis - INFO - 248 batches submitted to accumulate stats from 15872 documents (411915 virtual)
2024-05-03 02:44:47,922 - gensim.topic_coherence.text_analysis - INFO - 249 batches submitted to accumulate stats from 15936 documents (413538 virtual)
2024-05-03 02:44:47,922 - gensim.topic_coherence.text_analysis - INFO - 250 batches submitted to accumulate stats from 16000 documents (415307 virtual)
2024-05-03 02:44:47,937 - gensim.topic_coherence.text_analysis - INFO - 251 batches submitted to accumulate stats from 16064 documents (416993 virtual)
2024-05-03 02:44:47,937 - gensim.topic_coherence.text_analysis - INFO - 252 batches submitted to accumulate stats from 16128 documents (418582 virtual)
2024-05-03 02:44:47,937 - gensim.topic_coherence.text_analysis - INFO - 253 batches submitted to accumulate stats from 16192 documents (420282 virtual)
2024-05-03 02:44:47,937 - gensim.topic_coherence.text_analysis - INFO - 254 batches submitted to accumulate stats from 16256 documents (421859 virtual)
2024-05-03 02:44:47,937 - gensim.topic_coherence.text_analysis - INFO - 255 batches submitted to accumulate stats from 16320 documents (423441 virtual)
2024-05-03 02:44:47,937 - gensim.topic_coherence.text_analysis - INFO - 256 batches submitted to accumulate stats from 16384 documents (425169 virtual)
2024-05-03 02:44:47,953 - gensim.topic_coherence.text_analysis - INFO - 257 batches submitted to accumulate stats from 16448 documents (426804 virtual)
2024-05-03 02:44:47,953 - gensim.topic_coherence.text_analysis - INFO - 258 batches submitted to accumulate stats from 16512 documents (428503 virtual)
2024-05-03 02:44:47,953 - gensim.topic_coherence.text_analysis - INFO - 259 batches submitted to accumulate stats from 16576 documents (430063 virtual)
2024-05-03 02:44:47,953 - gensim.topic_coherence.text_analysis - INFO - 260 batches submitted to accumulate stats from 16640 documents (431672 virtual)
2024-05-03 02:44:47,953 - gensim.topic_coherence.text_analysis - INFO - 261 batches submitted to accumulate stats from 16704 documents (433298 virtual)
2024-05-03 02:44:47,968 - gensim.topic_coherence.text_analysis - INFO - 262 batches submitted to accumulate stats from 16768 documents (434951 virtual)
2024-05-03 02:44:47,968 - gensim.topic_coherence.text_analysis - INFO - 263 batches submitted to accumulate stats from 16832 documents (436505 virtual)
2024-05-03 02:44:47,968 - gensim.topic_coherence.text_analysis - INFO - 264 batches submitted to accumulate stats from 16896 documents (438134 virtual)
2024-05-03 02:44:47,968 - gensim.topic_coherence.text_analysis - INFO - 265 batches submitted to accumulate stats from 16960 documents (439831 virtual)
2024-05-03 02:44:47,968 - gensim.topic_coherence.text_analysis - INFO - 266 batches submitted to accumulate stats from 17024 documents (441666 virtual)
2024-05-03 02:44:47,968 - gensim.topic_coherence.text_analysis - INFO - 267 batches submitted to accumulate stats from 17088 documents (443389 virtual)
2024-05-03 02:44:47,968 - gensim.topic_coherence.text_analysis - INFO - 268 batches submitted to accumulate stats from 17152 documents (445008 virtual)
2024-05-03 02:44:47,984 - gensim.topic_coherence.text_analysis - INFO - 269 batches submitted to accumulate stats from 17216 documents (446691 virtual)
2024-05-03 02:44:47,984 - gensim.topic_coherence.text_analysis - INFO - 270 batches submitted to accumulate stats from 17280 documents (448258 virtual)
2024-05-03 02:44:47,984 - gensim.topic_coherence.text_analysis - INFO - 271 batches submitted to accumulate stats from 17344 documents (450177 virtual)
2024-05-03 02:44:47,984 - gensim.topic_coherence.text_analysis - INFO - 272 batches submitted to accumulate stats from 17408 documents (451838 virtual)
2024-05-03 02:44:48,000 - gensim.topic_coherence.text_analysis - INFO - 273 batches submitted to accumulate stats from 17472 documents (453765 virtual)
2024-05-03 02:44:48,000 - gensim.topic_coherence.text_analysis - INFO - 274 batches submitted to accumulate stats from 17536 documents (455302 virtual)
2024-05-03 02:44:48,000 - gensim.topic_coherence.text_analysis - INFO - 275 batches submitted to accumulate stats from 17600 documents (457026 virtual)
2024-05-03 02:44:48,000 - gensim.topic_coherence.text_analysis - INFO - 276 batches submitted to accumulate stats from 17664 documents (458678 virtual)
2024-05-03 02:44:48,000 - gensim.topic_coherence.text_analysis - INFO - 277 batches submitted to accumulate stats from 17728 documents (460363 virtual)
2024-05-03 02:44:48,000 - gensim.topic_coherence.text_analysis - INFO - 278 batches submitted to accumulate stats from 17792 documents (462290 virtual)
2024-05-03 02:44:48,015 - gensim.topic_coherence.text_analysis - INFO - 279 batches submitted to accumulate stats from 17856 documents (464043 virtual)
2024-05-03 02:44:48,015 - gensim.topic_coherence.text_analysis - INFO - 280 batches submitted to accumulate stats from 17920 documents (465601 virtual)
2024-05-03 02:44:48,015 - gensim.topic_coherence.text_analysis - INFO - 281 batches submitted to accumulate stats from 17984 documents (467208 virtual)
2024-05-03 02:44:48,015 - gensim.topic_coherence.text_analysis - INFO - 282 batches submitted to accumulate stats from 18048 documents (468985 virtual)
2024-05-03 02:44:48,015 - gensim.topic_coherence.text_analysis - INFO - 283 batches submitted to accumulate stats from 18112 documents (470540 virtual)
2024-05-03 02:44:48,015 - gensim.topic_coherence.text_analysis - INFO - 284 batches submitted to accumulate stats from 18176 documents (472187 virtual)
2024-05-03 02:44:48,031 - gensim.topic_coherence.text_analysis - INFO - 285 batches submitted to accumulate stats from 18240 documents (473657 virtual)
2024-05-03 02:44:48,031 - gensim.topic_coherence.text_analysis - INFO - 286 batches submitted to accumulate stats from 18304 documents (475433 virtual)
2024-05-03 02:44:48,031 - gensim.topic_coherence.text_analysis - INFO - 287 batches submitted to accumulate stats from 18368 documents (476949 virtual)
2024-05-03 02:44:48,031 - gensim.topic_coherence.text_analysis - INFO - 288 batches submitted to accumulate stats from 18432 documents (478519 virtual)
2024-05-03 02:44:48,031 - gensim.topic_coherence.text_analysis - INFO - 289 batches submitted to accumulate stats from 18496 documents (480346 virtual)
2024-05-03 02:44:48,047 - gensim.topic_coherence.text_analysis - INFO - 290 batches submitted to accumulate stats from 18560 documents (482113 virtual)
2024-05-03 02:44:48,047 - gensim.topic_coherence.text_analysis - INFO - 291 batches submitted to accumulate stats from 18624 documents (483726 virtual)
2024-05-03 02:44:48,047 - gensim.topic_coherence.text_analysis - INFO - 292 batches submitted to accumulate stats from 18688 documents (485454 virtual)
2024-05-03 02:44:48,047 - gensim.topic_coherence.text_analysis - INFO - 293 batches submitted to accumulate stats from 18752 documents (487120 virtual)
2024-05-03 02:44:48,047 - gensim.topic_coherence.text_analysis - INFO - 294 batches submitted to accumulate stats from 18816 documents (488702 virtual)
2024-05-03 02:44:48,047 - gensim.topic_coherence.text_analysis - INFO - 295 batches submitted to accumulate stats from 18880 documents (490511 virtual)
2024-05-03 02:44:48,062 - gensim.topic_coherence.text_analysis - INFO - 296 batches submitted to accumulate stats from 18944 documents (492226 virtual)
2024-05-03 02:44:48,062 - gensim.topic_coherence.text_analysis - INFO - 297 batches submitted to accumulate stats from 19008 documents (494165 virtual)
2024-05-03 02:44:48,062 - gensim.topic_coherence.text_analysis - INFO - 298 batches submitted to accumulate stats from 19072 documents (495678 virtual)
2024-05-03 02:44:48,062 - gensim.topic_coherence.text_analysis - INFO - 299 batches submitted to accumulate stats from 19136 documents (497503 virtual)
2024-05-03 02:44:48,062 - gensim.topic_coherence.text_analysis - INFO - 300 batches submitted to accumulate stats from 19200 documents (499131 virtual)
2024-05-03 02:44:48,078 - gensim.topic_coherence.text_analysis - INFO - 301 batches submitted to accumulate stats from 19264 documents (500986 virtual)
2024-05-03 02:44:48,078 - gensim.topic_coherence.text_analysis - INFO - 302 batches submitted to accumulate stats from 19328 documents (502687 virtual)
2024-05-03 02:44:48,078 - gensim.topic_coherence.text_analysis - INFO - 303 batches submitted to accumulate stats from 19392 documents (504106 virtual)
2024-05-03 02:44:48,078 - gensim.topic_coherence.text_analysis - INFO - 304 batches submitted to accumulate stats from 19456 documents (505909 virtual)
2024-05-03 02:44:48,078 - gensim.topic_coherence.text_analysis - INFO - 305 batches submitted to accumulate stats from 19520 documents (507546 virtual)
2024-05-03 02:44:48,078 - gensim.topic_coherence.text_analysis - INFO - 306 batches submitted to accumulate stats from 19584 documents (509312 virtual)
2024-05-03 02:44:48,093 - gensim.topic_coherence.text_analysis - INFO - 307 batches submitted to accumulate stats from 19648 documents (510954 virtual)
2024-05-03 02:44:48,093 - gensim.topic_coherence.text_analysis - INFO - 308 batches submitted to accumulate stats from 19712 documents (512465 virtual)
2024-05-03 02:44:48,093 - gensim.topic_coherence.text_analysis - INFO - 309 batches submitted to accumulate stats from 19776 documents (514132 virtual)
2024-05-03 02:44:48,093 - gensim.topic_coherence.text_analysis - INFO - 310 batches submitted to accumulate stats from 19840 documents (515846 virtual)
2024-05-03 02:44:48,093 - gensim.topic_coherence.text_analysis - INFO - 311 batches submitted to accumulate stats from 19904 documents (517436 virtual)
2024-05-03 02:44:48,109 - gensim.topic_coherence.text_analysis - INFO - 312 batches submitted to accumulate stats from 19968 documents (519076 virtual)
2024-05-03 02:44:48,109 - gensim.topic_coherence.text_analysis - INFO - 313 batches submitted to accumulate stats from 20032 documents (520797 virtual)
2024-05-03 02:44:48,109 - gensim.topic_coherence.text_analysis - INFO - 314 batches submitted to accumulate stats from 20096 documents (522328 virtual)
2024-05-03 02:44:48,484 - gensim.topic_coherence.text_analysis - INFO - 11 accumulators retrieved from output queue
2024-05-03 02:44:48,750 - gensim.topic_coherence.text_analysis - INFO - accumulated word occurrence stats for 523696 virtual documents
2024-05-03 02:44:49,817 - gensim.corpora.dictionary - INFO - adding document #0 to Dictionary<0 unique tokens: []>
2024-05-03 02:44:50,223 - gensim.corpora.dictionary - INFO - adding document #10000 to Dictionary<23643 unique tokens: ['-', '.', '?', 'a', 'africa']...>
2024-05-03 02:44:50,661 - gensim.corpora.dictionary - INFO - adding document #20000 to Dictionary<32428 unique tokens: ['-', '.', '?', 'a', 'africa']...>
2024-05-03 02:44:50,661 - gensim.corpora.dictionary - INFO - built Dictionary<32503 unique tokens: ['-', '.', '?', 'a', 'africa']...> from 20086 documents (total 703102 corpus positions)
2024-05-03 02:44:50,661 - gensim.utils - INFO - Dictionary lifecycle event {'msg': "built Dictionary<32503 unique tokens: ['-', '.', '?', 'a', 'africa']...> from 20086 documents (total 703102 corpus positions)", 'datetime': '2024-05-03T02:44:50.661285', 'gensim': '4.3.2', 'python': '3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}
2024-05-03 02:44:50,676 - gensim.topic_coherence.probability_estimation - INFO - using ParallelWordOccurrenceAccumulator<processes=11, batch_size=64> to estimate probabilities from sliding windows
2024-05-03 02:45:47,467 - gensim.topic_coherence.text_analysis - INFO - 11 accumulators retrieved from output queue
2024-05-03 02:45:47,530 - gensim.topic_coherence.text_analysis - INFO - accumulated word occurrence stats for 21620 virtual documents
2024-05-03 02:45:52,468 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=50, bias=False)
  (W_q): Linear(in_features=300, out_features=50, bias=False)
  (W_v): Linear(in_features=300, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,138,605
Freeze params: 0
2024-05-03 02:49:30,282 - trainer - INFO -     epoch          : 1
2024-05-03 02:49:30,282 - trainer - INFO -     loss           : 1.144569
2024-05-03 02:49:30,282 - trainer - INFO -     accuracy       : 0.670467
2024-05-03 02:49:30,282 - trainer - INFO -     macro_f        : 0.651974
2024-05-03 02:49:30,282 - trainer - INFO -     precision      : 0.682704
2024-05-03 02:49:30,282 - trainer - INFO -     recall         : 0.670467
2024-05-03 02:49:30,282 - trainer - INFO -     doc_entropy    : 2.4053
2024-05-03 02:49:30,282 - trainer - INFO -     val_loss       : 1.003746
2024-05-03 02:49:30,282 - trainer - INFO -     val_accuracy   : 0.70706
2024-05-03 02:49:30,282 - trainer - INFO -     val_macro_f    : 0.692312
2024-05-03 02:49:30,282 - trainer - INFO -     val_precision  : 0.72268
2024-05-03 02:49:30,297 - trainer - INFO -     val_recall     : 0.70706
2024-05-03 02:49:30,297 - trainer - INFO -     val_doc_entropy: 2.628753
2024-05-03 02:49:30,297 - trainer - INFO -     test_loss      : 0.995633
2024-05-03 02:49:30,297 - trainer - INFO -     test_accuracy  : 0.708703
2024-05-03 02:49:30,297 - trainer - INFO -     test_macro_f   : 0.693539
2024-05-03 02:49:30,297 - trainer - INFO -     test_precision : 0.723738
2024-05-03 02:49:30,297 - trainer - INFO -     test_recall    : 0.708703
2024-05-03 02:49:30,297 - trainer - INFO -     test_doc_entropy: 2.631323
2024-05-03 02:53:23,404 - trainer - INFO -     epoch          : 2
2024-05-03 02:53:23,404 - trainer - INFO -     loss           : 0.803798
2024-05-03 02:53:23,404 - trainer - INFO -     accuracy       : 0.759348
2024-05-03 02:53:23,404 - trainer - INFO -     macro_f        : 0.750427
2024-05-03 02:53:23,404 - trainer - INFO -     precision      : 0.782771
2024-05-03 02:53:23,404 - trainer - INFO -     recall         : 0.759348
2024-05-03 02:53:23,404 - trainer - INFO -     doc_entropy    : 1.982452
2024-05-03 02:53:23,404 - trainer - INFO -     val_loss       : 1.001739
2024-05-03 02:53:23,404 - trainer - INFO -     val_accuracy   : 0.706661
2024-05-03 02:53:23,404 - trainer - INFO -     val_macro_f    : 0.700233
2024-05-03 02:53:23,404 - trainer - INFO -     val_precision  : 0.741438
2024-05-03 02:53:23,404 - trainer - INFO -     val_recall     : 0.706661
2024-05-03 02:53:23,404 - trainer - INFO -     val_doc_entropy: 2.411159
2024-05-03 02:53:23,404 - trainer - INFO -     test_loss      : 0.994059
2024-05-03 02:53:23,404 - trainer - INFO -     test_accuracy  : 0.7093
2024-05-03 02:53:23,404 - trainer - INFO -     test_macro_f   : 0.701711
2024-05-03 02:53:23,404 - trainer - INFO -     test_precision : 0.740165
2024-05-03 02:53:23,404 - trainer - INFO -     test_recall    : 0.7093
2024-05-03 02:53:23,404 - trainer - INFO -     test_doc_entropy: 2.41423
2024-05-03 02:54:06,532 - gensim.corpora.dictionary - INFO - adding document #0 to Dictionary<0 unique tokens: []>
2024-05-03 02:54:06,922 - gensim.corpora.dictionary - INFO - adding document #10000 to Dictionary<23643 unique tokens: ['-', '.', '?', 'a', 'africa']...>
2024-05-03 02:54:07,313 - gensim.corpora.dictionary - INFO - adding document #20000 to Dictionary<32428 unique tokens: ['-', '.', '?', 'a', 'africa']...>
2024-05-03 02:54:07,313 - gensim.corpora.dictionary - INFO - built Dictionary<32503 unique tokens: ['-', '.', '?', 'a', 'africa']...> from 20086 documents (total 703102 corpus positions)
2024-05-03 02:54:07,329 - gensim.utils - INFO - Dictionary lifecycle event {'msg': "built Dictionary<32503 unique tokens: ['-', '.', '?', 'a', 'africa']...> from 20086 documents (total 703102 corpus positions)", 'datetime': '2024-05-03T02:54:07.329511', 'gensim': '4.3.2', 'python': '3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}
2024-05-03 02:54:07,345 - gensim.topic_coherence.probability_estimation - INFO - using ParallelWordOccurrenceAccumulator<processes=11, batch_size=64> to estimate probabilities from sliding windows
2024-05-03 02:55:10,114 - gensim.topic_coherence.text_analysis - INFO - 1 batches submitted to accumulate stats from 64 documents (1358 virtual)
2024-05-03 02:55:10,114 - gensim.topic_coherence.text_analysis - INFO - 2 batches submitted to accumulate stats from 128 documents (3142 virtual)
2024-05-03 02:55:10,114 - gensim.topic_coherence.text_analysis - INFO - 3 batches submitted to accumulate stats from 192 documents (4707 virtual)
2024-05-03 02:55:10,114 - gensim.topic_coherence.text_analysis - INFO - 4 batches submitted to accumulate stats from 256 documents (6346 virtual)
2024-05-03 02:55:10,130 - gensim.topic_coherence.text_analysis - INFO - 5 batches submitted to accumulate stats from 320 documents (7961 virtual)
2024-05-03 02:55:10,130 - gensim.topic_coherence.text_analysis - INFO - 6 batches submitted to accumulate stats from 384 documents (9298 virtual)
2024-05-03 02:55:10,130 - gensim.topic_coherence.text_analysis - INFO - 7 batches submitted to accumulate stats from 448 documents (11371 virtual)
2024-05-03 02:55:10,130 - gensim.topic_coherence.text_analysis - INFO - 8 batches submitted to accumulate stats from 512 documents (13011 virtual)
2024-05-03 02:55:10,130 - gensim.topic_coherence.text_analysis - INFO - 9 batches submitted to accumulate stats from 576 documents (14534 virtual)
2024-05-03 02:55:10,130 - gensim.topic_coherence.text_analysis - INFO - 10 batches submitted to accumulate stats from 640 documents (16161 virtual)
2024-05-03 02:55:10,130 - gensim.topic_coherence.text_analysis - INFO - 11 batches submitted to accumulate stats from 704 documents (17689 virtual)
2024-05-03 02:55:10,130 - gensim.topic_coherence.text_analysis - INFO - 12 batches submitted to accumulate stats from 768 documents (19256 virtual)
2024-05-03 02:55:10,145 - gensim.topic_coherence.text_analysis - INFO - 13 batches submitted to accumulate stats from 832 documents (21175 virtual)
2024-05-03 02:55:10,145 - gensim.topic_coherence.text_analysis - INFO - 14 batches submitted to accumulate stats from 896 documents (22850 virtual)
2024-05-03 02:55:10,145 - gensim.topic_coherence.text_analysis - INFO - 15 batches submitted to accumulate stats from 960 documents (24681 virtual)
2024-05-03 02:55:10,145 - gensim.topic_coherence.text_analysis - INFO - 16 batches submitted to accumulate stats from 1024 documents (26222 virtual)
2024-05-03 02:55:10,145 - gensim.topic_coherence.text_analysis - INFO - 17 batches submitted to accumulate stats from 1088 documents (27953 virtual)
2024-05-03 02:55:10,161 - gensim.topic_coherence.text_analysis - INFO - 18 batches submitted to accumulate stats from 1152 documents (29594 virtual)
2024-05-03 02:55:10,161 - gensim.topic_coherence.text_analysis - INFO - 19 batches submitted to accumulate stats from 1216 documents (31239 virtual)
2024-05-03 02:55:10,161 - gensim.topic_coherence.text_analysis - INFO - 20 batches submitted to accumulate stats from 1280 documents (32901 virtual)
2024-05-03 02:55:10,161 - gensim.topic_coherence.text_analysis - INFO - 21 batches submitted to accumulate stats from 1344 documents (34481 virtual)
2024-05-03 02:55:10,161 - gensim.topic_coherence.text_analysis - INFO - 22 batches submitted to accumulate stats from 1408 documents (36151 virtual)
2024-05-03 02:55:10,161 - gensim.topic_coherence.text_analysis - INFO - 23 batches submitted to accumulate stats from 1472 documents (37837 virtual)
2024-05-03 02:55:10,161 - gensim.topic_coherence.text_analysis - INFO - 24 batches submitted to accumulate stats from 1536 documents (39453 virtual)
2024-05-03 02:55:10,176 - gensim.topic_coherence.text_analysis - INFO - 25 batches submitted to accumulate stats from 1600 documents (41046 virtual)
2024-05-03 02:55:10,176 - gensim.topic_coherence.text_analysis - INFO - 26 batches submitted to accumulate stats from 1664 documents (42892 virtual)
2024-05-03 02:55:10,176 - gensim.topic_coherence.text_analysis - INFO - 27 batches submitted to accumulate stats from 1728 documents (44445 virtual)
2024-05-03 02:55:10,176 - gensim.topic_coherence.text_analysis - INFO - 28 batches submitted to accumulate stats from 1792 documents (46073 virtual)
2024-05-03 02:55:10,192 - gensim.topic_coherence.text_analysis - INFO - 29 batches submitted to accumulate stats from 1856 documents (47643 virtual)
2024-05-03 02:55:10,192 - gensim.topic_coherence.text_analysis - INFO - 30 batches submitted to accumulate stats from 1920 documents (49252 virtual)
2024-05-03 02:55:10,192 - gensim.topic_coherence.text_analysis - INFO - 31 batches submitted to accumulate stats from 1984 documents (50774 virtual)
2024-05-03 02:55:10,208 - gensim.topic_coherence.text_analysis - INFO - 32 batches submitted to accumulate stats from 2048 documents (52387 virtual)
2024-05-03 02:55:10,208 - gensim.topic_coherence.text_analysis - INFO - 33 batches submitted to accumulate stats from 2112 documents (53815 virtual)
2024-05-03 02:55:10,208 - gensim.topic_coherence.text_analysis - INFO - 34 batches submitted to accumulate stats from 2176 documents (55540 virtual)
2024-05-03 02:55:10,208 - gensim.topic_coherence.text_analysis - INFO - 35 batches submitted to accumulate stats from 2240 documents (57348 virtual)
2024-05-03 02:55:10,208 - gensim.topic_coherence.text_analysis - INFO - 36 batches submitted to accumulate stats from 2304 documents (59258 virtual)
2024-05-03 02:55:10,208 - gensim.topic_coherence.text_analysis - INFO - 37 batches submitted to accumulate stats from 2368 documents (60957 virtual)
2024-05-03 02:55:10,223 - gensim.topic_coherence.text_analysis - INFO - 38 batches submitted to accumulate stats from 2432 documents (62425 virtual)
2024-05-03 02:55:10,223 - gensim.topic_coherence.text_analysis - INFO - 39 batches submitted to accumulate stats from 2496 documents (64029 virtual)
2024-05-03 02:55:10,223 - gensim.topic_coherence.text_analysis - INFO - 40 batches submitted to accumulate stats from 2560 documents (65725 virtual)
2024-05-03 02:55:10,223 - gensim.topic_coherence.text_analysis - INFO - 41 batches submitted to accumulate stats from 2624 documents (67346 virtual)
2024-05-03 02:55:10,223 - gensim.topic_coherence.text_analysis - INFO - 42 batches submitted to accumulate stats from 2688 documents (68863 virtual)
2024-05-03 02:55:10,239 - gensim.topic_coherence.text_analysis - INFO - 43 batches submitted to accumulate stats from 2752 documents (70539 virtual)
2024-05-03 02:55:10,239 - gensim.topic_coherence.text_analysis - INFO - 44 batches submitted to accumulate stats from 2816 documents (71922 virtual)
2024-05-03 02:55:10,239 - gensim.topic_coherence.text_analysis - INFO - 45 batches submitted to accumulate stats from 2880 documents (73294 virtual)
2024-05-03 02:55:10,255 - gensim.topic_coherence.text_analysis - INFO - 46 batches submitted to accumulate stats from 2944 documents (75084 virtual)
2024-05-03 02:55:10,255 - gensim.topic_coherence.text_analysis - INFO - 47 batches submitted to accumulate stats from 3008 documents (76769 virtual)
2024-05-03 02:55:10,255 - gensim.topic_coherence.text_analysis - INFO - 48 batches submitted to accumulate stats from 3072 documents (78312 virtual)
2024-05-03 02:55:10,255 - gensim.topic_coherence.text_analysis - INFO - 49 batches submitted to accumulate stats from 3136 documents (80039 virtual)
2024-05-03 02:55:10,255 - gensim.topic_coherence.text_analysis - INFO - 50 batches submitted to accumulate stats from 3200 documents (81572 virtual)
2024-05-03 02:55:10,270 - gensim.topic_coherence.text_analysis - INFO - 51 batches submitted to accumulate stats from 3264 documents (83189 virtual)
2024-05-03 02:55:10,270 - gensim.topic_coherence.text_analysis - INFO - 52 batches submitted to accumulate stats from 3328 documents (84783 virtual)
2024-05-03 02:55:10,270 - gensim.topic_coherence.text_analysis - INFO - 53 batches submitted to accumulate stats from 3392 documents (86570 virtual)
2024-05-03 02:55:10,270 - gensim.topic_coherence.text_analysis - INFO - 54 batches submitted to accumulate stats from 3456 documents (88371 virtual)
2024-05-03 02:55:10,286 - gensim.topic_coherence.text_analysis - INFO - 55 batches submitted to accumulate stats from 3520 documents (90295 virtual)
2024-05-03 02:55:10,286 - gensim.topic_coherence.text_analysis - INFO - 56 batches submitted to accumulate stats from 3584 documents (92118 virtual)
2024-05-03 02:55:10,286 - gensim.topic_coherence.text_analysis - INFO - 57 batches submitted to accumulate stats from 3648 documents (93914 virtual)
2024-05-03 02:55:10,301 - gensim.topic_coherence.text_analysis - INFO - 58 batches submitted to accumulate stats from 3712 documents (95729 virtual)
2024-05-03 02:55:10,301 - gensim.topic_coherence.text_analysis - INFO - 59 batches submitted to accumulate stats from 3776 documents (97293 virtual)
2024-05-03 02:55:10,301 - gensim.topic_coherence.text_analysis - INFO - 60 batches submitted to accumulate stats from 3840 documents (98908 virtual)
2024-05-03 02:55:10,301 - gensim.topic_coherence.text_analysis - INFO - 61 batches submitted to accumulate stats from 3904 documents (100586 virtual)
2024-05-03 02:55:10,301 - gensim.topic_coherence.text_analysis - INFO - 62 batches submitted to accumulate stats from 3968 documents (102208 virtual)
2024-05-03 02:55:10,301 - gensim.topic_coherence.text_analysis - INFO - 63 batches submitted to accumulate stats from 4032 documents (103862 virtual)
2024-05-03 02:55:10,301 - gensim.topic_coherence.text_analysis - INFO - 64 batches submitted to accumulate stats from 4096 documents (105500 virtual)
2024-05-03 02:55:10,317 - gensim.topic_coherence.text_analysis - INFO - 65 batches submitted to accumulate stats from 4160 documents (106974 virtual)
2024-05-03 02:55:10,317 - gensim.topic_coherence.text_analysis - INFO - 66 batches submitted to accumulate stats from 4224 documents (108587 virtual)
2024-05-03 02:55:10,317 - gensim.topic_coherence.text_analysis - INFO - 67 batches submitted to accumulate stats from 4288 documents (110059 virtual)
2024-05-03 02:55:10,333 - gensim.topic_coherence.text_analysis - INFO - 68 batches submitted to accumulate stats from 4352 documents (111905 virtual)
2024-05-03 02:55:10,333 - gensim.topic_coherence.text_analysis - INFO - 69 batches submitted to accumulate stats from 4416 documents (113549 virtual)
2024-05-03 02:55:10,333 - gensim.topic_coherence.text_analysis - INFO - 70 batches submitted to accumulate stats from 4480 documents (115163 virtual)
2024-05-03 02:55:10,333 - gensim.topic_coherence.text_analysis - INFO - 71 batches submitted to accumulate stats from 4544 documents (117083 virtual)
2024-05-03 02:55:10,333 - gensim.topic_coherence.text_analysis - INFO - 72 batches submitted to accumulate stats from 4608 documents (118654 virtual)
2024-05-03 02:55:10,348 - gensim.topic_coherence.text_analysis - INFO - 73 batches submitted to accumulate stats from 4672 documents (120224 virtual)
2024-05-03 02:55:10,348 - gensim.topic_coherence.text_analysis - INFO - 74 batches submitted to accumulate stats from 4736 documents (121786 virtual)
2024-05-03 02:55:10,583 - gensim.topic_coherence.text_analysis - INFO - 75 batches submitted to accumulate stats from 4800 documents (123233 virtual)
2024-05-03 02:55:10,583 - gensim.topic_coherence.text_analysis - INFO - 76 batches submitted to accumulate stats from 4864 documents (124761 virtual)
2024-05-03 02:55:10,583 - gensim.topic_coherence.text_analysis - INFO - 77 batches submitted to accumulate stats from 4928 documents (126221 virtual)
2024-05-03 02:55:10,583 - gensim.topic_coherence.text_analysis - INFO - 78 batches submitted to accumulate stats from 4992 documents (127857 virtual)
2024-05-03 02:55:10,598 - gensim.topic_coherence.text_analysis - INFO - 79 batches submitted to accumulate stats from 5056 documents (129432 virtual)
2024-05-03 02:55:10,598 - gensim.topic_coherence.text_analysis - INFO - 80 batches submitted to accumulate stats from 5120 documents (130948 virtual)
2024-05-03 02:55:10,598 - gensim.topic_coherence.text_analysis - INFO - 81 batches submitted to accumulate stats from 5184 documents (132913 virtual)
2024-05-03 02:55:10,598 - gensim.topic_coherence.text_analysis - INFO - 82 batches submitted to accumulate stats from 5248 documents (134700 virtual)
2024-05-03 02:55:10,598 - gensim.topic_coherence.text_analysis - INFO - 83 batches submitted to accumulate stats from 5312 documents (136417 virtual)
2024-05-03 02:55:10,598 - gensim.topic_coherence.text_analysis - INFO - 84 batches submitted to accumulate stats from 5376 documents (138141 virtual)
2024-05-03 02:55:10,598 - gensim.topic_coherence.text_analysis - INFO - 85 batches submitted to accumulate stats from 5440 documents (139767 virtual)
2024-05-03 02:55:10,614 - gensim.topic_coherence.text_analysis - INFO - 86 batches submitted to accumulate stats from 5504 documents (141151 virtual)
2024-05-03 02:55:10,614 - gensim.topic_coherence.text_analysis - INFO - 87 batches submitted to accumulate stats from 5568 documents (142647 virtual)
2024-05-03 02:55:10,614 - gensim.topic_coherence.text_analysis - INFO - 88 batches submitted to accumulate stats from 5632 documents (144175 virtual)
2024-05-03 02:55:10,614 - gensim.topic_coherence.text_analysis - INFO - 89 batches submitted to accumulate stats from 5696 documents (145825 virtual)
2024-05-03 02:55:10,630 - gensim.topic_coherence.text_analysis - INFO - 90 batches submitted to accumulate stats from 5760 documents (147317 virtual)
2024-05-03 02:55:10,630 - gensim.topic_coherence.text_analysis - INFO - 91 batches submitted to accumulate stats from 5824 documents (149158 virtual)
2024-05-03 02:55:10,645 - gensim.topic_coherence.text_analysis - INFO - 92 batches submitted to accumulate stats from 5888 documents (150755 virtual)
2024-05-03 02:55:10,645 - gensim.topic_coherence.text_analysis - INFO - 93 batches submitted to accumulate stats from 5952 documents (152237 virtual)
2024-05-03 02:55:10,645 - gensim.topic_coherence.text_analysis - INFO - 94 batches submitted to accumulate stats from 6016 documents (154013 virtual)
2024-05-03 02:55:10,661 - gensim.topic_coherence.text_analysis - INFO - 95 batches submitted to accumulate stats from 6080 documents (155593 virtual)
2024-05-03 02:55:10,661 - gensim.topic_coherence.text_analysis - INFO - 96 batches submitted to accumulate stats from 6144 documents (157114 virtual)
2024-05-03 02:55:10,661 - gensim.topic_coherence.text_analysis - INFO - 97 batches submitted to accumulate stats from 6208 documents (158783 virtual)
2024-05-03 02:55:10,676 - gensim.topic_coherence.text_analysis - INFO - 98 batches submitted to accumulate stats from 6272 documents (160467 virtual)
2024-05-03 02:55:10,676 - gensim.topic_coherence.text_analysis - INFO - 99 batches submitted to accumulate stats from 6336 documents (162287 virtual)
2024-05-03 02:55:10,676 - gensim.topic_coherence.text_analysis - INFO - 100 batches submitted to accumulate stats from 6400 documents (163932 virtual)
2024-05-03 02:55:10,676 - gensim.topic_coherence.text_analysis - INFO - 101 batches submitted to accumulate stats from 6464 documents (165414 virtual)
2024-05-03 02:55:10,692 - gensim.topic_coherence.text_analysis - INFO - 102 batches submitted to accumulate stats from 6528 documents (166934 virtual)
2024-05-03 02:55:10,692 - gensim.topic_coherence.text_analysis - INFO - 103 batches submitted to accumulate stats from 6592 documents (168399 virtual)
2024-05-03 02:55:10,692 - gensim.topic_coherence.text_analysis - INFO - 104 batches submitted to accumulate stats from 6656 documents (170596 virtual)
2024-05-03 02:55:10,708 - gensim.topic_coherence.text_analysis - INFO - 105 batches submitted to accumulate stats from 6720 documents (172319 virtual)
2024-05-03 02:55:10,708 - gensim.topic_coherence.text_analysis - INFO - 106 batches submitted to accumulate stats from 6784 documents (173973 virtual)
2024-05-03 02:55:10,708 - gensim.topic_coherence.text_analysis - INFO - 107 batches submitted to accumulate stats from 6848 documents (175817 virtual)
2024-05-03 02:55:10,708 - gensim.topic_coherence.text_analysis - INFO - 108 batches submitted to accumulate stats from 6912 documents (177402 virtual)
2024-05-03 02:55:10,723 - gensim.topic_coherence.text_analysis - INFO - 109 batches submitted to accumulate stats from 6976 documents (179106 virtual)
2024-05-03 02:55:10,723 - gensim.topic_coherence.text_analysis - INFO - 110 batches submitted to accumulate stats from 7040 documents (181089 virtual)
2024-05-03 02:55:10,723 - gensim.topic_coherence.text_analysis - INFO - 111 batches submitted to accumulate stats from 7104 documents (182660 virtual)
2024-05-03 02:55:10,723 - gensim.topic_coherence.text_analysis - INFO - 112 batches submitted to accumulate stats from 7168 documents (184289 virtual)
2024-05-03 02:55:10,739 - gensim.topic_coherence.text_analysis - INFO - 113 batches submitted to accumulate stats from 7232 documents (185825 virtual)
2024-05-03 02:55:10,739 - gensim.topic_coherence.text_analysis - INFO - 114 batches submitted to accumulate stats from 7296 documents (187420 virtual)
2024-05-03 02:55:10,739 - gensim.topic_coherence.text_analysis - INFO - 115 batches submitted to accumulate stats from 7360 documents (189102 virtual)
2024-05-03 02:55:10,755 - gensim.topic_coherence.text_analysis - INFO - 116 batches submitted to accumulate stats from 7424 documents (190745 virtual)
2024-05-03 02:55:10,755 - gensim.topic_coherence.text_analysis - INFO - 117 batches submitted to accumulate stats from 7488 documents (192238 virtual)
2024-05-03 02:55:10,755 - gensim.topic_coherence.text_analysis - INFO - 118 batches submitted to accumulate stats from 7552 documents (194107 virtual)
2024-05-03 02:55:10,770 - gensim.topic_coherence.text_analysis - INFO - 119 batches submitted to accumulate stats from 7616 documents (195570 virtual)
2024-05-03 02:55:10,770 - gensim.topic_coherence.text_analysis - INFO - 120 batches submitted to accumulate stats from 7680 documents (197064 virtual)
2024-05-03 02:55:10,770 - gensim.topic_coherence.text_analysis - INFO - 121 batches submitted to accumulate stats from 7744 documents (198821 virtual)
2024-05-03 02:55:10,770 - gensim.topic_coherence.text_analysis - INFO - 122 batches submitted to accumulate stats from 7808 documents (200394 virtual)
2024-05-03 02:55:10,770 - gensim.topic_coherence.text_analysis - INFO - 123 batches submitted to accumulate stats from 7872 documents (202352 virtual)
2024-05-03 02:55:10,786 - gensim.topic_coherence.text_analysis - INFO - 124 batches submitted to accumulate stats from 7936 documents (204181 virtual)
2024-05-03 02:55:10,786 - gensim.topic_coherence.text_analysis - INFO - 125 batches submitted to accumulate stats from 8000 documents (206063 virtual)
2024-05-03 02:55:10,786 - gensim.topic_coherence.text_analysis - INFO - 126 batches submitted to accumulate stats from 8064 documents (207766 virtual)
2024-05-03 02:55:10,786 - gensim.topic_coherence.text_analysis - INFO - 127 batches submitted to accumulate stats from 8128 documents (209460 virtual)
2024-05-03 02:55:10,801 - gensim.topic_coherence.text_analysis - INFO - 128 batches submitted to accumulate stats from 8192 documents (211022 virtual)
2024-05-03 02:55:10,801 - gensim.topic_coherence.text_analysis - INFO - 129 batches submitted to accumulate stats from 8256 documents (212632 virtual)
2024-05-03 02:55:10,801 - gensim.topic_coherence.text_analysis - INFO - 130 batches submitted to accumulate stats from 8320 documents (214210 virtual)
2024-05-03 02:55:10,817 - gensim.topic_coherence.text_analysis - INFO - 131 batches submitted to accumulate stats from 8384 documents (215651 virtual)
2024-05-03 02:55:10,817 - gensim.topic_coherence.text_analysis - INFO - 132 batches submitted to accumulate stats from 8448 documents (217300 virtual)
2024-05-03 02:55:10,817 - gensim.topic_coherence.text_analysis - INFO - 133 batches submitted to accumulate stats from 8512 documents (219035 virtual)
2024-05-03 02:55:10,817 - gensim.topic_coherence.text_analysis - INFO - 134 batches submitted to accumulate stats from 8576 documents (220675 virtual)
2024-05-03 02:55:10,833 - gensim.topic_coherence.text_analysis - INFO - 135 batches submitted to accumulate stats from 8640 documents (222562 virtual)
2024-05-03 02:55:10,833 - gensim.topic_coherence.text_analysis - INFO - 136 batches submitted to accumulate stats from 8704 documents (224243 virtual)
2024-05-03 02:55:10,833 - gensim.topic_coherence.text_analysis - INFO - 137 batches submitted to accumulate stats from 8768 documents (225942 virtual)
2024-05-03 02:55:10,848 - gensim.topic_coherence.text_analysis - INFO - 138 batches submitted to accumulate stats from 8832 documents (227774 virtual)
2024-05-03 02:55:10,848 - gensim.topic_coherence.text_analysis - INFO - 139 batches submitted to accumulate stats from 8896 documents (229378 virtual)
2024-05-03 02:55:10,848 - gensim.topic_coherence.text_analysis - INFO - 140 batches submitted to accumulate stats from 8960 documents (231026 virtual)
2024-05-03 02:55:10,848 - gensim.topic_coherence.text_analysis - INFO - 141 batches submitted to accumulate stats from 9024 documents (232664 virtual)
2024-05-03 02:55:10,848 - gensim.topic_coherence.text_analysis - INFO - 142 batches submitted to accumulate stats from 9088 documents (234376 virtual)
2024-05-03 02:55:10,864 - gensim.topic_coherence.text_analysis - INFO - 143 batches submitted to accumulate stats from 9152 documents (236034 virtual)
2024-05-03 02:55:10,864 - gensim.topic_coherence.text_analysis - INFO - 144 batches submitted to accumulate stats from 9216 documents (237753 virtual)
2024-05-03 02:55:10,864 - gensim.topic_coherence.text_analysis - INFO - 145 batches submitted to accumulate stats from 9280 documents (239260 virtual)
2024-05-03 02:55:10,864 - gensim.topic_coherence.text_analysis - INFO - 146 batches submitted to accumulate stats from 9344 documents (240889 virtual)
2024-05-03 02:55:10,880 - gensim.topic_coherence.text_analysis - INFO - 147 batches submitted to accumulate stats from 9408 documents (242607 virtual)
2024-05-03 02:55:10,880 - gensim.topic_coherence.text_analysis - INFO - 148 batches submitted to accumulate stats from 9472 documents (244168 virtual)
2024-05-03 02:55:10,895 - gensim.topic_coherence.text_analysis - INFO - 149 batches submitted to accumulate stats from 9536 documents (245827 virtual)
2024-05-03 02:55:10,896 - gensim.topic_coherence.text_analysis - INFO - 150 batches submitted to accumulate stats from 9600 documents (247340 virtual)
2024-05-03 02:55:10,896 - gensim.topic_coherence.text_analysis - INFO - 151 batches submitted to accumulate stats from 9664 documents (248935 virtual)
2024-05-03 02:55:10,896 - gensim.topic_coherence.text_analysis - INFO - 152 batches submitted to accumulate stats from 9728 documents (250682 virtual)
2024-05-03 02:55:10,896 - gensim.topic_coherence.text_analysis - INFO - 153 batches submitted to accumulate stats from 9792 documents (252525 virtual)
2024-05-03 02:55:10,911 - gensim.topic_coherence.text_analysis - INFO - 154 batches submitted to accumulate stats from 9856 documents (253964 virtual)
2024-05-03 02:55:10,911 - gensim.topic_coherence.text_analysis - INFO - 155 batches submitted to accumulate stats from 9920 documents (255761 virtual)
2024-05-03 02:55:10,911 - gensim.topic_coherence.text_analysis - INFO - 156 batches submitted to accumulate stats from 9984 documents (257656 virtual)
2024-05-03 02:55:10,927 - gensim.topic_coherence.text_analysis - INFO - 157 batches submitted to accumulate stats from 10048 documents (259320 virtual)
2024-05-03 02:55:10,927 - gensim.topic_coherence.text_analysis - INFO - 158 batches submitted to accumulate stats from 10112 documents (261193 virtual)
2024-05-03 02:55:10,927 - gensim.topic_coherence.text_analysis - INFO - 159 batches submitted to accumulate stats from 10176 documents (262723 virtual)
2024-05-03 02:55:10,927 - gensim.topic_coherence.text_analysis - INFO - 160 batches submitted to accumulate stats from 10240 documents (264223 virtual)
2024-05-03 02:55:10,927 - gensim.topic_coherence.text_analysis - INFO - 161 batches submitted to accumulate stats from 10304 documents (265694 virtual)
2024-05-03 02:55:10,927 - gensim.topic_coherence.text_analysis - INFO - 162 batches submitted to accumulate stats from 10368 documents (267448 virtual)
2024-05-03 02:55:10,942 - gensim.topic_coherence.text_analysis - INFO - 163 batches submitted to accumulate stats from 10432 documents (269251 virtual)
2024-05-03 02:55:10,942 - gensim.topic_coherence.text_analysis - INFO - 164 batches submitted to accumulate stats from 10496 documents (270973 virtual)
2024-05-03 02:55:10,958 - gensim.topic_coherence.text_analysis - INFO - 165 batches submitted to accumulate stats from 10560 documents (272683 virtual)
2024-05-03 02:55:10,958 - gensim.topic_coherence.text_analysis - INFO - 166 batches submitted to accumulate stats from 10624 documents (274294 virtual)
2024-05-03 02:55:10,958 - gensim.topic_coherence.text_analysis - INFO - 167 batches submitted to accumulate stats from 10688 documents (276045 virtual)
2024-05-03 02:55:10,958 - gensim.topic_coherence.text_analysis - INFO - 168 batches submitted to accumulate stats from 10752 documents (277496 virtual)
2024-05-03 02:55:10,974 - gensim.topic_coherence.text_analysis - INFO - 169 batches submitted to accumulate stats from 10816 documents (279131 virtual)
2024-05-03 02:55:10,974 - gensim.topic_coherence.text_analysis - INFO - 170 batches submitted to accumulate stats from 10880 documents (280812 virtual)
2024-05-03 02:55:10,974 - gensim.topic_coherence.text_analysis - INFO - 171 batches submitted to accumulate stats from 10944 documents (282408 virtual)
2024-05-03 02:55:10,974 - gensim.topic_coherence.text_analysis - INFO - 172 batches submitted to accumulate stats from 11008 documents (284125 virtual)
2024-05-03 02:55:10,974 - gensim.topic_coherence.text_analysis - INFO - 173 batches submitted to accumulate stats from 11072 documents (285575 virtual)
2024-05-03 02:55:10,989 - gensim.topic_coherence.text_analysis - INFO - 174 batches submitted to accumulate stats from 11136 documents (287185 virtual)
2024-05-03 02:55:10,989 - gensim.topic_coherence.text_analysis - INFO - 175 batches submitted to accumulate stats from 11200 documents (289145 virtual)
2024-05-03 02:55:10,989 - gensim.topic_coherence.text_analysis - INFO - 176 batches submitted to accumulate stats from 11264 documents (290900 virtual)
2024-05-03 02:55:10,989 - gensim.topic_coherence.text_analysis - INFO - 177 batches submitted to accumulate stats from 11328 documents (292686 virtual)
2024-05-03 02:55:10,989 - gensim.topic_coherence.text_analysis - INFO - 178 batches submitted to accumulate stats from 11392 documents (294505 virtual)
2024-05-03 02:55:11,005 - gensim.topic_coherence.text_analysis - INFO - 179 batches submitted to accumulate stats from 11456 documents (296236 virtual)
2024-05-03 02:55:11,005 - gensim.topic_coherence.text_analysis - INFO - 180 batches submitted to accumulate stats from 11520 documents (297647 virtual)
2024-05-03 02:55:11,021 - gensim.topic_coherence.text_analysis - INFO - 181 batches submitted to accumulate stats from 11584 documents (299327 virtual)
2024-05-03 02:55:11,021 - gensim.topic_coherence.text_analysis - INFO - 182 batches submitted to accumulate stats from 11648 documents (300853 virtual)
2024-05-03 02:55:11,021 - gensim.topic_coherence.text_analysis - INFO - 183 batches submitted to accumulate stats from 11712 documents (302601 virtual)
2024-05-03 02:55:11,021 - gensim.topic_coherence.text_analysis - INFO - 184 batches submitted to accumulate stats from 11776 documents (304181 virtual)
2024-05-03 02:55:11,021 - gensim.topic_coherence.text_analysis - INFO - 185 batches submitted to accumulate stats from 11840 documents (305710 virtual)
2024-05-03 02:55:11,021 - gensim.topic_coherence.text_analysis - INFO - 186 batches submitted to accumulate stats from 11904 documents (307265 virtual)
2024-05-03 02:55:11,036 - gensim.topic_coherence.text_analysis - INFO - 187 batches submitted to accumulate stats from 11968 documents (309148 virtual)
2024-05-03 02:55:11,036 - gensim.topic_coherence.text_analysis - INFO - 188 batches submitted to accumulate stats from 12032 documents (310818 virtual)
2024-05-03 02:55:11,036 - gensim.topic_coherence.text_analysis - INFO - 189 batches submitted to accumulate stats from 12096 documents (312472 virtual)
2024-05-03 02:55:11,052 - gensim.topic_coherence.text_analysis - INFO - 190 batches submitted to accumulate stats from 12160 documents (314119 virtual)
2024-05-03 02:55:11,052 - gensim.topic_coherence.text_analysis - INFO - 191 batches submitted to accumulate stats from 12224 documents (316054 virtual)
2024-05-03 02:55:11,052 - gensim.topic_coherence.text_analysis - INFO - 192 batches submitted to accumulate stats from 12288 documents (317685 virtual)
2024-05-03 02:55:11,052 - gensim.topic_coherence.text_analysis - INFO - 193 batches submitted to accumulate stats from 12352 documents (319351 virtual)
2024-05-03 02:55:11,052 - gensim.topic_coherence.text_analysis - INFO - 194 batches submitted to accumulate stats from 12416 documents (321100 virtual)
2024-05-03 02:55:11,067 - gensim.topic_coherence.text_analysis - INFO - 195 batches submitted to accumulate stats from 12480 documents (323099 virtual)
2024-05-03 02:55:11,067 - gensim.topic_coherence.text_analysis - INFO - 196 batches submitted to accumulate stats from 12544 documents (324548 virtual)
2024-05-03 02:55:11,067 - gensim.topic_coherence.text_analysis - INFO - 197 batches submitted to accumulate stats from 12608 documents (326231 virtual)
2024-05-03 02:55:11,067 - gensim.topic_coherence.text_analysis - INFO - 198 batches submitted to accumulate stats from 12672 documents (327851 virtual)
2024-05-03 02:55:11,083 - gensim.topic_coherence.text_analysis - INFO - 199 batches submitted to accumulate stats from 12736 documents (329386 virtual)
2024-05-03 02:55:11,083 - gensim.topic_coherence.text_analysis - INFO - 200 batches submitted to accumulate stats from 12800 documents (331113 virtual)
2024-05-03 02:55:11,083 - gensim.topic_coherence.text_analysis - INFO - 201 batches submitted to accumulate stats from 12864 documents (332712 virtual)
2024-05-03 02:55:11,083 - gensim.topic_coherence.text_analysis - INFO - 202 batches submitted to accumulate stats from 12928 documents (334135 virtual)
2024-05-03 02:55:11,099 - gensim.topic_coherence.text_analysis - INFO - 203 batches submitted to accumulate stats from 12992 documents (335914 virtual)
2024-05-03 02:55:11,099 - gensim.topic_coherence.text_analysis - INFO - 204 batches submitted to accumulate stats from 13056 documents (337641 virtual)
2024-05-03 02:55:11,114 - gensim.topic_coherence.text_analysis - INFO - 205 batches submitted to accumulate stats from 13120 documents (339449 virtual)
2024-05-03 02:55:11,114 - gensim.topic_coherence.text_analysis - INFO - 206 batches submitted to accumulate stats from 13184 documents (341168 virtual)
2024-05-03 02:55:11,114 - gensim.topic_coherence.text_analysis - INFO - 207 batches submitted to accumulate stats from 13248 documents (342833 virtual)
2024-05-03 02:55:11,130 - gensim.topic_coherence.text_analysis - INFO - 208 batches submitted to accumulate stats from 13312 documents (344704 virtual)
2024-05-03 02:55:11,130 - gensim.topic_coherence.text_analysis - INFO - 209 batches submitted to accumulate stats from 13376 documents (346650 virtual)
2024-05-03 02:55:11,130 - gensim.topic_coherence.text_analysis - INFO - 210 batches submitted to accumulate stats from 13440 documents (348531 virtual)
2024-05-03 02:55:11,130 - gensim.topic_coherence.text_analysis - INFO - 211 batches submitted to accumulate stats from 13504 documents (350342 virtual)
2024-05-03 02:55:11,146 - gensim.topic_coherence.text_analysis - INFO - 212 batches submitted to accumulate stats from 13568 documents (352068 virtual)
2024-05-03 02:55:11,146 - gensim.topic_coherence.text_analysis - INFO - 213 batches submitted to accumulate stats from 13632 documents (353789 virtual)
2024-05-03 02:55:11,146 - gensim.topic_coherence.text_analysis - INFO - 214 batches submitted to accumulate stats from 13696 documents (355216 virtual)
2024-05-03 02:55:11,146 - gensim.topic_coherence.text_analysis - INFO - 215 batches submitted to accumulate stats from 13760 documents (356990 virtual)
2024-05-03 02:55:11,161 - gensim.topic_coherence.text_analysis - INFO - 216 batches submitted to accumulate stats from 13824 documents (358762 virtual)
2024-05-03 02:55:11,173 - gensim.topic_coherence.text_analysis - INFO - 217 batches submitted to accumulate stats from 13888 documents (360320 virtual)
2024-05-03 02:55:11,184 - gensim.topic_coherence.text_analysis - INFO - 218 batches submitted to accumulate stats from 13952 documents (361867 virtual)
2024-05-03 02:55:11,188 - gensim.topic_coherence.text_analysis - INFO - 219 batches submitted to accumulate stats from 14016 documents (363519 virtual)
2024-05-03 02:55:11,192 - gensim.topic_coherence.text_analysis - INFO - 220 batches submitted to accumulate stats from 14080 documents (365141 virtual)
2024-05-03 02:55:11,195 - gensim.topic_coherence.text_analysis - INFO - 221 batches submitted to accumulate stats from 14144 documents (366934 virtual)
2024-05-03 02:55:11,197 - gensim.topic_coherence.text_analysis - INFO - 222 batches submitted to accumulate stats from 14208 documents (368448 virtual)
2024-05-03 02:55:11,197 - gensim.topic_coherence.text_analysis - INFO - 223 batches submitted to accumulate stats from 14272 documents (370012 virtual)
2024-05-03 02:55:11,197 - gensim.topic_coherence.text_analysis - INFO - 224 batches submitted to accumulate stats from 14336 documents (371704 virtual)
2024-05-03 02:55:11,197 - gensim.topic_coherence.text_analysis - INFO - 225 batches submitted to accumulate stats from 14400 documents (373331 virtual)
2024-05-03 02:55:11,208 - gensim.topic_coherence.text_analysis - INFO - 226 batches submitted to accumulate stats from 14464 documents (375174 virtual)
2024-05-03 02:55:11,208 - gensim.topic_coherence.text_analysis - INFO - 227 batches submitted to accumulate stats from 14528 documents (377135 virtual)
2024-05-03 02:55:11,208 - gensim.topic_coherence.text_analysis - INFO - 228 batches submitted to accumulate stats from 14592 documents (378860 virtual)
2024-05-03 02:55:11,224 - gensim.topic_coherence.text_analysis - INFO - 229 batches submitted to accumulate stats from 14656 documents (380581 virtual)
2024-05-03 02:55:11,224 - gensim.topic_coherence.text_analysis - INFO - 230 batches submitted to accumulate stats from 14720 documents (382278 virtual)
2024-05-03 02:55:11,224 - gensim.topic_coherence.text_analysis - INFO - 231 batches submitted to accumulate stats from 14784 documents (383970 virtual)
2024-05-03 02:55:11,239 - gensim.topic_coherence.text_analysis - INFO - 232 batches submitted to accumulate stats from 14848 documents (385680 virtual)
2024-05-03 02:55:11,239 - gensim.topic_coherence.text_analysis - INFO - 233 batches submitted to accumulate stats from 14912 documents (387378 virtual)
2024-05-03 02:55:11,239 - gensim.topic_coherence.text_analysis - INFO - 234 batches submitted to accumulate stats from 14976 documents (388908 virtual)
2024-05-03 02:55:11,239 - gensim.topic_coherence.text_analysis - INFO - 235 batches submitted to accumulate stats from 15040 documents (390473 virtual)
2024-05-03 02:55:11,239 - gensim.topic_coherence.text_analysis - INFO - 236 batches submitted to accumulate stats from 15104 documents (391966 virtual)
2024-05-03 02:55:11,255 - gensim.topic_coherence.text_analysis - INFO - 237 batches submitted to accumulate stats from 15168 documents (393446 virtual)
2024-05-03 02:55:11,255 - gensim.topic_coherence.text_analysis - INFO - 238 batches submitted to accumulate stats from 15232 documents (394979 virtual)
2024-05-03 02:55:11,255 - gensim.topic_coherence.text_analysis - INFO - 239 batches submitted to accumulate stats from 15296 documents (396707 virtual)
2024-05-03 02:55:11,271 - gensim.topic_coherence.text_analysis - INFO - 240 batches submitted to accumulate stats from 15360 documents (398434 virtual)
2024-05-03 02:55:11,271 - gensim.topic_coherence.text_analysis - INFO - 241 batches submitted to accumulate stats from 15424 documents (400092 virtual)
2024-05-03 02:55:11,286 - gensim.topic_coherence.text_analysis - INFO - 242 batches submitted to accumulate stats from 15488 documents (402054 virtual)
2024-05-03 02:55:11,286 - gensim.topic_coherence.text_analysis - INFO - 243 batches submitted to accumulate stats from 15552 documents (403465 virtual)
2024-05-03 02:55:11,286 - gensim.topic_coherence.text_analysis - INFO - 244 batches submitted to accumulate stats from 15616 documents (405289 virtual)
2024-05-03 02:55:11,286 - gensim.topic_coherence.text_analysis - INFO - 245 batches submitted to accumulate stats from 15680 documents (406750 virtual)
2024-05-03 02:55:11,302 - gensim.topic_coherence.text_analysis - INFO - 246 batches submitted to accumulate stats from 15744 documents (408306 virtual)
2024-05-03 02:55:11,302 - gensim.topic_coherence.text_analysis - INFO - 247 batches submitted to accumulate stats from 15808 documents (410097 virtual)
2024-05-03 02:55:11,302 - gensim.topic_coherence.text_analysis - INFO - 248 batches submitted to accumulate stats from 15872 documents (411915 virtual)
2024-05-03 02:55:11,318 - gensim.topic_coherence.text_analysis - INFO - 249 batches submitted to accumulate stats from 15936 documents (413538 virtual)
2024-05-03 02:55:11,318 - gensim.topic_coherence.text_analysis - INFO - 250 batches submitted to accumulate stats from 16000 documents (415307 virtual)
2024-05-03 02:55:11,333 - gensim.topic_coherence.text_analysis - INFO - 251 batches submitted to accumulate stats from 16064 documents (416993 virtual)
2024-05-03 02:55:11,333 - gensim.topic_coherence.text_analysis - INFO - 252 batches submitted to accumulate stats from 16128 documents (418582 virtual)
2024-05-03 02:55:11,333 - gensim.topic_coherence.text_analysis - INFO - 253 batches submitted to accumulate stats from 16192 documents (420282 virtual)
2024-05-03 02:55:11,333 - gensim.topic_coherence.text_analysis - INFO - 254 batches submitted to accumulate stats from 16256 documents (421859 virtual)
2024-05-03 02:55:11,349 - gensim.topic_coherence.text_analysis - INFO - 255 batches submitted to accumulate stats from 16320 documents (423441 virtual)
2024-05-03 02:55:11,349 - gensim.topic_coherence.text_analysis - INFO - 256 batches submitted to accumulate stats from 16384 documents (425169 virtual)
2024-05-03 02:55:11,349 - gensim.topic_coherence.text_analysis - INFO - 257 batches submitted to accumulate stats from 16448 documents (426804 virtual)
2024-05-03 02:55:11,349 - gensim.topic_coherence.text_analysis - INFO - 258 batches submitted to accumulate stats from 16512 documents (428503 virtual)
2024-05-03 02:55:11,364 - gensim.topic_coherence.text_analysis - INFO - 259 batches submitted to accumulate stats from 16576 documents (430063 virtual)
2024-05-03 02:55:11,364 - gensim.topic_coherence.text_analysis - INFO - 260 batches submitted to accumulate stats from 16640 documents (431672 virtual)
2024-05-03 02:55:11,364 - gensim.topic_coherence.text_analysis - INFO - 261 batches submitted to accumulate stats from 16704 documents (433298 virtual)
2024-05-03 02:55:11,380 - gensim.topic_coherence.text_analysis - INFO - 262 batches submitted to accumulate stats from 16768 documents (434951 virtual)
2024-05-03 02:55:11,380 - gensim.topic_coherence.text_analysis - INFO - 263 batches submitted to accumulate stats from 16832 documents (436505 virtual)
2024-05-03 02:55:11,396 - gensim.topic_coherence.text_analysis - INFO - 264 batches submitted to accumulate stats from 16896 documents (438134 virtual)
2024-05-03 02:55:11,396 - gensim.topic_coherence.text_analysis - INFO - 265 batches submitted to accumulate stats from 16960 documents (439831 virtual)
2024-05-03 02:55:11,396 - gensim.topic_coherence.text_analysis - INFO - 266 batches submitted to accumulate stats from 17024 documents (441666 virtual)
2024-05-03 02:55:11,411 - gensim.topic_coherence.text_analysis - INFO - 267 batches submitted to accumulate stats from 17088 documents (443389 virtual)
2024-05-03 02:55:11,411 - gensim.topic_coherence.text_analysis - INFO - 268 batches submitted to accumulate stats from 17152 documents (445008 virtual)
2024-05-03 02:55:11,411 - gensim.topic_coherence.text_analysis - INFO - 269 batches submitted to accumulate stats from 17216 documents (446691 virtual)
2024-05-03 02:55:11,427 - gensim.topic_coherence.text_analysis - INFO - 270 batches submitted to accumulate stats from 17280 documents (448258 virtual)
2024-05-03 02:55:11,427 - gensim.topic_coherence.text_analysis - INFO - 271 batches submitted to accumulate stats from 17344 documents (450177 virtual)
2024-05-03 02:55:11,427 - gensim.topic_coherence.text_analysis - INFO - 272 batches submitted to accumulate stats from 17408 documents (451838 virtual)
2024-05-03 02:55:11,427 - gensim.topic_coherence.text_analysis - INFO - 273 batches submitted to accumulate stats from 17472 documents (453765 virtual)
2024-05-03 02:55:11,443 - gensim.topic_coherence.text_analysis - INFO - 274 batches submitted to accumulate stats from 17536 documents (455302 virtual)
2024-05-03 02:55:11,443 - gensim.topic_coherence.text_analysis - INFO - 275 batches submitted to accumulate stats from 17600 documents (457026 virtual)
2024-05-03 02:55:11,443 - gensim.topic_coherence.text_analysis - INFO - 276 batches submitted to accumulate stats from 17664 documents (458678 virtual)
2024-05-03 02:55:11,458 - gensim.topic_coherence.text_analysis - INFO - 277 batches submitted to accumulate stats from 17728 documents (460363 virtual)
2024-05-03 02:55:11,474 - gensim.topic_coherence.text_analysis - INFO - 278 batches submitted to accumulate stats from 17792 documents (462290 virtual)
2024-05-03 02:55:11,474 - gensim.topic_coherence.text_analysis - INFO - 279 batches submitted to accumulate stats from 17856 documents (464043 virtual)
2024-05-03 02:55:11,474 - gensim.topic_coherence.text_analysis - INFO - 280 batches submitted to accumulate stats from 17920 documents (465601 virtual)
2024-05-03 02:55:11,490 - gensim.topic_coherence.text_analysis - INFO - 281 batches submitted to accumulate stats from 17984 documents (467208 virtual)
2024-05-03 02:55:11,490 - gensim.topic_coherence.text_analysis - INFO - 282 batches submitted to accumulate stats from 18048 documents (468985 virtual)
2024-05-03 02:55:11,490 - gensim.topic_coherence.text_analysis - INFO - 283 batches submitted to accumulate stats from 18112 documents (470540 virtual)
2024-05-03 02:55:11,490 - gensim.topic_coherence.text_analysis - INFO - 284 batches submitted to accumulate stats from 18176 documents (472187 virtual)
2024-05-03 02:55:11,505 - gensim.topic_coherence.text_analysis - INFO - 285 batches submitted to accumulate stats from 18240 documents (473657 virtual)
2024-05-03 02:55:11,505 - gensim.topic_coherence.text_analysis - INFO - 286 batches submitted to accumulate stats from 18304 documents (475433 virtual)
2024-05-03 02:55:11,505 - gensim.topic_coherence.text_analysis - INFO - 287 batches submitted to accumulate stats from 18368 documents (476949 virtual)
2024-05-03 02:55:11,505 - gensim.topic_coherence.text_analysis - INFO - 288 batches submitted to accumulate stats from 18432 documents (478519 virtual)
2024-05-03 02:55:11,521 - gensim.topic_coherence.text_analysis - INFO - 289 batches submitted to accumulate stats from 18496 documents (480346 virtual)
2024-05-03 02:55:11,537 - gensim.topic_coherence.text_analysis - INFO - 290 batches submitted to accumulate stats from 18560 documents (482113 virtual)
2024-05-03 02:55:11,537 - gensim.topic_coherence.text_analysis - INFO - 291 batches submitted to accumulate stats from 18624 documents (483726 virtual)
2024-05-03 02:55:11,537 - gensim.topic_coherence.text_analysis - INFO - 292 batches submitted to accumulate stats from 18688 documents (485454 virtual)
2024-05-03 02:55:11,537 - gensim.topic_coherence.text_analysis - INFO - 293 batches submitted to accumulate stats from 18752 documents (487120 virtual)
2024-05-03 02:55:11,537 - gensim.topic_coherence.text_analysis - INFO - 294 batches submitted to accumulate stats from 18816 documents (488702 virtual)
2024-05-03 02:55:11,552 - gensim.topic_coherence.text_analysis - INFO - 295 batches submitted to accumulate stats from 18880 documents (490511 virtual)
2024-05-03 02:55:11,552 - gensim.topic_coherence.text_analysis - INFO - 296 batches submitted to accumulate stats from 18944 documents (492226 virtual)
2024-05-03 02:55:11,552 - gensim.topic_coherence.text_analysis - INFO - 297 batches submitted to accumulate stats from 19008 documents (494165 virtual)
2024-05-03 02:55:11,568 - gensim.topic_coherence.text_analysis - INFO - 298 batches submitted to accumulate stats from 19072 documents (495678 virtual)
2024-05-03 02:55:11,568 - gensim.topic_coherence.text_analysis - INFO - 299 batches submitted to accumulate stats from 19136 documents (497503 virtual)
2024-05-03 02:55:11,568 - gensim.topic_coherence.text_analysis - INFO - 300 batches submitted to accumulate stats from 19200 documents (499131 virtual)
2024-05-03 02:55:11,584 - gensim.topic_coherence.text_analysis - INFO - 301 batches submitted to accumulate stats from 19264 documents (500986 virtual)
2024-05-03 02:55:11,584 - gensim.topic_coherence.text_analysis - INFO - 302 batches submitted to accumulate stats from 19328 documents (502687 virtual)
2024-05-03 02:55:11,584 - gensim.topic_coherence.text_analysis - INFO - 303 batches submitted to accumulate stats from 19392 documents (504106 virtual)
2024-05-03 02:55:11,584 - gensim.topic_coherence.text_analysis - INFO - 304 batches submitted to accumulate stats from 19456 documents (505909 virtual)
2024-05-03 02:55:11,599 - gensim.topic_coherence.text_analysis - INFO - 305 batches submitted to accumulate stats from 19520 documents (507546 virtual)
2024-05-03 02:55:11,599 - gensim.topic_coherence.text_analysis - INFO - 306 batches submitted to accumulate stats from 19584 documents (509312 virtual)
2024-05-03 02:55:11,599 - gensim.topic_coherence.text_analysis - INFO - 307 batches submitted to accumulate stats from 19648 documents (510954 virtual)
2024-05-03 02:55:11,615 - gensim.topic_coherence.text_analysis - INFO - 308 batches submitted to accumulate stats from 19712 documents (512465 virtual)
2024-05-03 02:55:11,630 - gensim.topic_coherence.text_analysis - INFO - 309 batches submitted to accumulate stats from 19776 documents (514132 virtual)
2024-05-03 02:55:11,630 - gensim.topic_coherence.text_analysis - INFO - 310 batches submitted to accumulate stats from 19840 documents (515846 virtual)
2024-05-03 02:55:11,630 - gensim.topic_coherence.text_analysis - INFO - 311 batches submitted to accumulate stats from 19904 documents (517436 virtual)
2024-05-03 02:55:11,630 - gensim.topic_coherence.text_analysis - INFO - 312 batches submitted to accumulate stats from 19968 documents (519076 virtual)
2024-05-03 02:55:11,646 - gensim.topic_coherence.text_analysis - INFO - 313 batches submitted to accumulate stats from 20032 documents (520797 virtual)
2024-05-03 02:55:11,646 - gensim.topic_coherence.text_analysis - INFO - 314 batches submitted to accumulate stats from 20096 documents (522328 virtual)
2024-05-03 02:55:12,162 - gensim.topic_coherence.text_analysis - INFO - 11 accumulators retrieved from output queue
2024-05-03 02:55:12,334 - gensim.topic_coherence.text_analysis - INFO - accumulated word occurrence stats for 523696 virtual documents
2024-05-03 02:55:13,646 - gensim.corpora.dictionary - INFO - adding document #0 to Dictionary<0 unique tokens: []>
2024-05-03 02:55:14,021 - gensim.corpora.dictionary - INFO - adding document #10000 to Dictionary<23643 unique tokens: ['-', '.', '?', 'a', 'africa']...>
2024-05-03 02:55:14,413 - gensim.corpora.dictionary - INFO - adding document #20000 to Dictionary<32428 unique tokens: ['-', '.', '?', 'a', 'africa']...>
2024-05-03 02:55:14,413 - gensim.corpora.dictionary - INFO - built Dictionary<32503 unique tokens: ['-', '.', '?', 'a', 'africa']...> from 20086 documents (total 703102 corpus positions)
2024-05-03 02:55:14,427 - gensim.utils - INFO - Dictionary lifecycle event {'msg': "built Dictionary<32503 unique tokens: ['-', '.', '?', 'a', 'africa']...> from 20086 documents (total 703102 corpus positions)", 'datetime': '2024-05-03T02:55:14.427714', 'gensim': '4.3.2', 'python': '3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}
2024-05-03 02:55:14,427 - gensim.topic_coherence.probability_estimation - INFO - using ParallelWordOccurrenceAccumulator<processes=11, batch_size=64> to estimate probabilities from sliding windows
2024-05-03 02:56:10,366 - gensim.topic_coherence.text_analysis - INFO - 11 accumulators retrieved from output queue
2024-05-03 02:56:10,600 - gensim.topic_coherence.text_analysis - INFO - accumulated word occurrence stats for 21620 virtual documents
2024-05-03 02:56:15,289 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(84503, 300)
  (classifier): Linear(in_features=300, out_features=26, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1000, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1000, out_features=50, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=50, bias=False)
  (W_q): Linear(in_features=300, out_features=50, bias=False)
  (W_v): Linear(in_features=300, out_features=50, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((50, 100), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 27,138,605
Freeze params: 0
2024-05-03 03:00:08,251 - trainer - INFO -     epoch          : 1
2024-05-03 03:00:08,267 - trainer - INFO -     loss           : 1.143839
2024-05-03 03:00:08,267 - trainer - INFO -     accuracy       : 0.670978
2024-05-03 03:00:08,267 - trainer - INFO -     macro_f        : 0.653084
2024-05-03 03:00:08,267 - trainer - INFO -     precision      : 0.684929
2024-05-03 03:00:08,267 - trainer - INFO -     recall         : 0.670978
2024-05-03 03:00:08,267 - trainer - INFO -     doc_entropy    : 2.483231
2024-05-03 03:00:08,267 - trainer - INFO -     val_loss       : 1.00008
2024-05-03 03:00:08,267 - trainer - INFO -     val_accuracy   : 0.707209
2024-05-03 03:00:08,267 - trainer - INFO -     val_macro_f    : 0.690327
2024-05-03 03:00:08,267 - trainer - INFO -     val_precision  : 0.720786
2024-05-03 03:00:08,267 - trainer - INFO -     val_recall     : 0.707209
2024-05-03 03:00:08,267 - trainer - INFO -     val_doc_entropy: 2.671735
2024-05-03 03:00:08,267 - trainer - INFO -     test_loss      : 0.996858
2024-05-03 03:00:08,267 - trainer - INFO -     test_accuracy  : 0.705915
2024-05-03 03:00:08,267 - trainer - INFO -     test_macro_f   : 0.68861
2024-05-03 03:00:08,267 - trainer - INFO -     test_precision : 0.718707
2024-05-03 03:00:08,267 - trainer - INFO -     test_recall    : 0.705915
2024-05-03 03:00:08,267 - trainer - INFO -     test_doc_entropy: 2.673909
2024-05-03 03:04:01,758 - trainer - INFO -     epoch          : 2
2024-05-03 03:04:01,758 - trainer - INFO -     loss           : 0.797444
2024-05-03 03:04:01,758 - trainer - INFO -     accuracy       : 0.760699
2024-05-03 03:04:01,758 - trainer - INFO -     macro_f        : 0.751716
2024-05-03 03:04:01,758 - trainer - INFO -     precision      : 0.784059
2024-05-03 03:04:01,758 - trainer - INFO -     recall         : 0.760699
2024-05-03 03:04:01,758 - trainer - INFO -     doc_entropy    : 2.065815
2024-05-03 03:04:01,758 - trainer - INFO -     val_loss       : 1.010306
2024-05-03 03:04:01,758 - trainer - INFO -     val_accuracy   : 0.70686
2024-05-03 03:04:01,758 - trainer - INFO -     val_macro_f    : 0.700076
2024-05-03 03:04:01,758 - trainer - INFO -     val_precision  : 0.740073
2024-05-03 03:04:01,758 - trainer - INFO -     val_recall     : 0.70686
2024-05-03 03:04:01,758 - trainer - INFO -     val_doc_entropy: 2.460786
2024-05-03 03:04:01,758 - trainer - INFO -     test_loss      : 1.009629
2024-05-03 03:04:01,758 - trainer - INFO -     test_accuracy  : 0.708802
2024-05-03 03:04:01,758 - trainer - INFO -     test_macro_f   : 0.700987
2024-05-03 03:04:01,758 - trainer - INFO -     test_precision : 0.738379
2024-05-03 03:04:01,758 - trainer - INFO -     test_recall    : 0.708802
2024-05-03 03:04:01,758 - trainer - INFO -     test_doc_entropy: 2.463231
2024-05-03 03:04:42,232 - gensim.corpora.dictionary - INFO - adding document #0 to Dictionary<0 unique tokens: []>
2024-05-03 03:04:42,623 - gensim.corpora.dictionary - INFO - adding document #10000 to Dictionary<23643 unique tokens: ['-', '.', '?', 'a', 'africa']...>
2024-05-03 03:04:43,030 - gensim.corpora.dictionary - INFO - adding document #20000 to Dictionary<32428 unique tokens: ['-', '.', '?', 'a', 'africa']...>
2024-05-03 03:04:43,046 - gensim.corpora.dictionary - INFO - built Dictionary<32503 unique tokens: ['-', '.', '?', 'a', 'africa']...> from 20086 documents (total 703102 corpus positions)
2024-05-03 03:04:43,046 - gensim.utils - INFO - Dictionary lifecycle event {'msg': "built Dictionary<32503 unique tokens: ['-', '.', '?', 'a', 'africa']...> from 20086 documents (total 703102 corpus positions)", 'datetime': '2024-05-03T03:04:43.046110', 'gensim': '4.3.2', 'python': '3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}
2024-05-03 03:04:43,062 - gensim.topic_coherence.probability_estimation - INFO - using ParallelWordOccurrenceAccumulator<processes=11, batch_size=64> to estimate probabilities from sliding windows
2024-05-03 03:05:48,643 - gensim.topic_coherence.text_analysis - INFO - 1 batches submitted to accumulate stats from 64 documents (1358 virtual)
2024-05-03 03:05:48,643 - gensim.topic_coherence.text_analysis - INFO - 2 batches submitted to accumulate stats from 128 documents (3142 virtual)
2024-05-03 03:05:48,643 - gensim.topic_coherence.text_analysis - INFO - 3 batches submitted to accumulate stats from 192 documents (4707 virtual)
2024-05-03 03:05:48,643 - gensim.topic_coherence.text_analysis - INFO - 4 batches submitted to accumulate stats from 256 documents (6346 virtual)
2024-05-03 03:05:48,643 - gensim.topic_coherence.text_analysis - INFO - 5 batches submitted to accumulate stats from 320 documents (7961 virtual)
2024-05-03 03:05:48,658 - gensim.topic_coherence.text_analysis - INFO - 6 batches submitted to accumulate stats from 384 documents (9298 virtual)
2024-05-03 03:05:48,658 - gensim.topic_coherence.text_analysis - INFO - 7 batches submitted to accumulate stats from 448 documents (11371 virtual)
2024-05-03 03:05:48,658 - gensim.topic_coherence.text_analysis - INFO - 8 batches submitted to accumulate stats from 512 documents (13011 virtual)
2024-05-03 03:05:48,658 - gensim.topic_coherence.text_analysis - INFO - 9 batches submitted to accumulate stats from 576 documents (14534 virtual)
2024-05-03 03:05:48,658 - gensim.topic_coherence.text_analysis - INFO - 10 batches submitted to accumulate stats from 640 documents (16161 virtual)
2024-05-03 03:05:48,658 - gensim.topic_coherence.text_analysis - INFO - 11 batches submitted to accumulate stats from 704 documents (17689 virtual)
2024-05-03 03:05:48,658 - gensim.topic_coherence.text_analysis - INFO - 12 batches submitted to accumulate stats from 768 documents (19256 virtual)
2024-05-03 03:05:48,674 - gensim.topic_coherence.text_analysis - INFO - 13 batches submitted to accumulate stats from 832 documents (21175 virtual)
2024-05-03 03:05:48,674 - gensim.topic_coherence.text_analysis - INFO - 14 batches submitted to accumulate stats from 896 documents (22850 virtual)
2024-05-03 03:05:48,674 - gensim.topic_coherence.text_analysis - INFO - 15 batches submitted to accumulate stats from 960 documents (24681 virtual)
2024-05-03 03:05:48,689 - gensim.topic_coherence.text_analysis - INFO - 16 batches submitted to accumulate stats from 1024 documents (26222 virtual)
2024-05-03 03:05:48,689 - gensim.topic_coherence.text_analysis - INFO - 17 batches submitted to accumulate stats from 1088 documents (27953 virtual)
2024-05-03 03:05:48,689 - gensim.topic_coherence.text_analysis - INFO - 18 batches submitted to accumulate stats from 1152 documents (29594 virtual)
2024-05-03 03:05:48,689 - gensim.topic_coherence.text_analysis - INFO - 19 batches submitted to accumulate stats from 1216 documents (31239 virtual)
2024-05-03 03:05:48,705 - gensim.topic_coherence.text_analysis - INFO - 20 batches submitted to accumulate stats from 1280 documents (32901 virtual)
2024-05-03 03:05:48,705 - gensim.topic_coherence.text_analysis - INFO - 21 batches submitted to accumulate stats from 1344 documents (34481 virtual)
2024-05-03 03:05:48,705 - gensim.topic_coherence.text_analysis - INFO - 22 batches submitted to accumulate stats from 1408 documents (36151 virtual)
2024-05-03 03:05:48,721 - gensim.topic_coherence.text_analysis - INFO - 23 batches submitted to accumulate stats from 1472 documents (37837 virtual)
2024-05-03 03:05:48,721 - gensim.topic_coherence.text_analysis - INFO - 24 batches submitted to accumulate stats from 1536 documents (39453 virtual)
2024-05-03 03:05:48,721 - gensim.topic_coherence.text_analysis - INFO - 25 batches submitted to accumulate stats from 1600 documents (41046 virtual)
2024-05-03 03:05:48,721 - gensim.topic_coherence.text_analysis - INFO - 26 batches submitted to accumulate stats from 1664 documents (42892 virtual)
2024-05-03 03:05:48,721 - gensim.topic_coherence.text_analysis - INFO - 27 batches submitted to accumulate stats from 1728 documents (44445 virtual)
2024-05-03 03:05:48,736 - gensim.topic_coherence.text_analysis - INFO - 28 batches submitted to accumulate stats from 1792 documents (46073 virtual)
2024-05-03 03:05:48,736 - gensim.topic_coherence.text_analysis - INFO - 29 batches submitted to accumulate stats from 1856 documents (47643 virtual)
2024-05-03 03:05:48,736 - gensim.topic_coherence.text_analysis - INFO - 30 batches submitted to accumulate stats from 1920 documents (49252 virtual)
2024-05-03 03:05:48,752 - gensim.topic_coherence.text_analysis - INFO - 31 batches submitted to accumulate stats from 1984 documents (50774 virtual)
2024-05-03 03:05:48,752 - gensim.topic_coherence.text_analysis - INFO - 32 batches submitted to accumulate stats from 2048 documents (52387 virtual)
2024-05-03 03:05:48,752 - gensim.topic_coherence.text_analysis - INFO - 33 batches submitted to accumulate stats from 2112 documents (53815 virtual)
2024-05-03 03:05:48,752 - gensim.topic_coherence.text_analysis - INFO - 34 batches submitted to accumulate stats from 2176 documents (55540 virtual)
2024-05-03 03:05:48,752 - gensim.topic_coherence.text_analysis - INFO - 35 batches submitted to accumulate stats from 2240 documents (57348 virtual)
2024-05-03 03:05:48,768 - gensim.topic_coherence.text_analysis - INFO - 36 batches submitted to accumulate stats from 2304 documents (59258 virtual)
2024-05-03 03:05:48,768 - gensim.topic_coherence.text_analysis - INFO - 37 batches submitted to accumulate stats from 2368 documents (60957 virtual)
2024-05-03 03:05:48,768 - gensim.topic_coherence.text_analysis - INFO - 38 batches submitted to accumulate stats from 2432 documents (62425 virtual)
2024-05-03 03:05:48,768 - gensim.topic_coherence.text_analysis - INFO - 39 batches submitted to accumulate stats from 2496 documents (64029 virtual)
2024-05-03 03:05:48,783 - gensim.topic_coherence.text_analysis - INFO - 40 batches submitted to accumulate stats from 2560 documents (65725 virtual)
2024-05-03 03:05:48,783 - gensim.topic_coherence.text_analysis - INFO - 41 batches submitted to accumulate stats from 2624 documents (67346 virtual)
2024-05-03 03:05:48,783 - gensim.topic_coherence.text_analysis - INFO - 42 batches submitted to accumulate stats from 2688 documents (68863 virtual)
2024-05-03 03:05:48,783 - gensim.topic_coherence.text_analysis - INFO - 43 batches submitted to accumulate stats from 2752 documents (70539 virtual)
2024-05-03 03:05:48,799 - gensim.topic_coherence.text_analysis - INFO - 44 batches submitted to accumulate stats from 2816 documents (71922 virtual)
2024-05-03 03:05:48,799 - gensim.topic_coherence.text_analysis - INFO - 45 batches submitted to accumulate stats from 2880 documents (73294 virtual)
2024-05-03 03:05:48,799 - gensim.topic_coherence.text_analysis - INFO - 46 batches submitted to accumulate stats from 2944 documents (75084 virtual)
2024-05-03 03:05:48,814 - gensim.topic_coherence.text_analysis - INFO - 47 batches submitted to accumulate stats from 3008 documents (76769 virtual)
2024-05-03 03:05:48,814 - gensim.topic_coherence.text_analysis - INFO - 48 batches submitted to accumulate stats from 3072 documents (78312 virtual)
2024-05-03 03:05:48,814 - gensim.topic_coherence.text_analysis - INFO - 49 batches submitted to accumulate stats from 3136 documents (80039 virtual)
2024-05-03 03:05:48,814 - gensim.topic_coherence.text_analysis - INFO - 50 batches submitted to accumulate stats from 3200 documents (81572 virtual)
2024-05-03 03:05:48,814 - gensim.topic_coherence.text_analysis - INFO - 51 batches submitted to accumulate stats from 3264 documents (83189 virtual)
2024-05-03 03:05:48,830 - gensim.topic_coherence.text_analysis - INFO - 52 batches submitted to accumulate stats from 3328 documents (84783 virtual)
2024-05-03 03:05:48,830 - gensim.topic_coherence.text_analysis - INFO - 53 batches submitted to accumulate stats from 3392 documents (86570 virtual)
2024-05-03 03:05:48,830 - gensim.topic_coherence.text_analysis - INFO - 54 batches submitted to accumulate stats from 3456 documents (88371 virtual)
2024-05-03 03:05:48,846 - gensim.topic_coherence.text_analysis - INFO - 55 batches submitted to accumulate stats from 3520 documents (90295 virtual)
2024-05-03 03:05:48,846 - gensim.topic_coherence.text_analysis - INFO - 56 batches submitted to accumulate stats from 3584 documents (92118 virtual)
2024-05-03 03:05:48,846 - gensim.topic_coherence.text_analysis - INFO - 57 batches submitted to accumulate stats from 3648 documents (93914 virtual)
2024-05-03 03:05:48,846 - gensim.topic_coherence.text_analysis - INFO - 58 batches submitted to accumulate stats from 3712 documents (95729 virtual)
2024-05-03 03:05:48,846 - gensim.topic_coherence.text_analysis - INFO - 59 batches submitted to accumulate stats from 3776 documents (97293 virtual)
2024-05-03 03:05:48,861 - gensim.topic_coherence.text_analysis - INFO - 60 batches submitted to accumulate stats from 3840 documents (98908 virtual)
2024-05-03 03:05:48,861 - gensim.topic_coherence.text_analysis - INFO - 61 batches submitted to accumulate stats from 3904 documents (100586 virtual)
2024-05-03 03:05:48,861 - gensim.topic_coherence.text_analysis - INFO - 62 batches submitted to accumulate stats from 3968 documents (102208 virtual)
2024-05-03 03:05:48,861 - gensim.topic_coherence.text_analysis - INFO - 63 batches submitted to accumulate stats from 4032 documents (103862 virtual)
2024-05-03 03:05:48,861 - gensim.topic_coherence.text_analysis - INFO - 64 batches submitted to accumulate stats from 4096 documents (105500 virtual)
2024-05-03 03:05:48,861 - gensim.topic_coherence.text_analysis - INFO - 65 batches submitted to accumulate stats from 4160 documents (106974 virtual)
2024-05-03 03:05:48,861 - gensim.topic_coherence.text_analysis - INFO - 66 batches submitted to accumulate stats from 4224 documents (108587 virtual)
2024-05-03 03:05:48,877 - gensim.topic_coherence.text_analysis - INFO - 67 batches submitted to accumulate stats from 4288 documents (110059 virtual)
2024-05-03 03:05:48,877 - gensim.topic_coherence.text_analysis - INFO - 68 batches submitted to accumulate stats from 4352 documents (111905 virtual)
2024-05-03 03:05:48,877 - gensim.topic_coherence.text_analysis - INFO - 69 batches submitted to accumulate stats from 4416 documents (113549 virtual)
2024-05-03 03:05:48,877 - gensim.topic_coherence.text_analysis - INFO - 70 batches submitted to accumulate stats from 4480 documents (115163 virtual)
2024-05-03 03:05:48,877 - gensim.topic_coherence.text_analysis - INFO - 71 batches submitted to accumulate stats from 4544 documents (117083 virtual)
2024-05-03 03:05:48,893 - gensim.topic_coherence.text_analysis - INFO - 72 batches submitted to accumulate stats from 4608 documents (118654 virtual)
2024-05-03 03:05:48,893 - gensim.topic_coherence.text_analysis - INFO - 73 batches submitted to accumulate stats from 4672 documents (120224 virtual)
2024-05-03 03:05:48,893 - gensim.topic_coherence.text_analysis - INFO - 74 batches submitted to accumulate stats from 4736 documents (121786 virtual)
2024-05-03 03:05:48,908 - gensim.topic_coherence.text_analysis - INFO - 75 batches submitted to accumulate stats from 4800 documents (123233 virtual)
2024-05-03 03:05:48,908 - gensim.topic_coherence.text_analysis - INFO - 76 batches submitted to accumulate stats from 4864 documents (124761 virtual)
2024-05-03 03:05:48,908 - gensim.topic_coherence.text_analysis - INFO - 77 batches submitted to accumulate stats from 4928 documents (126221 virtual)
2024-05-03 03:05:48,908 - gensim.topic_coherence.text_analysis - INFO - 78 batches submitted to accumulate stats from 4992 documents (127857 virtual)
2024-05-03 03:05:48,908 - gensim.topic_coherence.text_analysis - INFO - 79 batches submitted to accumulate stats from 5056 documents (129432 virtual)
2024-05-03 03:05:48,908 - gensim.topic_coherence.text_analysis - INFO - 80 batches submitted to accumulate stats from 5120 documents (130948 virtual)
2024-05-03 03:05:48,924 - gensim.topic_coherence.text_analysis - INFO - 81 batches submitted to accumulate stats from 5184 documents (132913 virtual)
2024-05-03 03:05:48,924 - gensim.topic_coherence.text_analysis - INFO - 82 batches submitted to accumulate stats from 5248 documents (134700 virtual)
2024-05-03 03:05:48,924 - gensim.topic_coherence.text_analysis - INFO - 83 batches submitted to accumulate stats from 5312 documents (136417 virtual)
2024-05-03 03:05:48,924 - gensim.topic_coherence.text_analysis - INFO - 84 batches submitted to accumulate stats from 5376 documents (138141 virtual)
2024-05-03 03:05:48,939 - gensim.topic_coherence.text_analysis - INFO - 85 batches submitted to accumulate stats from 5440 documents (139767 virtual)
2024-05-03 03:05:48,939 - gensim.topic_coherence.text_analysis - INFO - 86 batches submitted to accumulate stats from 5504 documents (141151 virtual)
2024-05-03 03:05:48,939 - gensim.topic_coherence.text_analysis - INFO - 87 batches submitted to accumulate stats from 5568 documents (142647 virtual)
2024-05-03 03:05:48,939 - gensim.topic_coherence.text_analysis - INFO - 88 batches submitted to accumulate stats from 5632 documents (144175 virtual)
2024-05-03 03:05:48,939 - gensim.topic_coherence.text_analysis - INFO - 89 batches submitted to accumulate stats from 5696 documents (145825 virtual)
2024-05-03 03:05:48,955 - gensim.topic_coherence.text_analysis - INFO - 90 batches submitted to accumulate stats from 5760 documents (147317 virtual)
2024-05-03 03:05:48,955 - gensim.topic_coherence.text_analysis - INFO - 91 batches submitted to accumulate stats from 5824 documents (149158 virtual)
2024-05-03 03:05:48,955 - gensim.topic_coherence.text_analysis - INFO - 92 batches submitted to accumulate stats from 5888 documents (150755 virtual)
2024-05-03 03:05:48,955 - gensim.topic_coherence.text_analysis - INFO - 93 batches submitted to accumulate stats from 5952 documents (152237 virtual)
2024-05-03 03:05:48,955 - gensim.topic_coherence.text_analysis - INFO - 94 batches submitted to accumulate stats from 6016 documents (154013 virtual)
2024-05-03 03:05:48,971 - gensim.topic_coherence.text_analysis - INFO - 95 batches submitted to accumulate stats from 6080 documents (155593 virtual)
2024-05-03 03:05:48,971 - gensim.topic_coherence.text_analysis - INFO - 96 batches submitted to accumulate stats from 6144 documents (157114 virtual)
2024-05-03 03:05:48,971 - gensim.topic_coherence.text_analysis - INFO - 97 batches submitted to accumulate stats from 6208 documents (158783 virtual)
2024-05-03 03:05:48,971 - gensim.topic_coherence.text_analysis - INFO - 98 batches submitted to accumulate stats from 6272 documents (160467 virtual)
2024-05-03 03:05:48,971 - gensim.topic_coherence.text_analysis - INFO - 99 batches submitted to accumulate stats from 6336 documents (162287 virtual)
2024-05-03 03:05:48,971 - gensim.topic_coherence.text_analysis - INFO - 100 batches submitted to accumulate stats from 6400 documents (163932 virtual)
2024-05-03 03:05:48,986 - gensim.topic_coherence.text_analysis - INFO - 101 batches submitted to accumulate stats from 6464 documents (165414 virtual)
2024-05-03 03:05:48,986 - gensim.topic_coherence.text_analysis - INFO - 102 batches submitted to accumulate stats from 6528 documents (166934 virtual)
2024-05-03 03:05:48,986 - gensim.topic_coherence.text_analysis - INFO - 103 batches submitted to accumulate stats from 6592 documents (168399 virtual)
2024-05-03 03:05:48,986 - gensim.topic_coherence.text_analysis - INFO - 104 batches submitted to accumulate stats from 6656 documents (170596 virtual)
2024-05-03 03:05:48,986 - gensim.topic_coherence.text_analysis - INFO - 105 batches submitted to accumulate stats from 6720 documents (172319 virtual)
2024-05-03 03:05:49,002 - gensim.topic_coherence.text_analysis - INFO - 106 batches submitted to accumulate stats from 6784 documents (173973 virtual)
2024-05-03 03:05:49,002 - gensim.topic_coherence.text_analysis - INFO - 107 batches submitted to accumulate stats from 6848 documents (175817 virtual)
2024-05-03 03:05:49,002 - gensim.topic_coherence.text_analysis - INFO - 108 batches submitted to accumulate stats from 6912 documents (177402 virtual)
2024-05-03 03:05:49,002 - gensim.topic_coherence.text_analysis - INFO - 109 batches submitted to accumulate stats from 6976 documents (179106 virtual)
2024-05-03 03:05:49,002 - gensim.topic_coherence.text_analysis - INFO - 110 batches submitted to accumulate stats from 7040 documents (181089 virtual)
2024-05-03 03:05:49,018 - gensim.topic_coherence.text_analysis - INFO - 111 batches submitted to accumulate stats from 7104 documents (182660 virtual)
2024-05-03 03:05:49,018 - gensim.topic_coherence.text_analysis - INFO - 112 batches submitted to accumulate stats from 7168 documents (184289 virtual)
2024-05-03 03:05:49,018 - gensim.topic_coherence.text_analysis - INFO - 113 batches submitted to accumulate stats from 7232 documents (185825 virtual)
2024-05-03 03:05:49,018 - gensim.topic_coherence.text_analysis - INFO - 114 batches submitted to accumulate stats from 7296 documents (187420 virtual)
2024-05-03 03:05:49,033 - gensim.topic_coherence.text_analysis - INFO - 115 batches submitted to accumulate stats from 7360 documents (189102 virtual)
2024-05-03 03:05:49,033 - gensim.topic_coherence.text_analysis - INFO - 116 batches submitted to accumulate stats from 7424 documents (190745 virtual)
2024-05-03 03:05:49,033 - gensim.topic_coherence.text_analysis - INFO - 117 batches submitted to accumulate stats from 7488 documents (192238 virtual)
2024-05-03 03:05:49,033 - gensim.topic_coherence.text_analysis - INFO - 118 batches submitted to accumulate stats from 7552 documents (194107 virtual)
2024-05-03 03:05:49,033 - gensim.topic_coherence.text_analysis - INFO - 119 batches submitted to accumulate stats from 7616 documents (195570 virtual)
2024-05-03 03:05:49,049 - gensim.topic_coherence.text_analysis - INFO - 120 batches submitted to accumulate stats from 7680 documents (197064 virtual)
2024-05-03 03:05:49,049 - gensim.topic_coherence.text_analysis - INFO - 121 batches submitted to accumulate stats from 7744 documents (198821 virtual)
2024-05-03 03:05:49,049 - gensim.topic_coherence.text_analysis - INFO - 122 batches submitted to accumulate stats from 7808 documents (200394 virtual)
2024-05-03 03:05:49,064 - gensim.topic_coherence.text_analysis - INFO - 123 batches submitted to accumulate stats from 7872 documents (202352 virtual)
2024-05-03 03:05:49,064 - gensim.topic_coherence.text_analysis - INFO - 124 batches submitted to accumulate stats from 7936 documents (204181 virtual)
2024-05-03 03:05:49,064 - gensim.topic_coherence.text_analysis - INFO - 125 batches submitted to accumulate stats from 8000 documents (206063 virtual)
2024-05-03 03:05:49,080 - gensim.topic_coherence.text_analysis - INFO - 126 batches submitted to accumulate stats from 8064 documents (207766 virtual)
2024-05-03 03:05:49,080 - gensim.topic_coherence.text_analysis - INFO - 127 batches submitted to accumulate stats from 8128 documents (209460 virtual)
2024-05-03 03:05:49,080 - gensim.topic_coherence.text_analysis - INFO - 128 batches submitted to accumulate stats from 8192 documents (211022 virtual)
2024-05-03 03:05:49,080 - gensim.topic_coherence.text_analysis - INFO - 129 batches submitted to accumulate stats from 8256 documents (212632 virtual)
2024-05-03 03:05:49,080 - gensim.topic_coherence.text_analysis - INFO - 130 batches submitted to accumulate stats from 8320 documents (214210 virtual)
2024-05-03 03:05:49,096 - gensim.topic_coherence.text_analysis - INFO - 131 batches submitted to accumulate stats from 8384 documents (215651 virtual)
2024-05-03 03:05:49,096 - gensim.topic_coherence.text_analysis - INFO - 132 batches submitted to accumulate stats from 8448 documents (217300 virtual)
2024-05-03 03:05:49,096 - gensim.topic_coherence.text_analysis - INFO - 133 batches submitted to accumulate stats from 8512 documents (219035 virtual)
2024-05-03 03:05:49,096 - gensim.topic_coherence.text_analysis - INFO - 134 batches submitted to accumulate stats from 8576 documents (220675 virtual)
2024-05-03 03:05:49,111 - gensim.topic_coherence.text_analysis - INFO - 135 batches submitted to accumulate stats from 8640 documents (222562 virtual)
2024-05-03 03:05:49,111 - gensim.topic_coherence.text_analysis - INFO - 136 batches submitted to accumulate stats from 8704 documents (224243 virtual)
2024-05-03 03:05:49,111 - gensim.topic_coherence.text_analysis - INFO - 137 batches submitted to accumulate stats from 8768 documents (225942 virtual)
2024-05-03 03:05:49,127 - gensim.topic_coherence.text_analysis - INFO - 138 batches submitted to accumulate stats from 8832 documents (227774 virtual)
2024-05-03 03:05:49,127 - gensim.topic_coherence.text_analysis - INFO - 139 batches submitted to accumulate stats from 8896 documents (229378 virtual)
2024-05-03 03:05:49,127 - gensim.topic_coherence.text_analysis - INFO - 140 batches submitted to accumulate stats from 8960 documents (231026 virtual)
2024-05-03 03:05:49,127 - gensim.topic_coherence.text_analysis - INFO - 141 batches submitted to accumulate stats from 9024 documents (232664 virtual)
2024-05-03 03:05:49,143 - gensim.topic_coherence.text_analysis - INFO - 142 batches submitted to accumulate stats from 9088 documents (234376 virtual)
2024-05-03 03:05:49,143 - gensim.topic_coherence.text_analysis - INFO - 143 batches submitted to accumulate stats from 9152 documents (236034 virtual)
2024-05-03 03:05:49,174 - gensim.topic_coherence.text_analysis - INFO - 144 batches submitted to accumulate stats from 9216 documents (237753 virtual)
2024-05-03 03:05:49,174 - gensim.topic_coherence.text_analysis - INFO - 145 batches submitted to accumulate stats from 9280 documents (239260 virtual)
2024-05-03 03:05:49,174 - gensim.topic_coherence.text_analysis - INFO - 146 batches submitted to accumulate stats from 9344 documents (240889 virtual)
2024-05-03 03:05:49,174 - gensim.topic_coherence.text_analysis - INFO - 147 batches submitted to accumulate stats from 9408 documents (242607 virtual)
2024-05-03 03:05:49,174 - gensim.topic_coherence.text_analysis - INFO - 148 batches submitted to accumulate stats from 9472 documents (244168 virtual)
2024-05-03 03:05:49,189 - gensim.topic_coherence.text_analysis - INFO - 149 batches submitted to accumulate stats from 9536 documents (245827 virtual)
2024-05-03 03:05:49,189 - gensim.topic_coherence.text_analysis - INFO - 150 batches submitted to accumulate stats from 9600 documents (247340 virtual)
2024-05-03 03:05:49,189 - gensim.topic_coherence.text_analysis - INFO - 151 batches submitted to accumulate stats from 9664 documents (248935 virtual)
2024-05-03 03:05:49,205 - gensim.topic_coherence.text_analysis - INFO - 152 batches submitted to accumulate stats from 9728 documents (250682 virtual)
2024-05-03 03:05:49,205 - gensim.topic_coherence.text_analysis - INFO - 153 batches submitted to accumulate stats from 9792 documents (252525 virtual)
2024-05-03 03:05:49,205 - gensim.topic_coherence.text_analysis - INFO - 154 batches submitted to accumulate stats from 9856 documents (253964 virtual)
2024-05-03 03:05:49,221 - gensim.topic_coherence.text_analysis - INFO - 155 batches submitted to accumulate stats from 9920 documents (255761 virtual)
2024-05-03 03:05:49,221 - gensim.topic_coherence.text_analysis - INFO - 156 batches submitted to accumulate stats from 9984 documents (257656 virtual)
2024-05-03 03:05:49,221 - gensim.topic_coherence.text_analysis - INFO - 157 batches submitted to accumulate stats from 10048 documents (259320 virtual)
2024-05-03 03:05:49,221 - gensim.topic_coherence.text_analysis - INFO - 158 batches submitted to accumulate stats from 10112 documents (261193 virtual)
2024-05-03 03:05:49,236 - gensim.topic_coherence.text_analysis - INFO - 159 batches submitted to accumulate stats from 10176 documents (262723 virtual)
2024-05-03 03:05:49,236 - gensim.topic_coherence.text_analysis - INFO - 160 batches submitted to accumulate stats from 10240 documents (264223 virtual)
2024-05-03 03:05:49,236 - gensim.topic_coherence.text_analysis - INFO - 161 batches submitted to accumulate stats from 10304 documents (265694 virtual)
2024-05-03 03:05:49,236 - gensim.topic_coherence.text_analysis - INFO - 162 batches submitted to accumulate stats from 10368 documents (267448 virtual)
2024-05-03 03:05:49,252 - gensim.topic_coherence.text_analysis - INFO - 163 batches submitted to accumulate stats from 10432 documents (269251 virtual)
2024-05-03 03:05:49,252 - gensim.topic_coherence.text_analysis - INFO - 164 batches submitted to accumulate stats from 10496 documents (270973 virtual)
2024-05-03 03:05:49,252 - gensim.topic_coherence.text_analysis - INFO - 165 batches submitted to accumulate stats from 10560 documents (272683 virtual)
2024-05-03 03:05:49,252 - gensim.topic_coherence.text_analysis - INFO - 166 batches submitted to accumulate stats from 10624 documents (274294 virtual)
2024-05-03 03:05:49,268 - gensim.topic_coherence.text_analysis - INFO - 167 batches submitted to accumulate stats from 10688 documents (276045 virtual)
2024-05-03 03:05:49,268 - gensim.topic_coherence.text_analysis - INFO - 168 batches submitted to accumulate stats from 10752 documents (277496 virtual)
2024-05-03 03:05:49,268 - gensim.topic_coherence.text_analysis - INFO - 169 batches submitted to accumulate stats from 10816 documents (279131 virtual)
2024-05-03 03:05:49,268 - gensim.topic_coherence.text_analysis - INFO - 170 batches submitted to accumulate stats from 10880 documents (280812 virtual)
2024-05-03 03:05:49,299 - gensim.topic_coherence.text_analysis - INFO - 171 batches submitted to accumulate stats from 10944 documents (282408 virtual)
2024-05-03 03:05:49,393 - gensim.topic_coherence.text_analysis - INFO - 172 batches submitted to accumulate stats from 11008 documents (284125 virtual)
2024-05-03 03:05:49,393 - gensim.topic_coherence.text_analysis - INFO - 173 batches submitted to accumulate stats from 11072 documents (285575 virtual)
2024-05-03 03:05:49,393 - gensim.topic_coherence.text_analysis - INFO - 174 batches submitted to accumulate stats from 11136 documents (287185 virtual)
2024-05-03 03:05:49,393 - gensim.topic_coherence.text_analysis - INFO - 175 batches submitted to accumulate stats from 11200 documents (289145 virtual)
2024-05-03 03:05:49,393 - gensim.topic_coherence.text_analysis - INFO - 176 batches submitted to accumulate stats from 11264 documents (290900 virtual)
2024-05-03 03:05:49,393 - gensim.topic_coherence.text_analysis - INFO - 177 batches submitted to accumulate stats from 11328 documents (292686 virtual)
2024-05-03 03:05:49,393 - gensim.topic_coherence.text_analysis - INFO - 178 batches submitted to accumulate stats from 11392 documents (294505 virtual)
2024-05-03 03:05:49,393 - gensim.topic_coherence.text_analysis - INFO - 179 batches submitted to accumulate stats from 11456 documents (296236 virtual)
2024-05-03 03:05:49,393 - gensim.topic_coherence.text_analysis - INFO - 180 batches submitted to accumulate stats from 11520 documents (297647 virtual)
2024-05-03 03:05:49,408 - gensim.topic_coherence.text_analysis - INFO - 181 batches submitted to accumulate stats from 11584 documents (299327 virtual)
2024-05-03 03:05:49,408 - gensim.topic_coherence.text_analysis - INFO - 182 batches submitted to accumulate stats from 11648 documents (300853 virtual)
2024-05-03 03:05:49,408 - gensim.topic_coherence.text_analysis - INFO - 183 batches submitted to accumulate stats from 11712 documents (302601 virtual)
2024-05-03 03:05:49,408 - gensim.topic_coherence.text_analysis - INFO - 184 batches submitted to accumulate stats from 11776 documents (304181 virtual)
2024-05-03 03:05:49,408 - gensim.topic_coherence.text_analysis - INFO - 185 batches submitted to accumulate stats from 11840 documents (305710 virtual)
2024-05-03 03:05:49,408 - gensim.topic_coherence.text_analysis - INFO - 186 batches submitted to accumulate stats from 11904 documents (307265 virtual)
2024-05-03 03:05:49,408 - gensim.topic_coherence.text_analysis - INFO - 187 batches submitted to accumulate stats from 11968 documents (309148 virtual)
2024-05-03 03:05:49,424 - gensim.topic_coherence.text_analysis - INFO - 188 batches submitted to accumulate stats from 12032 documents (310818 virtual)
2024-05-03 03:05:49,424 - gensim.topic_coherence.text_analysis - INFO - 189 batches submitted to accumulate stats from 12096 documents (312472 virtual)
2024-05-03 03:05:49,424 - gensim.topic_coherence.text_analysis - INFO - 190 batches submitted to accumulate stats from 12160 documents (314119 virtual)
2024-05-03 03:05:49,440 - gensim.topic_coherence.text_analysis - INFO - 191 batches submitted to accumulate stats from 12224 documents (316054 virtual)
2024-05-03 03:05:49,440 - gensim.topic_coherence.text_analysis - INFO - 192 batches submitted to accumulate stats from 12288 documents (317685 virtual)
2024-05-03 03:05:49,440 - gensim.topic_coherence.text_analysis - INFO - 193 batches submitted to accumulate stats from 12352 documents (319351 virtual)
2024-05-03 03:05:49,455 - gensim.topic_coherence.text_analysis - INFO - 194 batches submitted to accumulate stats from 12416 documents (321100 virtual)
2024-05-03 03:05:49,455 - gensim.topic_coherence.text_analysis - INFO - 195 batches submitted to accumulate stats from 12480 documents (323099 virtual)
2024-05-03 03:05:49,455 - gensim.topic_coherence.text_analysis - INFO - 196 batches submitted to accumulate stats from 12544 documents (324548 virtual)
2024-05-03 03:05:49,471 - gensim.topic_coherence.text_analysis - INFO - 197 batches submitted to accumulate stats from 12608 documents (326231 virtual)
2024-05-03 03:05:49,471 - gensim.topic_coherence.text_analysis - INFO - 198 batches submitted to accumulate stats from 12672 documents (327851 virtual)
2024-05-03 03:05:49,471 - gensim.topic_coherence.text_analysis - INFO - 199 batches submitted to accumulate stats from 12736 documents (329386 virtual)
2024-05-03 03:05:49,471 - gensim.topic_coherence.text_analysis - INFO - 200 batches submitted to accumulate stats from 12800 documents (331113 virtual)
2024-05-03 03:05:49,471 - gensim.topic_coherence.text_analysis - INFO - 201 batches submitted to accumulate stats from 12864 documents (332712 virtual)
2024-05-03 03:05:49,487 - gensim.topic_coherence.text_analysis - INFO - 202 batches submitted to accumulate stats from 12928 documents (334135 virtual)
2024-05-03 03:05:49,487 - gensim.topic_coherence.text_analysis - INFO - 203 batches submitted to accumulate stats from 12992 documents (335914 virtual)
2024-05-03 03:05:49,487 - gensim.topic_coherence.text_analysis - INFO - 204 batches submitted to accumulate stats from 13056 documents (337641 virtual)
2024-05-03 03:05:49,487 - gensim.topic_coherence.text_analysis - INFO - 205 batches submitted to accumulate stats from 13120 documents (339449 virtual)
2024-05-03 03:05:49,502 - gensim.topic_coherence.text_analysis - INFO - 206 batches submitted to accumulate stats from 13184 documents (341168 virtual)
2024-05-03 03:05:49,502 - gensim.topic_coherence.text_analysis - INFO - 207 batches submitted to accumulate stats from 13248 documents (342833 virtual)
2024-05-03 03:05:49,502 - gensim.topic_coherence.text_analysis - INFO - 208 batches submitted to accumulate stats from 13312 documents (344704 virtual)
2024-05-03 03:05:49,502 - gensim.topic_coherence.text_analysis - INFO - 209 batches submitted to accumulate stats from 13376 documents (346650 virtual)
2024-05-03 03:05:49,502 - gensim.topic_coherence.text_analysis - INFO - 210 batches submitted to accumulate stats from 13440 documents (348531 virtual)
2024-05-03 03:05:49,518 - gensim.topic_coherence.text_analysis - INFO - 211 batches submitted to accumulate stats from 13504 documents (350342 virtual)
2024-05-03 03:05:49,518 - gensim.topic_coherence.text_analysis - INFO - 212 batches submitted to accumulate stats from 13568 documents (352068 virtual)
2024-05-03 03:05:49,518 - gensim.topic_coherence.text_analysis - INFO - 213 batches submitted to accumulate stats from 13632 documents (353789 virtual)
2024-05-03 03:05:49,518 - gensim.topic_coherence.text_analysis - INFO - 214 batches submitted to accumulate stats from 13696 documents (355216 virtual)
2024-05-03 03:05:49,533 - gensim.topic_coherence.text_analysis - INFO - 215 batches submitted to accumulate stats from 13760 documents (356990 virtual)
2024-05-03 03:05:49,533 - gensim.topic_coherence.text_analysis - INFO - 216 batches submitted to accumulate stats from 13824 documents (358762 virtual)
2024-05-03 03:05:49,533 - gensim.topic_coherence.text_analysis - INFO - 217 batches submitted to accumulate stats from 13888 documents (360320 virtual)
2024-05-03 03:05:49,533 - gensim.topic_coherence.text_analysis - INFO - 218 batches submitted to accumulate stats from 13952 documents (361867 virtual)
2024-05-03 03:05:49,533 - gensim.topic_coherence.text_analysis - INFO - 219 batches submitted to accumulate stats from 14016 documents (363519 virtual)
2024-05-03 03:05:49,533 - gensim.topic_coherence.text_analysis - INFO - 220 batches submitted to accumulate stats from 14080 documents (365141 virtual)
2024-05-03 03:05:49,549 - gensim.topic_coherence.text_analysis - INFO - 221 batches submitted to accumulate stats from 14144 documents (366934 virtual)
2024-05-03 03:05:49,549 - gensim.topic_coherence.text_analysis - INFO - 222 batches submitted to accumulate stats from 14208 documents (368448 virtual)
2024-05-03 03:05:49,549 - gensim.topic_coherence.text_analysis - INFO - 223 batches submitted to accumulate stats from 14272 documents (370012 virtual)
2024-05-03 03:05:49,549 - gensim.topic_coherence.text_analysis - INFO - 224 batches submitted to accumulate stats from 14336 documents (371704 virtual)
2024-05-03 03:05:49,549 - gensim.topic_coherence.text_analysis - INFO - 225 batches submitted to accumulate stats from 14400 documents (373331 virtual)
2024-05-03 03:05:49,549 - gensim.topic_coherence.text_analysis - INFO - 226 batches submitted to accumulate stats from 14464 documents (375174 virtual)
2024-05-03 03:05:49,565 - gensim.topic_coherence.text_analysis - INFO - 227 batches submitted to accumulate stats from 14528 documents (377135 virtual)
2024-05-03 03:05:49,565 - gensim.topic_coherence.text_analysis - INFO - 228 batches submitted to accumulate stats from 14592 documents (378860 virtual)
2024-05-03 03:05:49,565 - gensim.topic_coherence.text_analysis - INFO - 229 batches submitted to accumulate stats from 14656 documents (380581 virtual)
2024-05-03 03:05:49,565 - gensim.topic_coherence.text_analysis - INFO - 230 batches submitted to accumulate stats from 14720 documents (382278 virtual)
2024-05-03 03:05:49,580 - gensim.topic_coherence.text_analysis - INFO - 231 batches submitted to accumulate stats from 14784 documents (383970 virtual)
2024-05-03 03:05:49,580 - gensim.topic_coherence.text_analysis - INFO - 232 batches submitted to accumulate stats from 14848 documents (385680 virtual)
2024-05-03 03:05:49,580 - gensim.topic_coherence.text_analysis - INFO - 233 batches submitted to accumulate stats from 14912 documents (387378 virtual)
2024-05-03 03:05:49,596 - gensim.topic_coherence.text_analysis - INFO - 234 batches submitted to accumulate stats from 14976 documents (388908 virtual)
2024-05-03 03:05:49,596 - gensim.topic_coherence.text_analysis - INFO - 235 batches submitted to accumulate stats from 15040 documents (390473 virtual)
2024-05-03 03:05:49,596 - gensim.topic_coherence.text_analysis - INFO - 236 batches submitted to accumulate stats from 15104 documents (391966 virtual)
2024-05-03 03:05:49,596 - gensim.topic_coherence.text_analysis - INFO - 237 batches submitted to accumulate stats from 15168 documents (393446 virtual)
2024-05-03 03:05:49,596 - gensim.topic_coherence.text_analysis - INFO - 238 batches submitted to accumulate stats from 15232 documents (394979 virtual)
2024-05-03 03:05:49,596 - gensim.topic_coherence.text_analysis - INFO - 239 batches submitted to accumulate stats from 15296 documents (396707 virtual)
2024-05-03 03:05:49,611 - gensim.topic_coherence.text_analysis - INFO - 240 batches submitted to accumulate stats from 15360 documents (398434 virtual)
2024-05-03 03:05:49,611 - gensim.topic_coherence.text_analysis - INFO - 241 batches submitted to accumulate stats from 15424 documents (400092 virtual)
2024-05-03 03:05:49,611 - gensim.topic_coherence.text_analysis - INFO - 242 batches submitted to accumulate stats from 15488 documents (402054 virtual)
2024-05-03 03:05:49,611 - gensim.topic_coherence.text_analysis - INFO - 243 batches submitted to accumulate stats from 15552 documents (403465 virtual)
2024-05-03 03:05:49,611 - gensim.topic_coherence.text_analysis - INFO - 244 batches submitted to accumulate stats from 15616 documents (405289 virtual)
2024-05-03 03:05:49,627 - gensim.topic_coherence.text_analysis - INFO - 245 batches submitted to accumulate stats from 15680 documents (406750 virtual)
2024-05-03 03:05:49,627 - gensim.topic_coherence.text_analysis - INFO - 246 batches submitted to accumulate stats from 15744 documents (408306 virtual)
2024-05-03 03:05:49,627 - gensim.topic_coherence.text_analysis - INFO - 247 batches submitted to accumulate stats from 15808 documents (410097 virtual)
2024-05-03 03:05:49,643 - gensim.topic_coherence.text_analysis - INFO - 248 batches submitted to accumulate stats from 15872 documents (411915 virtual)
2024-05-03 03:05:49,643 - gensim.topic_coherence.text_analysis - INFO - 249 batches submitted to accumulate stats from 15936 documents (413538 virtual)
2024-05-03 03:05:49,643 - gensim.topic_coherence.text_analysis - INFO - 250 batches submitted to accumulate stats from 16000 documents (415307 virtual)
2024-05-03 03:05:49,643 - gensim.topic_coherence.text_analysis - INFO - 251 batches submitted to accumulate stats from 16064 documents (416993 virtual)
2024-05-03 03:05:49,658 - gensim.topic_coherence.text_analysis - INFO - 252 batches submitted to accumulate stats from 16128 documents (418582 virtual)
2024-05-03 03:05:49,658 - gensim.topic_coherence.text_analysis - INFO - 253 batches submitted to accumulate stats from 16192 documents (420282 virtual)
2024-05-03 03:05:49,658 - gensim.topic_coherence.text_analysis - INFO - 254 batches submitted to accumulate stats from 16256 documents (421859 virtual)
2024-05-03 03:05:49,658 - gensim.topic_coherence.text_analysis - INFO - 255 batches submitted to accumulate stats from 16320 documents (423441 virtual)
2024-05-03 03:05:49,658 - gensim.topic_coherence.text_analysis - INFO - 256 batches submitted to accumulate stats from 16384 documents (425169 virtual)
2024-05-03 03:05:49,674 - gensim.topic_coherence.text_analysis - INFO - 257 batches submitted to accumulate stats from 16448 documents (426804 virtual)
2024-05-03 03:05:49,674 - gensim.topic_coherence.text_analysis - INFO - 258 batches submitted to accumulate stats from 16512 documents (428503 virtual)
2024-05-03 03:05:49,674 - gensim.topic_coherence.text_analysis - INFO - 259 batches submitted to accumulate stats from 16576 documents (430063 virtual)
2024-05-03 03:05:49,690 - gensim.topic_coherence.text_analysis - INFO - 260 batches submitted to accumulate stats from 16640 documents (431672 virtual)
2024-05-03 03:05:49,690 - gensim.topic_coherence.text_analysis - INFO - 261 batches submitted to accumulate stats from 16704 documents (433298 virtual)
2024-05-03 03:05:49,690 - gensim.topic_coherence.text_analysis - INFO - 262 batches submitted to accumulate stats from 16768 documents (434951 virtual)
2024-05-03 03:05:49,690 - gensim.topic_coherence.text_analysis - INFO - 263 batches submitted to accumulate stats from 16832 documents (436505 virtual)
2024-05-03 03:05:49,690 - gensim.topic_coherence.text_analysis - INFO - 264 batches submitted to accumulate stats from 16896 documents (438134 virtual)
2024-05-03 03:05:49,705 - gensim.topic_coherence.text_analysis - INFO - 265 batches submitted to accumulate stats from 16960 documents (439831 virtual)
2024-05-03 03:05:49,705 - gensim.topic_coherence.text_analysis - INFO - 266 batches submitted to accumulate stats from 17024 documents (441666 virtual)
2024-05-03 03:05:49,705 - gensim.topic_coherence.text_analysis - INFO - 267 batches submitted to accumulate stats from 17088 documents (443389 virtual)
2024-05-03 03:05:49,705 - gensim.topic_coherence.text_analysis - INFO - 268 batches submitted to accumulate stats from 17152 documents (445008 virtual)
2024-05-03 03:05:49,721 - gensim.topic_coherence.text_analysis - INFO - 269 batches submitted to accumulate stats from 17216 documents (446691 virtual)
2024-05-03 03:05:49,721 - gensim.topic_coherence.text_analysis - INFO - 270 batches submitted to accumulate stats from 17280 documents (448258 virtual)
2024-05-03 03:05:49,721 - gensim.topic_coherence.text_analysis - INFO - 271 batches submitted to accumulate stats from 17344 documents (450177 virtual)
2024-05-03 03:05:49,721 - gensim.topic_coherence.text_analysis - INFO - 272 batches submitted to accumulate stats from 17408 documents (451838 virtual)
2024-05-03 03:05:49,736 - gensim.topic_coherence.text_analysis - INFO - 273 batches submitted to accumulate stats from 17472 documents (453765 virtual)
2024-05-03 03:05:49,736 - gensim.topic_coherence.text_analysis - INFO - 274 batches submitted to accumulate stats from 17536 documents (455302 virtual)
2024-05-03 03:05:49,736 - gensim.topic_coherence.text_analysis - INFO - 275 batches submitted to accumulate stats from 17600 documents (457026 virtual)
2024-05-03 03:05:49,736 - gensim.topic_coherence.text_analysis - INFO - 276 batches submitted to accumulate stats from 17664 documents (458678 virtual)
2024-05-03 03:05:49,752 - gensim.topic_coherence.text_analysis - INFO - 277 batches submitted to accumulate stats from 17728 documents (460363 virtual)
2024-05-03 03:05:49,752 - gensim.topic_coherence.text_analysis - INFO - 278 batches submitted to accumulate stats from 17792 documents (462290 virtual)
2024-05-03 03:05:49,752 - gensim.topic_coherence.text_analysis - INFO - 279 batches submitted to accumulate stats from 17856 documents (464043 virtual)
2024-05-03 03:05:49,752 - gensim.topic_coherence.text_analysis - INFO - 280 batches submitted to accumulate stats from 17920 documents (465601 virtual)
2024-05-03 03:05:49,768 - gensim.topic_coherence.text_analysis - INFO - 281 batches submitted to accumulate stats from 17984 documents (467208 virtual)
2024-05-03 03:05:49,768 - gensim.topic_coherence.text_analysis - INFO - 282 batches submitted to accumulate stats from 18048 documents (468985 virtual)
2024-05-03 03:05:49,768 - gensim.topic_coherence.text_analysis - INFO - 283 batches submitted to accumulate stats from 18112 documents (470540 virtual)
2024-05-03 03:05:49,783 - gensim.topic_coherence.text_analysis - INFO - 284 batches submitted to accumulate stats from 18176 documents (472187 virtual)
2024-05-03 03:05:49,783 - gensim.topic_coherence.text_analysis - INFO - 285 batches submitted to accumulate stats from 18240 documents (473657 virtual)
2024-05-03 03:05:49,783 - gensim.topic_coherence.text_analysis - INFO - 286 batches submitted to accumulate stats from 18304 documents (475433 virtual)
2024-05-03 03:05:49,783 - gensim.topic_coherence.text_analysis - INFO - 287 batches submitted to accumulate stats from 18368 documents (476949 virtual)
2024-05-03 03:05:49,799 - gensim.topic_coherence.text_analysis - INFO - 288 batches submitted to accumulate stats from 18432 documents (478519 virtual)
2024-05-03 03:05:49,799 - gensim.topic_coherence.text_analysis - INFO - 289 batches submitted to accumulate stats from 18496 documents (480346 virtual)
2024-05-03 03:05:49,799 - gensim.topic_coherence.text_analysis - INFO - 290 batches submitted to accumulate stats from 18560 documents (482113 virtual)
2024-05-03 03:05:49,799 - gensim.topic_coherence.text_analysis - INFO - 291 batches submitted to accumulate stats from 18624 documents (483726 virtual)
2024-05-03 03:05:49,815 - gensim.topic_coherence.text_analysis - INFO - 292 batches submitted to accumulate stats from 18688 documents (485454 virtual)
2024-05-03 03:05:49,815 - gensim.topic_coherence.text_analysis - INFO - 293 batches submitted to accumulate stats from 18752 documents (487120 virtual)
2024-05-03 03:05:49,815 - gensim.topic_coherence.text_analysis - INFO - 294 batches submitted to accumulate stats from 18816 documents (488702 virtual)
2024-05-03 03:05:49,815 - gensim.topic_coherence.text_analysis - INFO - 295 batches submitted to accumulate stats from 18880 documents (490511 virtual)
2024-05-03 03:05:49,830 - gensim.topic_coherence.text_analysis - INFO - 296 batches submitted to accumulate stats from 18944 documents (492226 virtual)
2024-05-03 03:05:49,830 - gensim.topic_coherence.text_analysis - INFO - 297 batches submitted to accumulate stats from 19008 documents (494165 virtual)
2024-05-03 03:05:49,830 - gensim.topic_coherence.text_analysis - INFO - 298 batches submitted to accumulate stats from 19072 documents (495678 virtual)
2024-05-03 03:05:49,830 - gensim.topic_coherence.text_analysis - INFO - 299 batches submitted to accumulate stats from 19136 documents (497503 virtual)
2024-05-03 03:05:49,830 - gensim.topic_coherence.text_analysis - INFO - 300 batches submitted to accumulate stats from 19200 documents (499131 virtual)
2024-05-03 03:05:49,846 - gensim.topic_coherence.text_analysis - INFO - 301 batches submitted to accumulate stats from 19264 documents (500986 virtual)
2024-05-03 03:05:49,846 - gensim.topic_coherence.text_analysis - INFO - 302 batches submitted to accumulate stats from 19328 documents (502687 virtual)
2024-05-03 03:05:49,862 - gensim.topic_coherence.text_analysis - INFO - 303 batches submitted to accumulate stats from 19392 documents (504106 virtual)
2024-05-03 03:05:49,862 - gensim.topic_coherence.text_analysis - INFO - 304 batches submitted to accumulate stats from 19456 documents (505909 virtual)
2024-05-03 03:05:49,862 - gensim.topic_coherence.text_analysis - INFO - 305 batches submitted to accumulate stats from 19520 documents (507546 virtual)
2024-05-03 03:05:49,877 - gensim.topic_coherence.text_analysis - INFO - 306 batches submitted to accumulate stats from 19584 documents (509312 virtual)
2024-05-03 03:05:49,877 - gensim.topic_coherence.text_analysis - INFO - 307 batches submitted to accumulate stats from 19648 documents (510954 virtual)
2024-05-03 03:05:49,877 - gensim.topic_coherence.text_analysis - INFO - 308 batches submitted to accumulate stats from 19712 documents (512465 virtual)
2024-05-03 03:05:49,877 - gensim.topic_coherence.text_analysis - INFO - 309 batches submitted to accumulate stats from 19776 documents (514132 virtual)
2024-05-03 03:05:49,877 - gensim.topic_coherence.text_analysis - INFO - 310 batches submitted to accumulate stats from 19840 documents (515846 virtual)
2024-05-03 03:05:49,877 - gensim.topic_coherence.text_analysis - INFO - 311 batches submitted to accumulate stats from 19904 documents (517436 virtual)
2024-05-03 03:05:49,893 - gensim.topic_coherence.text_analysis - INFO - 312 batches submitted to accumulate stats from 19968 documents (519076 virtual)
2024-05-03 03:05:49,893 - gensim.topic_coherence.text_analysis - INFO - 313 batches submitted to accumulate stats from 20032 documents (520797 virtual)
2024-05-03 03:05:49,893 - gensim.topic_coherence.text_analysis - INFO - 314 batches submitted to accumulate stats from 20096 documents (522328 virtual)
2024-05-03 03:05:50,330 - gensim.topic_coherence.text_analysis - INFO - 11 accumulators retrieved from output queue
2024-05-03 03:05:50,455 - gensim.topic_coherence.text_analysis - INFO - accumulated word occurrence stats for 523696 virtual documents
2024-05-03 03:05:51,127 - gensim.corpora.dictionary - INFO - adding document #0 to Dictionary<0 unique tokens: []>
2024-05-03 03:05:51,565 - gensim.corpora.dictionary - INFO - adding document #10000 to Dictionary<23643 unique tokens: ['-', '.', '?', 'a', 'africa']...>
2024-05-03 03:05:51,987 - gensim.corpora.dictionary - INFO - adding document #20000 to Dictionary<32428 unique tokens: ['-', '.', '?', 'a', 'africa']...>
2024-05-03 03:05:52,002 - gensim.corpora.dictionary - INFO - built Dictionary<32503 unique tokens: ['-', '.', '?', 'a', 'africa']...> from 20086 documents (total 703102 corpus positions)
2024-05-03 03:05:52,002 - gensim.utils - INFO - Dictionary lifecycle event {'msg': "built Dictionary<32503 unique tokens: ['-', '.', '?', 'a', 'africa']...> from 20086 documents (total 703102 corpus positions)", 'datetime': '2024-05-03T03:05:52.002772', 'gensim': '4.3.2', 'python': '3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}
2024-05-03 03:05:52,002 - gensim.topic_coherence.probability_estimation - INFO - using ParallelWordOccurrenceAccumulator<processes=11, batch_size=64> to estimate probabilities from sliding windows
2024-05-03 03:06:51,319 - gensim.topic_coherence.text_analysis - INFO - 11 accumulators retrieved from output queue
2024-05-03 03:06:51,398 - gensim.topic_coherence.text_analysis - INFO - accumulated word occurrence stats for 21620 virtual documents
