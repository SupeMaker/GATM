{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Union, Any, List, Mapping, Union\n",
    "import json\n",
    "from collections import OrderedDict, defaultdict\n",
    "import torch\n",
    "import logging\n",
    "import logging.config\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from nltk.corpus import stopwords as stop_words\n",
    "from torchtext.data import get_tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.backends import cudnn\n",
    "from scipy.stats import entropy\n",
    "import heapq\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel\n",
    "from tqdm import tqdm\n",
    "import torch.distributed\n",
    "from abc import abstractmethod\n",
    "from numpy import inf\n",
    "from logger import TensorboardWriter\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_json(file: Union[str, os.PathLike]):\n",
    "    \"\"\"\n",
    "    Read json from file\n",
    "    :param file: the path to the json file\n",
    "    :return: ordered dictionary content\n",
    "    \"\"\"\n",
    "    file = Path(file)\n",
    "    with file.open(\"rt\") as handle:\n",
    "        return json.load(handle, object_hook=OrderedDict)\n",
    "\n",
    "\n",
    "def write_json(content: Dict, file: Union[str, os.PathLike]):\n",
    "    \"\"\"\n",
    "    Write content to a json file\n",
    "    :param content: the content dictionary\n",
    "    :param file: the path to save json file\n",
    "    \"\"\"\n",
    "    file = Path(file)\n",
    "    with file.open(\"wt\") as handle:\n",
    "        json.dump(content, handle, indent=4, sort_keys=False)\n",
    "\n",
    "\n",
    "def write_to_file(file: Union[str, os.PathLike], text: Union[str, list]):\n",
    "    with open(file, \"w\", encoding=\"utf-8\") as w:\n",
    "        if isinstance(text, str):\n",
    "            w.write(text)\n",
    "        elif isinstance(text, list):\n",
    "            w.write(\"\\n\".join(text))\n",
    "\n",
    "\n",
    "def prepare_device(n_gpu_use):\n",
    "    \"\"\"\n",
    "    setup GPU device if available. get gpu device indices which are used for DataParallel\n",
    "    \"\"\"\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    if n_gpu_use > 0 and n_gpu == 0:\n",
    "        print(\"Warning: There\\'s no GPU available on this machine,\"\n",
    "              \"training will be performed on CPU.\")\n",
    "        n_gpu_use = 0\n",
    "    if n_gpu_use > n_gpu:\n",
    "        print(f\"Warning: The number of GPU\\'s configured to use is {n_gpu_use}, but only {n_gpu} are \"\n",
    "              \"available on this machine.\")\n",
    "        n_gpu_use = n_gpu\n",
    "    device = torch.device('cuda:0' if n_gpu_use > 0 else 'cpu')\n",
    "    list_ids = list(range(n_gpu_use))\n",
    "    return device, list_ids\n",
    "\n",
    "\n",
    "def del_index_column(df):\n",
    "    return df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]\n",
    "\n",
    "\n",
    "def get_project_root(**kwargs):\n",
    "    project_name = kwargs.pop(\"project_name\", \"nc_base\") # nc_base\n",
    "    file_parts = Path(os.getcwd()).parts # D:\\AI\\Graduation_Project\\model\\BATM\\experiment\\runner\n",
    "    # abs_path = Path(f\"{os.sep}\".join(file_parts[:file_parts.index(project_name) + 1]))\n",
    "    abs_path = Path(f\"{os.sep}\".join(file_parts))\n",
    "    return os.path.relpath(abs_path, os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class Configuration:\n",
    "    \"\"\"\n",
    "    This is the base class for all configuration class. Deal with the common hyper-parameters to all models'\n",
    "    configuration, and include the methods for loading/saving configurations.\n",
    "    For each sub configuration, a variable named 'type' is defined to indicate which class it belongs to\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        # parameters in general\n",
    "        self.n_gpu = kwargs.pop(\"n_gpu\", 1)  # default using gpu for training\n",
    "        self.embedding_type = kwargs.pop(\"embedding_type\", \"glove\")\n",
    "        self.max_length = kwargs.pop(\"max_length\", 100)\n",
    "        self.loss = kwargs.pop(\"loss\", \"cross_entropy\")\n",
    "        self.metrics = kwargs.pop(\"metrics\", [\"accuracy\", \"macro_f\"])\n",
    "        self.save_model = kwargs.pop(\"save_model\", False)\n",
    "        self.resume = kwargs.pop(\"resume\", None)\n",
    "        # setup default relative project path\n",
    "        self.project_name = kwargs.pop(\"project_name\", \"nc_base\")\n",
    "\n",
    "        self.project_root = kwargs.pop(\"project_root\", get_project_root(project_name=self.project_name))\n",
    "        self.data_root = os.path.join(self.project_root, \"dataset\")\n",
    "        self.save_dir = kwargs.pop(\"save_dir\", os.path.join(self.project_root, \"saved\"))\n",
    "        self.seed = kwargs.pop(\"seed\", 42)\n",
    "        self.sub_configs = [\"arch_config\", \"data_config\", \"trainer_config\", \"optimizer_config\", \"scheduler_config\"]\n",
    "\n",
    "        # parameters for architecture by default\n",
    "        self.arch_config = {\n",
    "            \"type\": \"Baseline\", \"dropout_rate\": 0.2, \"embedding_type\": self.embedding_type,\n",
    "            \"max_length\": self.max_length,\n",
    "        }\n",
    "        self.arch_config.update(kwargs.pop(\"arch_config\", {}))\n",
    "\n",
    "        # parameters for loading data\n",
    "        self.data_config = {\n",
    "            \"type\": \"NewsDataLoader\", \"batch_size\": 32, \"num_workers\": 1, \"name\": \"MIND15/keep_all\",\n",
    "            \"max_length\": self.max_length, \"data_root\": self.data_root, \"embedding_type\": self.embedding_type\n",
    "        }\n",
    "        self.data_config.update(kwargs.pop(\"data_config\", {}))\n",
    "        # identifier of experiment, default is identified by dataset name and architecture type.\n",
    "        self.run_name = kwargs.pop(\"run_name\", f\"{self.data_config['name']}/{self.arch_config['type']}\")\n",
    "\n",
    "        # parameters for optimizer\n",
    "        self.optimizer_config = {\"type\": \"Adam\", \"lr\": 1e-3, \"weight_decay\": 0}\n",
    "        self.optimizer_config.update(kwargs.pop(\"optimizer_config\", {}))\n",
    "\n",
    "        # parameters for scheduler\n",
    "        self.scheduler_config = {\"type\": \"StepLR\", \"step_size\": 50, \"gamma\": 0.1}\n",
    "        self.scheduler_config.update(kwargs.pop(\"scheduler_config\", {}))\n",
    "\n",
    "        # parameters for trainer\n",
    "        self.trainer_config = {\n",
    "            \"epochs\": 3, \"early_stop\": 3, \"monitor\": \"max val_accuracy\", \"verbosity\": 2, \"tensorboard\": False\n",
    "        }\n",
    "        self.trainer_config.update(kwargs.pop(\"trainer_config\", {}))\n",
    "\n",
    "        # Additional attributes without default values\n",
    "        for key, value in kwargs.items():\n",
    "            try:\n",
    "                setattr(self, key, value)\n",
    "            except AttributeError as err:\n",
    "                raise err\n",
    "    # 实现了get函数\n",
    "    def get(self, key, default=None):\n",
    "        if hasattr(self, key):\n",
    "            return getattr(self, key)\n",
    "        else:\n",
    "            for sub in self.sub_configs:\n",
    "                sub_config = getattr(self, sub)\n",
    "                if key in sub_config:\n",
    "                    return sub_config[key]\n",
    "            return default\n",
    "\n",
    "    def set(self, key, value):\n",
    "        if hasattr(self, key):\n",
    "            setattr(self, key, value)\n",
    "        for sub in self.sub_configs:\n",
    "            sub_config = getattr(self, sub)\n",
    "            if key in sub_config:\n",
    "                sub_config[key] = value\n",
    "\n",
    "    def update(self, config_dict: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Updates attributes of this class with attributes from ``config_dict``.\n",
    "\n",
    "        Args:\n",
    "            config_dict (:obj:`Dict[str, Any]`): Dictionary of attributes that should be updated for this class.\n",
    "        \"\"\"\n",
    "        for key, value in config_dict.items():\n",
    "            if isinstance(value, dict):\n",
    "                self.update_sub_config(key, **value)\n",
    "            else:\n",
    "                setattr(self, key, value)\n",
    "\n",
    "    def update_sub_config(self, sub_name: str, **kwargs):\n",
    "        \"\"\"\n",
    "        update corresponding sub configure dictionary\n",
    "        :param sub_name: the name of sub-configuration, such as arch_config\n",
    "        \"\"\"\n",
    "        getattr(self, sub_name).update(kwargs)\n",
    "\n",
    "    def save_config(self, save_dir: Union[str, os.PathLike], config_name: str = \"config.json\"):\n",
    "        \"\"\"\n",
    "        Save configuration with the saved directory with corresponding configuration name in a json file\n",
    "        :param config_name: default is config.json, should be a json filename\n",
    "        :param save_dir: the directory to save the configuration\n",
    "        \"\"\"\n",
    "        if os.path.isfile(save_dir):\n",
    "            raise AssertionError(f\"Provided path ({save_dir}) should be a directory, not a file\")\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        config_file = Path(save_dir) / config_name\n",
    "        write_json(copy.deepcopy(self.__dict__), config_file)\n",
    "\n",
    "    @classmethod\n",
    "    def from_json_file(cls, json_file: Union[str, os.PathLike]):\n",
    "        \"\"\"\n",
    "        load configuration from a json file\n",
    "        :param json_file: the path to the json file\n",
    "        :return: a configuration object\n",
    "        \"\"\"\n",
    "        return cls(**read_json(json_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def setup_logging(save_dir):\n",
    "    \"\"\"\n",
    "    Setup logging configuration\n",
    "    \"\"\"\n",
    "    log_config = {\n",
    "        \"version\": 1,\n",
    "        \"disable_existing_loggers\": False,\n",
    "        \"formatters\": {\n",
    "            \"simple\": {\"format\": \"%(message)s\"},\n",
    "            \"datetime\": {\"format\": \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"}\n",
    "        },\n",
    "        \"handlers\": {\n",
    "            \"console\": {\n",
    "                \"class\": \"logging.StreamHandler\",\n",
    "                \"level\": \"DEBUG\",\n",
    "                \"formatter\": \"simple\",\n",
    "                \"stream\": \"ext://sys.stdout\"\n",
    "            },\n",
    "            \"info_file_handler\": {\n",
    "                \"class\": \"logging.handlers.RotatingFileHandler\",\n",
    "                \"level\": \"INFO\",\n",
    "                \"formatter\": \"datetime\",\n",
    "                \"filename\": \"info.log\",\n",
    "                \"maxBytes\": 10485760,\n",
    "                \"backupCount\": 20, \"encoding\": \"utf8\"\n",
    "            }\n",
    "        },\n",
    "        \"root\": {\n",
    "            \"level\": \"INFO\",\n",
    "            \"handlers\": [\n",
    "                \"console\",\n",
    "                \"info_file_handler\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    # modify logging paths based on run config\n",
    "    for _, handler in log_config['handlers'].items():\n",
    "        if 'filename' in handler:\n",
    "            handler['filename'] = str(save_dir / handler['filename'])\n",
    "\n",
    "    logging.config.dictConfig(log_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "default_configs = {\n",
    "    \"PretrainedBaseline\": {\n",
    "        \"n_layers\": 1,\n",
    "    },\n",
    "    \"TextCNNClassifyModel\": {\n",
    "        \"num_filters\": 100, \"filter_sizes\": (2, )\n",
    "    },\n",
    "    \"NRMSNewsEncoderModel\": {\n",
    "        \"variant_name\": \"base\"\n",
    "    },\n",
    "    \"GRUAttClassifierModel\": {\n",
    "        \"variant_name\": \"gru_att\"\n",
    "    },\n",
    "    \"BiAttentionClassifyModel\": {\n",
    "        \"head_num\": None, \"head_dim\": 20, \"entropy_constraint\": False, \"alpha\": 0.01, \"n_layers\": 1, \"variant_name\": \"base\",\n",
    "    },\n",
    "    \"TopicExtractorClassifyModel\": {\n",
    "        \"head_num\": None, \"head_dim\": 20, \"entropy_constraint\": False, \"alpha\": 0.01, \"n_layers\": 1\n",
    "    },\n",
    "    \"FastformerClassifyModel\": {\n",
    "        \"embedding_dim\": 300, \"n_layers\": 2, \"hidden_act\": \"gelu\", \"head_num\": 15, \"type_vocab_size\": 2,\n",
    "        \"vocab_size\": 100000, \"layer_norm_eps\": 1e-12, \"initializer_range\": 0.02, \"pooler_type\": \"weightpooler\",\n",
    "        \"enable_fp16\": \"False\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def arch_default_config(arch_type: str):\n",
    "    default_config = {\"type\": arch_type}\n",
    "    default_config.update(default_configs[arch_type])\n",
    "    return default_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ConfigParser:\n",
    "    def __init__(self, config: Configuration, modification: dict = None):\n",
    "        \"\"\"\n",
    "        class to parse configuration json file. Handles hyper-parameters for training, initializations of modules,\n",
    "        checkpoint saving and logging module.\n",
    "        :param config: Dict containing configurations, hyper-parameters for training. Normal saved in configs directory.\n",
    "        :param modification: Dict keychain:value, specifying position values to be replaced from config dict.\n",
    "        Timestamp is being used as default\n",
    "        \"\"\"\n",
    "        # load config file and apply modification.\n",
    "        self.config = config\n",
    "        if modification:\n",
    "            self.config.update(modification)\n",
    "\n",
    "        # set save_dir where training model and log will be saved.\n",
    "        save_dir = Path(self.config.save_dir)\n",
    "        run_name = self.config.run_name\n",
    "\n",
    "        self._save_dir = save_dir / \"models\" / run_name\n",
    "        self._log_dir = save_dir / \"log\" / run_name\n",
    "        # configure logging module\n",
    "        self.log_levels = {0: logging.WARNING, 1: logging.INFO, 2: logging.DEBUG}\n",
    "        # make directory for saving checkpoints and log\n",
    "        self._save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self._log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        setup_logging(self.log_dir)\n",
    "        # save updated config file to the checkpoint directory\n",
    "        self.config.save_config(self._save_dir)\n",
    "\n",
    "    @property\n",
    "    def save_dir(self):\n",
    "        return self._save_dir\n",
    "\n",
    "    @property\n",
    "    def log_dir(self):\n",
    "        return self._log_dir\n",
    "\n",
    "# 返回Configuration对象\n",
    "    @classmethod\n",
    "    def from_args(cls, args, options: list = None):\n",
    "        \"\"\"\n",
    "        Initialize this class from some cli arguments. Used in train, test.\n",
    "        \"\"\"\n",
    "        for opt in options:\n",
    "            default = opt.default if hasattr(opt, \"default\") else None\n",
    "            args.add_argument(*opt.flags, default=default, type=opt.type)\n",
    "        if not isinstance(args, tuple):\n",
    "            args = args.parse_args()\n",
    "        # parse custom cli options into dictionary\n",
    "        modification = defaultdict()\n",
    "        if hasattr(args, \"arch_type\") and args.arch_type is not None:\n",
    "            modification[\"arch_config\"] = arch_default_config(args.arch_type)  # setup default arch params\n",
    "        for opt in options:\n",
    "            name = opt.flags[-1].replace(\"--\", \"\")  # acquire param name\n",
    "            if opt.target:\n",
    "                if opt.target not in modification:\n",
    "                    modification[opt.target] = {}\n",
    "                if getattr(args, name):\n",
    "                    modification[opt.target][name] = getattr(args, name)  # setup custom params values\n",
    "            else:\n",
    "                if getattr(args, name):\n",
    "                    modification[name] = getattr(args, name)  # setup custom params values\n",
    "        if hasattr(args, \"resume\") and args.resume is not None:\n",
    "            config_file = Path(args.resume).parent / \"config.json\"\n",
    "            config = Configuration.from_json_file(config_file)\n",
    "        else:\n",
    "            # 到最后 config都会转变为Configuration对象\n",
    "            config = Configuration(**modification)\n",
    "        return cls(config)\n",
    "\n",
    "    def init_obj(self, module_config: str, module: object, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Finds a function handle with the name given as 'type' in config, and returns the\n",
    "        instance initialized with corresponding arguments given.\n",
    "\n",
    "        `object = config.init_obj('trainer_config', module, a, b=1)`\n",
    "        is equivalent to\n",
    "        `object = module.module_name(a, b=1)`\n",
    "        \"\"\"\n",
    "        module_args = copy.deepcopy(getattr(self.config, module_config))\n",
    "        module_args.update(kwargs)\n",
    "        module_name = module_args.pop(\"type\")\n",
    "        # getattr()返回module对象里面的module_name,也可以返回名为 module的类的对象\n",
    "        return getattr(module, module_name)(*args, **module_args)\n",
    "\n",
    "    def get_logger(self, name, verbosity=2):\n",
    "        msg_verbosity = f\"verbosity option{verbosity} is invalid. Valid options are {self.log_levels.keys()}.\"\n",
    "        assert verbosity in self.log_levels, msg_verbosity\n",
    "        logger = logging.getLogger(name)\n",
    "        logger.setLevel(self.log_levels[verbosity])\n",
    "        return logger\n",
    "\n",
    "    def __getitem__(self, name):\n",
    "        \"\"\"Access items like ordinary dict.\"\"\"\n",
    "        return getattr(self.config, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def text2index(text, word_dict, method=\"keep\", ignore=True):\n",
    "    return word2index(word_dict, tokenize(text, method), ignore)\n",
    "\n",
    "'''\n",
    "这个 clean_text 函数的作用是清洗文本，它主要做的是移除文本中的标点符号以及数字。让我们来具体看一下这个函数是如何工作的：\n",
    "string.punctuation + \"0123456789\"：此处创建了一个字符串规则（rule），这个字符串包含了所有的标点符号以及数字0到9。\n",
    "re.sub(rf'([^{rule}a-zA-Z ])', r\" \", text)：这个是 Python 的正则表达式 re.sub() 方法，它用于替换字符串中的匹配项。这里它将所有不在 rule 中，也不是英文字母和空格的字符替换为一个空格。rf'([^{rule}a-zA-Z ])' 表示匹配所有不在 rule、不是英文小写字母、不是英文大写字母、也不是空格的字符。\n",
    "'''\n",
    "def clean_text(text):\n",
    "    rule = string.punctuation + \"0123456789\"\n",
    "    return re.sub(rf'([^{rule}a-zA-Z ])', r\" \", text)\n",
    "\n",
    "\n",
    "def aggressive_process(text):\n",
    "    stopwords = set(stop_words.words(\"english\"))\n",
    "    text = text.lower().translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
    "    text = text.translate(str.maketrans(\"0123456789\", ' ' * len(\"0123456789\")))\n",
    "    text = [w for w in text.split() if len(w) > 0 and w not in stopwords]\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize(text, method=\"keep_all\"):\n",
    "    tokens = []\n",
    "    text = clean_text(text)\n",
    "    rule = string.punctuation + \"0123456789\"\n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "    if method == \"keep_all\":\n",
    "        tokens = tokenizer(re.sub(rf'([{rule}])', r\" \\1 \", text.lower()))\n",
    "    elif method == \"aggressive\":\n",
    "        tokens = aggressive_process(text)\n",
    "    elif method == \"alphabet_only\":\n",
    "        tokens = tokenizer(re.sub(rf'([{rule}])', r\" \", text.lower()))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def word2index(word_dict, sent, ignore=True):\n",
    "    word_index = []\n",
    "    for word in sent:\n",
    "        if ignore:\n",
    "            index = word_dict[word] if word in word_dict else 0\n",
    "        else:\n",
    "            if word not in word_dict:\n",
    "                word_dict[word] = len(word_dict)\n",
    "            index = word_dict[word]\n",
    "        word_index.append(index)\n",
    "    return word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, texts, labels, label_dict, max_length, word_dict, process_method=\"keep_all\"):\n",
    "        super().__init__()\n",
    "        self.texts, self.labels, self.label_dict, self.max_length = texts, labels, label_dict, max_length\n",
    "        self.word_dict = word_dict\n",
    "        self.process_method = process_method\n",
    "\n",
    "        if self.label_dict is None and labels is not None:\n",
    "            self.label_dict = dict(zip(sorted(set(labels)), range(len(set(labels)))))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        data = text2index(self.texts[i], self.word_dict, self.process_method, True)[:self.max_length]\n",
    "        data.extend([0 for _ in range(max(0, self.max_length - len(data)))])\n",
    "        data = torch.tensor(data, dtype=torch.long)\n",
    "        label = torch.tensor(self.label_dict.get(self.labels[i], -1), dtype=torch.long).squeeze(0)\n",
    "        mask = torch.tensor(np.where(data == 0, 0, 1), dtype=torch.long)\n",
    "        return {\"data\": data, \"label\": label, \"mask\": mask}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "class BaseDatasetBert(Dataset):\n",
    "    def __init__(self, texts: List[str], labels: List[str] = None, label_dict: Mapping[str, int] = None,\n",
    "                 max_length: int = 512, embedding_type: str = 'distilbert-base-uncased', is_local=False):\n",
    "\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.label_dict = label_dict\n",
    "        self.max_length = max_length\n",
    "\n",
    "        if self.label_dict is None and labels is not None:\n",
    "            self.label_dict = dict(zip(sorted(set(labels)), range(len(set(labels)))))\n",
    "\n",
    "        if is_local:\n",
    "            model_root = \"D:\\\\AI\\\\model\\\\\"\n",
    "        else:\n",
    "            model_root = ''\n",
    "        # 这里下载 tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_root + embedding_type)\n",
    "        logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.FATAL)\n",
    "\n",
    "        # self.sep_vid = self.tokenizer.sep_token_id\n",
    "        # self.cls_vid = self.tokenizer.cls_token_id\n",
    "        if embedding_type == \"transfo-xl-wt103\":\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            self.pad_vid = self.tokenizer.pad_token_id\n",
    "        else:\n",
    "            self.pad_vid = self.tokenizer.pad_token_id\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index) -> Mapping[str, torch.Tensor]:\n",
    "\n",
    "        x = self.texts[index]\n",
    "        x_encoded = self.tokenizer.encode(x, add_special_tokens=True, max_length=self.max_length, truncation=True,\n",
    "                                          return_tensors=\"pt\").squeeze(0)\n",
    "\n",
    "        # 这里是得到等长的embedding，会进行填充，获得处理过的X和mask\n",
    "        true_seq_length = x_encoded.size(0)\n",
    "        pad_size = self.max_length - true_seq_length\n",
    "        pad_ids = torch.Tensor([self.pad_vid] * pad_size).long()\n",
    "        x_tensor = torch.cat((x_encoded, pad_ids))\n",
    "\n",
    "        mask = torch.ones_like(x_encoded, dtype=torch.int8)\n",
    "        mask_pad = torch.zeros_like(pad_ids, dtype=torch.int8)\n",
    "        mask = torch.cat((mask, mask_pad))\n",
    "\n",
    "        output_dict = {\"data\": x_tensor, 'mask': mask}\n",
    "\n",
    "        if self.labels is not None:\n",
    "            y = self.labels[index]\n",
    "            y_encoded = torch.Tensor([self.label_dict.get(y, -1)]).long().squeeze(0)\n",
    "            output_dict[\"label\"] = y_encoded\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def clean_df(data_df):\n",
    "    # 这行代码的作用是从数据表 data_df 中删除那些在\"标题\" (title) 和 \"正文\" (body) 字段中同时为空的行。参数 inplace=True 表示这一操作直接就地对 data_df 进行修改。\n",
    "    data_df.dropna(subset=[\"title\", \"body\"], inplace=True, how=\"all\")\n",
    "    # 这行代码的作用是将 data_df 中的空值（NA 或 NaN）替换为字符串 \"empty\"。参数 inplace=True 表示这一操作直接在 data_df 上进行修改。\n",
    "    data_df.fillna(\"empty\", inplace=True)\n",
    "    # 这行代码使用一个匿名函数（lambda 函数）去处理 data_df 中的 title 列。函数 clean_text(s) 应该是一个对字符串进行清洗的函数，即对每一篇文章的标题进行清洗。\n",
    "    data_df[\"title\"] = data_df.title.apply(lambda s: clean_text(s))\n",
    "    data_df[\"body\"] = data_df.body.apply(lambda s: clean_text(s))\n",
    "    return data_df\n",
    "\n",
    "'''\n",
    "这段代码是对Pandas DataFrame的一组操作，主要用于创建一个随机筛选的验证集。以下是对每行代码的解析：\n",
    "df是一个Pandas的DataFrame对象，你可以将它视为一个二维的数据表格。\n",
    "indices = df.index.values：这行代码获取df的索引，也就是行号，并赋值给indices变量。例如，如果df有10行，indices就是一个包含0到9的数组。\n",
    "random.Random(42).shuffle(indices)：这行代码使用shuffle方法对indices数组进行随机排序。注意这里的42是随机数生成器的种子，保证了每次运行这段代码，得到的随机排序都是一样的。\n",
    "split_len = round(split * len(df))：这行代码计算验证集的长度。split是一个介于0和1之间的浮点数，代表验证集在所有数据中的占比。len(df)则是df的行数。所以split * len(df)就是我们期望的验证集大小，然后通过round函数进行四舍五入。\n",
    "df.loc[indices[:split_len], \"split\"] = \"valid\"：这行代码实际执行了数据集的分割。它选取了df中索引在乱序indices数组前split_len部分的行，也就是随机选取的split_len数量的数据，并在这些行下新增了一列名为\"split\"的属性，标记为\"valid\"。\n",
    "'''\n",
    "# 这个函数就是划分test,train,valid数据集的\n",
    "def split_df(df, split=0.1, split_test=False):\n",
    "    indices = df.index.values\n",
    "    random.Random(42).shuffle(indices)\n",
    "    split_len = round(split * len(df))\n",
    "    df.loc[indices[:split_len], \"split\"] = \"valid\"\n",
    "    if split_test:\n",
    "        df.loc[indices[split_len:split_len*2], \"split\"] = \"test\"\n",
    "        df.loc[indices[split_len*2:], \"split\"] = \"train\"\n",
    "    else:\n",
    "        df.loc[indices[split_len:], \"split\"] = \"train\"\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_set_by_type(dataset, set_type: str) -> pd.DataFrame:\n",
    "    df = {k: [] for k in [\"data\", \"category\"]}\n",
    "    for text, label in zip(dataset[set_type][\"text\"], dataset[set_type][\"label\"]):\n",
    "        for c, v in zip([\"data\", \"category\"], [text, label]):\n",
    "            df[c].append(v)\n",
    "    df[\"split\"] = set_type\n",
    "    return pd.DataFrame(df)\n",
    "\n",
    "\n",
    "def load_dataset_df(dataset_name, data_path):\n",
    "\n",
    "    if dataset_name in [\"MIND15\", \"News26\"]:\n",
    "        # 这里是直接读取本地的数据，完蛋\n",
    "        df = clean_df(pd.read_csv(data_path, encoding=\"utf-8\"))\n",
    "        df[\"data\"] = df.title + \"\\n\" + df.body\n",
    "    elif dataset_name in [\"ag_news\", \"yelp_review_full\", \"imdb\"]:\n",
    "        # load corresponding dataset from datasets library，使用 NewsDataLoader 类里面的 load_dataset\n",
    "        dataset = load_dataset(dataset_name)\n",
    "        train_set, test_set = split_df(load_set_by_type(dataset, \"train\")), load_set_by_type(dataset, \"test\")\n",
    "        df = train_set.append(test_set)\n",
    "    else:\n",
    "        raise ValueError(\"dataset name should be in one of MIND15, IMDB, News26, and ag_news...\")\n",
    "    labels = df[\"category\"].values.tolist()\n",
    "    label_dict = dict(zip(sorted(set(labels)), range(len(set(labels)))))\n",
    "    return df, label_dict\n",
    "\n",
    "\n",
    "def load_word_dict(data_root, dataset_name, process_method, **kwargs):\n",
    "    embed_method = kwargs.get(\"embed_method\", \"use_all\")\n",
    "    wd_path = Path(data_root) / \"utils\" / \"word_dict\" / f\"{dataset_name}_{process_method}_{embed_method}.json\"\n",
    "    if os.path.exists(wd_path):\n",
    "        word_dict = read_json(wd_path)\n",
    "    else:\n",
    "        word_dict = {}\n",
    "        data_path = kwargs.get(\"data_path\", Path(data_root) / \"data\" / f\"{dataset_name}.csv\")\n",
    "        df = kwargs.get(\"df\", load_dataset_df(dataset_name, data_path)[0])\n",
    "        df.data.apply(lambda s: text2index(s, word_dict, process_method, False))\n",
    "        os.makedirs(wd_path.parent, exist_ok=True)\n",
    "        write_json(word_dict, wd_path)\n",
    "    return word_dict\n",
    "\n",
    "\n",
    "def load_glove_embedding(glove_path=None):\n",
    "    if not glove_path:\n",
    "        # glove_path = \"D:\\\\AI\\\\Graduation_Project\\\\model\\\\BATM\\\\dataset\\\\glove\\\\glove.840B.300d.txt\"\n",
    "        # 这里要使用的是相对路径\n",
    "        glove_path = '../../dataset/glove/glove.840B.300d.txt'\n",
    "    glove = pd.read_csv(glove_path, sep=\" \", quoting=3, header=None, index_col=0)\n",
    "    return {key: val.values for key, val in glove.T.items()}\n",
    "\n",
    "'''\n",
    "这个函数其实就是加载glove embedding, 并使用glove的embedding将词典word_dict中出现过的单词重新赋值给new_wd,就是使用跟 glove embedding一样的词表索引\n",
    "'''\n",
    "def load_embeddings(data_root, dataset_name, process_method, word_dict, glove_path=None, embed_method=\"use_all\"):\n",
    "    embed_path = Path(data_root) / \"utils\" / \"embed_dict\" / f\"{dataset_name}_{process_method}_{embed_method}.npy\"\n",
    "    wd_path = Path(data_root) / \"utils\" / \"word_dict\" / f\"{dataset_name}_{process_method}_{embed_method}.json\"\n",
    "    if os.path.exists(embed_path):\n",
    "        embeddings = np.load(embed_path.__str__())\n",
    "        word_dict = read_json(wd_path)\n",
    "    else:\n",
    "        new_wd = {\"[UNK]\": 0}\n",
    "        # 这里加载已经处理好的embedding_dict\n",
    "        embedding_dict = load_glove_embedding(glove_path)\n",
    "        embeddings, exclude_words = [np.zeros(300)], []\n",
    "        for i, w in enumerate(word_dict.keys()):\n",
    "            if w in embedding_dict:\n",
    "                embeddings.append(embedding_dict[w])\n",
    "                new_wd[w] = len(new_wd)\n",
    "            else:\n",
    "                exclude_words.append(w)\n",
    "        if embed_method == \"use_all\":\n",
    "            mean, std = np.mean(embeddings), np.std(embeddings)\n",
    "            # append random embedding\n",
    "            for i, w in enumerate(exclude_words):\n",
    "                new_wd[w] = len(new_wd)\n",
    "                embeddings.append(np.random.normal(loc=mean, scale=std, size=300))\n",
    "        os.makedirs(embed_path.parent, exist_ok=True)\n",
    "        np.save(embed_path.__str__(), np.array(embeddings))\n",
    "        word_dict = new_wd\n",
    "        write_json(word_dict, wd_path)\n",
    "    return np.array(embeddings), word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NewsDataLoader:\n",
    "    def load_dataset(self, df):\n",
    "        pretrained_models = [\"distilbert-base-uncased\", \"bert-base-uncased\", \"xlnet-base-cased\", \"roberta-base\",\n",
    "                             \"longformer-base-4096\", \"transfo-xl-wt103\"]\n",
    "        if self.embedding_type in pretrained_models:\n",
    "            # df[\"data\"] = df.title + \"\\n\" + df.body\n",
    "            # 这里是根据 embedding_type 得到对应模型的dataset,dataset中含有tokenizer最为关键\n",
    "            dataset = BaseDatasetBert(texts=df[\"data\"].values.tolist(), labels=df[\"category\"].values.tolist(),\n",
    "                                      label_dict=self.label_dict, max_length=self.max_length,\n",
    "                                      embedding_type=self.embedding_type)\n",
    "            if self.embedding_type == \"transfo-xl-wt103\":\n",
    "                # 根据给定的预训练模型类型(embedding_type)生成相应的分词器(tokenizer)并获取其词汇表中每个符号的索引。\n",
    "                self.word_dict = dataset.tokenizer.sym2idx\n",
    "            else:\n",
    "                self.word_dict = dataset.tokenizer.vocab\n",
    "        elif self.embedding_type in [\"glove\", \"init\"]:\n",
    "            # if we use glove embedding, then we ignore the unknown words\n",
    "            dataset = BaseDatasetBert(df[\"data\"].values.tolist(), df[\"category\"].values.tolist(), self.label_dict,\n",
    "                                  self.max_length, self.word_dict, self.method)\n",
    "        else:\n",
    "            raise ValueError(f\"Embedding type should be one of {','.join(pretrained_models)} or glove and init\")\n",
    "        return dataset\n",
    "\n",
    "    def __init__(self, batch_size=32, shuffle=True, num_workers=1, max_length=128, name=\"MIND15/keep\", **kwargs):\n",
    "        self.set_name, self.method = name.split(\"/\")[0], name.split(\"/\")[1]\n",
    "        print(\"self.set_name, self.method: \", self.set_name, self.method) #  News26 keep_all\n",
    "        # kwargs.get(\"embedding_type\", \"glove\") 尝试从kwargs中获取\"embedding_type\"的值。如果字典中有\"embedding_type\"这个键，那么就返回其对应的值；如果字典中没有\"embedding_type\"这个键，那么方法就会返回默认值\"glove\"。\n",
    "        self.max_length, self.embedding_type = max_length, kwargs.get(\"embedding_type\", \"glove\")\n",
    "        # self.data_root = kwargs.get(\"data_root\", \"../../dataset\")\n",
    "        self.data_root = \"../../dataset\"\n",
    "        print(\"self.data_root: \", self.data_root) # self.data_root:  .\\dataset\n",
    "        data_path = Path(self.data_root) / \"data\" / f\"{self.set_name}.csv\"\n",
    "        print(\"data_path: \", data_path) # data_path:  dataset\\data\\News26.csv\n",
    "        # 加载数据\n",
    "        df, self.label_dict = load_dataset_df(self.set_name, data_path)\n",
    "        train_set, valid_set, test_set = df[\"split\"] == \"train\", df[\"split\"] == \"valid\", df[\"split\"] == \"test\"\n",
    "        if self.embedding_type in [\"glove\", \"init\"]:\n",
    "            # setup word dictionary for glove or init embedding\n",
    "            self.word_dict = load_word_dict(self.data_root, self.set_name, self.method, df=df)\n",
    "        if self.embedding_type == \"glove\":\n",
    "            # 这里加载 glove的embedding表示\n",
    "            self.embeds, self.word_dict = load_embeddings(self.data_root, self.set_name, self.method, self.word_dict,\n",
    "                                                          embed_method=kwargs.get(\"embed_method\", \"use_all\"))\n",
    "        self.init_params = {'batch_size': batch_size, 'shuffle': shuffle, 'num_workers': num_workers}\n",
    "        # initialize train loader\n",
    "        self.train_loader = DataLoader(self.load_dataset(df[train_set]), **self.init_params)\n",
    "        # initialize validation loader\n",
    "        self.valid_loader = DataLoader(self.load_dataset(df[valid_set]), **self.init_params)\n",
    "        # initialize test loader\n",
    "        self.test_loader = DataLoader(self.load_dataset(df[test_set]), **self.init_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_topic_dist(trainer, word_seq):\n",
    "    topic_dist = np.zeros((trainer.model.head_num, len(word_seq)))\n",
    "    with torch.no_grad():\n",
    "        bs = 512\n",
    "        num = bs * (len(word_seq) // bs)\n",
    "        word_feat = np.array(word_seq[:num]).reshape(-1, bs).tolist() + [word_seq[num:]]\n",
    "        for words in word_feat:\n",
    "            input_feat = {\"data\": torch.tensor(words).unsqueeze(0), \"mask\": torch.ones(len(words)).unsqueeze(0)}\n",
    "            input_feat = trainer.load_batch_data(input_feat)\n",
    "            _, topic_weight = trainer.best_model.extract_topic(input_feat)  # (B, H, N)\n",
    "            topic_dist[:, words] = topic_weight.squeeze().cpu().data\n",
    "        return topic_dist\n",
    "\n",
    "\n",
    "def get_coherence(topics, texts, method):\n",
    "    dictionary = Dictionary(texts)\n",
    "    return CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, coherence=method, topn=25)\n",
    "\n",
    "\n",
    "def get_topic_list(matrix, top_n, reverse_dict):\n",
    "    top_index = [heapq.nlargest(top_n, range(len(vec)), vec.take) for vec in matrix]\n",
    "    topic_list = [[reverse_dict[i] for i in index] for index in top_index]\n",
    "    return topic_list\n",
    "\n",
    "\n",
    "def evaluate_topic(topic_list, data_loader):\n",
    "    texts = [tokenize(s, data_loader.method) for s in data_loader.test_loader.dataset.texts]\n",
    "    npmi = get_coherence(topic_list, texts, \"c_npmi\").get_coherence_per_topic()\n",
    "    c_v = get_coherence(topic_list, texts, \"c_v\").get_coherence_per_topic()\n",
    "    return npmi, c_v\n",
    "\n",
    "\n",
    "def save_topic_info(path, weights, reverse_dict, data_loader, top_n=25):\n",
    "    topic_list = get_topic_list(weights, top_n, reverse_dict)\n",
    "    npmi, c_v = evaluate_topic(topic_list, data_loader)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    write_to_file(os.path.join(path, \"topic_list.txt\"), [\" \".join(topics) for topics in topic_list])\n",
    "    topic_result = {\"NPMI\": np.mean(npmi), \"CV\": np.mean(c_v)}\n",
    "    write_to_file(os.path.join(path, f\"cv_coherence_{topic_result['CV']}.txt\"), [str(s) for s in np.round(c_v, 4)])\n",
    "    write_to_file(os.path.join(path, f\"npmi_coherence_{topic_result['NPMI']}.txt\"), [str(s) for s in np.round(npmi, 4)])\n",
    "    return topic_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def nll_loss(output, target):\n",
    "    return F.nll_loss(output, target)\n",
    "\n",
    "\n",
    "def cross_entropy(output, target):\n",
    "    return F.cross_entropy(output, target)\n",
    "\n",
    "\n",
    "def categorical_loss(output, target, epsilon=1e-12):\n",
    "    \"\"\"\n",
    "    Computes cross entropy between target (encoded as one-hot vectors) and output.\n",
    "    Input: output (N, k) ndarray\n",
    "           target (N, k) ndarray\n",
    "    Returns: scalar\n",
    "    \"\"\"\n",
    "    output, target = output.float(), target.float()\n",
    "    output = torch.clamp(output, epsilon, 1. - epsilon)\n",
    "    return -torch.sum(target * torch.log(output + 1e-9)) / output.shape[0]\n",
    "\n",
    "loss_dict = {\"nll_loss\": nll_loss, \"cross_entropy\": cross_entropy, \"categorical_loss\": categorical_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MetricTracker:\n",
    "    def __init__(self, *keys, writer=None):\n",
    "        self.writer = writer\n",
    "        self._data = pd.DataFrame(index=keys, columns=[\"total\", \"counts\", \"average\"])\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        for col in self._data.columns:\n",
    "            self._data[col].values[:] = 0\n",
    "\n",
    "    def update(self, key, value, n=1):\n",
    "        if self.writer is not None:\n",
    "            self.writer.add_scalar(key, value)\n",
    "        self._data.total[key] += value * n\n",
    "        self._data.counts[key] += n\n",
    "        self._data.average[key] = round(self._data.total[key] / self._data.counts[key], 6)\n",
    "\n",
    "    def avg(self, key):\n",
    "        return self._data.average[key]\n",
    "\n",
    "    def result(self):\n",
    "        return dict(self._data.average)\n",
    "\n",
    "\n",
    "def accuracy(output, target):\n",
    "    with torch.no_grad():\n",
    "        pred = torch.argmax(output, dim=1)\n",
    "        assert pred.shape[0] == len(target)\n",
    "        return torch.sum(pred == target).item() / len(target)\n",
    "\n",
    "\n",
    "def macro_f(output, target):\n",
    "    with torch.no_grad():\n",
    "        pred = torch.argmax(output, dim=1)\n",
    "        score = f1_score(target.cpu(), pred.cpu(), average=\"macro\")\n",
    "        return score\n",
    "\n",
    "metric_dict = {\"accuracy\": accuracy, \"macro_f\": macro_f}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BaseTrainer:\n",
    "    \"\"\"\n",
    "    Base class for all trainers\n",
    "    \"\"\"\n",
    "    def __init__(self, model, config):\n",
    "        self.config = config.config\n",
    "        # 设置 epochs等\n",
    "        cfg_trainer = config[\"trainer_config\"]\n",
    "        self.logger = config.get_logger(\"trainer\", cfg_trainer[\"verbosity\"])\n",
    "        # prepare for (multi-device) GPU training\n",
    "        # 使用gpu训练\n",
    "        self.device, device_ids = prepare_device(config[\"n_gpu\"])\n",
    "        self.model = model.to(self.device)\n",
    "        if len(device_ids) > 1:\n",
    "            self.model = torch.nn.DataParallel(self.model, device_ids=device_ids)\n",
    "        # set up model parameters\n",
    "        self.best_model = model\n",
    "        # get function handles of loss and metrics\n",
    "        # self.criterion = getattr(module_loss, config[\"loss\"])\n",
    "        self.criterion = loss_dict.pop(config[\"loss\"])\n",
    "        # 评价函数  [\"accuracy\", \"macro_f\"]\n",
    "        # self.metric_ftns = [getattr(module_metric, met) for met in config[\"metrics\"]]\n",
    "        self.metric_ftns = [metric_dict(met) for met in config[\"metrics\"]]\n",
    "        # build optimizer, learning rate scheduler. delete every lines containing lr_scheduler for disabling scheduler\n",
    "        trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "        self.optimizer = config.init_obj(\"optimizer_config\", torch.optim, trainable_params)\n",
    "        self.lr_scheduler = config.init_obj(\"scheduler_config\", torch.optim.lr_scheduler, self.optimizer)\n",
    "        # set up trainer parameters\n",
    "        self.epochs = cfg_trainer[\"epochs\"]\n",
    "        self.save_model = config[\"save_model\"]\n",
    "        self.monitor = cfg_trainer.get(\"monitor\", \"off\")\n",
    "        self.last_best_path = None\n",
    "        self.not_improved_count = 0\n",
    "\n",
    "        # configuration to monitor model performance and save best\n",
    "        if self.monitor == \"off\":\n",
    "            self.mnt_mode = \"off\"\n",
    "            self.mnt_best = 0\n",
    "        else:\n",
    "            self.mnt_mode, self.mnt_metric = self.monitor.split()\n",
    "            assert self.mnt_mode in [\"min\", \"max\"]\n",
    "\n",
    "            self.mnt_best = inf if self.mnt_mode == \"min\" else -inf\n",
    "            self.early_stop = cfg_trainer.get(\"early_stop\", inf)\n",
    "            if self.early_stop <= 0:\n",
    "                self.early_stop = inf\n",
    "\n",
    "        self.start_epoch = 1\n",
    "        self.checkpoint_dir = config.save_dir\n",
    "\n",
    "        # setup visualization writer instance\n",
    "        self.writer = TensorboardWriter(config.log_dir, self.logger, cfg_trainer[\"tensorboard\"])\n",
    "\n",
    "        if config[\"resume\"] is not None:\n",
    "            self._resume_checkpoint(config[\"resume\"])\n",
    "\n",
    "    @abstractmethod\n",
    "    def _train_epoch(self, epoch):\n",
    "        \"\"\"\n",
    "        Training logic for an epoch\n",
    "\n",
    "        :param epoch: Current epoch number\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _log_info(self, log):\n",
    "        # print logged information to the screen\n",
    "        for key, value in log.items():\n",
    "            self.logger.info(\"    {:15s}: {}\".format(str(key), value))\n",
    "\n",
    "    def save_log(self, log, **kwargs):\n",
    "        log[\"seed\"] = self.config[\"seed\"]\n",
    "        arch_config = self.config[\"arch_config\"]\n",
    "        default_config = arch_default_config(arch_config.get(\"type\"))\n",
    "        for key in arch_config.keys():\n",
    "            if default_config.get(key, None) != arch_config.get(key):\n",
    "                log[key] = arch_config.get(key)\n",
    "        log[\"run_id\"] = self.config[\"run_name\"]\n",
    "        saved_path = kwargs.get(\"saved_path\", Path(self.checkpoint_dir) / \"model_best.csv\")\n",
    "        log_df = pd.DataFrame(log, index=[0])\n",
    "        if os.path.exists(saved_path):\n",
    "            log_df = log_df.append(pd.read_csv(saved_path, float_precision=\"round_trip\"), ignore_index=True)\n",
    "        log_df = log_df.loc[:, ~log_df.columns.str.contains(\"^Unnamed\")]\n",
    "        log_df.to_csv(saved_path)\n",
    "\n",
    "    def _monitor(self, log, epoch):\n",
    "        # evaluate model performance according to configured metric, save best checkpoint as model_best with score\n",
    "        if self.mnt_mode != \"off\":\n",
    "            try:\n",
    "                # check whether model performance improved or not, according to specified metric(mnt_metric)\n",
    "                improved = (self.mnt_mode == \"min\" and log[self.mnt_metric] <= self.mnt_best) or \\\n",
    "                           (self.mnt_mode == \"max\" and log[self.mnt_metric] >= self.mnt_best)\n",
    "            except KeyError:\n",
    "                err_msg = f\"Warning:Metric {self.mnt_metric} is not found.Model performance monitoring is disabled.\"\n",
    "                self.logger.warning(err_msg)\n",
    "                self.mnt_mode = \"off\"\n",
    "                improved = False\n",
    "            log[\"split\"] = \"valid\"\n",
    "            self.save_log(log)\n",
    "\n",
    "            if improved:\n",
    "                self.mnt_best = log[self.mnt_metric]\n",
    "                self.not_improved_count = 0\n",
    "                self.best_model = copy.deepcopy(self.model)\n",
    "                if self.save_model:\n",
    "                    self._save_checkpoint(epoch, log[self.mnt_metric])\n",
    "            else:\n",
    "                self.not_improved_count += 1\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Full training logic\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        for epoch in range(self.start_epoch, self.epochs + 1):\n",
    "            result = self._train_epoch(epoch)\n",
    "\n",
    "            # save logged information into log dict\n",
    "            log = {\"epoch\": epoch}\n",
    "            log.update(result)\n",
    "            self._log_info(log)\n",
    "            self._monitor(log, epoch)\n",
    "            if self.not_improved_count > self.early_stop:\n",
    "                self.logger.info(f\"Validation performance did not improve for {self.early_stop} epochs. \"\n",
    "                                 \"Training stops.\")\n",
    "                break\n",
    "\n",
    "    def _save_checkpoint(self, epoch, score=0.0):\n",
    "        \"\"\"\n",
    "        Saving checkpoints\n",
    "        :param epoch: current epoch number\n",
    "        :param score: current score of monitor metric\n",
    "        \"\"\"\n",
    "        arch = type(self.model).__name__\n",
    "        state = {\n",
    "            \"arch\": arch,\n",
    "            \"epoch\": epoch,\n",
    "            \"state_dict\": self.model.state_dict(),\n",
    "            \"optimizer\": self.optimizer.state_dict(),\n",
    "            \"monitor_best\": self.mnt_best,\n",
    "            \"config\": self.config\n",
    "        }\n",
    "        best_path = str(self.checkpoint_dir / f\"{round(score, 4)}_model_best-epoch{epoch}.pth\")\n",
    "        if self.last_best_path:\n",
    "            if os.path.exists(self.last_best_path):\n",
    "                os.remove(self.last_best_path)\n",
    "        torch.save(state, best_path)\n",
    "        self.logger.info(f\"Saving current best: {best_path}\")\n",
    "        self.last_best_path = best_path\n",
    "\n",
    "    def _resume_checkpoint(self, resume_path):\n",
    "        \"\"\"\n",
    "        Resume from saved checkpoints\n",
    "\n",
    "        :param resume_path: Checkpoint path to be resumed\n",
    "        \"\"\"\n",
    "        resume_path = str(resume_path)\n",
    "        self.logger.info(f\"Loading checkpoint: {resume_path} ...\")\n",
    "        checkpoint = torch.load(resume_path)\n",
    "        self.start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        self.mnt_best = checkpoint[\"monitor_best\"]\n",
    "\n",
    "        # load architecture params from checkpoint.\n",
    "        if checkpoint[\"config\"][\"arch_config\"] != self.config[\"arch_config\"]:\n",
    "            self.logger.warning(\"Warning: Architecture configuration given in config file is different from that of \"\n",
    "                                \"checkpoint. This may yield an exception while state_dict is being loaded.\")\n",
    "        # if torch.distributed.is_initialized():\n",
    "        #     self.model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        # else:\n",
    "        #     self.model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        self.model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        # load optimizer state from checkpoint only when optimizer type is not changed.\n",
    "        if checkpoint[\"config\"][\"optimizer_config\"] != self.config[\"optimizer_config\"]:\n",
    "            self.logger.warning(\"Warning: Optimizer type given in config file is different from that of checkpoint. \"\n",
    "                                \"Optimizer parameters not being resumed.\")\n",
    "        else:\n",
    "            self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "        self.logger.info(f\"Checkpoint loaded. Resume training from epoch {self.start_epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NCTrainer(BaseTrainer):\n",
    "    \"\"\"\n",
    "    Trainer class\n",
    "    \"\"\"\n",
    "    def __init__(self, model, config, data_loader, **kwargs):\n",
    "        super().__init__(model, config)\n",
    "        self.config = config\n",
    "        self.data_loader = data_loader.train_loader\n",
    "        # Configuration类实现了 get() 函数,这里是获取 arch_config 属性\n",
    "        arch_config = self.config[\"arch_config\"]\n",
    "        self.entropy_constraint = arch_config.get(\"entropy_constraint\", False)\n",
    "        self.calculate_entropy = arch_config.get(\"calculate_entropy\", self.entropy_constraint)\n",
    "        # 训练步长\n",
    "        self.alpha = arch_config.get(\"alpha\", 0.001)\n",
    "        self.len_epoch = len(self.data_loader)\n",
    "        self.valid_loader = data_loader.valid_loader\n",
    "        self.do_validation = self.valid_loader is not None\n",
    "        self.log_step = int(np.sqrt(self.data_loader.batch_size))\n",
    "        metrics = [\"loss\"] + [m.__name__ for m in self.metric_ftns]\n",
    "        if self.calculate_entropy:\n",
    "            metrics.extend([\"doc_entropy\"])\n",
    "        self.train_metrics = MetricTracker(*metrics, writer=self.writer)\n",
    "        self.valid_metrics = MetricTracker(*metrics, writer=self.writer)\n",
    "\n",
    "    # 将数据放入GPU中\n",
    "    def load_batch_data(self, batch_dict):\n",
    "        \"\"\"\n",
    "        load batch data to default device\n",
    "        \"\"\"\n",
    "        return {k: v.to(self.device) for k, v in batch_dict.items()}\n",
    "\n",
    "# 训练模型并返回真实结果和loss\n",
    "    def run_model(self, batch_dict, model=None):\n",
    "        \"\"\"\n",
    "        run model with the batch data\n",
    "        :param batch_dict: the dictionary of data with format like {\"data\": Tensor(), \"label\": Tensor()}\n",
    "        :param model: by default we use the self model\n",
    "        :return: the output of running, label used for evaluation, and loss item\n",
    "        \"\"\"\n",
    "        # 将数据放入GPU中\n",
    "        batch_dict = self.load_batch_data(batch_dict)\n",
    "        # 训练模型\n",
    "        output = model(batch_dict) if model is not None else self.model(batch_dict)\n",
    "        loss = self.criterion(output[0], batch_dict[\"label\"])\n",
    "        out_dict = {\"label\": batch_dict[\"label\"], \"loss\": loss, \"predict\": output[0]}\n",
    "        # 使用商约束\n",
    "        if self.entropy_constraint:\n",
    "            loss += self.alpha * output[2]\n",
    "        if self.calculate_entropy:\n",
    "            out_dict.update({\"attention_weight\": output[1], \"entropy\": output[2]})\n",
    "        return out_dict\n",
    "\n",
    "    # 更新评价函数  [\"accuracy\", \"macro_f\"]\n",
    "    def update_metrics(self, metrics, out_dict):\n",
    "        n = len(out_dict[\"label\"])\n",
    "        metrics.update(\"loss\", out_dict[\"loss\"].item(), n=n)  # update metrix\n",
    "        if self.calculate_entropy:\n",
    "            metrics.update(\"doc_entropy\", out_dict[\"entropy\"].item() / n, n=n)\n",
    "        for met in self.metric_ftns:  # run metric functions\n",
    "            metrics.update(met.__name__, met(out_dict[\"predict\"], out_dict[\"label\"]), n=n)\n",
    "\n",
    "    def _train_epoch(self, epoch):\n",
    "        \"\"\"\n",
    "        Training logic for an epoch\n",
    "        :param epoch: Integer, current training epoch.\n",
    "        :return: A log that contains average loss and metric in this epoch.\n",
    "        \"\"\"\n",
    "        # 这里进入训练模式\n",
    "        self.model.train()\n",
    "        # self.train_metrics.reset(): 这一行代码调用了一个名为 reset 的方法，该方法可能是用于重置训练过程中的度量（metrics）或统计信息的。这样可以确保每个训练周期（epoch）开始时，度量的状态是干净的，而不会受到之前周期的影响\n",
    "        self.train_metrics.reset()\n",
    "        # tqdm 是一个 Python 库，用于在命令行界面中显示进度条，以提供对代码执行进度的实时可视化反馈。它的名称取自阿拉伯语中的“taqaddum”（进展）。\n",
    "        bar = tqdm(enumerate(self.data_loader), total=len(self.data_loader))\n",
    "        for batch_idx, batch_dict in bar:\n",
    "            self.optimizer.zero_grad()  # setup gradient to zero\n",
    "\n",
    "            out_dict = self.run_model(batch_dict, self.model)  # run model\n",
    "            out_dict[\"loss\"].backward()  # backpropagation\n",
    "            self.optimizer.step()  # gradient descent\n",
    "            self.writer.set_step((epoch - 1) * self.len_epoch + batch_idx, \"train\")\n",
    "            self.update_metrics(self.train_metrics, out_dict)\n",
    "            if batch_idx % self.log_step == 0:  # set bar\n",
    "                bar.set_description(f\"Train Epoch: {epoch} Loss: {out_dict['loss'].item()}\")\n",
    "            if batch_idx == self.len_epoch:\n",
    "                break\n",
    "        log = self.train_metrics.result()\n",
    "        if self.do_validation:\n",
    "            log.update(self.evaluate(self.valid_loader, self.model, epoch))  # update validation log\n",
    "\n",
    "        if self.lr_scheduler is not None:\n",
    "            # 是否调整 lr\n",
    "            self.lr_scheduler.step()\n",
    "        return log\n",
    "\n",
    "    def evaluate(self, loader, model, epoch=0, prefix=\"val\"):\n",
    "        model.eval()\n",
    "        self.valid_metrics.reset()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch_dict in tqdm(enumerate(loader), total=len(loader)):\n",
    "                out_dict = self.run_model(batch_dict, model)\n",
    "                self.writer.set_step((epoch - 1) * len(loader) + batch_idx, \"evaluate\")\n",
    "                self.update_metrics(self.valid_metrics, out_dict)\n",
    "        for name, p in model.named_parameters():  # add histogram of model parameters to the tensorboard\n",
    "            self.writer.add_histogram(name, p, bins='auto')\n",
    "        return {f\"{prefix}_{k}\": v for k, v in self.valid_metrics.result().items()}  # return log with prefix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def init_default_model(config_parser: ConfigParser, data_loader: NewsDataLoader):\n",
    "    # build a default model architecture\n",
    "    # word_dict获得词表中每个词的索引列表\n",
    "    model_params = {\"num_classes\": len(data_loader.label_dict), \"word_dict\": data_loader.word_dict}\n",
    "    # 如果 object 对象具有名为 name 的属性或方法，则 hasattr() 函数返回 True。如果 object 对象不具有名为 name 的属性或方法，则返回 False\n",
    "    if hasattr(data_loader, \"embeds\"):\n",
    "        # 可以看 NewsDataLoader的代码，如果有这个属性说明为glove嵌入\n",
    "        model_params.update({\"embeds\": data_loader.embeds})\n",
    "        # arch_config的定义在 /home/zhouyonglin/work/model/BATM/experiment/config/configuration.py\n",
    "        # 获取相应的模型\n",
    "    model = config_parser.init_obj(\"arch_config\", module_arch, **model_params)\n",
    "    return model\n",
    "\n",
    "\n",
    "def init_data_loader(config_parser: ConfigParser):\n",
    "    # setup data_loader instances\n",
    "    # 从 data_config(/home/zhouyonglin/work/model/BATM/experiment/config/configuration.py)中取出type,这里的type是指类NewsDataLoader，返回的就是这个类\n",
    "    data_loader = config_parser.init_obj(\"data_config\", module_data)\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "def run(config_parser: ConfigParser, data_loader: NewsDataLoader):\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True\n",
    "    logger = config_parser.get_logger(\"train\")\n",
    "    # 得到base模型\n",
    "    model = init_default_model(config_parser, data_loader)\n",
    "    logger.info(model)\n",
    "    trainer = NCTrainer(model, config_parser, data_loader)\n",
    "    # 训练模型\n",
    "    # train(),起源于base_trainer的抽象方法 train(),里面调用 _train_epoch抽象方法，这里实际调用的是 nc_trainer中的_train_epoch方法\n",
    "    trainer.train()\n",
    "    return trainer\n",
    "\n",
    "\n",
    "def test(trainer: NCTrainer, data_loader: NewsDataLoader):\n",
    "    log = {}\n",
    "    # run validation\n",
    "    log.update(trainer.evaluate(data_loader.valid_loader, trainer.best_model, prefix=\"val\"))\n",
    "    # run test\n",
    "    log.update(trainer.evaluate(data_loader.test_loader, trainer.best_model, prefix=\"test\"))\n",
    "    return log\n",
    "\n",
    "\n",
    "def topic_evaluation(trainer: NCTrainer, data_loader: NewsDataLoader, path: Union[str, os.PathLike]):\n",
    "    # statistic topic distribution of Topic Attention network\n",
    "    reverse_dict = {v: k for k, v in data_loader.word_dict.items()}\n",
    "    topic_dist = get_topic_dist(trainer, list(data_loader.word_dict.values()))\n",
    "    topic_result = save_topic_info(path, topic_dist, reverse_dict, data_loader)\n",
    "    topic_result.update({\"token_entropy\": np.mean(entropy(topic_dist, axis=1))})\n",
    "    return topic_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
