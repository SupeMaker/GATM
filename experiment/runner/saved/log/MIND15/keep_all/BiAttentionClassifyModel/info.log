2024-03-11 18:36:18,794 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=3600, bias=True)
    (1): Tanh()
    (2): Linear(in_features=3600, out_features=180, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 35,040,788
Freeze params: 0
2024-03-11 18:50:49,151 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=3600, bias=True)
    (1): Tanh()
    (2): Linear(in_features=3600, out_features=180, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 35,040,788
Freeze params: 0
2024-03-11 19:58:46,795 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Linear(in_features=768, out_features=180, bias=True)
  (query): Linear(in_features=768, out_features=3600, bias=True)
  (key): Linear(in_features=768, out_features=3600, bias=True)
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 37,299,428
Freeze params: 0
2024-03-11 20:19:35,617 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Linear(in_features=768, out_features=180, bias=True)
  (query): Linear(in_features=768, out_features=3600, bias=True)
  (key): Linear(in_features=768, out_features=3600, bias=True)
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 37,299,428
Freeze params: 0
2024-03-11 22:03:55,082 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Linear(in_features=768, out_features=16, bias=True)
  (mha): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (query): Linear(in_features=768, out_features=320, bias=True)
  (key): Linear(in_features=768, out_features=320, bias=True)
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 34,491,040
Freeze params: 0
2024-03-11 23:29:13,639 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Linear(in_features=768, out_features=16, bias=True)
  (mha): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (W_q): Linear(in_features=768, out_features=320, bias=True)
  (W_k): Linear(in_features=768, out_features=320, bias=True)
  (W_v): Linear(in_features=768, out_features=320, bias=True)
  (W_o): Linear(in_features=320, out_features=320, bias=True)
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 34,839,840
Freeze params: 0
2024-03-11 23:34:15,274 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Linear(in_features=768, out_features=32, bias=True)
  (mha): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (W_q): Linear(in_features=768, out_features=768, bias=True)
  (W_k): Linear(in_features=768, out_features=768, bias=True)
  (W_v): Linear(in_features=768, out_features=768, bias=True)
  (W_o): Linear(in_features=768, out_features=768, bias=True)
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 36,373,552
Freeze params: 0
2024-03-11 23:39:57,489 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Linear(in_features=768, out_features=32, bias=True)
  (mha): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (W_q): Linear(in_features=768, out_features=768, bias=True)
  (W_k): Linear(in_features=768, out_features=768, bias=True)
  (W_v): Linear(in_features=768, out_features=768, bias=True)
  (W_o): Linear(in_features=768, out_features=768, bias=True)
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 36,373,552
Freeze params: 0
2024-03-11 23:48:57,190 - trainer - INFO -     epoch          : 1
2024-03-11 23:48:57,190 - trainer - INFO -     loss           : 0.718998
2024-03-11 23:48:57,190 - trainer - INFO -     accuracy       : 0.758186
2024-03-11 23:48:57,190 - trainer - INFO -     macro_f        : 0.557648
2024-03-11 23:48:57,190 - trainer - INFO -     doc_entropy    : 4.432046
2024-03-11 23:48:57,190 - trainer - INFO -     val_loss       : 0.617828
2024-03-11 23:48:57,190 - trainer - INFO -     val_accuracy   : 0.78624
2024-03-11 23:48:57,190 - trainer - INFO -     val_macro_f    : 0.603219
2024-03-11 23:48:57,190 - trainer - INFO -     val_doc_entropy: 4.425529
2024-03-11 23:58:03,506 - trainer - INFO -     epoch          : 2
2024-03-11 23:58:03,506 - trainer - INFO -     loss           : 0.487989
2024-03-11 23:58:03,506 - trainer - INFO -     accuracy       : 0.830861
2024-03-11 23:58:03,506 - trainer - INFO -     macro_f        : 0.683314
2024-03-11 23:58:03,506 - trainer - INFO -     doc_entropy    : 4.430749
2024-03-11 23:58:03,506 - trainer - INFO -     val_loss       : 0.6073
2024-03-11 23:58:03,506 - trainer - INFO -     val_accuracy   : 0.791916
2024-03-11 23:58:03,506 - trainer - INFO -     val_macro_f    : 0.624773
2024-03-11 23:58:03,506 - trainer - INFO -     val_doc_entropy: 4.426017
2024-03-12 00:07:10,099 - trainer - INFO -     epoch          : 3
2024-03-12 00:07:10,099 - trainer - INFO -     loss           : 0.323992
2024-03-12 00:07:10,099 - trainer - INFO -     accuracy       : 0.885608
2024-03-12 00:07:10,099 - trainer - INFO -     macro_f        : 0.779823
2024-03-12 00:07:10,099 - trainer - INFO -     doc_entropy    : 4.431243
2024-03-12 00:07:10,099 - trainer - INFO -     val_loss       : 0.683177
2024-03-12 00:07:10,099 - trainer - INFO -     val_accuracy   : 0.791226
2024-03-12 00:07:10,099 - trainer - INFO -     val_macro_f    : 0.620703
2024-03-12 00:07:10,099 - trainer - INFO -     val_doc_entropy: 4.427705
2024-03-12 00:08:19,433 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Linear(in_features=768, out_features=32, bias=True)
  (mha): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (W_q): Linear(in_features=768, out_features=768, bias=True)
  (W_k): Linear(in_features=768, out_features=768, bias=True)
  (W_v): Linear(in_features=768, out_features=768, bias=True)
  (W_o): Linear(in_features=768, out_features=768, bias=True)
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 36,373,552
Freeze params: 0
2024-03-12 20:06:28,317 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Linear(in_features=768, out_features=32, bias=True)
  (mha): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (W_q): Linear(in_features=768, out_features=768, bias=True)
  (W_k): Linear(in_features=768, out_features=768, bias=True)
  (W_v): Linear(in_features=768, out_features=768, bias=True)
  (W_o): Linear(in_features=768, out_features=768, bias=True)
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 36,373,552
Freeze params: 0
2024-03-12 20:17:38,682 - trainer - INFO -     epoch          : 1
2024-03-12 20:17:38,697 - trainer - INFO -     loss           : 0.724394
2024-03-12 20:17:38,697 - trainer - INFO -     accuracy       : 0.756182
2024-03-12 20:17:38,697 - trainer - INFO -     macro_f        : 0.5556
2024-03-12 20:17:38,697 - trainer - INFO -     doc_entropy    : 4.506613
2024-03-12 20:17:38,697 - trainer - INFO -     val_loss       : 0.635351
2024-03-12 20:17:38,697 - trainer - INFO -     val_accuracy   : 0.779261
2024-03-12 20:17:38,697 - trainer - INFO -     val_macro_f    : 0.592385
2024-03-12 20:17:38,697 - trainer - INFO -     val_doc_entropy: 4.497758
2024-03-12 20:17:38,697 - trainer - INFO -     test_loss      : 0.635177
2024-03-12 20:17:38,697 - trainer - INFO -     test_accuracy  : 0.783632
2024-03-12 20:17:38,697 - trainer - INFO -     test_macro_f   : 0.598408
2024-03-12 20:17:38,697 - trainer - INFO -     test_doc_entropy: 4.50471
2024-03-12 20:19:06,132 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Linear(in_features=768, out_features=32, bias=True)
  (mha): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (W_q): Linear(in_features=768, out_features=768, bias=True)
  (W_k): Linear(in_features=768, out_features=768, bias=True)
  (W_v): Linear(in_features=768, out_features=768, bias=True)
  (W_o): Linear(in_features=768, out_features=768, bias=True)
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 36,373,552
Freeze params: 0
2024-03-12 20:28:30,861 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Linear(in_features=768, out_features=32, bias=True)
  (mha): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 34,011,184
Freeze params: 0
2024-03-12 22:20:57,446 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Linear(in_features=768, out_features=32, bias=True)
  (mha1): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (mha2): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (add_norm1): AddNorm(
    (dropout): Dropout(p=0.5, inplace=False)
    (ln): LayerNorm((100, 768), eps=1e-05, elementwise_affine=True)
  )
  (add_norm2): AddNorm(
    (dropout): Dropout(p=0.5, inplace=False)
    (ln): LayerNorm((100, 768), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 36,680,752
Freeze params: 0
2024-03-12 22:22:11,608 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Linear(in_features=768, out_features=32, bias=True)
  (mha1): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (mha2): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (add_norm1): AddNorm(
    (dropout): Dropout(p=0.5, inplace=False)
    (ln): LayerNorm((100, 768), eps=1e-05, elementwise_affine=True)
  )
  (add_norm2): AddNorm(
    (dropout): Dropout(p=0.5, inplace=False)
    (ln): LayerNorm((100, 768), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 36,680,752
Freeze params: 0
2024-03-12 22:23:51,972 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Linear(in_features=768, out_features=32, bias=True)
  (mha1): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (mha2): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (add_norm1): AddNorm(
    (dropout): Dropout(p=0.5, inplace=False)
    (ln): LayerNorm((100, 768), eps=1e-05, elementwise_affine=True)
  )
  (add_norm2): AddNorm(
    (dropout): Dropout(p=0.5, inplace=False)
    (ln): LayerNorm((100, 768), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 36,680,752
Freeze params: 0
2024-03-12 22:25:01,151 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Linear(in_features=768, out_features=32, bias=True)
  (mha1): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (mha2): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (add_norm1): AddNorm(
    (dropout): Dropout(p=0.5, inplace=False)
    (ln): LayerNorm((100, 768), eps=1e-05, elementwise_affine=True)
  )
  (add_norm2): AddNorm(
    (dropout): Dropout(p=0.5, inplace=False)
    (ln): LayerNorm((100, 768), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 36,680,752
Freeze params: 0
2024-03-12 22:37:03,162 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Linear(in_features=768, out_features=32, bias=True)
  (mha1): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (mha2): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (add_norm1): AddNorm(
    (dropout): Dropout(p=0.5, inplace=False)
    (ln): LayerNorm((100, 768), eps=1e-05, elementwise_affine=True)
  )
  (add_norm2): AddNorm(
    (dropout): Dropout(p=0.5, inplace=False)
    (ln): LayerNorm((100, 768), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 36,680,752
Freeze params: 0
2024-03-12 22:41:39,083 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Linear(in_features=768, out_features=32, bias=True)
  (mha1): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (mha2): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (add_norm1): AddNorm(
    (dropout): Dropout(p=0.5, inplace=False)
    (ln): LayerNorm((100, 768), eps=1e-05, elementwise_affine=True)
  )
  (add_norm2): AddNorm(
    (dropout): Dropout(p=0.5, inplace=False)
    (ln): LayerNorm((100, 768), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 36,680,752
Freeze params: 0
2024-03-12 22:45:23,492 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Linear(in_features=768, out_features=32, bias=True)
  (mha1): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (mha2): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (add_norm1): AddNorm(
    (dropout): Dropout(p=0.5, inplace=False)
    (ln): LayerNorm((100, 768), eps=1e-05, elementwise_affine=True)
  )
  (add_norm2): AddNorm(
    (dropout): Dropout(p=0.5, inplace=False)
    (ln): LayerNorm((100, 768), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 36,680,752
Freeze params: 0
2024-03-13 13:12:42,870 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Linear(in_features=768, out_features=32, bias=True)
  (mha1): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (mha2): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (add_norm1): AddNorm(
    (dropout): Dropout(p=0.5, inplace=False)
    (ln): LayerNorm((100, 768), eps=1e-05, elementwise_affine=True)
  )
  (add_norm2): AddNorm(
    (dropout): Dropout(p=0.5, inplace=False)
    (ln): LayerNorm((100, 768), eps=1e-05, elementwise_affine=True)
  )
  (feature_network): Linear(in_features=768, out_features=768, bias=True)
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 37,271,344
Freeze params: 0
2024-03-16 23:54:33,677 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Linear(in_features=300, out_features=10, bias=True)
  (mha1): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)
  )
  (mha2): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)
  )
  (mha3): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)
  )
  (add_norm1): AddNorm(
    (dropout): Dropout(p=0.5, inplace=False)
    (ln): LayerNorm((100, 300), eps=1e-05, elementwise_affine=True)
  )
  (add_norm2): AddNorm(
    (dropout): Dropout(p=0.5, inplace=False)
    (ln): LayerNorm((100, 300), eps=1e-05, elementwise_affine=True)
  )
  (add_norm3): AddNorm(
    (dropout): Dropout(p=0.5, inplace=False)
    (ln): LayerNorm((100, 300), eps=1e-05, elementwise_affine=True)
  )
  (feature_network1): Linear(in_features=300, out_features=300, bias=True)
  (feature_network2): Linear(in_features=300, out_features=300, bias=True)
  (feature_network3): Linear(in_features=300, out_features=300, bias=True)
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 71,291,382
Freeze params: 0
2024-03-16 23:58:54,862 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Linear(in_features=300, out_features=10, bias=True)
  (gru): GRU(300, 300, num_layers=2, batch_first=True)
  (mha1): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)
  )
  (mha2): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)
  )
  (mha3): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)
  )
  (add_norm1): AddNorm(
    (dropout): Dropout(p=0.5, inplace=False)
    (ln): LayerNorm((100, 300), eps=1e-05, elementwise_affine=True)
  )
  (add_norm2): AddNorm(
    (dropout): Dropout(p=0.5, inplace=False)
    (ln): LayerNorm((100, 300), eps=1e-05, elementwise_affine=True)
  )
  (add_norm3): AddNorm(
    (dropout): Dropout(p=0.5, inplace=False)
    (ln): LayerNorm((100, 300), eps=1e-05, elementwise_affine=True)
  )
  (feature_network1): Linear(in_features=300, out_features=300, bias=True)
  (feature_network2): Linear(in_features=300, out_features=300, bias=True)
  (feature_network3): Linear(in_features=300, out_features=300, bias=True)
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 72,374,982
Freeze params: 0
2024-03-17 02:40:05,619 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Linear(in_features=300, out_features=10, bias=True)
  (gru): LSTM(300, 300, batch_first=True, bidirectional=True)
  (mha1): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=600, out_features=600, bias=True)
  )
  (add_norm1): AddNorm(
    (dropout): Dropout(p=0.5, inplace=False)
    (ln): LayerNorm((100, 600), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 72,764,082
Freeze params: 0
2024-03-17 02:44:24,810 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Linear(in_features=600, out_features=10, bias=True)
  (gru): LSTM(300, 300, batch_first=True, bidirectional=True)
  (mha1): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=600, out_features=600, bias=True)
  )
  (add_norm1): AddNorm(
    (dropout): Dropout(p=0.5, inplace=False)
    (ln): LayerNorm((100, 600), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 72,767,082
Freeze params: 0
2024-03-17 03:00:39,740 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=600, out_features=300, bias=True)
  (topic_layer): Linear(in_features=600, out_features=10, bias=True)
  (gru): LSTM(300, 300, batch_first=True, bidirectional=True)
  (mha1): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=600, out_features=600, bias=True)
  )
  (add_norm1): AddNorm(
    (dropout): Dropout(p=0.5, inplace=False)
    (ln): LayerNorm((100, 600), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=600, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 72,895,482
Freeze params: 0
2024-03-17 03:03:46,634 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=600, out_features=300, bias=True)
  (topic_layer): Linear(in_features=600, out_features=10, bias=True)
  (gru): LSTM(300, 300, batch_first=True, bidirectional=True)
  (mha1): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=600, out_features=600, bias=True)
  )
  (add_norm1): AddNorm(
    (dropout): Dropout(p=0.5, inplace=False)
    (ln): LayerNorm((100, 600), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 72,857,082
Freeze params: 0
2024-03-17 12:28:24,919 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=600, out_features=300, bias=True)
  (topic_layer): Linear(in_features=600, out_features=10, bias=True)
  (gru): LSTM(300, 300, batch_first=True, bidirectional=True)
  (mha1): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=600, out_features=600, bias=True)
  )
  (add_norm1): AddNorm(
    (dropout): Dropout(p=0.5, inplace=False)
    (ln): LayerNorm((100, 600), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 72,857,082
Freeze params: 0
2024-03-20 12:27:53,103 - train - INFO - BiAttentionClassifyModel(
  (extract_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 384, padding_idx=0)
      (position_embeddings): Embedding(512, 384)
      (token_type_embeddings): Embedding(2, 384)
      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-5): 6 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=384, out_features=384, bias=True)
              (key): Linear(in_features=384, out_features=384, bias=True)
              (value): Linear(in_features=384, out_features=384, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=384, out_features=384, bias=True)
              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=384, out_features=1536, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=1536, out_features=384, bias=True)
            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=384, out_features=384, bias=True)
      (activation): Tanh()
    )
  )
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=8, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 54,461,752
Freeze params: 0
2024-03-20 12:31:56,212 - train - INFO - BiAttentionClassifyModel(
  (extract_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 384, padding_idx=0)
      (position_embeddings): Embedding(512, 384)
      (token_type_embeddings): Embedding(2, 384)
      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-5): 6 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=384, out_features=384, bias=True)
              (key): Linear(in_features=384, out_features=384, bias=True)
              (value): Linear(in_features=384, out_features=384, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=384, out_features=384, bias=True)
              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=384, out_features=1536, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=1536, out_features=384, bias=True)
            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=384, out_features=384, bias=True)
      (activation): Tanh()
    )
  )
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=8, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 54,461,752
Freeze params: 0
2024-03-20 12:39:29,376 - train - INFO - BiAttentionClassifyModel(
  (extract_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 384, padding_idx=0)
      (position_embeddings): Embedding(512, 384)
      (token_type_embeddings): Embedding(2, 384)
      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-5): 6 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=384, out_features=384, bias=True)
              (key): Linear(in_features=384, out_features=384, bias=True)
              (value): Linear(in_features=384, out_features=384, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=384, out_features=384, bias=True)
              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=384, out_features=1536, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=1536, out_features=384, bias=True)
            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=384, out_features=384, bias=True)
      (activation): Tanh()
    )
  )
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=8, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 54,461,752
Freeze params: 0
2024-03-20 13:17:02,772 - train - INFO - BiAttentionClassifyModel(
  (extract_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 384, padding_idx=0)
      (position_embeddings): Embedding(512, 384)
      (token_type_embeddings): Embedding(2, 384)
      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-5): 6 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=384, out_features=384, bias=True)
              (key): Linear(in_features=384, out_features=384, bias=True)
              (value): Linear(in_features=384, out_features=384, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=384, out_features=384, bias=True)
              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=384, out_features=1536, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=1536, out_features=384, bias=True)
            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=384, out_features=384, bias=True)
      (activation): Tanh()
    )
  )
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=8, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 54,461,752
Freeze params: 0
2024-03-20 13:38:35,766 - train - INFO - BiAttentionClassifyModel(
  (extract_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 384, padding_idx=0)
      (position_embeddings): Embedding(512, 384)
      (token_type_embeddings): Embedding(2, 384)
      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-5): 6 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=384, out_features=384, bias=True)
              (key): Linear(in_features=384, out_features=384, bias=True)
              (value): Linear(in_features=384, out_features=384, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=384, out_features=384, bias=True)
              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=384, out_features=1536, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=1536, out_features=384, bias=True)
            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=384, out_features=384, bias=True)
      (activation): Tanh()
    )
  )
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=8, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 54,461,752
Freeze params: 0
2024-03-20 13:40:48,093 - train - INFO - BiAttentionClassifyModel(
  (extract_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 384, padding_idx=0)
      (position_embeddings): Embedding(512, 384)
      (token_type_embeddings): Embedding(2, 384)
      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-5): 6 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=384, out_features=384, bias=True)
              (key): Linear(in_features=384, out_features=384, bias=True)
              (value): Linear(in_features=384, out_features=384, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=384, out_features=384, bias=True)
              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=384, out_features=1536, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=1536, out_features=384, bias=True)
            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=384, out_features=384, bias=True)
      (activation): Tanh()
    )
  )
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=8, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 54,461,752
Freeze params: 0
2024-03-20 13:42:35,324 - train - INFO - BiAttentionClassifyModel(
  (extract_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 384, padding_idx=0)
      (position_embeddings): Embedding(512, 384)
      (token_type_embeddings): Embedding(2, 384)
      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-5): 6 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=384, out_features=384, bias=True)
              (key): Linear(in_features=384, out_features=384, bias=True)
              (value): Linear(in_features=384, out_features=384, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=384, out_features=384, bias=True)
              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=384, out_features=1536, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=1536, out_features=384, bias=True)
            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=384, out_features=384, bias=True)
      (activation): Tanh()
    )
  )
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=8, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 54,461,752
Freeze params: 0
2024-03-20 14:06:57,002 - train - INFO - BiAttentionClassifyModel(
  (extract_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 384, padding_idx=0)
      (position_embeddings): Embedding(512, 384)
      (token_type_embeddings): Embedding(2, 384)
      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-5): 6 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=384, out_features=384, bias=True)
              (key): Linear(in_features=384, out_features=384, bias=True)
              (value): Linear(in_features=384, out_features=384, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=384, out_features=384, bias=True)
              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=384, out_features=1536, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=1536, out_features=384, bias=True)
            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=384, out_features=384, bias=True)
      (activation): Tanh()
    )
  )
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=8, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 54,461,752
Freeze params: 0
2024-03-20 14:10:52,313 - train - INFO - BiAttentionClassifyModel(
  (extract_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 384, padding_idx=0)
      (position_embeddings): Embedding(512, 384)
      (token_type_embeddings): Embedding(2, 384)
      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-5): 6 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=384, out_features=384, bias=True)
              (key): Linear(in_features=384, out_features=384, bias=True)
              (value): Linear(in_features=384, out_features=384, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=384, out_features=384, bias=True)
              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=384, out_features=1536, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=1536, out_features=384, bias=True)
            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=384, out_features=384, bias=True)
      (activation): Tanh()
    )
  )
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=8, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 54,461,752
Freeze params: 0
2024-03-20 14:11:37,569 - train - INFO - BiAttentionClassifyModel(
  (extract_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 384, padding_idx=0)
      (position_embeddings): Embedding(512, 384)
      (token_type_embeddings): Embedding(2, 384)
      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-5): 6 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=384, out_features=384, bias=True)
              (key): Linear(in_features=384, out_features=384, bias=True)
              (value): Linear(in_features=384, out_features=384, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=384, out_features=384, bias=True)
              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=384, out_features=1536, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=1536, out_features=384, bias=True)
            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=384, out_features=384, bias=True)
      (activation): Tanh()
    )
  )
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=8, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 54,461,752
Freeze params: 0
2024-03-20 14:12:41,951 - train - INFO - BiAttentionClassifyModel(
  (extract_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 384, padding_idx=0)
      (position_embeddings): Embedding(512, 384)
      (token_type_embeddings): Embedding(2, 384)
      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-5): 6 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=384, out_features=384, bias=True)
              (key): Linear(in_features=384, out_features=384, bias=True)
              (value): Linear(in_features=384, out_features=384, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=384, out_features=384, bias=True)
              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=384, out_features=1536, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=1536, out_features=384, bias=True)
            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=384, out_features=384, bias=True)
      (activation): Tanh()
    )
  )
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=8, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 54,461,752
Freeze params: 0
2024-03-20 14:31:01,971 - train - INFO - BiAttentionClassifyModel(
  (extract_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 384, padding_idx=0)
      (position_embeddings): Embedding(512, 384)
      (token_type_embeddings): Embedding(2, 384)
      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-5): 6 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=384, out_features=384, bias=True)
              (key): Linear(in_features=384, out_features=384, bias=True)
              (value): Linear(in_features=384, out_features=384, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=384, out_features=384, bias=True)
              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=384, out_features=1536, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=1536, out_features=384, bias=True)
            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=384, out_features=384, bias=True)
      (activation): Tanh()
    )
  )
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=8, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 54,461,752
Freeze params: 0
2024-03-20 14:35:20,271 - train - INFO - BiAttentionClassifyModel(
  (extract_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 384, padding_idx=0)
      (position_embeddings): Embedding(512, 384)
      (token_type_embeddings): Embedding(2, 384)
      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-5): 6 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=384, out_features=384, bias=True)
              (key): Linear(in_features=384, out_features=384, bias=True)
              (value): Linear(in_features=384, out_features=384, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=384, out_features=384, bias=True)
              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=384, out_features=1536, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=1536, out_features=384, bias=True)
            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=384, out_features=384, bias=True)
      (activation): Tanh()
    )
  )
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=8, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 54,461,752
Freeze params: 0
2024-03-20 14:37:33,546 - train - INFO - BiAttentionClassifyModel(
  (extract_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 384, padding_idx=0)
      (position_embeddings): Embedding(512, 384)
      (token_type_embeddings): Embedding(2, 384)
      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-5): 6 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=384, out_features=384, bias=True)
              (key): Linear(in_features=384, out_features=384, bias=True)
              (value): Linear(in_features=384, out_features=384, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=384, out_features=384, bias=True)
              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=384, out_features=1536, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=1536, out_features=384, bias=True)
            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=384, out_features=384, bias=True)
      (activation): Tanh()
    )
  )
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=8, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 54,461,752
Freeze params: 0
2024-03-20 16:13:46,022 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\distilbert-base-uncased
2024-03-20 16:13:46,022 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\distilbert-base-uncased. Creating a new one with MEAN pooling.
2024-03-20 16:13:46,288 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-20 16:13:52,685 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=8, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 31,748,536
Freeze params: 0
2024-03-20 16:17:39,498 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\distilbert-base-uncased
2024-03-20 16:17:39,514 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\distilbert-base-uncased. Creating a new one with MEAN pooling.
2024-03-20 16:17:39,827 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-20 16:17:40,186 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=8, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 31,748,536
Freeze params: 0
2024-03-20 16:19:24,287 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\distilbert-base-uncased
2024-03-20 16:19:24,287 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\distilbert-base-uncased. Creating a new one with MEAN pooling.
2024-03-20 16:19:24,532 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-20 16:19:24,783 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=8, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 31,748,536
Freeze params: 0
2024-03-20 16:26:06,131 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\distilbert-base-uncased
2024-03-20 16:26:06,131 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\distilbert-base-uncased. Creating a new one with MEAN pooling.
2024-03-20 16:26:06,381 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-20 16:26:06,647 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=8, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 31,748,536
Freeze params: 0
2024-03-20 16:34:41,434 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\distilbert-base-uncased
2024-03-20 16:34:41,434 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\distilbert-base-uncased. Creating a new one with MEAN pooling.
2024-03-20 16:34:41,684 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-20 16:34:41,950 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=8, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 31,748,536
Freeze params: 0
2024-03-20 16:38:01,251 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\distilbert-base-uncased
2024-03-20 16:38:01,251 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\distilbert-base-uncased. Creating a new one with MEAN pooling.
2024-03-20 16:38:01,501 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-20 16:38:01,767 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=8, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 31,748,536
Freeze params: 0
2024-03-20 16:46:45,301 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\distilbert-base-uncased
2024-03-20 16:46:45,317 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\distilbert-base-uncased. Creating a new one with MEAN pooling.
2024-03-20 16:46:45,567 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-20 16:46:45,912 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=8, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 31,748,536
Freeze params: 0
2024-03-20 17:00:20,167 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\distilbert-base-uncased
2024-03-20 17:00:20,167 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\distilbert-base-uncased. Creating a new one with MEAN pooling.
2024-03-20 17:00:21,381 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-20 17:00:21,771 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\distilbert-base-uncased
2024-03-20 17:00:21,771 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\distilbert-base-uncased. Creating a new one with MEAN pooling.
2024-03-20 17:00:22,572 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-20 17:00:22,759 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\distilbert-base-uncased
2024-03-20 17:00:22,759 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\distilbert-base-uncased. Creating a new one with MEAN pooling.
2024-03-20 17:00:23,092 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-20 17:00:23,538 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=8, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 31,748,536
Freeze params: 0
2024-03-20 17:01:35,426 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\distilbert-base-uncased
2024-03-20 17:01:35,426 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\distilbert-base-uncased. Creating a new one with MEAN pooling.
2024-03-20 17:01:36,555 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-20 17:01:36,869 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\distilbert-base-uncased
2024-03-20 17:01:36,869 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\distilbert-base-uncased. Creating a new one with MEAN pooling.
2024-03-20 17:01:37,063 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-20 17:01:37,204 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\distilbert-base-uncased
2024-03-20 17:01:37,204 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\distilbert-base-uncased. Creating a new one with MEAN pooling.
2024-03-20 17:01:37,439 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-20 17:01:37,801 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=8, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 31,748,536
Freeze params: 0
2024-03-20 17:13:31,219 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=8, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 31,748,536
Freeze params: 0
2024-03-20 17:13:31,475 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\distilbert-base-uncased
2024-03-20 17:13:31,475 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\distilbert-base-uncased. Creating a new one with MEAN pooling.
2024-03-20 17:13:31,715 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-20 17:14:48,683 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=8, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 31,748,536
Freeze params: 0
2024-03-20 17:14:48,962 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\distilbert-base-uncased
2024-03-20 17:14:48,962 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\distilbert-base-uncased. Creating a new one with MEAN pooling.
2024-03-20 17:14:49,228 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-20 17:19:25,521 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=8, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 31,748,536
Freeze params: 0
2024-03-20 17:19:25,756 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\distilbert-base-uncased
2024-03-20 17:19:25,756 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\distilbert-base-uncased. Creating a new one with MEAN pooling.
2024-03-20 17:19:26,007 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-20 17:23:06,052 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=8, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 31,748,536
Freeze params: 0
2024-03-20 17:23:06,299 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\distilbert-base-uncased
2024-03-20 17:23:06,299 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\distilbert-base-uncased. Creating a new one with MEAN pooling.
2024-03-20 17:23:06,539 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-20 17:28:32,409 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=8, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 31,748,536
Freeze params: 0
2024-03-20 17:28:32,627 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\distilbert-base-uncased
2024-03-20 17:28:32,627 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\distilbert-base-uncased. Creating a new one with MEAN pooling.
2024-03-20 17:28:32,879 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-20 17:35:53,349 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=8, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 31,748,536
Freeze params: 0
2024-03-20 17:35:53,548 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\distilbert-base-uncased
2024-03-20 17:35:53,548 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\distilbert-base-uncased. Creating a new one with MEAN pooling.
2024-03-20 17:35:53,782 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-20 17:41:58,021 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=8, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 31,748,536
Freeze params: 0
2024-03-20 17:41:58,240 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: D:\AI\model\distilbert-base-uncased
2024-03-20 17:41:58,240 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name D:\AI\model\distilbert-base-uncased. Creating a new one with MEAN pooling.
2024-03-20 17:41:58,491 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda
2024-03-23 00:58:48,327 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,271,448
Freeze params: 0
2024-03-23 01:00:50,726 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,271,448
Freeze params: 0
2024-03-23 01:13:31,388 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,271,448
Freeze params: 0
2024-03-23 01:15:45,605 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,271,448
Freeze params: 0
2024-03-23 01:22:40,667 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,271,448
Freeze params: 0
2024-03-23 01:25:06,551 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,271,448
Freeze params: 0
2024-03-23 01:31:26,037 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,271,448
Freeze params: 0
2024-03-23 01:33:38,597 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,271,448
Freeze params: 0
2024-03-23 01:37:21,353 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,271,448
Freeze params: 0
2024-03-23 01:38:19,174 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,271,448
Freeze params: 0
2024-03-23 01:43:22,176 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,271,448
Freeze params: 0
2024-03-23 01:46:06,131 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,271,448
Freeze params: 0
2024-03-23 02:00:40,594 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,271,448
Freeze params: 0
2024-03-23 02:02:40,572 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,271,448
Freeze params: 0
2024-03-23 02:03:32,208 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,271,448
Freeze params: 0
2024-03-23 02:08:30,703 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,271,448
Freeze params: 0
2024-03-23 02:10:20,868 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 32,271,448
Freeze params: 0
2024-03-23 10:04:59,006 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (gru): GRU(768, 768, num_layers=2, batch_first=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=1200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1200, out_features=60, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 39,706,172
Freeze params: 0
2024-03-23 10:17:36,665 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (gru): GRU(768, 768, num_layers=2, batch_first=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=1200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1200, out_features=60, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 39,706,172
Freeze params: 0
2024-03-23 11:40:44,900 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (gru): GRU(768, 768, num_layers=2, batch_first=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=1200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1200, out_features=60, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 39,706,172
Freeze params: 0
2024-03-23 11:41:32,909 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (gru): GRU(768, 768, num_layers=2, batch_first=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=1200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1200, out_features=60, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 39,706,172
Freeze params: 0
2024-03-23 11:57:07,182 - train - INFO - BiAttentionClassifyModel(
  (embedding): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): Linear(in_features=768, out_features=15, bias=True)
  (gru): GRU(768, 768, num_layers=2, batch_first=True)
  (mha1): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (add_norm1): AddNorm(
    (dropout): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((100, 768), eps=1e-05, elementwise_affine=True)
  )
  (feature_network1): Linear(in_features=768, out_features=768, bias=True)
  (final): Linear(in_features=768, out_features=768, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=768, out_features=768, bias=True)
    (1): Tanh()
    (2): Linear(in_features=768, out_features=24, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=768, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 42,426,920
Freeze params: 0
2024-04-23 14:16:00,611 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (gru): GRU(300, 300, batch_first=True, bidirectional=True)
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (droputout): Dropout(p=0.2, inplace=False)
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 71,134,312
Freeze params: 0
2024-04-23 14:18:28,636 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (gru): GRU(300, 300, batch_first=True, bidirectional=True)
  (W_k): Linear(in_features=600, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (droputout): Dropout(p=0.2, inplace=False)
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 71,146,312
Freeze params: 0
2024-04-24 01:42:31,628 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRU(300, 300, batch_first=True, bidirectional=True)
  (W_k): Linear(in_features=600, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (droputout): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 71,146,312
Freeze params: 0
2024-04-24 01:50:04,082 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRU(300, 300, batch_first=True, bidirectional=True)
  (W_k): Linear(in_features=600, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (droputout): Dropout(p=0.2, inplace=False)
  (pooling): AvgPool1d(kernel_size=(3,), stride=(1,), padding=(0,))
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 71,146,312
Freeze params: 0
2024-04-24 01:55:25,746 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRU(300, 300, batch_first=True, bidirectional=True)
  (W_k): Linear(in_features=600, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (droputout): Dropout(p=0.2, inplace=False)
  (pooling): AvgPool1d(kernel_size=(3,), stride=(1,), padding=(1,))
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 71,146,312
Freeze params: 0
2024-04-25 01:26:52,864 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=40, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (W_k): Linear(in_features=255, out_features=40, bias=False)
  (W_q): Linear(in_features=256, out_features=40, bias=False)
  (droputout): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((256, 40), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
  (conv_layers): ModuleList(
    (0): Conv2d(1, 40, kernel_size=(3, 300), stride=(1, 1), padding=(1, 0))
  )
)
Trainable params: 70,099,772
Freeze params: 0
2024-04-25 01:29:37,688 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 70,026,712
Freeze params: 0
2024-04-25 01:31:15,800 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=40, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (W_k): Linear(in_features=255, out_features=40, bias=False)
  (W_q): Linear(in_features=256, out_features=40, bias=False)
  (droputout): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((256, 40), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
  (conv_layers): ModuleList(
    (0): Conv2d(1, 40, kernel_size=(3, 300), stride=(1, 1), padding=(1, 0))
  )
)
Trainable params: 70,099,772
Freeze params: 0
2024-04-25 01:32:42,402 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=40, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (W_k): Linear(in_features=255, out_features=40, bias=False)
  (W_q): Linear(in_features=256, out_features=40, bias=False)
  (droputout): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((256, 40), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
  (conv_layers): ModuleList(
    (0): Conv2d(1, 40, kernel_size=(3, 300), stride=(1, 1), padding=(1, 0))
  )
)
Trainable params: 70,099,772
Freeze params: 0
2024-04-25 01:35:58,266 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=40, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (W_k): Linear(in_features=255, out_features=40, bias=False)
  (W_q): Linear(in_features=256, out_features=40, bias=False)
  (droputout): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((256, 40), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
  (conv_layers): ModuleList(
    (0): Conv2d(1, 40, kernel_size=(3, 300), stride=(1, 1), padding=(1, 0))
  )
)
Trainable params: 70,099,772
Freeze params: 0
2024-04-25 01:38:12,962 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 70,026,712
Freeze params: 0
2024-04-25 01:39:55,625 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=40, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (W_k): Linear(in_features=255, out_features=40, bias=False)
  (W_q): Linear(in_features=256, out_features=40, bias=False)
  (droputout): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((256, 40), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
  (conv_layers): ModuleList(
    (0): Conv2d(1, 40, kernel_size=(3, 300), stride=(1, 1), padding=(1, 0))
  )
)
Trainable params: 70,099,772
Freeze params: 0
2024-04-25 01:43:34,177 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 70,026,712
Freeze params: 0
2024-04-25 01:45:44,198 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=40, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (W_k): Linear(in_features=255, out_features=40, bias=False)
  (W_q): Linear(in_features=256, out_features=40, bias=False)
  (droputout): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((256, 40), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
  (conv_layers): ModuleList(
    (0): Conv2d(1, 40, kernel_size=(3, 300), stride=(1, 1), padding=(1, 0))
  )
)
Trainable params: 70,099,772
Freeze params: 0
2024-04-25 01:48:04,042 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=40, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (W_k): Linear(in_features=255, out_features=40, bias=False)
  (W_q): Linear(in_features=256, out_features=40, bias=False)
  (droputout): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((256, 40), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
  (conv_layers): ModuleList(
    (0): Conv2d(1, 40, kernel_size=(3, 300), stride=(1, 1), padding=(1, 0))
  )
)
Trainable params: 70,099,772
Freeze params: 0
2024-04-25 20:23:11,384 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRU(300, 300, batch_first=True)
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((256, 300), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
  (conv_layers): ModuleList(
    (0): Conv2d(1, 40, kernel_size=(1, 300), stride=(1, 1))
  )
  (add_norm2): AddNorm(
    (dropout): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((256, 600), eps=1e-05, elementwise_affine=True)
  )
)
Trainable params: 71,058,024
Freeze params: 0
2024-04-25 20:45:49,266 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRU(300, 300, batch_first=True)
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((256, 300), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
  (conv_layers): ModuleList(
    (0): Conv2d(1, 40, kernel_size=(1, 300), stride=(1, 1))
  )
  (add_norm2): AddNorm(
    (dropout): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((256, 600), eps=1e-05, elementwise_affine=True)
  )
)
Trainable params: 71,058,024
Freeze params: 0
2024-04-25 20:50:56,074 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRU(300, 300, batch_first=True)
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 300), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
  (conv_layers): ModuleList(
    (0): Conv2d(1, 40, kernel_size=(1, 300), stride=(1, 1))
  )
  (add_norm2): AddNorm(
    (dropout): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((256, 600), eps=1e-05, elementwise_affine=True)
  )
)
Trainable params: 70,928,424
Freeze params: 0
2024-04-25 21:23:47,051 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRU(300, 300, batch_first=True, bidirectional=True)
  (W_k): Linear(in_features=600, out_features=40, bias=False)
  (W_q): Linear(in_features=600, out_features=300, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
  (conv_layers): ModuleList(
    (0): Conv2d(1, 40, kernel_size=(1, 300), stride=(1, 1))
  )
)
Trainable params: 71,339,504
Freeze params: 0
2024-04-26 14:42:31,741 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300, batch_first=True)
    (attention): GRU_Attention(
      (attn): Linear(in_features=600, out_features=1, bias=True)
    )
  )
  (W_k): Linear(in_features=600, out_features=40, bias=False)
  (W_q): Linear(in_features=600, out_features=300, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (W1): Linear(in_features=300, out_features=300, bias=False)
  (W2): Linear(in_features=300, out_features=300, bias=False)
  (W3): Linear(in_features=300, out_features=300, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (droputout2): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
  (conv_layers): ModuleList(
    (0): Conv2d(1, 40, kernel_size=(1, 300), stride=(1, 1))
  )
)
Trainable params: 71,068,305
Freeze params: 0
2024-04-26 14:48:27,711 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300, batch_first=True)
    (attention): GRU_Attention(
      (attn): Linear(in_features=600, out_features=1, bias=True)
    )
  )
  (W_k): Linear(in_features=600, out_features=40, bias=False)
  (W_q): Linear(in_features=600, out_features=300, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (W1): Linear(in_features=300, out_features=300, bias=False)
  (W2): Linear(in_features=300, out_features=300, bias=False)
  (W3): Linear(in_features=300, out_features=300, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (droputout2): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
  (conv_layers): ModuleList(
    (0): Conv2d(1, 40, kernel_size=(1, 300), stride=(1, 1))
  )
)
Trainable params: 71,068,305
Freeze params: 0
2024-04-26 14:56:55,508 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300, batch_first=True)
    (attention): GRU_Attention(
      (attn): Linear(in_features=600, out_features=1, bias=True)
    )
  )
  (W_k): Linear(in_features=600, out_features=40, bias=False)
  (W_q): Linear(in_features=600, out_features=300, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (W1): Linear(in_features=300, out_features=300, bias=False)
  (W2): Linear(in_features=300, out_features=300, bias=False)
  (W3): Linear(in_features=300, out_features=300, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (droputout2): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
  (conv_layers): ModuleList(
    (0): Conv2d(1, 40, kernel_size=(1, 300), stride=(1, 1))
  )
)
Trainable params: 71,068,305
Freeze params: 0
2024-04-26 14:59:49,851 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300, batch_first=True)
    (attention): GRU_Attention(
      (attn): Linear(in_features=600, out_features=1, bias=True)
    )
  )
  (W_k): Linear(in_features=600, out_features=40, bias=False)
  (W_q): Linear(in_features=600, out_features=300, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (W1): Linear(in_features=300, out_features=300, bias=False)
  (W2): Linear(in_features=300, out_features=300, bias=False)
  (W3): Linear(in_features=300, out_features=300, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (droputout2): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
  (conv_layers): ModuleList(
    (0): Conv2d(1, 40, kernel_size=(1, 300), stride=(1, 1))
  )
)
Trainable params: 71,068,305
Freeze params: 0
2024-04-26 15:01:41,996 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300, batch_first=True)
    (attention): GRU_Attention(
      (attn): Linear(in_features=600, out_features=1, bias=True)
    )
  )
  (W_k): Linear(in_features=600, out_features=40, bias=False)
  (W_q): Linear(in_features=600, out_features=300, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (W1): Linear(in_features=300, out_features=300, bias=False)
  (W2): Linear(in_features=300, out_features=300, bias=False)
  (W3): Linear(in_features=300, out_features=300, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (droputout2): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
  (conv_layers): ModuleList(
    (0): Conv2d(1, 40, kernel_size=(1, 300), stride=(1, 1))
  )
)
Trainable params: 71,068,305
Freeze params: 0
2024-04-26 15:07:54,842 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300, batch_first=True)
    (attention): GRU_Attention(
      (attn): Linear(in_features=600, out_features=1, bias=True)
    )
  )
  (W_k): Linear(in_features=600, out_features=40, bias=False)
  (W_q): Linear(in_features=600, out_features=300, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (W1): Linear(in_features=300, out_features=300, bias=False)
  (W2): Linear(in_features=300, out_features=300, bias=False)
  (W3): Linear(in_features=300, out_features=300, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (droputout2): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
  (conv_layers): ModuleList(
    (0): Conv2d(1, 40, kernel_size=(1, 300), stride=(1, 1))
  )
)
Trainable params: 71,068,305
Freeze params: 0
2024-04-26 15:23:16,253 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300, num_layers=2, batch_first=True)
    (attention): GRU_Attention(
      (attn): Linear(in_features=600, out_features=1, bias=True)
    )
  )
  (W_k): Linear(in_features=600, out_features=40, bias=False)
  (W_q): Linear(in_features=600, out_features=300, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (W1): Linear(in_features=300, out_features=300, bias=False)
  (W2): Linear(in_features=300, out_features=300, bias=False)
  (W3): Linear(in_features=300, out_features=300, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (droputout2): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
  (conv_layers): ModuleList(
    (0): Conv2d(1, 40, kernel_size=(1, 300), stride=(1, 1))
  )
)
Trainable params: 71,610,105
Freeze params: 0
2024-04-26 15:26:36,396 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300, num_layers=2, batch_first=True)
    (attention): GRU_Attention(
      (attn): Linear(in_features=600, out_features=1, bias=True)
    )
  )
  (W_k): Linear(in_features=600, out_features=40, bias=False)
  (W_q): Linear(in_features=600, out_features=300, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (W1): Linear(in_features=300, out_features=300, bias=False)
  (W2): Linear(in_features=300, out_features=300, bias=False)
  (W3): Linear(in_features=300, out_features=300, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (droputout2): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
  (conv_layers): ModuleList(
    (0): Conv2d(1, 40, kernel_size=(1, 300), stride=(1, 1))
  )
)
Trainable params: 71,610,105
Freeze params: 0
2024-04-26 15:40:30,224 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300, batch_first=True)
    (attention): GRU_Attention(
      (attn): Linear(in_features=300, out_features=1, bias=True)
    )
  )
  (W_k): Linear(in_features=600, out_features=40, bias=False)
  (W_q): Linear(in_features=600, out_features=300, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (W1): Linear(in_features=300, out_features=300, bias=False)
  (W2): Linear(in_features=300, out_features=300, bias=False)
  (W3): Linear(in_features=300, out_features=300, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (droputout2): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
  (conv_layers): ModuleList(
    (0): Conv2d(1, 40, kernel_size=(1, 300), stride=(1, 1))
  )
)
Trainable params: 71,068,005
Freeze params: 0
2024-04-26 15:42:24,611 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300, batch_first=True)
    (attention): GRU_Attention(
      (attn): Linear(in_features=300, out_features=1, bias=True)
    )
  )
  (W_k): Linear(in_features=600, out_features=40, bias=False)
  (W_q): Linear(in_features=600, out_features=300, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (W1): Linear(in_features=300, out_features=300, bias=False)
  (W2): Linear(in_features=300, out_features=300, bias=False)
  (W3): Linear(in_features=300, out_features=300, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (droputout2): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
  (conv_layers): ModuleList(
    (0): Conv2d(1, 40, kernel_size=(1, 300), stride=(1, 1))
  )
)
Trainable params: 71,068,005
Freeze params: 0
2024-04-26 15:44:51,228 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300, batch_first=True)
    (attention): GRU_Attention(
      (attn): Linear(in_features=300, out_features=1, bias=True)
    )
  )
  (W_k): Linear(in_features=600, out_features=40, bias=False)
  (W_q): Linear(in_features=600, out_features=300, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (W1): Linear(in_features=300, out_features=300, bias=False)
  (W2): Linear(in_features=300, out_features=300, bias=False)
  (W3): Linear(in_features=300, out_features=300, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (droputout2): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
  (conv_layers): ModuleList(
    (0): Conv2d(1, 40, kernel_size=(1, 300), stride=(1, 1))
  )
)
Trainable params: 71,068,005
Freeze params: 0
2024-04-26 15:52:09,664 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300, batch_first=True)
    (attention): GRU_Attention(
      (attn): Linear(in_features=300, out_features=1, bias=True)
    )
  )
  (W_k): Linear(in_features=600, out_features=40, bias=False)
  (W_q): Linear(in_features=600, out_features=300, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (W1): Linear(in_features=300, out_features=300, bias=False)
  (W2): Linear(in_features=300, out_features=300, bias=False)
  (W3): Linear(in_features=300, out_features=300, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (droputout2): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
  (conv_layers): ModuleList(
    (0): Conv2d(1, 40, kernel_size=(1, 300), stride=(1, 1))
  )
)
Trainable params: 71,068,005
Freeze params: 0
2024-04-26 16:39:49,800 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (attention): GRU_Attention(
      (attn): Linear(in_features=300, out_features=1, bias=True)
    )
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=600, out_features=300, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (W1): Linear(in_features=300, out_features=300, bias=False)
  (W2): Linear(in_features=300, out_features=300, bias=False)
  (W3): Linear(in_features=300, out_features=300, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (droputout2): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
  (conv_layers): ModuleList(
    (0): Conv2d(1, 40, kernel_size=(1, 300), stride=(1, 1))
  )
)
Trainable params: 71,597,805
Freeze params: 0
2024-04-26 17:01:11,735 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=600, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (attention): GRU_Attention(
      (attn): Linear(in_features=600, out_features=1, bias=True)
    )
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=600, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (W1): Linear(in_features=300, out_features=300, bias=False)
  (W2): Linear(in_features=300, out_features=300, bias=False)
  (W3): Linear(in_features=300, out_features=300, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (droputout2): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=600, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
  (conv_layers): ModuleList(
    (0): Conv2d(1, 40, kernel_size=(1, 300), stride=(1, 1))
  )
)
Trainable params: 71,551,305
Freeze params: 0
2024-04-26 17:11:10,125 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=600, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (attention): GRU_Attention(
      (attn): Linear(in_features=600, out_features=1, bias=True)
    )
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=600, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (W1): Linear(in_features=300, out_features=300, bias=False)
  (W2): Linear(in_features=300, out_features=300, bias=False)
  (W3): Linear(in_features=300, out_features=300, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (droputout2): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=600, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
  (conv_layers): ModuleList(
    (0): Conv2d(1, 40, kernel_size=(1, 300), stride=(1, 1))
  )
)
Trainable params: 71,551,305
Freeze params: 0
2024-04-26 17:13:15,893 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=600, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (attention): GRU_Attention(
      (attn): Linear(in_features=600, out_features=1, bias=True)
    )
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=600, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (W1): Linear(in_features=300, out_features=300, bias=False)
  (W2): Linear(in_features=300, out_features=300, bias=False)
  (W3): Linear(in_features=300, out_features=300, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (droputout2): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=600, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
  (conv_layers): ModuleList(
    (0): Conv2d(1, 40, kernel_size=(1, 300), stride=(1, 1))
  )
)
Trainable params: 71,551,305
Freeze params: 0
2024-04-26 17:34:28,960 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=600, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (attention): GRU_Attention(
      (attn): Linear(in_features=600, out_features=1, bias=True)
    )
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=600, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (W1): Linear(in_features=300, out_features=300, bias=False)
  (W2): Linear(in_features=300, out_features=300, bias=False)
  (W3): Linear(in_features=300, out_features=300, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (droputout2): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
  (conv_layers): ModuleList(
    (0): Conv2d(1, 40, kernel_size=(1, 300), stride=(1, 1))
  )
)
Trainable params: 71,532,105
Freeze params: 0
2024-04-26 17:58:01,584 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=600, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (attention): GRU_Attention(
      (attn): Linear(in_features=600, out_features=1, bias=True)
    )
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=600, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (W1): Linear(in_features=300, out_features=300, bias=False)
  (W2): Linear(in_features=300, out_features=300, bias=False)
  (W3): Linear(in_features=300, out_features=300, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (droputout2): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
  (conv_layers): ModuleList(
    (0): Conv2d(1, 40, kernel_size=(1, 300), stride=(1, 1))
  )
)
Trainable params: 71,532,105
Freeze params: 0
2024-04-26 19:47:05,697 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=600, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300, batch_first=True)
    (attention): GRU_Attention(
      (attn): Linear(in_features=600, out_features=1, bias=True)
      (attlayer): AttLayer(
        (attention): Sequential(
          (0): Linear(in_features=300, out_features=64, bias=True)
          (1): Tanh()
          (2): Linear(in_features=64, out_features=1, bias=True)
          (3): Flatten(start_dim=1, end_dim=-1)
          (4): Softmax(dim=-1)
        )
      )
    )
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (W1): Linear(in_features=300, out_features=300, bias=False)
  (W2): Linear(in_features=300, out_features=300, bias=False)
  (W3): Linear(in_features=300, out_features=300, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (droputout2): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
  (conv_layers): ModuleList(
    (0): Conv2d(1, 40, kernel_size=(1, 300), stride=(1, 1))
  )
)
Trainable params: 70,997,634
Freeze params: 0
2024-04-26 21:00:49,989 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (attention): GRU_Attention(
      (attn): Linear(in_features=600, out_features=1, bias=True)
      (attlayer): AttLayer(
        (attention): Sequential(
          (0): Linear(in_features=600, out_features=64, bias=True)
          (1): Tanh()
          (2): Linear(in_features=64, out_features=1, bias=True)
          (3): Flatten(start_dim=1, end_dim=-1)
          (4): Softmax(dim=-1)
        )
      )
    )
  )
  (W_k): Linear(in_features=600, out_features=40, bias=False)
  (W_q): Linear(in_features=600, out_features=300, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (W1): Linear(in_features=300, out_features=300, bias=False)
  (W2): Linear(in_features=300, out_features=300, bias=False)
  (W3): Linear(in_features=300, out_features=300, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (droputout2): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
  (conv_layers): ModuleList(
    (0): Conv2d(1, 40, kernel_size=(1, 300), stride=(1, 1))
  )
)
Trainable params: 71,648,634
Freeze params: 0
2024-04-28 14:35:18,685 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 71,328,064
Freeze params: 0
2024-04-28 14:38:05,085 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 71,328,064
Freeze params: 0
2024-04-28 14:39:25,480 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 71,328,064
Freeze params: 0
2024-04-28 14:42:59,564 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 71,328,064
Freeze params: 0
2024-04-28 14:48:10,920 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 71,328,064
Freeze params: 0
2024-04-28 14:51:48,782 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 71,328,064
Freeze params: 0
2024-04-28 16:41:38,052 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 71,328,064
Freeze params: 0
2024-04-28 16:48:22,699 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 71,328,064
Freeze params: 0
2024-04-28 16:50:14,103 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 71,328,064
Freeze params: 0
2024-04-30 02:47:13,531 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): RNNBase(
    (attn): Linear(in_features=300, out_features=1, bias=True)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 70,064,165
Freeze params: 0
2024-04-30 02:49:55,886 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): RNNBase(
    (attn): Linear(in_features=300, out_features=1, bias=True)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 70,064,165
Freeze params: 0
2024-04-30 03:00:37,617 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): RNNBase(
    (attn): Linear(in_features=300, out_features=1, bias=True)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 70,064,165
Freeze params: 0
2024-04-30 03:09:15,065 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): RNNBase(
    (attn): Linear(in_features=300, out_features=1, bias=True)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 70,064,165
Freeze params: 0
2024-04-30 03:11:14,561 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): RNNBase(
    (attn): Linear(in_features=300, out_features=1, bias=True)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 70,064,165
Freeze params: 0
2024-04-30 03:31:55,247 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): RNNBase(
    (attn): Linear(in_features=300, out_features=1, bias=True)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 70,064,165
Freeze params: 0
2024-04-30 03:36:32,925 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): RNNBase(
    (attn): Linear(in_features=300, out_features=1, bias=True)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 70,064,165
Freeze params: 0
2024-04-30 03:38:23,843 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): RNNBase(
    (attn): Linear(in_features=300, out_features=1, bias=True)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 70,064,165
Freeze params: 0
2024-04-30 03:39:56,706 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): RNNBase(
    (attn): Linear(in_features=300, out_features=1, bias=True)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 70,064,165
Freeze params: 0
2024-04-30 03:41:44,713 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): RNNBase(
    (attn): Linear(in_features=300, out_features=1, bias=True)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 70,064,165
Freeze params: 0
2024-04-30 03:44:10,867 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): RNNBase(
    (attn): Linear(in_features=300, out_features=1, bias=True)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 70,064,165
Freeze params: 0
2024-04-30 03:46:24,943 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): RNNBase(
    (attn): Linear(in_features=300, out_features=1, bias=True)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 70,064,165
Freeze params: 0
2024-04-30 03:51:28,447 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): RNNBase(
    (attn): Linear(in_features=300, out_features=1, bias=True)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 70,064,165
Freeze params: 0
2024-04-30 03:54:56,557 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): RNNBase(
    (attn): Linear(in_features=300, out_features=1, bias=True)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 70,064,165
Freeze params: 0
2024-04-30 03:56:35,395 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): RNNBase(
    (attn): Linear(in_features=300, out_features=1, bias=True)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 70,064,165
Freeze params: 0
2024-04-30 04:01:40,510 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): RNNBase(
    (attn): Linear(in_features=300, out_features=1, bias=True)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 70,064,165
Freeze params: 0
2024-04-30 04:04:42,051 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): RNNBase(
    (attn): Linear(in_features=300, out_features=1, bias=True)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 70,064,165
Freeze params: 0
2024-04-30 04:08:51,866 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): RNNBase(
    (attn): Linear(in_features=300, out_features=1, bias=True)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 70,064,165
Freeze params: 0
2024-04-30 04:37:00,998 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): RNNBase(
    (attn): Linear(in_features=300, out_features=1, bias=True)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 70,064,165
Freeze params: 0
2024-04-30 04:38:18,197 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=40, bias=True)
  )
  (rnn): RNNBase(
    (attn): Linear(in_features=300, out_features=1, bias=True)
  )
  (W_k): Linear(in_features=300, out_features=40, bias=False)
  (W_q): Linear(in_features=300, out_features=40, bias=False)
  (W_v): Linear(in_features=300, out_features=40, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((40, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 70,064,165
Freeze params: 0
2024-05-03 00:10:23,736 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1200, out_features=60, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=60, bias=False)
  (W_q): Linear(in_features=300, out_features=60, bias=False)
  (W_v): Linear(in_features=300, out_features=60, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((60, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 71,516,724
Freeze params: 0
2024-05-03 00:11:23,279 - train - INFO - BiAttentionClassifyModel(
  (embedding): Embedding(232068, 300)
  (classifier): Linear(in_features=300, out_features=15, bias=True)
  (final): Linear(in_features=300, out_features=300, bias=True)
  (topic_layer): Sequential(
    (0): Linear(in_features=300, out_features=1200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=1200, out_features=60, bias=True)
  )
  (rnn): GRUWithAttention(
    (gru): GRU(300, 300)
    (lstm): LSTM(300, 300)
  )
  (W_k): Linear(in_features=300, out_features=60, bias=False)
  (W_q): Linear(in_features=300, out_features=60, bias=False)
  (W_v): Linear(in_features=300, out_features=60, bias=False)
  (droputout1): Dropout(p=0.2, inplace=False)
  (pooling): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
  (add_norm1): AddNorm(
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
    (ln): LayerNorm((60, 256), eps=1e-05, elementwise_affine=True)
  )
  (projection): AttLayer(
    (attention): Sequential(
      (0): Linear(in_features=300, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
      (3): Flatten(start_dim=1, end_dim=-1)
      (4): Softmax(dim=-1)
    )
  )
)
Trainable params: 71,516,724
Freeze params: 0
2024-05-03 00:17:07,613 - trainer - INFO -     epoch          : 1
2024-05-03 00:17:07,613 - trainer - INFO -     loss           : 0.695802
2024-05-03 00:17:07,613 - trainer - INFO -     accuracy       : 0.764887
2024-05-03 00:17:07,613 - trainer - INFO -     macro_f        : 0.749782
2024-05-03 00:17:07,613 - trainer - INFO -     precision      : 0.769229
2024-05-03 00:17:07,613 - trainer - INFO -     recall         : 0.764887
2024-05-03 00:17:07,613 - trainer - INFO -     doc_entropy    : 4.181623
2024-05-03 00:17:07,613 - trainer - INFO -     val_loss       : 0.598194
2024-05-03 00:17:07,613 - trainer - INFO -     val_accuracy   : 0.793987
2024-05-03 00:17:07,613 - trainer - INFO -     val_macro_f    : 0.78917
2024-05-03 00:17:07,613 - trainer - INFO -     val_precision  : 0.813403
2024-05-03 00:17:07,613 - trainer - INFO -     val_recall     : 0.793987
2024-05-03 00:17:07,613 - trainer - INFO -     val_doc_entropy: 4.716448
2024-05-03 00:17:07,613 - trainer - INFO -     test_loss      : 0.589813
2024-05-03 00:17:07,613 - trainer - INFO -     test_accuracy  : 0.800813
2024-05-03 00:17:07,613 - trainer - INFO -     test_macro_f   : 0.796364
2024-05-03 00:17:07,613 - trainer - INFO -     test_precision : 0.820675
2024-05-03 00:17:07,613 - trainer - INFO -     test_recall    : 0.800813
2024-05-03 00:17:07,613 - trainer - INFO -     test_doc_entropy: 4.727534
2024-05-03 00:22:59,234 - trainer - INFO -     epoch          : 2
2024-05-03 00:22:59,234 - trainer - INFO -     loss           : 0.483787
2024-05-03 00:22:59,234 - trainer - INFO -     accuracy       : 0.831427
2024-05-03 00:22:59,234 - trainer - INFO -     macro_f        : 0.825958
2024-05-03 00:22:59,234 - trainer - INFO -     precision      : 0.847659
2024-05-03 00:22:59,234 - trainer - INFO -     recall         : 0.831427
2024-05-03 00:22:59,234 - trainer - INFO -     doc_entropy    : 4.551355
2024-05-03 00:22:59,234 - trainer - INFO -     val_loss       : 0.575481
2024-05-03 00:22:59,234 - trainer - INFO -     val_accuracy   : 0.80273
2024-05-03 00:22:59,234 - trainer - INFO -     val_macro_f    : 0.791631
2024-05-03 00:22:59,234 - trainer - INFO -     val_precision  : 0.812771
2024-05-03 00:22:59,234 - trainer - INFO -     val_recall     : 0.80273
2024-05-03 00:22:59,234 - trainer - INFO -     val_doc_entropy: 4.830549
2024-05-03 00:22:59,234 - trainer - INFO -     test_loss      : 0.558839
2024-05-03 00:22:59,234 - trainer - INFO -     test_accuracy  : 0.808329
2024-05-03 00:22:59,234 - trainer - INFO -     test_macro_f   : 0.796681
2024-05-03 00:22:59,234 - trainer - INFO -     test_precision : 0.81252
2024-05-03 00:22:59,234 - trainer - INFO -     test_recall    : 0.808329
2024-05-03 00:22:59,234 - trainer - INFO -     test_doc_entropy: 4.841625
